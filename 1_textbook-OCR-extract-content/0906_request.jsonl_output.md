<markdown>
# An Infinite Descent into Pure Mathematics

**by Clive Newstead**

*Version 0.5*  
*Last updated on Saturday 24th December 2022*  
[https://infinitedescent.xyz](https://infinitedescent.xyz)
</markdown><markdown>
¬© 2022 Clive Newstead  
All Rights Reserved.

Preview of First Edition (forthcoming)

ISBN 978-1-950215-00-3 (paperback)  
ISBN 978-1-950215-01-0 (hardback)

A free PDF copy of *An Infinite Descent into Pure Mathematics* can be obtained from the book‚Äôs website:  
[https://infinitedescent.xyz](https://infinitedescent.xyz)

This book, its figures and its \(\TeX\) source are released under a Creative Commons Attribution‚ÄìShareAlike 4.0 International Licence. The full text of the licence is replicated at the end of the book, and can be found on the Creative Commons website:  
[https://creativecommons.org/licenses/by/4.0/legalcode](https://creativecommons.org/licenses/by/4.0/legalcode)

0 2 4 6 8 10 9 7 5 3 1
</markdown><markdown>
To my parents,  
Imogen and Matthew (1952‚Äì2021),  
who taught me so much.
</markdown>It seems the image is blank. Could you please provide a different image or check if the file uploaded correctly?<markdown>
## Contents

**Preface** xi

**Acknowledgements** xvii

0 **Getting started** 1  
0.E Chapter 0 exercises 19

I **Core concepts** 23

1 **Logical structure** 25  
1.1 Propositional logic 26  
1.2 Variables and quantifiers 49  
1.3 Logical equivalence 63  
1.E Chapter 1 exercises 81

2 **Sets** 85  
2.1 Sets 86  
2.2 Set operations 98
</markdown><markdown>
## 2.E Chapter 2 exercises
- Page 108

## 3 Functions
- **3.1 Functions**  
  Page 114
- **3.2 Injections and surjections**  
  Page 130
- **3.E Chapter 3 exercises**  
  Page 145

## 4 Mathematical induction
- **4.1 Peano‚Äôs axioms**  
  Page 152
- **4.2 Weak induction**  
  Page 159
- **4.3 Strong induction**  
  Page 174
- **4.E Chapter 4 exercises**  
  Page 182

## 5 Relations
- **5.1 Relations**  
  Page 186
- **5.2 Equivalence relations and partitions**  
  Page 196
- **5.E Chapter 5 exercises**  
  Page 208

## 6 Finite and infinite sets
- **6.1 Finite sets**  
  Page 212
- **6.2 Countable and uncountable sets**  
  Page 222
- **6.E Chapter 6 exercises**  
  Page 236

## II Topics in pure mathematics

## 7 Number theory
- **7.1 Division**  
  Page 240
</markdown><markdown>
## Contents

### 7.2 Prime numbers
- Page 255

### 7.3 Modular arithmetic
- Page 264

### 7.E Chapter 7 exercises
- Page 290

## 8 Enumerative combinatorics
- Page 293

### 8.1 Counting principles
- Page 294

### 8.2 Alternating sums
- Page 314

### 8.E Chapter 8 exercises
- Page 330

## 9 Real numbers
- Page 333

### 9.1 Inequalities and means
- Page 334

### 9.2 Completeness and convergence
- Page 353

### 9.3 Series and sums
- Page 378

### 9.E Chapter 9 exercises
- Page 401

## 10 Infinity
- Page 405

### 10.1 Cardinality
- Page 406

### 10.2 Cardinal arithmetic
- Page 415

### 10.E Chapter 10 exercises
- Page 427

## 11 Discrete probability theory
- Page 431

### 11.1 Discrete probability spaces
- Page 432

### 11.2 Discrete random variables
- Page 451

### 11.E Chapter 11 exercises
- Page 468

## 12 Additional topics
- Page 469

### 12.1 Orders and lattices
- Page 470
</markdown><markdown>
12.2 Inductively defined sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<markdown>
## Appendices

### A Proof-writing
- A.1 Elements of proof-writing . . . . . . . . . . . . . . . . . . . . . . . . . 508
- A.2 Vocabulary for proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

### B Mathematical miscellany
- B.1 Set theoretic foundations . . . . . . . . . . . . . . . . . . . . . . . . . 532
- B.2 Constructions of the number sets . . . . . . . . . . . . . . . . . . . 537
- B.3 Limits of functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554

### C Hints for selected exercises
- 559

### D Typesetting mathematics with LaTeX
- 575

## Indices

- Index of topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .The page appears to be blank with only the word "Contents" and the letter "x" on it. There is no additional text to extract.<markdown>
## Preface

Hello, and thank you for taking the time to read this quick introduction to *An Infinite Descent into Pure Mathematics*! The most recent version of the book is freely available for download from the following website:

[https://infinitedescent.xyz](https://infinitedescent.xyz)

The website also includes information about changes between different versions of the book, an archive of previous versions, and some resources for using LaTeX (see also Appendix D).

## About the book

A student in a typical calculus class will learn the chain rule and then use it to solve some prescribed ‚Äòchain rule problems‚Äô such as computing the derivative of \(\sin(1 + x^2)\) with respect to \(x\), or perhaps solving a word problem involving related rates of change. The expectation is that the student correctly apply the chain rule to derive the correct answer, and show enough work to be believed. In this sense, the student is a *consumer* of mathematics. They are given the chain rule as a tool to be accepted without question, and then use the tool to solve a narrow range of problems.

The goal of this book is to help the reader make the transition from being a *consumer* of mathematics to a *producer* of it. This is what is meant by ‚Äòpure‚Äô mathematics. While a consumer of mathematics might learn the chain rule and use it to compute a derivative, a producer of mathematics might derive the chain rule from the rigorous definition of a derivative, and then prove more abstract versions of the chain rule in more general contexts (such as multivariate analysis).

Consumers of mathematics are expected to say how they used their tools to find their answers. Producers of mathematics, on the other hand, have to do much more: they must be able to keep track of definitions and hypotheses, piece together facts in new
</markdown><markdown>
and interesting ways, and make their own definitions of mathematical concepts. But even more importantly, once they have done this, they must communicate their findings in a way that others find intelligible, and they must convince others that what they have done is correct, appropriate and worthwhile.

It is this transition from consumption to production of mathematics that guided the principles I used to design and write this book. In particular:

- **Communication.** Above all, this book aims to help the reader to obtain mathematical literacy and express themselves mathematically. This occurs at many levels of magnification. For example, consider the following expression:

  \[
  \forall x \in \mathbb{R}, [\neg(x = 0) \Rightarrow (\exists y \in \mathbb{R}, y^2 < x^2)]
  \]

  After working through this book, you will be able to say what the symbols \(\forall, \in, \mathbb{R}, \neg, \Rightarrow\) and \(\exists\) all mean intuitively and how they are defined precisely. But you will also be able to interpret what the expression means as a whole, explain what it means in plain terms to another person *without* using a jumble of symbols, prove that it is true, and communicate your proof to another person in a clear and concise way.

  The kinds of tools needed to do this are developed in the main chapters of the book, and more focus is given to the writing side of things in Appendix A.

- **Inquiry.** The research is clear that people learn more when they find things out for themselves. If I took this to the extreme, the book would be blank; however, I do believe it is important to incorporate aspects of inquiry-based learning into the text.

  This principle manifests itself in that there are exercises scattered throughout the text, many of which simply require you to prove a result that has been stated. Many readers will find this frustrating, but this is for good reason: these exercises serve as checkpoints to make sure your understanding of the material is sufficient to proceed. That feeling of frustration is what learning feels like‚Äîembrace it!

- **Strategy.** Mathematical proof is much like a puzzle. At any given stage in a proof, you will have some definitions, assumptions and results that are available to be used, and you must piece them together using the logical rules at your disposal. Throughout the book, and particularly in the early chapters, I have made an effort to highlight useful proof strategies whenever they arise.

- **Content.** There isn‚Äôt much point learning about mathematics if you don‚Äôt have any concepts to prove results about. With this in mind, Part II includes several chapters dedicated to introducing some topic areas in pure mathematics, such as number theory, combinatorics, analysis and probability theory.

- **\LaTeX.** The *de facto* standard for typesetting mathematics is \LaTeX. I think it is important for mathematicians to learn this early in a guided way, so I wrote a brief tutorial in Appendix D and have included \LaTeX\ code for all new notation as it is defined throughout the book.
</markdown><markdown>
## Navigating the book

This book need not, and, emphatically should not, be read from front to back. The order of material is chosen so that material appearing later depends only on material appearing earlier, but following the material in the order it is presented may be a fairly dry experience.

The majority of introductory pure mathematics courses cover, at a minimum, symbolic logic, sets, functions and relations. This material is the content of Part I. Such courses usually cover additional topics from pure mathematics, with exactly which topics depending on what the course is preparing students for. With this in mind, Part II serves as an introduction to a range of areas of pure mathematics, including number theory, combinatorics, set theory, real analysis, probability theory and order theory.

It is not necessary to cover all of Part I before proceeding to topics in Part II. In fact, interspersing material from Part II can be a useful way of motivating many of the abstract concepts that arise in Part I.

The following table shows dependencies between sections. Previous sections within the same chapter as a section should be considered ‚Äòessential‚Äô prerequisites unless indicated otherwise.

| Part | Section | Essential | Recommended | Useful |
|------|---------|-----------|-------------|--------|
| I    | 1.1     | 0         |             |        |
|      | 2.1     | 1.3       |             |        |
|      | 3.1     | 2.2       |             |        |
|      | 4.1     | 1.3       | 3.1         | 3.2    |
|      | 5.1     | 2.1       | 3.1         | 3.2, 4.2|
|      | 6.1     | 3.2, 4.3  | 5.2         |        |
| II   | 7.1     | 1.3       | 2.1, 4.3    | 3.1    |
|      | 7.3     |           | 5.2         |        |
|      | 8.1     | 6.1       |             |        |
|      | 9.1     | 4.1, 2.1  |             | 5.2    |
|      | 9.2     | 3.1       | 9.1         |        |
|      | 9.3     | 3.1       | 9.1         | 7.3, 6.2|
|      | 10.1    | 6.2       |             |        |
|      | 10.2    | 8.1       |             |        |
|      | 11.1    | 8.1       | 6.2, 9.3    |        |
|      | 12.1    | 5.2       |             |        |
|      | 12.2    | 4.3, 3.2  | 6.2         | 12.1   |

Prerequisites are cumulative. For example, in order to cover Section 10.2, you should first cover Chapters 0 to 4 and Sections 6.1, 6.2, 8.1 and 10.1.
</markdown><markdown>
## What the numbers, colours and symbols mean

Broadly speaking, the material in the book is broken down into enumerated items that fall into one of five categories: definitions, results, remarks, examples and exercises. In Appendix A, we also have proof extracts. To improve navigability, these categories are distinguished by name, colour and symbol, as indicated in the following table.

| Category    | Symbol | Colour  | Category      | Symbol | Colour |
|-------------|--------|---------|---------------|--------|--------|
| Definitions | ‚óÜ      | Red     | Examples      | üñâ     | Teal   |
| Results     | ‚ú∂      | Blue    | Exercises     | ‚úé      | Gold   |
| Remarks     | ‚ùñ      | Purple  | Proof extracts| „ÄÉ     | Teal   |

These items are enumerated according to their section‚Äîfor example, Theorem 9.2.41 is in Section 9.2. Definitions and theorems (important results) appear in a box.

You will also encounter the symbols ‚ñ° and ‚óÅ whose meanings are as follows:

- **‚ñ° End of proof.** It is standard in mathematical documents to identify when a proof has ended by drawing a small square or by writing ‚ÄòQ.E.D.‚Äô (The latter stands for *quod erat demonstrandum*, which is Latin for *which was to be shown*.)

- **‚óÅ End of item.** This is *not* a standard usage, and is included only to help you to identify when an item has finished and the main content of the book continues.

Some subsections are labelled with the symbol ‚òÖ. This indicates that the material in that subsection can be skipped without dire consequences.

## Licence

This book is licensed under the Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0) licence. This means you‚Äôre welcome to share this book, provided that you give credit to the author and that any copies or derivatives of this book are released under the same licence.

The licence can be read in its full glory at the end of the book or by following the following URL:

[http://creativecommons.org/licenses/by-sa/4.0/](http://creativecommons.org/licenses/by-sa/4.0/)
</markdown><markdown>
### Comments and corrections

Any feedback, be it from students, teaching assistants, instructors or any other readers, would be very much appreciated. Particularly useful are corrections of typographical errors, suggestions for alternative ways to describe concepts or prove theorems, and requests for new content (e.g. if you know of a nice example that illustrates a concept, or if there is a relevant concept you wish were included in the book).

Such feedback can be sent to the author by email (clive@infinitedescent.xyz).
</markdown>The page appears to be blank. There is no text to extract.<markdown>
# Acknowledgements

When I reflect on the time I have spent writing this book, I am overwhelmed by the number of people who have had some kind of influence on its content.

This book would never have come to exist were it not for Chad Hershock‚Äôs course 38-801 *Evidence-Based Teaching in the Sciences*, which I took in Fall 2014 as a graduate student at Carnegie Mellon University. His course heavily influenced my approach to teaching, and it motivated me to write this book in the first place. Many of the pedagogical decisions I made when writing this book were informed by research that I was exposed to as a student in Chad‚Äôs class.

The legendary Carnegie Mellon professor, John Mackey, has been using this book (in various forms) as course notes for 21-128 *Mathematical Concepts and Proofs* and 15-151 *Mathematical Foundations of Computer Science* since Fall 2016. His influence can be felt throughout the book: thanks to discussions with John, many proofs have been reworded, sections restructured, and explanations improved. As a result, there is some overlap between the exercises in this book and the questions on his problem sheets. I am extremely grateful for his ongoing support.

Steve Awodey, who was my doctoral thesis advisor, has for a long time been a source of inspiration for me. Many of the choices I made when choosing how to present the material in this book are grounded in my desire to do mathematics *the right way*‚Äîit was this desire that led me to study category theory, and ultimately to become Steve‚Äôs PhD student. I learnt a great deal from him and I greatly appreciated his patience and flexibility in helping direct my research despite my busy teaching schedule and extracurricular interests (such as writing this book).

Perhaps unbeknownst to them, many insightful conversations with the following people have helped shape the material in this book in one way or another: Jeremy Avigad, Deb Brandon, Santiago Ca√±ez, Heather Dwyer, Thomas Forster, Will Gunther, Kate Hamilton, Jessica Harrell, Bob Harper, Brian Kell, Marsha Lovett, Ben Millwood,
</markdown><markdown>
David Offner, Ruth Poproski, Emily Riehl, Hilary Schuldt, Gareth Taylor, Katie Walsh, Emily Weiss and Andy Zucker.

The Stack Exchange network has influenced the development of this book in two important ways. First, I have been an active member of Mathematics Stack Exchange (https://math.stackexchange.com/) since early 2012 and have learnt a great deal about how to effectively explain mathematical concepts; occasionally, a question on Mathematics Stack Exchange inspires me to add a new example or exercise to the book. Second, I have made frequent use of \(\LaTeX\) Stack Exchange (https://tex.stackexchange.com) for implementing some of the more technical aspects of writing a book using \(\LaTeX\).

The Department of Mathematical Sciences at Carnegie Mellon University supported me academically, professionally and financially throughout my PhD and presented me with more opportunities than I could possibly have hoped for to develop as a teacher. This support is now continued by the Department of Mathematics at Northwestern University, where I am currently employed as a lecturer.

I would also like to thank everyone at Carnegie Mellon‚Äôs and Northwestern‚Äôs teaching centres, the Eberly Center and the Searle Center, respectively. Through various workshops, programs and fellowships at both teaching centres, I have learnt an incredible amount about how people learn, and I have transformed as a teacher. Their student-centred, evidence-based approach to the science of teaching and learning underlies everything I do as a teacher, including writing this book‚Äîtheir influence cannot be understated.

Finally, and importantly, I am grateful to the 1000+ students who have already used this book to learn mathematics. Every time a student contacts me to ask a question or point out an error, the book gets better; this is reflected in the dozens of typographical errors that have been fixed as a consequence.

Clive Newstead  
January 2020  
Evanston, Illinois
</markdown><markdown>
## Chapter 0

# Getting started

Before we can start proving things, we need to eliminate certain kinds of statements that we might try to prove. Consider the following statement:

*This sentence is false.*

Is it true or false? If you think about this for a couple of seconds then you‚Äôll get into a bit of a pickle.

Now consider the following statement:

*The happiest donkey in the world.*

Is it true or false? Well it‚Äôs not even a sentence; it doesn‚Äôt make sense to even *ask* if it‚Äôs true or false!

Clearly we‚Äôll be wasting our time trying to write proofs of statements like the two listed above‚Äîwe need to narrow our scope to statements that we might actually have a chance of proving (or perhaps refuting)! This motivates the following (informal) definition.

‚ú¶ **Definition 0.1**  
A *proposition* is a statement to which it is possible to assign a *truth value* (‚Äòtrue‚Äô or ‚Äòfalse‚Äô). If a proposition is true, a *proof* of the proposition is a logically valid argument demonstrating that it is true, which is pitched at such a level that a member of the intended audience can verify its correctness.

Thus the statements given above are not propositions because there is no possible way of assigning them a truth value. Note that, in **Definition 0.1**, all that matters is that it
</markdown><markdown>
makes sense to say that it is true or false, regardless of whether it actually *is* true or false‚Äîthe truth value of many propositions is unknown, even very simple ones.

### Exercise 0.2
Think of an example of a true proposition, a false proposition, a proposition whose truth value you don‚Äôt know, and a statement that is not a proposition.

Results in mathematical papers and textbooks may be referred to as *propositions*, but they may also be referred to as *theorems*, *lemmas* or *corollaries* depending on their intended usage.

- A **proposition** is an umbrella term which can be used for any result.
- A **theorem** is a key result which is particularly important.
- A **lemma** is a result which is proved for the purposes of being used in the proof of a theorem.
- A **corollary** is a result which follows from a theorem without much additional effort.

These are not precise definitions, and they are not meant to be‚Äîyou could call every result a *proposition* if you wanted to‚Äîbut using these words appropriately helps readers work out how to read a paper. For example, if you just want to skim a paper and find its key results, you‚Äôd look for results labelled as *theorems*.

It is not much good trying to prove results if we don‚Äôt have anything to prove results about. With this in mind, we will now introduce the *number sets* and prove some results about them in the context of four topics, namely: division of integers, number bases, rational and irrational numbers, and polynomials. These topics will provide context for the material in Part I, and serve as an introduction to the topics covered in Part II.

We will not go into very much depth in this chapter. Rather, think of this as a warm-up exercise‚Äîa quick, light introduction, with more proofs to be provided in the rest of the book.

### Number sets

Later in this chapter, and then in much more detail in Chapter 2, we will encounter the notion of a *set*; a set can be thought of as being a collection of objects. This seemingly simple notion is fundamental to mathematics, and is so involved that we will not treat sets formally in this book. For now, the following definition will suffice.
</markdown><markdown>
### Definition 0.3 (to be revised in Definition 2.1.1)

A set is a collection of objects. The objects in the set are called **elements** of the set. If \( X \) is a set and \( x \) is an object, then we write \( x \in X \) (LaTeX code: `x \in X`) to denote the assertion that \( x \) is an element of \( X \).

The sets of concern to us first and foremost are the **number sets**‚Äîthat is, sets whose elements are particular types of **number**. At this introductory level, many details will be temporarily swept under the rug; we will work at a level of precision which is appropriate for our current stage, but still allows us to develop a reasonable amount of intuition.

In order to define the number sets, we will need three things: an infinite line, a fixed point on this line, and a fixed unit of length.

So here we go. Here is an infinite line:

The arrows indicate that it is supposed to extend in both directions without end. The points on the line will represent numbers (specifically, **real numbers**, a misleading term that will be defined in Definition 0.25). Now let‚Äôs fix a point on this line, and label it ‚Äò0‚Äô:

This point can be thought of as representing the number zero; it is the point against which all other numbers will be measured. Finally, let‚Äôs fix a unit of length:

This unit of length will be used, amongst other things, to compare the extent to which the other numbers differ from zero.

### Definition 0.4

The above infinite line, together with its fixed zero point and fixed unit length, constitute the (**real**) **number line**.

We will use the number line to construct five sets of numbers of interest to us:

- The set \( \mathbb{N} \) of **natural numbers**‚ÄîDefinition 0.5;
</markdown><markdown>
- The set \(\mathbb{Z}\) of **integers**‚Äî[Definition 0.11](#);
- The set \(\mathbb{Q}\) of **rational numbers**‚Äî[Definition 0.24](#);
- The set \(\mathbb{R}\) of **real numbers**‚Äî[Definition 0.25](#); and
- The set \(\mathbb{C}\) of **complex numbers**‚Äî[Definition 0.31](#).

Each of these sets has a different character and is used for different purposes, as we will see both later in this chapter and throughout this book.

## Natural numbers (\(\mathbb{N}\))

The **natural numbers** are the numbers used for counting‚Äîthey are the answers to questions of the form ‚Äòhow many‚Äô‚Äîfor example, I have **three** uncles, **one** dog and **zero** cats.

Counting is a skill humans have had for a very long time; we know this because there is evidence of people using tally marks tens of thousands of years ago. Tally marks provide one method of counting small numbers: starting with nothing, proceed through the objects you want to count one by one, and make a mark for every object. When you are finished, there will be as many marks as there are objects. We are taught from a young age to count with our fingers; this is another instance of making tally marks, where now instead of making a mark we raise a finger.

Making a tally mark represents an **increment** in quantity‚Äîthat is, adding one. On our number line, we can represent an increment in quantity by moving to the right by the unit length. Then the distance from zero we have moved, which is equal to the number of times we moved right by the unit length, is therefore equal to the number of objects being counted.

### Definition 0.5

The **natural numbers** are represented by the points on the number line which can be obtained by starting at 0 and moving right by the unit length any number of times:

\[
\begin{array}{cccccc}
0 & 1 & 2 & 3 & 4 & 5 \\
\end{array}
\]

In more familiar terms, they are the **non-negative whole numbers**. We write \(\mathbb{N}\) (\(\LaTeX\) code: `\mathbb{N}`) for the set of all natural numbers; thus, the notation ‚Äò\(n \in \mathbb{N}\)‚Äô means that \(n\) is a natural number.
</markdown><markdown>
The natural numbers have very important and interesting mathematical structure, and are central to the material in Chapter 8. A more precise characterisation of the natural numbers will be provided in Section 4.1, and a mathematical construction of the set of natural numbers can be found in Section B.1 (see Construction B.2.5). Central to these more precise characterisations will be the notions of ‚Äòzero‚Äô and of ‚Äòadding one‚Äô‚Äîjust like making tally marks.

‚ùñ **Aside**

Some authors define the natural numbers to be the positive whole numbers, thus excluding zero. We take 0 to be a natural number since our main use of the natural numbers will be for counting finite sets, and a set with nothing in it is certainly finite! That said, as with any mathematical definition, the choice about whether \(0 \in \mathbb{N}\) or \(0 \notin \mathbb{N}\) is a matter of taste or convenience, and is merely a convention‚Äîit is not something that can be proved or refuted.

## Number bases

Writing numbers down is something that may seem easy to you now, but it likely took you several years as a child to truly understand what was going on. Historically, there have been many different systems for representing numbers symbolically, called *numeral systems*. First came the most primitive of all, tally marks, appearing in the Stone Age and still being used for some purposes today. Thousands of years and hundreds of numeral systems later, there is one dominant numeral system, understood throughout the world: the *Hindu‚ÄìArabic numeral system*. This numeral system consists of ten symbols, called *digits*. It is a *positional numeral system*, meaning that the position of a symbol in a string determines its numerical value.

In English, the *Arabic numerals* are used as the ten digits:

\[
0 \quad 1 \quad 2 \quad 3 \quad 4 \quad 5 \quad 6 \quad 7 \quad 8 \quad 9
\]

The right-most digit in a string is in the units place, and the value of each digit increases by a factor of ten moving to the left. For example, when we write ‚Äò2812‚Äô, the left-most ‚Äò2‚Äô represents the number two thousand, whereas the last ‚Äò2‚Äô represents the number two.

The fact that there are ten digits, and that the numeral system is based on powers of ten, is a biological accident corresponding with the fact that most humans have ten fingers. For many purposes, this is inconvenient. For example, ten does not have many positive divisors (only four)‚Äîthis has implications for the ease of performing arithmetic; a system based on the number twelve, which has six positive divisors, might be more convenient. Another example is in computing and digital electronics, where it is more convenient to work in a *binary* system, with just two digits, which represent ‚Äòoff‚Äô and ‚Äòon‚Äô (or ‚Äòlow voltage‚Äô and ‚Äòhigh voltage‚Äô), respectively; arithmetic can then be performed directly using sequences of *logic gates* in an electrical circuit.
</markdown><markdown>
It is therefore worthwhile to have some understanding of positional numeral systems based on numbers other than ten. The mathematical abstraction we make leads to the definition of **base-b expansion**.

### Definition 0.6
Let \( b > 1 \). The **base-b expansion** of a natural number \( n \) is the\(^a\) string \( d_r d_{r-1} \ldots d_0 \) such that

- \( n = d_r \cdot b^r + d_{r-1} \cdot b^{r-1} + \cdots + d_0 \cdot b^0 \),
- \( 0 \leq d_i < b \) for each \( i \); and
- If \( n > 0 \) then \( d_r \neq 0 \)‚Äîthe base-b expansion of zero is 0 in all bases \( b \).

Certain number bases have names; for instance, the base-2, 3, 8, 10 and 16 expansions are respectively called **binary**, **ternary**, **octal**, **decimal** and **hexadecimal**.

\(^a\)The use of the word ‚Äòthe‚Äô is troublesome here, since it assumes that every natural number has only one base-b expansion. This fact actually requires proof‚Äîsee Theorem 7.3.51.

### Example 0.7
Consider the number 1023. Its decimal (base-10) expansion is 1023, since

\[
1023 = 1 \cdot 10^3 + 0 \cdot 10^2 + 2 \cdot 10^1 + 3 \cdot 10^0
\]

Its binary (base-2) expansion is 1111111111, since

\[
1023 = 1 \cdot 2^9 + 1 \cdot 2^8 + 1 \cdot 2^7 + 1 \cdot 2^6 + 1 \cdot 2^5 + 1 \cdot 2^4 + 1 \cdot 2^3 + 1 \cdot 2^2 + 1 \cdot 2^1 + 1 \cdot 2^0
\]

We can express numbers in base-36 by using the ten usual digits 0 through 9 and the twenty-six letters A through Z; for instance, A represents 10, M represents 22 and Z represents 35. The base-36 expansion of 1023 is SF, since

\[
1023 = 28 \cdot 36^1 + 15 \cdot 36^0 = S \cdot 36^1 + F \cdot 36^0
\]

### Exercise 0.8
Find the binary, ternary, octal, decimal, hexadecimal and base-36 expansions of the number 21127, using the letters A‚ÄìF as additional digits for the hexadecimal expansion and the letters A‚ÄìZ as additional digits for the base-36 expansion.

We sometimes wish to specify a natural number in terms of its base-b expansion; we have some notation for this.

### Notation 0.9
Let \( b > 1 \). If the numbers \( d_0, d_1, \ldots, d_r \) are base-b digits (in the sense of Definition 0.6),
</markdown><markdown>
then we write

\[ d_r d_{r-1} \ldots d_0_{(b)} = d_r \cdot b^r + d_{r-1} \cdot b^{r-1} + \ldots + d_0 \cdot b^0 \]

for the natural number whose base-\(b\) expansion is \(d_r d_{r-1} \ldots d_0\). If there is no subscript \((b)\) and a base is not specified explicitly, the expansion will be assumed to be in base-10.

‚úé **Example 0.10**  
Using our new notation, we have

\[ 1023 = 1111111111_{(2)} = 1101220_{(3)} = 1777_{(8)} = 1023_{(10)} = 3FF_{(16)} = SF_{(36)} \]

## Integers (\(\mathbb{Z}\))

The integers can be used for measuring the difference between two instances of counting. For example, suppose I have five apples and five bananas. Another person, also holding apples and bananas, wishes to trade. After our exchange, I have seven apples and only one banana. Thus I have two more apples and four fewer bananas.

Since an increment in quantity can be represented by moving to the right on the number line by the unit length, a decrement in quantity can therefore be represented by moving to the left by the unit length. Doing so gives rise to the integers.

‚ú¶ **Definition 0.11**  
The integers are represented by the points on the number line which can be obtained by starting at 0 and moving in either direction by the unit length any number of times:

\[
\begin{array}{cccccccccc}
\ldots & -5 & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & 5 & \ldots \\
\end{array}
\]

We write \(\mathbb{Z}\) (LaTeX code: `\mathbb{Z}`) for the set of all integers; thus, the notation ‚Äò\(n \in \mathbb{Z}\)‚Äô means that \(n\) is an integer.

The integers have such a fascinating structure that a whole chapter of this book is devoted to them‚Äîsee Chapter 7. This is to do with the fact that, although you can add, subtract and multiply two integers and obtain another integer, the same is not true of division. This ‚Äòbad behaviour‚Äô of division is what makes the integers interesting. We will now see some basic results about division.
</markdown><markdown>
## Division of integers

The motivation we will soon give for the definition of the rational numbers ([Definition 0.24](#)) is that the result of dividing one integer by another integer is not necessarily another integer. However, the result is *sometimes* another integer; for example, I can divide six apples between three people, and each person will receive an integral number of apples. This makes division interesting: how can we measure the failure of one integer‚Äôs divisibility by another? How can we deduce when one integer is divisible by another? What is the structure of the set of integers when viewed through the lens of division? This motivates [Definition 0.12](#).

‚ú¶ **Definition 0.12** (to be repeated in Definition 7.1.4)  
Let \( a, b \in \mathbb{Z} \). We say \( b \) *divides* \( a \) if \( a = qb \) for some integer \( q \). Other ways of saying that \( b \) divides \( a \) are: \( b \) is a *divisor* of \( a \), \( b \) is a *factor* of \( a \), or \( a \) is a *multiple* of \( b \).

‚úê **Example 0.13**  
The integer 12 is divisible by 1, 2, 3, 4, 6 and 12, since

\[
12 = 12 \cdot 1 = 6 \cdot 2 = 4 \cdot 3 = 3 \cdot 4 = 2 \cdot 6 = 1 \cdot 12
\]

It is also divisible by the negatives of all of those numbers; for example, 12 is divisible by \(-3\) since \( 12 = (-4) \cdot (-3) \).

‚úé **Exercise 0.14**  
Prove that 1 divides every integer, and that every integer divides 0.

Using [Definition 0.12](#), we can prove some general basic facts about divisibility.

‚ú¢ **Proposition 0.15**  
Let \( a, b, c \in \mathbb{Z} \). If \( c \) divides \( b \) and \( b \) divides \( a \), then \( c \) divides \( a \).

**Proof**  
Suppose that \( c \) divides \( b \) and \( b \) divides \( a \). By [Definition 0.12](#), it follows that

\[
b = qc \quad \text{and} \quad a = rb
\]

for some integers \( q \) and \( r \). Using the first equation, we may substitute \( qc \) for \( b \) in the second equation, to obtain

\[
a = r(qc)
\]

But \( r(qc) = (rq)c \), and \( rq \) is an integer, so it follows from [Definition 0.12](#) that \( c \) divides \( a \).

‚úé **Exercise 0.16**  
Let \( a, b, d \in \mathbb{Z} \). Suppose that \( d \) divides \( a \) and \( d \) divides \( b \). Given integers \( u \) and \( v \), prove that \( d \) divides \( au + bv \).
</markdown><markdown>
Some familiar concepts, such as evenness and oddness, can be characterised in terms of divisibility.

‚ú¶ **Definition 0.17**  
An integer \( n \) is even if it is divisible by 2; otherwise, \( n \) is odd.

It is not just interesting to know when one integer does divide another; however, proving that one integer doesn‚Äôt divide another is much harder. Indeed, to prove that an integer \( b \) does not divide an integer \( a \), we must prove that \( a \neq qb \) for any integer \( q \) at all. We will look at methods for doing this in Chapter 1; these methods use the following extremely important result, which will underlie all of Chapter 7.

‚ú£ **Theorem 0.18 (Division theorem, to be repeated in Theorem 7.1.1)**  
Let \( a, b \in \mathbb{Z} \) with \( b \neq 0 \). There is exactly one way to write

\[ a = qb + r \]

such that \( q \) and \( r \) are integers, and \( 0 \leq r < b \) (if \( b > 0 \)) or \( 0 \leq r < -b \) (if \( b < 0 \)).

The number \( q \) in Theorem 0.18 is called the **quotient** of \( a \) when divided by \( b \), and the number \( r \) is called the **remainder**.

‚úê **Example 0.19**  
The number 12 leaves a remainder of 2 when divided by 5, since \( 12 = 2 \cdot 5 + 2 \).

Here‚Äôs a slightly more involved example.

‚ú£ **Proposition 0.20**  
Suppose an integer \( a \) leaves a remainder of \( r \) when divided by an integer \( b \), and that \( r > 0 \). Then \(-a\) leaves a remainder of \( b - r \) when divided by \( b \).

**Proof**  
Suppose \( a \) leaves a remainder of \( r \) when divided by \( b \). Then

\[ a = qb + r \]

for some integer \( q \). A bit of algebra yields

\[
-a = -qb - r = -qb - r + (b - b) = -(q + 1)b + (b - r)
\]

Since \( 0 < r < b \), we have \( 0 < b - r < b \). Hence \(-(q + 1)\) is the quotient of \(-a\) when divided by \( b \), and \( b - r \) is the remainder.
</markdown><markdown>
### Exercise 0.21
Prove that if an integer \( a \) leaves a remainder of \( r \) when divided by an integer \( b \), then \( a \) leaves a remainder of \( r \) when divided by \(-b\).

We will finish this part on division of integers by connecting it with the material on number bases‚Äîwe can use the division theorem (Theorem 0.18) to find the base-\( b \) expansion of a given natural number. It is based on the following observation: the natural number \( n \) whose base-\( b \) expansion is \( d_r d_{r-1} \cdots d_1 d_0 \) is equal to

\[
d_0 + b(d_1 + b(d_2 + \cdots + b(d_{r-1} + b d_r) \cdots ))
\]

Moreover, \( 0 \leq d_i < b \) for all \( i \). In particular \( n \) leaves a remainder of \( d_0 \) when divided by \( b \). Hence

\[
\frac{n - d_0}{b} = d_1 + d_2 b + \cdots + d_r b^{r-1}
\]

The base-\( b \) expansion of \(\frac{n - d_0}{b}\) is therefore

\[
d_r d_{r-1} \cdots d_1
\]

In other words, the remainder of \( n \) when divided by \( b \) is the last base-\( b \) digit of \( n \), and then subtracting this number from \( n \) and dividing the result by \( b \) truncates the final digit. Repeating this process gives us \( d_1 \), and then \( d_2 \), and so on, until we end up with 0.

This suggests the following algorithm for computing the base-\( b \) expansion of a number \( n \):

- **Step 1.** Let \( d_0 \) be the remainder when \( n \) is divided by \( b \), and let \( n_0 = \frac{n - d_0}{b} \) be the quotient. Fix \( i = 0 \).

- **Step 2.** Suppose \( n_i \) and \( d_i \) have been defined. If \( n_i = 0 \), then proceed to Step 3. Otherwise, define \( d_{i+1} \) to be the remainder when \( n_i \) is divided by \( b \), and define \( n_{i+1} = \frac{n_i - d_{i+1}}{b} \). Increment \( i \), and repeat Step 2.

- **Step 3.** The base-\( b \) expansion of \( n \), is

\[
d_i d_{i-1} \cdots d_0
\]

### Example 0.22
We compute the base-17 expansion of 15213, using the letters A‚ÄìG to represent the numbers 10 through 16.

- \( 15213 = 894 \cdot 17 + 15 \), so \( d_0 = 15 = \text{F} \) and \( n_0 = 894 \).

- \( 894 = 52 \cdot 17 + 10 \), so \( d_1 = 10 = \text{A} \) and \( n_1 = 52 \).

- \( 52 = 3 \cdot 17 + 1 \), so \( d_2 = 1 \) and \( n_2 = 3 \).
</markdown><markdown>
- \(3 = 0 \cdot 17 + 3\), so \(d_3 = 3\) and \(n_3 = 0\).

- The base-17 expansion of 15213 is therefore 31AF.

A quick verification gives

\[
31AF_{(17)} = 3 \cdot 17^3 + 1 \cdot 17^2 + 10 \cdot 17 + 15 = 15213
\]

as desired.

üìé **Exercise 0.23**  
Find the base-17 expansion of 408735787 and the base-36 expansion of 1442151747.

## Rational numbers (\(\mathbb{Q}\))

Bored of eating apples and bananas, I buy a pizza which is divided into eight slices. A friend and I decide to share the pizza. I don‚Äôt have much of an appetite, so I eat three slices and my friend eats five. Unfortunately, we cannot represent the proportion of the pizza each of us has eaten using natural numbers or integers. However, we‚Äôre not far off: we can count the number of equal parts the pizza was split into, and of those parts, we can count how many we had. On the number line, this could be represented by splitting the unit line segment from 0 to 1 into eight equal pieces, and proceeding from there. This kind of procedure gives rise to the **rational numbers**.

‚ú¶ **Definition 0.24**  
The **rational numbers** are represented by the points at the number line which can be obtained by dividing any of the unit line segments between integers into an equal number of parts.

\[
\begin{array}{cccccccccccc}
-5 & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & 5 \\
\end{array}
\]

The rational numbers are those of the form \(\frac{a}{b}\), where \(a, b \in \mathbb{Z}\) and \(b \neq 0\). We write \(\mathbb{Q}\) (LaTeX code: \(\mathbb{Q}\)) for the set of all rational numbers; thus, the notation ‚Äò\(q \in \mathbb{Q}\)‚Äô means that \(q\) is a rational number.

The rational numbers are a very important example of a type of algebraic structure known as a **field**‚Äîthey are particularly central to algebraic number theory and algebraic geometry.
</markdown><markdown>
# Real numbers (\(\mathbb{R}\))

Quantity and change can be measured in the abstract using *real numbers*.

‚ú¶ **Definition 0.25**  
The *real numbers* are the points on the number line. We write \(\mathbb{R}\) (LaTeX code: `\mathbb{R}`) for the set of all real numbers; thus, the notation ‚Äò\(a \in \mathbb{R}\)‚Äô means that \(a\) is a real number.

The real numbers are central to real analysis, a branch of mathematics introduced in Chapter 9. They turn the rationals into a *continuum* by ‚Äòfilling in the gaps‚Äô‚Äîspecifically, they have the property of *completeness*, meaning that if a quantity can be approximated with arbitrary precision by real numbers, then that quantity is itself a real number.

We can define the basic arithmetic operations (addition, subtraction, multiplication and division) on the real numbers, and a notion of ordering of the real numbers, in terms of the infinite number line.

- **Ordering.** A real number \(a\) is less than a real number \(b\), written \(a < b\), if \(a\) lies to the left of \(b\) on the number line. The usual conventions for the symbols \(\leq\) (LaTeX code: `\leq`), \(>\) and \(\geq\) (LaTeX code: `\geq`) apply, for instance ‚Äò\(a \leq b\)‚Äô means that either \(a < b\) or \(a = b\).

- **Addition.** Suppose we want to add a real number \(a\) to a real number \(b\). To do this, we *translate* \(a\) by \(b\) units to the right‚Äîif \(b < 0\) then this amounts to translating \(a\) by an equivalent number of units to the left. Concretely, take two copies of the number line, one above the other, with the same choice of unit length; move the 0 of the lower number line beneath the point \(a\) of the upper number line. Then \(a + b\) is the point on the upper number line lying above the point \(b\) of the lower number line.

Here is an illustration of the fact that \((-3) + 5 = 2\):

\[
\begin{align*}
&\quad -8 \quad -7 \quad -6 \quad -5 \quad -4 \quad -3 \quad -2 \quad -1 \quad 0 \quad 1 \quad 2 \quad 3 \quad 4 \quad 5 \\
&\quad \longleftarrow \quad \longrightarrow \\
&\quad -5 \quad -4 \quad -3 \quad -2 \quad -1 \quad 0 \quad 1 \quad 2 \quad 3 \quad 4 \quad 5 \quad 6 \quad 7 \quad 8 \\
\end{align*}
\]

- **Multiplication.** This one is fun. Suppose we want to multiply a real number \(a\) by a real number \(b\). To do this, we *scale* the number line, and perhaps *reflect* it. Concretely, take two copies of the number line, one above the other; align the 0 points on both number lines, and stretch the lower number line evenly until the point 1 on the lower number line is below the point \(a\) on the upper number line‚Äînote that if \(a < 0\) then
</markdown><markdown>
the number line must be reflected in order for this to happen. Then \( a \cdot b \) is the point on the upper number line lying above \( b \) on the lower number line.

Here is an illustration of the fact that \( 5 \cdot 4 = 20 \).

and here is an illustration of the fact that \( (-5) \cdot 4 = -20 \):

### ‚ú¶ Exercise 0.26
Interpret the operations of subtraction and division as geometric transformations of the real number line.

We will take for granted the arithmetic properties of the real numbers in this chapter, waiting until Section 9.1 to sink our teeth into the details. For example, we will take for granted the basic properties of rational numbers, for instance

\[
\frac{a}{b} + \frac{c}{d} = \frac{ad + bc}{bd} \quad \text{and} \quad \frac{a}{b} \cdot \frac{c}{d} = \frac{ac}{bd}
\]

## Rational and irrational numbers

Before we can talk about irrational numbers, we should say what they are.

### ‚ú¶ Definition 0.27
An **irrational number** is a real number that is not rational.

Unlike \( \mathbb{N}, \mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C} \), there is no standard single letter expressing the irrational numbers. However, by the end of Section 2.2, we will be able to write the set of irrational numbers as \( \mathbb{R} \setminus \mathbb{Q} \).

Note in particular that ‚Äòirrational‚Äô does not simply mean ‚Äònot rational‚Äô‚Äîthat would imply that all complex numbers which are not real are irrational‚Äîrather, the term ‚Äòirrational‚Äô means ‚Äòreal and not rational‚Äô.
</markdown><markdown>
Proving that a real number is *irrational* is not particularly easy. We will get our foot in the door by allowing ourselves to assume the following result, which is restated and proved in Proposition 4.3.12.

‚ú£ **Proposition 0.28**  
The real number \(\sqrt{2}\) is irrational.

We can use the fact that \(\sqrt{2}\) is irrational to prove some facts about the relationship between rational numbers and irrational numbers.

‚ú£ **Proposition 0.29**  
Let \(a\) and \(b\) be irrational numbers. It is possible that \(ab\) be rational.

*Proof*  
Let \(a = b = \sqrt{2}\). Then \(a\) and \(b\) are irrational, and \(ab = 2 = \frac{2}{1}\), which is rational.

‚úê **Exercise 0.30**  
Let \(r\) be a rational number and let \(a\) be an irrational number. Prove that it is possible that \(ra\) be rational, and it is possible that \(ra\) be irrational.

## Complex numbers (\(\mathbb{C}\))

We have seen that multiplication by real numbers corresponds with scaling and reflection of the number line‚Äîscaling alone when the multiplicand is positive, and scaling with reflection when it is negative. We could alternatively interpret this reflection as a *rotation* by half a turn, since the effect on the number line is the same. You might then wonder what happens if we rotate by arbitrary angles, rather than only half turns.

What we end up with is a *plane* of numbers, not merely a line‚Äîsee Figure 1. Moreover, it happens that the rules that we expect arithmetic operations to satisfy still hold‚Äîaddition corresponds with translation, and multiplication corresponds with scaling and rotation. This resulting number set is that of the *complex numbers*.

‚ú¶ **Definition 0.31**  
The *complex numbers* are those obtained by the non-negative real numbers upon rotation by any angle about the point 0. We write \(\mathbb{C}\) (LaTeX code: `\mathbb{C}`) for the set of all complex numbers; thus, the notation ‚Äò\(z \in \mathbb{C}\)‚Äô means that \(z\) is a complex number.

There is a particularly important complex number, \(i\), which is the point in the complex plane exactly one unit above 0‚Äîthis is illustrated in Figure 1. Multiplication by \(i\) has the effect of rotating the plane by a quarter turn anticlockwise. In particular, we have \(i^2 = i \cdot i = -1\); the complex numbers have the astonishing property that square roots of *all* complex numbers exist (including all the real numbers).
</markdown><markdown>
Figure 1: Illustration of the complex plane, with some points labelled.
</markdown><markdown>
In fact, every complex number can be written in the form \(a + bi\), where \(a, b \in \mathbb{R}\); this number corresponds with the point on the complex plane obtained by moving \(a\) units to the right and \(b\) units up, reversing directions as usual if \(a\) or \(b\) is negative. Arithmetic on the complex numbers works just as with the real numbers; in particular, using the fact that \(i^2 = -1\), we obtain

\[
(a + bi) + (c + di) = (a + c) + (b + d)i \quad \text{and} \quad (a + bi) \cdot (c + di) = (ac - bd) + (ad + bc)i
\]

We will discuss complex numbers further in the portion of this chapter on polynomials below.

## Polynomials

The natural numbers, integers, rational numbers, real numbers and complex numbers are all examples of semirings, which means that they come equipped with nicely behaving notions of addition and multiplication.

### Definition 0.32
Let \(\mathbb{S} = \mathbb{N}, \mathbb{Z}, \mathbb{Q}, \mathbb{R}\) or \(\mathbb{C}\). A (univariate) polynomial over \(\mathbb{S}\) in the indeterminate \(x\) is an expression of the form

\[
a_0 + a_1x + \cdots + a_nx^n
\]

where \(n \in \mathbb{N}\) and each \(a_k \in \mathbb{S}\). The numbers \(a_k\) are called the **coefficients** of the polynomial. If not all coefficients are zero, the largest value of \(k\) for which \(a_k \neq 0\) is called the **degree** of the polynomial. By convention, the degree of the polynomial 0 is \(-\infty\).

Polynomials of degree 1, 2, 3, 4 and 5 are respectively called **linear**, **quadratic**, **cubic**, **quartic** and **quintic** polynomials.

### Example 0.33
The following expressions are all polynomials:

\[
3 \quad 2x - 1 \quad (3 + i)x^2 - x
\]

Their degrees are 0, 1 and 2, respectively. The first two are polynomials over \(\mathbb{Z}\), and the third is a polynomial over \(\mathbb{C}\).

### Exercise 0.34
Write down a polynomial of degree 4 over \(\mathbb{R}\) which is not a polynomial over \(\mathbb{Q}\).

### Notation 0.35
Instead of writing out the coefficients of a polynomial each time, we may write something like \(p(x)\) or \(q(x)\). The ‚Äò\((x)\)‚Äô indicates that \(x\) is the indeterminate of the polynomial.
</markdown><markdown>
If \( \alpha \) is a number[^a] and \( p(x) \) is a polynomial in indeterminate \( x \), we write \( p(\alpha) \) for the result of substituting \( \alpha \) for \( x \) in the expression \( p(x) \).

Note that, if \( A \) is any of the sets \( \mathbb{N}, \mathbb{Z}, \mathbb{Q}, \mathbb{R} \) or \( \mathbb{C} \), and \( p(x) \) is a polynomial over \( A \), then \( p(\alpha) \in A \) for all \( \alpha \in A \).

### Example 0.36
Let \( p(x) = x^3 - 3x^2 + 3x - 1 \). Then \( p(x) \) is a polynomial over \( \mathbb{Z} \) with indeterminate \( x \). For any integer \( \alpha \), the value \( p(\alpha) \) will also be an integer. For example

\[
p(0) = 0^3 - 3 \cdot 0^2 + 3 \cdot 0 - 1 = -1 \quad \text{and} \quad p(3) = 3^3 - 3 \cdot 3^2 + 3 \cdot 3 - 1 = 8
\]

### Definition 0.37
Let \( p(x) \) be a polynomial. A **root** of \( p(x) \) is a complex number \( \alpha \) such that \( p(\alpha) = 0 \).

The quadratic formula ([Theorem 1.1.31](#)) tells us that the roots of the polynomial \( x^2 + ax + b \), where \( a, b \in \mathbb{C} \), are precisely the complex numbers

\[
\frac{-a + \sqrt{a^2 - 4b}}{2} \quad \text{and} \quad \frac{-a - \sqrt{a^2 - 4b}}{2}
\]

Note our avoidance of the symbol ‚Äò\( \pm \)‚Äô, which is commonly found in discussions of quadratic polynomials. The symbol ‚Äò\( \pm \)‚Äô is dangerous because it may suppress the word ‚Äòand‚Äô or the word ‚Äòor‚Äô, depending on context‚Äîthis kind of ambiguity is not something that we will want to deal with when discussing the logical structure of a proposition in [Chapter 1](#)!

### Example 0.38
Let \( p(x) = x^2 - 2x + 5 \). The quadratic formula tells us that the roots of \( p \) are

\[
\frac{2 + \sqrt{4 - 4 \cdot 5}}{2} = 1 + \sqrt{-4} = 1 + 2i \quad \text{and} \quad \frac{2 - \sqrt{4 - 4 \cdot 5}}{2} = 1 - \sqrt{-4} = 1 - 2i
\]

The numbers \( 1 + 2i \) and \( 1 - 2i \) are related in that their real parts are equal and their imaginary parts differ only by a sign. [Exercise 0.39](#) generalises this observation.

### Exercise 0.39
Let \( \alpha = a + bi \) be a complex number, where \( a, b \in \mathbb{R} \). Prove that \( \alpha \) is the root of a quadratic polynomial over \( \mathbb{R} \), and find the other root of this polynomial.

[^a]: When dealing with polynomials, we will typically reserve the letter \( x \) for the indeterminate variable, and use the Greek letters \( \alpha, \beta, \gamma \) (LaTeX code: \texttt{\textbackslash alpha}, \texttt{\textbackslash beta}, \texttt{\textbackslash gamma}) for numbers to be substituted into a polynomial.
</markdown><markdown>
The following exercise proves the well-known result which classifies the number of real roots of a polynomial over \(\mathbb{R}\) in terms of its coefficients.

### Exercise 0.40
Let \(a, b \in \mathbb{C}\) and let \(p(x) = x^2 + ax + b\). The value \(\Delta = a^2 - 4b\) is called the **discriminant** of \(p\). Prove that \(p\) has two roots if \(\Delta \neq 0\) and one root if \(\Delta = 0\). Moreover, if \(a, b \in \mathbb{R}\), prove that \(p\) has no real roots if \(\Delta < 0\), one real root if \(\Delta = 0\), and two real roots if \(\Delta > 0\).

### Example 0.41
Consider the polynomial \(x^2 - 2x + 5\). Its discriminant is equal to \((-2)^2 - 4 \cdot 5 = -16\), which is negative. Exercise 0.40 tells us that it has two roots, neither of which are real. This was verified by Example 0.38, where we found that the roots of \(x^2 - 2x + 5\) are \(1 + 2i\) and \(1 - 2i\).

Now consider the polynomial \(x^2 - 2x - 3\). Its discriminant is equal to \((-2)^2 - 4 \cdot (-3) = 16\), which is positive. Exercise 0.40 tells us that it has two roots, both of which are real; and indeed

\[
x^2 - 2x - 3 = (x + 1)(x - 3)
\]

so the roots of \(x^2 - 2x - 3\) are \(-1\) and \(3\).
</markdown><markdown>
# Section 0.E

## Chapter 0 exercises

0.1. The video-sharing website *YouTube* assigns to each video a unique identifier, which is a string of 11 characters from the set

\[
\{A, B, \ldots, Z, a, b, \ldots, z, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, -, \_\}
\]

This string is actually a natural number expressed in base-64, where the characters in the above set represent the numbers 0 through 63, in the same order‚Äîthus C represents 2, c represents 28, 3 represents 55, and \_ represents 63. According to this schema, find the natural number whose base-64 expansion is dQw4w9WgXcQ, and find the base-64 expansion of the natural number 7 159 047 702 620 056 984.

0.2. Let \(a, b, c, d \in \mathbb{Z}\). Under what conditions is \((a + b\sqrt{2})(c + d\sqrt{2})\) an integer?

0.3. Suppose an integer \(m\) leaves a remainder of \(i\) when divided by 3, and an integer \(n\) leaves a remainder of \(j\) when divided by 3, where \(0 \leq i, j < 3\). Prove that \(m + n\) and \(i + j\) leave the same remainder when divided by 3.

0.4. What are the possible remainders of \(n^2\) when divided by 3, where \(n \in \mathbb{Z}\)?

### Definition 0.E.1
A set \(X\) is **closed** under an operation \(\odot\) if, whenever \(a\) and \(b\) are elements of \(X\), \(a \odot b\) is also an element of \(X\).

In Questions 0.5 to 0.11, determine which of the number sets \(\mathbb{N}, \mathbb{Z}, \mathbb{Q}\) and \(\mathbb{R}\) are closed under the operation \(\odot\) defined in the question.

0.5. \(a \odot b = a + b\)

0.6. \(a \odot b = a - b\)

0.7. \(a \odot b = (a - b)(a + b)\)

0.8. \(a \odot b = (a - 1)(b - 1) + 2(a + b)\)

0.9. \(a \odot b = \frac{a}{b^2 + 1}\)

0.10. \(a \odot b = \frac{a}{\sqrt{b^2 + 1}}\)

0.11. \(a \odot b = 
\begin{cases} 
ab & \text{if } b > 0 \\
0 & \text{if } b \not\in \mathbb{Q}
\end{cases}
\)

### Definition 0.E.2
A complex number \(\alpha\) is **algebraic** if \(p(\alpha) = 0\) for some nonzero polynomial \(p(x)\) over \(\mathbb{Q}\).

0.12. Let \(x\) be a rational number. Prove that \(x\) is an algebraic number.
</markdown><markdown>
0.13. Prove that \(\sqrt{2}\) is an algebraic number.

0.14. Prove that \(\sqrt{2} + \sqrt{3}\) is an algebraic number.

0.15. Prove that \(x + yi\) is an algebraic number, where \(x\) and \(y\) are any two rational numbers.

**True‚ÄìFalse questions**

In Questions 0.16 to 0.23, determine (with proof) whether the statement is true or false.

0.16. Every integer is a natural number.

0.17. Every integer is a rational number.

0.18. Every integer divides zero.

0.19. Every integer divides its square.

0.20. The square of every rational number is a rational number.

0.21. The square root of every positive rational number is a rational number.

0.22. When an integer \(a\) is divided by a positive integer \(b\), the remainder is always less than \(a\).

0.23. Every quadratic polynomial has two distinct complex roots.

**Always‚ÄìSometimes‚ÄìNever questions**

In Questions 0.24 to 0.32, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

0.24. Let \(n, b_1, b_2 \in \mathbb{N}\) with \(1 < n < b_1 < b_2\). Then the base-\(b_1\) expansion of \(n\) is equal to the base-\(b_2\) expansion of \(n\).

0.25. Let \(n, b_1, b_2 \in \mathbb{N}\) with \(1 < b_1 < b_2 < n\). Then the base-\(b_1\) expansion of \(n\) is equal to the base-\(b_2\) expansion of \(n\).

0.26. Let \(a, b, c \in \mathbb{Z}\) and suppose that \(a\) divides \(c\) and \(b\) divides \(c\). Then \(ab\) divides \(c\).

0.27. Let \(a, b, c \in \mathbb{Z}\) and suppose that \(a\) divides \(c\) and \(b\) divides \(c\). Then \(ab\) divides \(c^2\).

0.28. Let \(x, y \in \mathbb{Q}\) and let \(a, b, c, d \in \mathbb{Z}\) with \(cy + d \neq 0\). Then \(\frac{ax + b}{cy + d} \in \mathbb{Q}\).

0.29. Let \(\frac{a}{b}\) be a rational number. Then \(a \in \mathbb{Z}\) and \(b \in \mathbb{Z}\).
</markdown><markdown>
0.30. Let \( x \in \mathbb{R} \) and assume that \( x^2 \in \mathbb{Q} \). Then \( x \in \mathbb{Q} \).

0.31. Let \( x \in \mathbb{R} \) and assume that \( x^2 + 1 \in \mathbb{Q} \) and \( x^5 + 1 \in \mathbb{Q} \). Then \( x \in \mathbb{Q} \).

0.32. Let \( p(x) = ax^2 + bx + c \) be a polynomial with \( a, b, c \in \mathbb{R} \) and \( a \neq 0 \), and suppose that \( u + vi \) be a complex root of \( p(x) \) with \( v \neq 0 \). Then \( u - vi \) is a root of \( p(x) \).
</markdown>It seems the page is blank, so there is no text to extract.<markdown>
# Part I

## Core concepts
</markdown>It seems the image is blank. Could you please provide a different image or check if the file is correct?<markdown>
# Chapter 1

## Logical structure

The goal of this chapter is to develop a methodical way of breaking up a proposition into smaller components and seeing how these components fit together‚Äîthis is called the *logical structure* of a proposition. The logical structure of a proposition is very informative: it tells us what we need to do in order to prove it, what we need to write in order to communicate our proof, and how to explore the consequences of the proposition after it has been proved.

```
logical structure of a proposition
   /            |            \
  /             |             \
 /              |              \
strategies for  structure and   consequences of
proving the     wording of      the proposition
proposition     the proof
```

[Sections 1.1](#) and [1.2](#) are dedicated to developing a system of *symbolic logic* for reasoning about propositions. We will be able to represent a proposition using a string of variables and symbols, and this expression will guide how we can prove the proposition and explore its consequences. In [Section 1.3](#) we will develop techniques for manipulating these logical expressions algebraically‚Äîthis, in turn, will yield new proof techniques (some have fancy names like ‚Äòproof by contraposition‚Äô, but some do not).

Exploring how the logical structure of a proposition informs the structure and wording of its proof is the content of [Appendix A.2](#).
</markdown><markdown>
## Section 1.1

# Propositional logic

Every mathematical proof is written in the context of certain *assumptions* being made, and certain *goals* to be achieved.

- **Assumptions** are the propositions which are known to be true, or which we are assuming to be true for the purposes of proving something. They include theorems that have already been proved, prior knowledge which is assumed of the reader, and assumptions which are explicitly made using words like ‚Äòsuppose‚Äô or ‚Äòassume‚Äô.

- **Goals** are the propositions we are trying to prove in order to complete the proof of a result, or perhaps just a step in the proof.

With every phrase we write, our assumptions and goals change. This is perhaps best illustrated by example. In [Example 1.1.1](#), we will examine the proof of [Proposition 0.15](#) in detail, so that we can see how the words we wrote affected the assumptions and goals at each stage in the proof. We will indicate our assumptions and goals at any given stage using tables‚Äîthe assumptions listed will only be those assumptions which are made explicitly; prior knowledge and previously proved theorems will be left implicit.

‚úê **Example 1.1.1**  
The statement of [Proposition 0.15](#) was as follows:

Let \( a, b, c \in \mathbb{Z} \). If \( c \) divides \( b \) and \( b \) divides \( a \), then \( c \) divides \( a \).

The set-up of the proposition instantly gives us our initial assumptions and goals:

| Assumptions     | Goals                                      |
|-----------------|--------------------------------------------|
| \( a, b, c \in \mathbb{Z} \) | If \( c \) divides \( b \) and \( b \) divides \( a \), then \( c \) divides \( a \) |

We will now proceed through the proof, line by line, to see what effect the words we wrote had on the assumptions and goals.

Since our goal was an expression of the form ‚Äòif‚Ä¶ then‚Ä¶‚Äô, it made sense to start by assuming the ‚Äòif‚Äô statement, and using that assumption to prove the ‚Äòthen‚Äô statement. As such, the first thing we wrote in our proof was:

Suppose that \( c \) divides \( b \) and \( b \) divides \( a \).
</markdown><markdown>
Our updated assumptions and goals are reflected in the following table.

| Assumptions          | Goals        |
|----------------------|--------------|
| \(a, b, c \in \mathbb{Z}\) | \(c\) divides \(a\) |
| \(c\) divides \(b\)  |              |
| \(b\) divides \(a\)  |              |

Our next step in the proof was to unpack the definitions of ‚Äò\(c\) divides \(b\)‚Äô and ‚Äò\(b\) divides \(a\)‚Äô, giving us more to work with.

Suppose that \(c\) divides \(b\) and \(b\) divides \(a\). By Definition 0.12, it follows that

\[ 
b = qc \quad \text{and} \quad a = rb 
\]

for some integers \(q\) and \(r\).

This introduces two new variables \(q, r\) and allows us to replace the assumptions ‚Äò\(c\) divides \(b\)‚Äô and ‚Äò\(b\) divides \(a\)‚Äô with their definitions.

| Assumptions          | Goals        |
|----------------------|--------------|
| \(a, b, c, q, r \in \mathbb{Z}\) | \(c\) divides \(a\) |
| \(b = qc\)           |              |
| \(a = rb\)           |              |

At this point we have pretty much exhausted all of the assumptions we can make, and so our attention turns towards the goal‚Äîthat is, we must prove that \(c\) divides \(a\). At this point, it helps to ‚Äòwork backwards‚Äô by unpacking the goal: what does it mean for \(c\) to divide \(a\)? Well, by Definition 0.12, we need to prove that \(a\) is equal to some integer multiplied by \(c\)‚Äîthis will be reflected in the following table of assumptions and goals.

Since we are now trying to express \(a\) in terms of \(c\), it makes sense to use the equations we have relating \(a\) with \(b\), and \(b\) with \(c\), to relate \(a\) with \(c\).

Suppose that \(c\) divides \(b\) and \(b\) divides \(a\). By Definition 0.12, it follows that

\[ 
b = qc \quad \text{and} \quad a = rb 
\]

for some integers \(q\) and \(r\). Using the first equation, we may substitute \(qc\) for \(b\) in the second equation, to obtain

\[ 
a = r(qc) 
\]
</markdown><markdown>
We are now very close, as indicated in the following table.

| Assumptions          | Goals                      |
|----------------------|----------------------------|
| \(a, b, c, q, r \in \mathbb{Z}\) | \(a = [\text{some integer}] \cdot c\) |
| \(b = qc\)           |                            |
| \(a = rb\)           |                            |
| \(a = r(qc)\)        |                            |

Our final step was to observe that the goal has at last been achieved:

Suppose that \(c\) divides \(b\) and \(b\) divides \(a\). By Definition 0.12, it follows that

\[ 
b = qc \quad \text{and} \quad a = rb 
\]

for some integers \(q\) and \(r\). Using the first equation, we may substitute \(qc\) for \(b\) in the second equation, to obtain

\[ 
a = r(qc) 
\]

But \(r(qc) = (rq)c\), and \(rq\) is an integer,

| Assumptions          | Goals                      |
|----------------------|----------------------------|
| \(a, b, c, q, r \in \mathbb{Z}\) |                            |
| \(b = qc\)           |                            |
| \(a = rb\)           |                            |
| \(a = r(qc)\)        |                            |
| \(a = (rq)c\)        |                            |
| \(rq \in \mathbb{Z}\) |                            |

Now that there is nothing left to prove, it is helpful to reiterate that point so that the reader has some closure on the matter.

Suppose that \(c\) divides \(b\) and \(b\) divides \(a\). By Definition 0.12, it follows that

\[ 
b = qc \quad \text{and} \quad a = rb 
\]

for some integers \(q\) and \(r\). Using the first equation, we may substitute \(qc\) for \(b\) in the second equation, to obtain

\[ 
a = r(qc) 
\]

But \(r(qc) = (rq)c\), and \(rq\) is an integer, so it follows from Definition 0.12 that \(c\) divides \(a\).
</markdown><markdown>
## Symbolic logic

Consider again the proposition that we proved in Proposition 0.15 (for given integers \(a, b, c\)):

\[ 
\text{If } c \text{ divides } b \text{ and } b \text{ divides } a, \text{ then } c \text{ divides } a.
\]

The three statements ‚Äò\(c\) divides \(b\)‚Äô, ‚Äò\(b\) divides \(a\)‚Äô and ‚Äò\(c\) divides \(a\)‚Äô are all propositions in their own right, despite the fact that they all appear inside a more complex proposition. We can examine the logical structure of the proposition by replacing these simpler propositions with symbols, called *propositional variables*. Writing \(P\) to represent ‚Äò\(c\) divides \(b\)‚Äô, \(Q\) to represent ‚Äò\(b\) divides \(a\)‚Äô and \(R\) to represent ‚Äò\(c\) divides \(a\)‚Äô, we obtain:

\[ 
\text{If } P \text{ and } Q, \text{ then } R.
\]

Breaking down the proposition in this way makes it clear that a feasible way to prove it is to *assume* \(P\) and \(Q\), and then *derive* \(R\) from these assumptions‚Äîthis is exactly what we did in the proof, which we examined in great detail in Example 1.1.1. But importantly, it suggests that the same proof strategy might work for other propositions which are also of the form ‚Äòif \(P\) and \(Q\), then \(R\)‚Äô, such as the following proposition (for a given integer \(n\)):

\[ 
\text{If } n > 2 \text{ and } n \text{ is prime, then } n \text{ is odd.}
\]

Observe that the simpler propositions are joined together to form a more complex proposition using language, namely the word ‚Äòand‚Äô and the construction ‚Äòif... then...‚Äô‚Äîwe will represent these constructions symbolically using *logical operators*, which will be introduced in Definition 1.1.3.

Zooming in even more closely, we can use Definition 0.12 to observe that ‚Äò\(c\) divides \(b\)‚Äô really means ‚Äò\(b = qc\) for some \(q \in \mathbb{Z}\)‚Äô. The expression ‚Äòfor some \(q \in \mathbb{Z}\)‚Äô introduces a new variable \(q\), which we must deal with appropriately in our proof. Words which we attach to variables in our proofs‚Äîsuch as ‚Äòany‚Äô, ‚Äòexists‚Äô, ‚Äòall‚Äô, ‚Äòsome‚Äô, ‚Äòunique‚Äô and ‚Äòonly‚Äô‚Äîwill be represented symbolically using *quantifiers*, which we will study in Section 1.2.

By breaking down a complex proposition into simpler statements which are connected together using logical operators and quantifiers, we can more precisely identify what assumptions we can make at any given stage in a proof of the proposition, and what steps are needed in order to finish the proof.
</markdown><markdown>
## Propositional formulae

We begin our development of symbolic logic with some definitions to fix our terminology.

‚ú¶ **Definition 1.1.2**  
A **propositional variable** is a symbol that represents a proposition. Propositional variables may be assigned **truth values** (‚Äòtrue‚Äô or ‚Äòfalse‚Äô).

We will typically use the lower-case letters \( p, q, r \) and \( s \) as our propositional variables.

We will be able to form more complex expressions representing propositions by connecting together simpler ones using **logical operators** such as \(\land\) (which represents ‚Äòand‚Äô), \(\lor\) (which represents ‚Äòor‚Äô), \(\Rightarrow\) (which represents ‚Äòif‚Ä¶then‚Ä¶‚Äô) and \(\lnot\) (which represents ‚Äònot‚Äô).

The definition of the notions of **logical operator** and **propositional formula** given below is a little bit difficult to digest, so it is best understood by considering examples of propositional formulae and instances of logical operators. Fortunately we will see plenty of these, since they are the central objects of study for the rest of this section.

‚ú¶ **Definition 1.1.3**  
A **propositional formula** is an expression that is either a propositional variable, or is built up from simpler propositional formulae (‚Äòsubformulae‚Äô) using a **logical operator**. In the latter case, the truth value of the propositional formula is determined by the truth values of the subformulae according to the rules of the logical operator.

On first sight, **Definition 1.1.3** seems circular‚Äîit defines the term ‚Äòpropositional formula‚Äô in terms of propositional formulae! But in fact it is not circular; it is an example of a **recursive** definition (we avoid circularity with the word ‚Äòsimpler‚Äô). To illustrate, consider the following example of a propositional formula:

\[
(p \land q) \Rightarrow r
\]

This expression represents a proposition of the form ‚Äòif \( p \) and \( q \), then \( r \)‚Äô, where \( p, q, r \) are themselves propositions. It is built from the subformula \( p \land q \) and \( r \) using the logical operator \(\Rightarrow\), and \( p \land q \) is itself built up from the subformulae \( p \) and \( q \) using the logical operator \(\land\).

The truth value of \((p \land q) \Rightarrow r\) is then determined by the truth values of the constituent propositional variables (\( p, q \) and \( r \)) according to the rules for the logical operators \(\land\) and \(\Rightarrow\).
</markdown><markdown>
If this all seems a bit abstract, that is because it *is* abstract, and you are forgiven if it makes no sense to you yet. From this point onwards, we will only study particular instances of logical operators, which will make it all much easier to understand.

## Conjunction (‚Äòand‚Äô, ‚àß)

Conjunction is the logical operator which makes precise what we mean when we say ‚Äòand‚Äô.

‚ú¶ **Definition 1.1.4**  
The **conjunction** operator is the logical operator ‚àß (LaTeX code: `\wedge`), defined according to the following rules:

- (‚àßI) If *p* is true and *q* is true, then *p* ‚àß *q* is true;
- (‚àßE‚ÇÅ) If *p* ‚àß *q* is true, then *p* is true;
- (‚àßE‚ÇÇ) If *p* ‚àß *q* is true, then *q* is true.

The expression *p* ‚àß *q* represents ‚Äò*p* and *q*‚Äô.

It is not always obvious when conjunction is being used; sometimes it sneaks in without the word ‚Äòand‚Äô ever being mentioned! Be on the look-out for occasions like this, such as in the following exercise.

‚úé **Example 1.1.5**  
We can express the proposition ‚Äò7 is a prime factor of 28‚Äô in the form *p* ‚àß *q*, by letting *p* represent the proposition ‚Äò7 is prime‚Äô and letting *q* represent the proposition ‚Äò7 divides 28‚Äô.

‚úé **Exercise 1.1.6**  
Express the proposition ‚ÄòJohn is a mathematician who lives in Pittsburgh‚Äô in the form *p* ‚àß *q*, for propositions *p* and *q*.

The rules in **Definition 1.1.4** are examples of **rules of inference**‚Äîthey tell us how to deduce (or ‚Äòinfer‚Äô) the truth of one propositional formula from the truth of other propositional formulae. In particular, rules of inference never directly tell us when a proposition is *false*‚Äîin order to prove something is false, we will prove its negation is true (see **Definition 1.1.37**).

Rules of inference tell us how to use the logical structure of propositions in proofs:

- The rule (‚àßI) is an **introduction rule**, meaning that it tells us how to **prove a goal of**
</markdown><markdown>
the form \( p \land q \)‚Äîindeed, if we want to prove that \( p \land q \) is true, \((\land I)\) tells us that it suffices to prove that \( p \) is true and prove that \( q \) is true.

- The rules \((\land E_1)\) and \((\land E_2)\) are elimination rules, meaning that they tell us how to use an assumption of the form \( p \land q \)‚Äîindeed, if we are assuming that \( p \land q \) is true, we are then free to use the fact that \( p \) is true and the fact that \( q \) is true.

Each logical operator will come equipped with some introduction and/or elimination rules, which tell us how to prove goals or use assumptions which include the logical operator in question. It is in this way that the logical structure of a proposition informs proof strategies, like the following:

‚ùñ **Strategy 1.1.7 (Proving conjunctions)**  
A proof of the proposition \( p \land q \) can be obtained by tying together two proofs, one being a proof that \( p \) is true and one being a proof that \( q \) is true.

‚úê **Example 1.1.8**  
Suppose we are required to prove that 7 is a prime factor of 28. In Example 1.1.5 we expressed ‚Äò7 is a prime factor of 28‚Äô as the conjunction of the propositions ‚Äò7 is prime‚Äô and ‚Äò7 divides 28‚Äô, and so Strategy 1.1.7 breaks down the proof into two steps: first prove that 7 is prime, and then prove that 7 divides 28.

Much like Strategy 1.1.7 was informed by the introduction rule for \(\land\), the elimination rules inform how we may make use of an assumption involving a conjunction.

‚ùñ **Strategy 1.1.9 (Assuming conjunctions)**  
If an assumption in a proof has the form \( p \land q \), then we may assume \( p \) and assume \( q \) in the proof.

‚úê **Example 1.1.10**  
Suppose that, somewhere in the process of proving a proposition, we arrive at the fact that 7 is a prime factor of 28. Strategy 1.1.9 then allows us to use the separate facts that 7 is prime and that 7 divides 28.

Strategies 1.1.7 and 1.1.9 seem almost obvious. To an extent they are obvious, and that is why we are stating them first. But the real reason we are going through the process of precisely defining logical operators, their introduction and elimination rules, and the corresponding proof strategies, is that when you are in the middle of the proof of a complicated result, it is all too easy to lose track of what you have already proved and what remains to be proved. Keeping track of the assumptions and goals in a proof, and understanding what must be done in order to complete the proof, is a difficult task.
</markdown><markdown>
To avoid drawing this process out too long, we need a compact way of expressing rules of inference that allows us to simply read off corresponding proof strategies. We could use tables of assumptions and goals like in Example 1.1.1, but this quickly becomes clunky‚Äîindeed, even the very simple conjunction introduction rule (\(\land I\)) doesn‚Äôt look very nice in this format:

| Assumptions | Goals   | Assumptions | Goals |
|-------------|---------|-------------|-------|
| ...         | \(p \land q\) | ...         | \(p\)   |
| ...         |         | ...         | \(q\)   |

Instead, we will represent rules of inference in the style of natural deduction. In this style, we write the premises \(p_1, p_2, \ldots, p_k\) of a rule above a line, with a single conclusion \(q\) below the line, representing the assertion that the truth of a proposition \(q\) follows from the truth of (all of) the premises \(p_1, p_2, \ldots, p_k\).

\[
\frac{p_1 \quad p_2 \quad \cdots \quad p_k}{q}
\]

For instance, the introduction and elimination rules for conjunction can be expressed concisely as follows:

\[
\frac{p \quad q}{p \land q} \quad (\land I) \quad \frac{p \land q}{p} \quad (\land E_1) \quad \frac{p \land q}{q} \quad (\land E_2)
\]

In addition to its clean and compact nature, this way of writing rules of inference is useful because we can combine them into proof trees in order to see how to prove more complicated propositions. For example, consider the following proof tree, which combines two instances of the conjunction introduction rule.

\[
\frac{\frac{p \quad q}{p \land q} \quad r}{(p \land q) \land r}
\]

From this proof tree, we obtain a strategy for proving a proposition of the form \((p \land q) \land r\). Namely, first prove \(p\) and prove \(q\), to conclude \(p \land q\); and then prove \(r\), to conclude \((p \land q) \land r\). This illustrates that the logical structure of a proposition informs how we may structure a proof of the proposition.
</markdown><markdown>
### Exercise 1.1.11

Write a proof tree whose conclusion is the propositional formula \((p \land q) \land (r \land s)\), where \(p, q, r, s\) are propositional variables. Express ‚Äò2 is an even prime number and 3 is an odd prime number‚Äô in the form \((p \land q) \land (r \land s)\), for appropriate propositions \(p, q, r\) and \(s\), and describe how your proof tree suggests what a proof might look like.

### Disjunction (‚Äòor‚Äô, \(\vee\))

‚ú¶ **Definition 1.1.12**  
The **disjunction** operator is the logical operator \(\vee\) (LaTeX code: `\vee`), defined according to the following rules:

- (‚à®I‚ÇÅ) If \(p\) is true, then \(p \vee q\) is true;
- (‚à®I‚ÇÇ) If \(q\) is true, then \(p \vee q\) is true;
- (‚à®E) If \(p \vee q\) is true, and if \(r\) can be derived from \(p\) and from \(q\), then \(r\) is true.

The expression \(p \vee q\) represents ‚Äò\(p\) or \(q\)‚Äô.

The introduction and elimination rules for disjunction are represented diagrammatically as follows.

\[
\begin{array}{c}
\frac{p}{p \vee q} (\vee I_1) \quad \frac{q}{p \vee q} (\vee I_2) \\
\begin{array}{c}
[p] \quad [q] \\
\downarrow \quad \downarrow \\
r \quad r
\end{array} \quad \frac{p \vee q}{r} (\vee E)
\end{array}
\]

We will discuss what the notation \([p] \leadsto r\) and \([q] \leadsto r\) means momentarily. First, we zoom in on how the disjunction introduction rules inform proofs of propositions of the form ‚Äò\(p\) or \(q\)‚Äô.

‚ú∂ **Strategy 1.1.13 (Proving disjunctions)**  
In order to prove a proposition of the form \(p \vee q\), it suffices to prove just one of \(p\) or \(q\).

‚úê **Example 1.1.14**  
Suppose we want prove that 8192 is not divisible by 3. We know by the division theorem (Theorem 0.18) that an integer is not divisible by 3 if and only if it leaves a remainder of 1 or 2 when divided by 3, and so it suffices to prove the following:

\[ 
8192 \text{ leaves a remainder of 1 when divided by 3} \vee 8192 \text{ leaves a remainder of 2 when divided by 3} 
\]
</markdown><markdown>
A quick computation reveals that \(8192 = 2730 \times 3 + 2\), so that 8192 leaves a remainder of 2 when divided by 3. By [Strategy 1.1.13](#), the proof is now complete, since the full disjunction follows by (\(\vee\)12).

### Example 1.1.15

Let \(p, q, r, s\) be propositional variables. The propositional formula \((p \vee q) \land (r \vee s)\) represents ‚Äò\(p\) or \(q\), and \(r\) or \(s\)‚Äô. What follows are two examples of truth trees for this propositional formula.

\[
\begin{array}{c}
\frac{p}{p \vee q} \quad (\vee 11) \quad \frac{r}{r \vee s} \quad (\vee 11) \\
\hline
(p \vee q) \land (r \vee s) \quad (\land I)
\end{array}
\]

\[
\begin{array}{c}
\frac{q}{p \vee q} \quad (\vee 12) \quad \frac{s}{r \vee s} \quad (\vee 12) \\
\hline
(p \vee q) \land (r \vee s) \quad (\land I)
\end{array}
\]

The proof tree on the left suggests the following proof strategy for \((p \vee q) \land (r \vee s)\). First prove \(p\), and deduce \(p \vee q\); then prove \(r\), and deduce \(r \vee s\); and finally deduce \((p \vee q) \land (r \vee s)\). The proof tree on the right suggests a different strategy, where \(p \vee q\) is deduced by proving \(q\) instead of \(p\), and \(r \vee s\) is deduced by proving \(s\) instead of \(r\).

Selecting which (if any) of these to use in a proof might depend on what we are trying to prove. For example, for a fixed natural number \(n\), let \(p\) represent ‚Äò\(n\) is even‚Äô, let \(q\) represent ‚Äò\(n\) is odd‚Äô, let \(r\) represent ‚Äò\(n \geq 2\)‚Äô and let \(s\) represent ‚Äò\(n\) is a perfect square‚Äô. Proving \((p \vee q) \land (r \vee s)\) when \(n = 2\) would be most easily done using the left-hand proof tree above, since \(p\) and \(r\) are evidently true when \(n = 2\). However, the second proof tree would be more appropriate for proving \((p \vee q) \land (r \vee s)\) when \(n = 1\).

### Aside

If you haven‚Äôt already mixed up \(\land\) and \(\vee\), you probably will soon, so here‚Äôs a way of remembering which is which:

**fish n chips**

If you forget whether it‚Äôs \(\land\) or \(\vee\) that means ‚Äòand‚Äô, just write it in place of the ‚Äòn‚Äô in ‚Äòfish n chips‚Äô:

- fish \(\land\) chips
- fish \(\vee\) chips

Clearly the first looks more correct, so \(\land\) means ‚Äòand‚Äô. If you don‚Äôt eat fish (or chips), then worry not, as this mnemonic can be modified to accommodate a wide variety of dietary restrictions; for instance ‚Äòmac n cheese‚Äô or ‚Äòquinoa n kale‚Äô or, for the meat lovers, ‚Äòribs n brisket‚Äô.

Recall the diagrammatic statement of the disjunction elimination rule:
</markdown><markdown>
The curious notation \([p] \leadsto r\) indicates that \(p\) is a temporary assumption. In the part of the proof corresponding to \([p] \leadsto r\), we would assume that \(p\) is true and derive \(r\) from that assumption, and remove the assumption that \(p\) is true from that point onwards. Likewise for \([q] \leadsto r\).

The proof strategy obtained from the disjunction elimination rule is called proof by cases.

### Strategy 1.1.16 (Assuming disjunctions‚Äîproof by cases)
If an assumption in a proof has the form \(p \lor q\), then we may derive a proposition \(r\) by splitting into two cases: first, derive \(r\) from the temporary assumption that \(p\) is true, and then derive \(r\) from the assumption that \(q\) is true.

The following example illustrates how Strategies 1.1.13 and 1.1.16 can be used together in a proof.

### Example 1.1.17
Let \(n\) be a positive proper factor of 4, and suppose we want to prove that \(n\) is either even or a perfect square.

- Our assumption that \(n\) is a positive proper factor of 4 can be expressed as the disjunction \(n = 1 \lor n = 2\).

- Our goal is to prove the disjunction ‚Äò\(n\) is even \(\lor\) \(n\) is a perfect square‚Äô.

According to Strategy 1.1.16, we split into two cases, one in which \(n = 1\) and one in which \(n = 2\). In each case, we must derive ‚Äò\(n\) is even \(\lor\) \(n\) is a perfect square‚Äô, for which it suffices by Strategy 1.1.13 to derive either that \(n\) is even or that \(n\) is a perfect square. Thus a proof might look something like this:

Since \(n\) is a positive proper factor of 4, either \(n = 1\) or \(n = 2\).

- **Case 1.** Suppose \(n = 1\). Then since \(1^2 = 1\) we have \(n = 1^2\), so that \(n\) is a perfect square.

- **Case 2.** Suppose \(n = 2\). Then since \(2 = 2 \times 1\), we have that \(n\) is even.

Hence \(n\) is either even or a perfect square.
</markdown><markdown>
Notice that in both Case 1 and Case 2, we did not explicitly mention that we had proved that ‚Äò\( n \) is even \(\lor n\) is a perfect square‚Äô, leaving that deduction to the reader‚Äîwe only mentioned it after the proofs in each case were complete.

The proof of [Proposition 1.1.18](#) below splits into three cases, rather than just two.

### Proposition 1.1.18
Let \( n \in \mathbb{Z} \). Then \( n^2 \) leaves a remainder of 0 or 1 when divided by 3.

**Proof**  
Let \( n \in \mathbb{Z} \). By the division theorem ([Theorem 0.18](#)), one of the following must be true for some \( k \in \mathbb{Z} \):

\[ n = 3k \quad \text{or} \quad n = 3k + 1 \quad \text{or} \quad n = 3k + 2 \]

- Suppose \( n = 3k \). Then

  \[
  n^2 = (3k)^2 = 9k^2 = 3 \cdot (3k^2)
  \]

  So \( n^2 \) leaves a remainder of 0 when divided by 3.

- Suppose \( n = 3k + 1 \). Then

  \[
  n^2 = (3k + 1)^2 = 9k^2 + 6k + 1 = 3(3k^2 + 2k) + 1
  \]

  So \( n^2 \) leaves a remainder of 1 when divided by 3.

- Suppose \( n = 3k + 2 \). Then

  \[
  n^2 = (3k + 2)^2 = 9k^2 + 12k + 4 = 3(3k^2 + 4k + 1) + 1
  \]

  So \( n^2 \) leaves a remainder of 1 when divided by 3.

In all cases, \( n^2 \) leaves a remainder of 0 or 1 when divided by 3. ‚ñ°

Note that in the proof of [Proposition 1.1.18](#), unlike in [Example 1.1.17](#), we did not explicitly use the word ‚Äòcase‚Äô, even though we were using proof by cases. Whether or not to make your proof strategies explicit is up to you‚Äîdiscussion of this kind of matter can be found in [Appendix A.2](#).

When completing the following exercises, try to keep track of exactly where you use the introduction and elimination rules that we have seen so far.

### Exercise 1.1.19
Let \( n \) be an integer. Prove that \( n^2 \) leaves a remainder of 0, 1 or 4 when divided by 5.

### Exercise 1.1.20
Let \( a, b \in \mathbb{R} \) and suppose \( a^2 - 4b \neq 0 \). Let \( \alpha \) and \( \beta \) be the (distinct) roots of the polynomial \( x^2 + ax + b \). Prove that there is a real number \( c \) such that either \( \alpha - \beta = c \) or \( \alpha - \beta = ci \).
</markdown><markdown>
## Implication (‚Äòif‚Ä¶then‚Ä¶‚Äô, ‚áí)

### Definition 1.1.21
The **implication** operator is the logical operator ‚áí (LaTeX code: \(\Rightarrow\)), defined according to the following rules:

- (‚áíI) If \(q\) can be derived from the assumption that \(p\) is true, then \(p \Rightarrow q\) is true;
- (‚áíE) If \(p \Rightarrow q\) is true and \(p\) is true, then \(q\) is true.

The expression \(p \Rightarrow q\) represents ‚Äòif \(p\), then \(q\)‚Äô.

\[
\begin{array}{c}
[p] \\
\downarrow \\
\frac{q}{p \Rightarrow q} \; (\Rightarrow I) \quad \frac{p \Rightarrow q \quad p}{q} \; (\Rightarrow E)
\end{array}
\]

### Strategy 1.1.22 (Proving implications)
In order to prove a proposition of the form \(p \Rightarrow q\), it suffices to assume that \(p\) is true, and then derive \(q\) from that assumption.

The following proposition illustrates how Strategy 1.1.22 can be used in a proof.

### Proposition 1.1.23
Let \(x\) and \(y\) be real numbers. If \(x\) and \(x+y\) are rational, then \(y\) is rational.

**Proof**  
Suppose \(x\) and \(x+y\) are rational. Then there exist integers \(a, b, c, d\) with \(b, d \neq 0\) such that

\[
x = \frac{a}{b} \quad \text{and} \quad x+y = \frac{c}{d}
\]

It then follows that

\[
y = (x+y) - x = \frac{c}{d} - \frac{a}{b} = \frac{bc - ad}{bd}
\]

Since \(bc - ad\) and \(bd\) are integers, and \(bd \neq 0\), it follows that \(y\) is rational.

The key phrase in the above proof was ‚ÄòSuppose \(x\) and \(x+y\) are rational.‚Äô This introduced the assumptions \(x \in \mathbb{Q}\) and \(x+y \in \mathbb{Q}\), and reduced our goal to that of deriving a proof that \(y\) is rational‚Äîthis was the content of the rest of the proof.
</markdown><markdown>
### Exercise 1.1.24

Let \( p(x) \) be a polynomial over \( \mathbb{C} \). Prove that if \(\alpha\) is a root of \( p(x) \), and \( a \in \mathbb{C} \), then \(\alpha\) is a root of \((x-a)p(x)\).

The elimination rule for implication (\(\Rightarrow \mathcal{E}\)) is more commonly known by the Latin name *modus ponens*.

### Strategy 1.1.25 (Assuming implications‚Äîmodus ponens)

If an assumption in a proof has the form \( p \Rightarrow q \), and \( p \) is also assumed to be true, then we may deduce that \( q \) is true.

**Strategy 1.1.16** is frequently used to reduce a more complicated goal to a simpler one. Indeed, if we know that \( p \Rightarrow q \) is true, and if \( p \) is easy to verify, then it allows us to prove \( q \) by proving \( p \) instead.

### Example 1.1.26

Let \( f(x) = x^2 + ax + b \) be a polynomial with \( a, b \in \mathbb{R} \), and let \(\Delta = a^2 - 4b\) be its discriminant. Part of Exercise 0.40 was to prove that:

(i) If \(\Delta > 0\), then \( f \) has two real roots;

(ii) If \(\Delta = 0\), then \( f \) has one real root;

(iii) If \(\Delta < 0\), then \( f \) has no real roots.

Given the polynomial \( f(x) = x^2 - 68x + 1156 \), it would be a pain to go through the process of solving the equation \( f(x) = 0 \) in order to determine how many real roots \( f \) has. However, each of the propositions (i), (ii) and (iii) take the form \( p \Rightarrow q \), so **Strategy 1.1.25** reduces the problem of finding how many real roots \( f \) has to that of evaluating \(\Delta\) and comparing it with 0. And indeed, \((-68)^2 - 4 \times 1156 = 0\), so the implication (ii) together with (\(\Rightarrow \mathcal{E}\)) tell us that \( f \) has one real root.

A common task faced by mathematicians is to prove that two conditions are equivalent. For example, given a polynomial \( f(x) = x^2 + ax + b \) with \( a, b \in \mathbb{R} \), we know that if \( a^2 - 4b > 0 \) then \( f \) has two real roots, but is it also true that if \( f \) has two real roots then \( a^2 - 4b > 0 \)? (The answer is ‚Äòyes‚Äô.) The relationship between these two implications is that each is the *converse* of the other.

### Definition 1.1.27

The *converse* of a proposition of the form \( p \Rightarrow q \) is the proposition \( q \Rightarrow p \).

A quick remark on terminology is pertinent. The following table summarises some common ways of referring to the propositions ‚Äò\( p \Rightarrow q \)‚Äô and ‚Äò\( q \Rightarrow p \)‚Äô.
</markdown><markdown>
| \(p \Rightarrow q\) | \(q \Rightarrow p\) |
|---------------------|---------------------|
| if \(p\), then \(q\) | if \(q\), then \(p\) |
| \(p\) only if \(q\) | \(p\) if \(q\) |
| \(p\) is sufficient for \(q\) | \(p\) is necessary for \(q\) |

We so often encounter the problem of proving both an implication and its converse that we introduce a new logical operator that represents the conjunction of both.

‚ú¶ **Definition 1.1.28**  
The biconditional operator is the logical operator \(\Leftrightarrow\) (LaTeX code: `\Leftrightarrow`), defined by declaring \(p \Leftrightarrow q\) to mean \((p \Rightarrow q) \land (q \Rightarrow p)\). The expression \(p \Leftrightarrow q\) represents ‚Äò\(p\) if and only if \(q\)‚Äô.

Many examples of biconditional statements come from solving equations; indeed, to say that the values \(\alpha_1, \ldots, \alpha_n\) are the solutions to a particular equation is precisely to say that

\[ x \text{ is a solution} \quad \Leftrightarrow \quad x = \alpha_1 \text{ or } x = \alpha_2 \text{ or } \cdots \text{ or } x = \alpha_n \]

‚úê **Example 1.1.29**  
We find all real solutions \(x\) to the equation

\[
\sqrt{x-3} + \sqrt{x+4} = 7
\]

Let's rearrange the equation to find out what the possible solutions may be.

\[
\sqrt{x-3} + \sqrt{x+4} = 7
\]

\[
\Rightarrow (x-3) + 2\sqrt{(x-3)(x+4)} + (x+4) = 49 \quad \text{squaring}
\]

\[
\Rightarrow 2\sqrt{(x-3)(x+4)} = 48 - 2x \quad \text{rearranging}
\]

\[
\Rightarrow 4(x-3)(x+4) = (48 - 2x)^2 \quad \text{squaring}
\]

\[
\Rightarrow 4x^2 + 4x - 48 = 2304 - 192x + 4x^2 \quad \text{expanding}
\]

\[
\Rightarrow 196x = 2352 \quad \text{rearranging}
\]

\[
\Rightarrow x = 12 \quad \text{dividing by 196}
\]

You might be inclined to stop here. Unfortunately, all we have proved is that, given a real number \(x\), if \(x\) solves the equation \(\sqrt{x-3} + \sqrt{x+4} = 7\), then \(x = 12\). This narrows down the set of possible solutions to just one candidate‚Äîbut we still need to check the converse, namely that if \(x = 12\), then \(x\) is a solution to the equation.

As such, to finish off the proof, note that

\[
\sqrt{12-3} + \sqrt{12+4} = \sqrt{9} + \sqrt{16} = 3 + 4 = 7
\]

and so the value \(x = 12\) is indeed a solution to the equation.
</markdown><markdown>
The last step in Example 1.1.29 may have seemed a little bit silly; but Example 1.1.30 demonstrates that proving the converse when solving equations truly is necessary.

### Example 1.1.30
We find all real solutions \( x \) to the equation

\[ x + \sqrt{x} = 0 \]

We proceed as before, rearranging the equation to find all possible solutions.

\[
\begin{align*}
x + \sqrt{x} &= 0 \\
x &= -\sqrt{x} & \text{rearranging} \\
x^2 &= x & \text{squaring} \\
x(x - 1) &= 0 & \text{rearranging} \\
x &= 0 \text{ or } x = 1
\end{align*}
\]

Now certainly 0 is a solution to the equation, since

\[ 0 + \sqrt{0} = 0 + 0 = 0 \]

However, 1 is not a solution, since

\[ 1 + \sqrt{1} = 1 + 1 = 2 \]

Hence it is actually the case that, given a real number \( x \), we have

\[ x + \sqrt{x} = 0 \quad \iff \quad x = 0 \]

Checking the converse here was vital to our success in solving the equation!

A slightly more involved example of a biconditional statement arising from the solution to an equation‚Äîin fact, a class of equations‚Äîis the proof of the quadratic formula.

### Theorem 1.1.31 (Quadratic formula)
Let \( a, b \in \mathbb{C} \). A complex number \( \alpha \) is a root of the polynomial \( x^2 + ax + b \) if and only if

\[
\alpha = \frac{-a + \sqrt{a^2 - 4b}}{2} \quad \text{or} \quad \alpha = \frac{-a - \sqrt{a^2 - 4b}}{2}
\]

**Proof**

First we prove that if \( \alpha \) is a root, then \( \alpha \) is one of the values given in the statement of the proposition. So suppose \( \alpha \) be a root of the polynomial \( x^2 + ax + b \). Then

\[ \alpha^2 + a\alpha + b = 0 \]
</markdown><markdown>
The algebraic technique of ‚Äòcompleting the square‚Äô tells us that

\[
\alpha^2 + a\alpha = \left( \alpha + \frac{a}{2} \right)^2 - \frac{a^2}{4}
\]

and hence

\[
\left( \alpha + \frac{a}{2} \right)^2 - \frac{a^2}{4} + b = 0
\]

Rearranging yields

\[
\left( \alpha + \frac{a}{2} \right)^2 = \frac{a^2}{4} - b = \frac{a^2 - 4b}{4}
\]

Taking square roots gives

\[
\alpha + \frac{a}{2} = \frac{\sqrt{a^2 - 4b}}{2} \quad \text{or} \quad \alpha + \frac{a}{2} = \frac{-\sqrt{a^2 - 4b}}{2}
\]

and, finally, subtracting \(\frac{a}{2}\) from both sides gives the desired result.

The proof of the converse is Exercise 1.1.32.

‚úé **Exercise 1.1.32**  
Complete the proof of the quadratic formula. That is, for fixed \(a, b \in \mathbb{C}\), prove that if

\[
\alpha = \frac{-a + \sqrt{a^2 - 4b}}{2} \quad \text{or} \quad \alpha = \frac{-a - \sqrt{a^2 - 4b}}{2}
\]

then \(\alpha\) is a root of the polynomial \(x^2 + ax + b\).

Another class of examples of biconditional propositions arise in finding necessary and sufficient criteria for an integer \(n\) to be divisible by some number‚Äîfor example, that an integer is divisible by 10 if and only if its base-10 expansion ends with the digit 0.

‚úé **Example 1.1.33**  
Let \(n \in \mathbb{N}\). We will prove that \(n\) is divisible by 8 if and only if the number formed of the last three digits of the base-10 expansion of \(n\) is divisible by 8.

First, we will do some ‚Äòscratch work‚Äô. Let \(d_r d_{r-1} \ldots d_1 d_0\) be the base-10 expansion of \(n\). Then

\[
n = d_r \cdot 10^r + d_{r-1} \cdot 10^{r-1} + \cdots + d_1 \cdot 10 + d_0
\]

Define

\[
n' = d_2 d_1 d_0 \quad \text{and} \quad n'' = n - n' = d_r d_{r-1} \ldots d_4 d_3 000
\]

Now \(n - n' = 1000 \cdot d_r d_{r-1} \ldots d_4 d_3\) and \(1000 = 8 \cdot 125\), so it follows that 8 divides \(n''\).

Our goal is now to prove that 8 divides \(n\) if and only if 8 divides \(n'\).
</markdown><markdown>
- (‚áí) Suppose 8 divides \( n \). Since 8 divides \( n'' \), it follows from Exercise 0.16 that 8 divides \( an + bn'' \) for all \( a, b \in \mathbb{Z} \). But

  \[
  n' = n - (n - n') = n - n'' = 1 \cdot n + (-1) \cdot n''
  \]

  so indeed 8 divides \( n' \), as required.

- (‚áê) Suppose 8 divides \( n' \). Since 8 divides \( n'' \), it follows from Exercise 0.16 that 8 divides \( an' + bn'' \) for all \( a, b \in \mathbb{Z} \). But

  \[
  n = n' + (n - n') = n' + n'' = 1 \cdot n' + 1 \cdot n''
  \]

  so indeed 8 divides \( n \), as required.

### Exercise 1.1.34
Prove that a natural number \( n \) is divisible by 3 if and only if the sum of its base-10 digits is divisible by 3.

### Negation (‚Äònot‚Äô, ¬¨)

So far we only officially know how to prove that true propositions are *true*. The negation operator makes precise what we mean by ‚Äònot‚Äô, which allows us to prove that false propositions are *false*.

### Definition 1.1.35
A *contradiction* is a proposition that is known or assumed to be false. We will use the symbol ‚ä• (LaTeX code: \(\bot\)) to represent an arbitrary contradiction.

### Example 1.1.36
Some examples of contradictions include the assertion that \( 0 = 1 \), or that \( \sqrt{2} \) is rational, or that the equation \( x^2 = -1 \) has a solution \( x \in \mathbb{R} \).

### Definition 1.1.37
The *negation* operator is the logical operator ¬¨ (LaTeX code: \(\neg\)), defined according to the following rules:

- (¬¨I) If a contradiction can be derived from the assumption that \( p \) is true, then ¬¨\( p \) is true;
- (¬¨E) If ¬¨\( p \) and \( p \) are both true, then a contradiction may be derived.

The expression ¬¨\( p \) represents ‚Äònot \( p \)‚Äô (or ‚Äò\( p \) is false‚Äô).
</markdown><markdown>
\[ 
\begin{array}{c}
[p] \\
\hline
\bot \quad \neg p \quad (\neg \text{I}) \\
\neg p \quad \bot \quad p \quad (\neg \text{E})
\end{array}
\]

‚ùñ **Aside**

The rules (¬¨I) and (¬¨E) closely resemble (‚áíI) and (‚áíE)‚Äîindeed, we could simply define ¬¨p to mean ‚Äò\( p \Rightarrow \bot \)‚Äô, where \( \bot \) represents an arbitrary contradiction, but it will be easier later on to have a primitive notion of negation.

The introduction rule for negation (¬¨I) gives rise to a proof strategy called **proof by contradiction**, which turns out to be extremely useful.

‚ùñ **Strategy 1.1.38** (Proving negations‚Äîproof by contradiction)

In order to prove a proposition \( p \) is false (that is, that ¬¨p is true), it suffices to assume that \( p \) is true and derive a contradiction.

The following proposition has a classic proof by contradiction.

√∑ **Proposition 1.1.39**

Let \( r \) be a rational number and let \( a \) be an irrational number. Then \( r + a \) is irrational.

*Proof*

By [Definition 0.27](#), we need to prove that \( r + a \) is real and not rational. It is certainly real, since \( r \) and \( a \) are real, so it remains to prove that \( r + a \) is not rational.

Suppose \( r + a \) is rational. Since \( r \) is rational, it follows from [Proposition 1.1.23](#) that \( a \) is rational, since

\[
a = (r + a) - r
\]

This contradicts the assumption that \( a \) is irrational. It follows that \( r + a \) is not rational, and is therefore irrational. ‚ñ°

Now you can try proving some elementary facts by contradiction.

‚úé **Exercise 1.1.40**

Let \( x \in \mathbb{R} \). Prove by contradiction that if \( x \) is irrational then \(-x\) and \(\frac{1}{x}\) are irrational.

‚úé **Exercise 1.1.41**

Prove by contradiction that there is no least positive real number. That is, prove that there is not a positive real number \( a \) such that \( a \leq b \) for all positive real numbers \( b \).

A proof need not be a ‚Äòproof by contradiction‚Äô in its entirety‚Äîindeed, it may be that only a small portion of the proof uses contradiction. This is exhibited in the proof of the following proposition.
</markdown><markdown>
### Proposition 1.1.42

Let \( a \) be an integer. Then \( a \) is odd if and only if \( a = 2b + 1 \) for some integer \( b \).

**Proof**  
Suppose \( a \) is odd. By the division theorem ([Theorem 0.18](#)), either \( a = 2b \) or \( a = 2b + 1 \), for some \( b \in \mathbb{Z} \). If \( a = 2b \), then 2 divides \( a \), contradicting the assumption that \( a \) is odd; so it must be the case that \( a = 2b + 1 \).

Conversely, suppose \( a = 2b + 1 \). Then \( a \) leaves a remainder of 1 when divided by 2. However, by the division theorem, the even numbers are precisely those that leave a remainder of 0 when divided by 2. It follows that \( a \) is not even, so is odd. ‚ñ°

The elimination rule for the negation operator (\( \neg \mathsf{E} \)) simply says that a proposition can‚Äôt be true and false at the same time.

### Strategy 1.1.43 (Assuming negations)

If an assumption in a proof has the form \( \neg p \), then any derivation of \( p \) leads to a contradiction.

The main use of [Strategy 1.1.43](#) is for obtaining the contradiction in a proof by contradiction‚Äîin fact, we have already used it in our examples of proof by contradiction! As such, we will not dwell on it further.

### Logical axioms

We wrap up this section by introducing a couple of additional logical rules (axioms) that we will use in our proofs.

The first is the so-called law of excluded middle, which appears so obvious that it is not even worth stating (let alone naming)‚Äîwhat it says is that every proposition is either true or false. But beware, as looks can be deceiving; the law of excluded middle is a non-constructive axiom, meaning that it should not be accepted in settings it is important to keep track of how a proposition is proved‚Äîsimply knowing that a proposition is either true or false tells us nothing about how it might be proved or refuted. In most mathematical contexts, though, it is accepted without a second‚Äôs thought.

### Axiom 1.1.44 (Law of excluded middle)

Let \( p \) be a propositional formula. Then \( p \lor (\neg p) \) is true.

The law of excluded middle can be represented diagrammatically as follows; there are no premises above the line, since we are simply asserting that it is true.
</markdown><markdown>
\[
p \lor (\neg p) \quad \text{LEM}
\]

**Strategy 1.1.45 (Using the law of excluded middle)**  
In order to prove a proposition \( q \) is true, it suffices to split into cases based on whether some other proposition \( p \) is true or false, and prove that \( q \) is true in each case.

The proof of [Proposition 1.1.46](#) below makes use of the law of excluded middle‚Äînote that we defined ‚Äòodd‚Äô to mean ‚Äònot even‚Äô ([Definition 0.17](#)).

**Proposition 1.1.46**  
Let \( a, b \in \mathbb{Z} \). If \( ab \) is even, then either \( a \) is even or \( b \) is even (or both).

*Proof*  
Suppose \( a, b \in \mathbb{Z} \) with \( ab \) even.

- Suppose \( a \) is even‚Äîthen we‚Äôre done.

- Suppose \( a \) is odd. If \( b \) is also odd, then by [Proposition 1.1.42](#) can write

  \[
  a = 2k + 1 \quad \text{and} \quad b = 2\ell + 1
  \]

  for some integers \( k, \ell \). This implies that

  \[
  ab = (2k + 1)(2\ell + 1) = 4k\ell + 2k + 2\ell + 1 = 2(2k\ell + k + \ell) + 1 \quad \in \mathbb{Z}
  \]

  so that \( ab \) is odd. This contradicts the assumption that \( ab \) is even, and so \( b \) must in fact be even.

In both cases, either \( a \) or \( b \) is even. \(\square\)

**Exercise 1.1.47**  
Reflect on the proof of [Proposition 1.1.46](#). Where in the proof did we use the law of excluded middle? Where in the proof did we use proof by contradiction? What was the contradiction in this case? Prove [Proposition 1.1.46](#) twice more, once using contradiction and not using the law of excluded middle, and once using the law of excluded middle and not using contradiction.

**Exercise 1.1.48**  
Let \( a \) and \( b \) be irrational numbers. By considering the number \( \sqrt{2}^{\sqrt{2}} \), prove that it is possible that \( a^b \) be rational.

Another logical rule that we will use is the *principle of explosion*, which is also known by its Latin name, *ex falso sequitur quodlibet*, which approximately translates to ‚Äòfrom falsity follows whatever you like‚Äô.
</markdown><markdown>
### Axiom 1.1.49 (Principle of explosion)

If a contradiction is assumed, any consequence may be derived.

\[
\frac{\bot}{p} \quad \text{Expl}
\]

The principle of explosion is a bit confusing on first sight. To shed a tiny bit of intuition on it, think of it as saying that both true and false propositions are consequences of a contradictory assumption. For instance, suppose that \(-1 = 1\). From this we can obtain consequences that are false, such as \(0 = 2\) by adding 1 to both sides of the equation, and consequences that are true, such as \(1 = 1\) by squaring both sides of the equation.

We will rarely use the principle of explosion directly in our mathematical proofs, but we will use it in [Section 1.3](#) for proving logical formulae are equivalent.
</markdown><markdown>
## TL;DR ‚Äî summary of [Section 1.1](#)

### Propositional formulae

**1.1.2** Propositional variables \( p, q, r, \ldots \) are used to express simple propositions.

**1.1.3** Propositional formulae are more complicated expressions built from propositional variables using logical operators, which represent phrases like ‚Äòand‚Äô, ‚Äòor‚Äô and ‚Äòif‚Ä¶ then‚Ä¶‚Äô.

### Logical operators

**1.1.4** The conjunction operator (\(\land\)) represents ‚Äòand‚Äô. We prove \( p \land q \) by proving both \( p \) and \( q \) separately; we can use an assumption of the form \( p \land q \) by assuming both \( p \) and \( q \) separately.

**1.1.12** The disjunction operator (\(\lor\)) represents ‚Äòor‚Äô. We prove \( p \lor q \) by proving at least one of \( p \) or \( q \); we can use an assumption of the form \( p \lor q \) by splitting into cases, assuming that \( p \) is true in one case, and assuming that \( q \) is true in the other.

**1.1.21** The implication operator (\(\Rightarrow\)) represents ‚Äòif‚Ä¶ then‚Ä¶‚Äô. We prove \( p \Rightarrow q \) by assuming \( p \) and deriving \( q \); we can use an assumption of the form \( p \Rightarrow q \) by deducing \( q \) whenever we know that \( p \) is true.

**1.1.37** The negation operator (\(\lnot\)) represents ‚Äònot‚Äô. We prove \(\lnot p\) by assuming \( p \) and deriving something known or assumed to be false (this is called proof by contradiction); we can use an assumption of the form \(\lnot p\) by arriving at a contradiction whenever we find that \( p \) is true.

**1.1.28** The biconditional operator (\(\Leftrightarrow\)) represents ‚Äòif and only if‚Äô. The expression \( p \Leftrightarrow q \) is shorthand for \((p \Rightarrow q) \land (q \Rightarrow p)\).

### Logical axioms

**1.1.44** The law of excluded middle says that \( p \lor (\lnot p) \) is always true. So at any point in a proof, we may split into two cases: one where \( p \) is assumed to be true, and one where \( p \) is assumed to be false.

**1.1.49** The principle of explosion says that if a contradiction is introduced as an assumption, then any conclusion at all can be derived.
</markdown><markdown>
## Section 1.2
Variables and quantifiers

### Free and bound variables

Everything we did in [Section 1.1](#) concerned *propositions* and the logical rules concerning their proofs. Unfortunately if all we have to work with is propositions then our ability to do mathematical reasoning will be halted pretty quickly. For example, consider the following statement:

\[ x \text{ is divisible by } 7 \]

This statement seems like the kind of thing we should probably be able to work with if we‚Äôre doing mathematics. It makes sense if \( x \) is an integer, such as 28 or 41; but it doesn‚Äôt make sense at all if \( x \) is a parrot called Alex.[^1] In any case, even when it does make sense, its truth value depends on \( x \); indeed, ‚Äò28 is divisible by 7‚Äô is a true proposition, but ‚Äò41 is divisible by 7‚Äô is a false proposition.

This means that the statement ‚Äò\( x \) is divisible by 7‚Äô isn‚Äôt a proposition‚Äî*quel horreur*! But it *almost* is a proposition: if we know that \( x \) refers somehow to an integer, then it becomes a proposition as soon as a particular numerical value of \( x \) is specified. The symbol \( x \) is called a *free variable*.

‚ú¶ **Definition 1.2.1**  
Let \( x \) be a variable that is understood to refer to an element of a set \( X \). In a statement involving \( x \), we say \( x \) is *free* if it makes sense to substitute particular elements of \( X \) in the statement; otherwise, we say \( x \) is *bound*.

To represent statements that have free variables in them abstractly, we generalise the notion of a propositional variable ([Definition 1.1.2](#)) to that of a *predicate*.

‚ú¶ **Definition 1.2.2**  
A *predicate* is a symbol \( p \) together with a specified list of free variables \( x_1, x_2, \ldots, x_n \) (where \( n \in \mathbb{N} \)) and, for each free variable \( x_i \), a specification of a set \( X_i \) called the *domain of discourse* (or *range*) of \( x_i \). We will typically write \( p(x_1, x_2, \ldots, x_n) \) in order to make the variables explicit.

[^1]: Alex the parrot is the only non-human animal to have ever been observed to ask an existential question; he died in September 2007 so we may never know if he was divisible by 7, but it is unlikely. According to *Time*, his last words were ‚Äòyou be good, see you tomorrow, I love you‚Äô. The reader is advised to finish crying before they continue reading about variables and quantifiers.
</markdown><markdown>
The statements represented by predicates are those that become propositions when specific values are substituted for their free variables from their respective domains of discourse. For example, ‚Äò\(x\) is divisible by 7‚Äô is not a proposition, but it becomes a proposition when specific integers (such as 28 or 41) are substituted for \(x\).

This is a lot to take in, so let‚Äôs look at some examples.

‚úê **Example 1.2.3**

(i) We can represent the statement ‚Äò\(x\) is divisible by 7‚Äô discussed above by a predicate \(p(x)\) whose only free variable \(x\) has \(\mathbb{Z}\) as its domain of discourse. Then \(p(28)\) is the true proposition ‚Äò28 is divisible by 7‚Äô and \(p(41)\) is the false proposition ‚Äò41 is divisible by 7‚Äô.

(ii) A predicate with no free variables is precisely a propositional variable. This means that the notion of a predicate generalises that of a propositional variable.

(iii) The expression ‚Äò\(2^n - 1\) is prime‚Äô can be represented by a predicate \(p(n)\) with one free variable \(n\), whose domain of discourse is the set \(\mathbb{N}\) of natural numbers. Then \(p(3)\) is the true proposition ‚Äò\(2^3 - 1\) is prime‚Äô and \(p(4)\) is the false proposition ‚Äò\(2^4 - 1\) is prime‚Äô.

(iv) The expression ‚Äò\(x - y\) is rational‚Äô can be represented by a predicate \(q(x, y)\) with free variables \(x\) and \(y\), whose domain of discourse is the set \(\mathbb{R}\) of real numbers.

(v) The expression ‚Äòthere exist integers \(a\) and \(b\) such that \(x = a^2 + b^2\)‚Äô has free variable \(x\) and bound variables \(a, b\). It can be represented by a predicate \(r(x)\) with one free variable \(x\), whose domain of discourse is \(\mathbb{Z}\).

(vi) The expression ‚Äòevery even natural number \(n \geq 2\) is divisible by \(k\)‚Äô has free variable \(k\) and bound variable \(n\). It can be represented by a predicate \(s(k)\) with one free variable \(k\), whose domain of discourse is \(\mathbb{N}\).

### Quantifiers

Look again at the statements in parts (v) and (vi) of **Example 1.2.3**. Both contained bound variables, which were so because we used words like ‚Äòthere exists‚Äô and ‚Äòevery‚Äô‚Äîhad we not used these words, those variables would be free, as in ‚Äò\(x = a^2 + b^2\)‚Äô and ‚Äò\(n\) is divisible by \(k\)‚Äô.

Expressions that refer to *how many* elements of a set make a statement true, such as ‚Äòthere exists‚Äô and ‚Äòevery‚Äô, turn free variables into bound variables. We represent such expressions using symbols called *quantifiers*, which are the central objects of study of this section.
</markdown><markdown>
The two main quantifiers used throughout mathematics are the universal quantifier \(\forall\) and the existential quantifier \(\exists\). We will define these quantifiers formally later in this section, but for now, the following informal definitions suffice:

- The expression ‚Äò\(\forall x \in X, \ldots\)‚Äô denotes ‚Äòfor all \(x \in X, \ldots\)‚Äô and will be defined formally in [Definition 1.2.9](#).
- The expression ‚Äò\(\exists x \in X, \ldots\)‚Äô denotes ‚Äòthere exists \(x \in X\) such that \ldots‚Äô and will be defined formally in [Definition 1.2.17](#).

Note that we always place the quantifier *before* the statement, so even though we might write or say things like ‚Äò\(n = 2k\) for some integer \(k\)‚Äô or ‚Äò\(x^2 \geq 0\) for all \(x \in \mathbb{R}\)‚Äô, we would express these statements symbolically as ‚Äò\(\exists k \in \mathbb{Z}, n = 2k\)‚Äô and ‚Äò\(\forall x \in \mathbb{R}, x^2 \geq 0\)‚Äô, respectively.

We will define a third quantifier \(\exists!\) in terms of \(\forall\) and \(\exists\) to say that there is *exactly one* element of a set making a statement true. There are plenty of other quantifiers out there, but they tend to be specific to particular fields‚Äîexamples include ‚Äòalmost everywhere‚Äô in measure theory, ‚Äòalmost surely‚Äô in probability theory, ‚Äòfor all but finitely many‚Äô in set theory and related disciplines, and ‚Äòfor fresh‚Äô in the theory of nominal sets.

Using predicates, logical formulae and quantifiers, we are able to build up more complicated expressions, called *logical formulae*. Logical formulae generalise propositional formulae ([Definition 1.1.3](#)) in by allowing (free and bound) variables and quantification to occur.

### Definition 1.2.4
A *logical formula* is an expression that is built from predicates using logical operators and quantifiers; it may have both free and bound variables. The truth value of a logical formula depends on its free variables according to the rules for logical operators and quantifiers.

Translating between plain English statements and purely symbolic logical formulae is an important skill to obtain:

- The plain English statements are easier to understand and are the kinds of things you would speak aloud or write down when discussing the mathematical ideas involved.
- The symbolic logical formulae are what provide the precision needed to guide a proof of the statement being discussed‚Äîwe will see strategies for proving statements involving quantifiers soon.

The following examples and exercise concern translating between plain English statements and purely symbolic logical formulae.
</markdown><markdown>
### Example 1.2.5

Recall that an integer \( n \) is even if and only if it is divisible by 2. According to Definition 0.12, that is to say that ‚Äò\( n \) is even‚Äô means ‚Äò\( n = 2k \) for some integer \( k \)‚Äô. Using quantifiers, we can express ‚Äò\( n \) is even‚Äô as ‚Äò\(\exists k \in \mathbb{Z}, n = 2k\)‚Äô.

The (false) proposition ‚Äòevery integer is even‚Äô can then be written symbolically as follows. First introduce a variable \( n \) to refer to an integer; to say ‚Äòevery integer is even‚Äô is to say ‚Äò\(\forall n \in \mathbb{Z}, n \) is even‚Äô, and so using the symbolic representation of ‚Äò\( n \) is even‚Äô, we can express ‚Äòevery integer is even‚Äô as \(\forall n \in \mathbb{Z}, \exists k \in \mathbb{Z}, n = 2k\).

### Exercise 1.2.6

Find logical formulae that represent each of the following English statements.

(a) There is an integer that is divisible by every integer.

(b) There is no greatest odd integer.

(c) Between any two distinct rational numbers is a third distinct rational number.

(d) If an integer has a rational square root, then that root is an integer.

### Example 1.2.7

Consider the following logical formula.

\[
\forall a \in \mathbb{R}, (a \geq 0 \Rightarrow \exists b \in \mathbb{R}, a = b^2)
\]

If we translate this expression symbol-for-symbol, what it says is:

For every real number \( a \), if \( a \) is non-negative, then there exists a real number \( b \) such that \( a = b^2 \).

Read in this way, it is not a particularly enlightening statement. However, we can distill the robotic nature of the symbol-for-symbol reading by thinking more carefully about what the statement *really* means.

Indeed, to say ‚Äò\( a = b^2 \) for some real number \( b \)‚Äô is exactly to say that \( a \) has a real square root‚Äîafter all, what is a square root of \( a \) if not a real number whose square is equal to \( a \)? This translation eliminates explicit reference to the bound variable \( b \), so that the statement now reads:

For every real number \( a \), if \( a \) is non-negative, then \( a \) has a real square root.
</markdown><markdown>
We‚Äôre getting closer. Next note that instead of the clunky expression ‚Äòfor every real number \( a \), if \( a \) is non-negative, then . . . ‚Äô, we could just say ‚Äòfor every non-negative real number \( a \), . . . ‚Äô.

> For every non-negative real number \( a \), \( a \) has a real square root.

Finally, we can eliminate the bound variable \( a \) by simply saying:

> Every non-negative real number has a real square root.

This is now a meaningful expression that is much easier to understand than the logical formula we started with.

### Exercise 1.2.8
Find statements in plain English, involving as few variables as possible, that are represented by each of the following logical formulae. (The domains of discourse of the free variables are indicated in each case.)

(a) \(\exists q \in \mathbb{Z}, a = qb\) ‚Äî free variables \(a, b \in \mathbb{Z}\)

(b) \(\exists a \in \mathbb{Z}, \exists b \in \mathbb{Z}, (b \neq 0 \land bx = a)\) ‚Äî free variable \(x \in \mathbb{R}\)

(c) \(\forall d \in \mathbb{N}, [(\exists q \in \mathbb{Z}, n = qd) \Rightarrow (d = 1 \lor d = n)]\) ‚Äî free variable \(n \in \mathbb{N}\)

(d) \(\forall a \in \mathbb{R}, [a > 0 \Rightarrow \exists b \in \mathbb{R}, (b > 0 \land a < b)]\) ‚Äî no free variables

Now that we have a better understanding of how to translate between plain English statements and logical formulae, we are ready to give a precise mathematical treatment of quantifiers. Just like with logical operators in Section 1.1, quantifiers will be defined according to **introduction rules**, which tell us how to prove a quantified formula, and **elimination rules**, which tell us how to use an assumption that involves a quantifier.

### Universal quantification (‚Äòfor all‚Äô, \(\forall\))

The universal quantifier makes precise what we mean when we say ‚Äòfor all‚Äô, or ‚Äò\( p(x) \) is always true no matter what value \( x \) takes‚Äô.
</markdown><markdown>
### Definition 1.2.9

The **universal quantifier** is the quantifier \(\forall\) (LaTeX code: `\forall`); if \(p(x)\) is a logical formula with free variable \(x\) with range \(X\), then \(\forall x \in X, p(x)\) is the logical formula defined according to the following rules:

- (\(\forall\)I) If \(p(x)\) can be derived from the assumption that \(x\) is an arbitrary element of \(X\), then \(\forall x \in X, p(x)\);
- (\(\forall\)E) If \(a \in X\) and \(\forall x \in X, p(x)\) is true, then \(p(a)\) is true.

The expression \(\forall x \in X, p(x)\) represents ‚Äòfor all \(x \in X, p(x)\)‚Äô.

\[
\begin{array}{c}
[x \in X] \\
\downarrow \\
\frac{p(x)}{\forall x \in X, p(x)} \quad \frac{\forall x \in X, p(x)}{p(a)} \quad a \in X
\end{array}
\]

### Strategy 1.2.10 (Proving universally quantified statements)

To prove a proposition of the form \(\forall x \in X, p(x)\), it suffices to prove \(p(x)\) for an arbitrary element \(x \in X\)‚Äîin other words, prove \(p(x)\) whilst assuming nothing about the variable \(x\) other than that it is an element of \(X\).

Useful phrases for introducing an arbitrary variable of a set \(X\) in a proof include ‚Äòfix \(x \in X\)‚Äô or ‚Äòlet \(x \in X\)‚Äô or ‚Äòtake \(x \in X\)‚Äô‚Äîmore on this is discussed in [Appendix A.2](#).

The proofs of the following propositions illustrate how a proof of a universally quantified statement might look.

### Proposition 1.2.11

The square of every odd integer is odd.

**Proof**

Let \(n\) be an odd integer. Then \(n = 2k + 1\) for some \(k \in \mathbb{Z}\) by the division theorem ([Theorem 0.18](#)), and so

\[
n^2 = (2k + 1)^2 = 4k^2 + 4k + 1 = 2(2k^2 + 2k) + 1
\]

Since \(2k^2 + 2k \in \mathbb{Z}\), we have that \(n^2\) is odd, as required.

Note that in the proof of [Proposition 1.2.11](#), we did not assume anything about \(n\) other than that it is an odd integer.
</markdown><markdown>
### Proposition 1.2.12

The base-10 expansion of the square of every natural number ends in one of the digits 0, 1, 4, 5, 6 or 9.

*Proof*  
Fix \( n \in \mathbb{N} \), and let

\[ n = d_r d_{r-1} \ldots d_0 \]

be its base-10 expansion. Write

\[ n = 10m + d_0 \]

where \( m \in \mathbb{N} \)‚Äîthat is, \( m \) is the natural number obtained by removing the final digit from \( n \). Then

\[ n^2 = 100m^2 + 20md_0 + d_0^2 = 10m(10m + 2d_0) + d_0^2 \]

Hence the final digit of \( n^2 \) is equal to the final digit of \( d_0^2 \). But the possible values of \( d_0^2 \) are

\[ 0 \quad 1 \quad 4 \quad 9 \quad 16 \quad 25 \quad 36 \quad 49 \quad 64 \quad 81 \]

all of which end in one of the digits 0, 1, 4, 5, 6 or 9. ‚ñ°

#### Exercise 1.2.13
Prove that every integer is rational.

#### Exercise 1.2.14
Prove that every linear polynomial over \( \mathbb{Q} \) has a rational root.

#### Exercise 1.2.15
Prove that, for all real numbers \( x \) and \( y \), if \( x \) is irrational, then \( x + y \) and \( x - y \) are not both rational.

Before advancing too much further, beware of the following common error that arises when dealing with universal quantifiers.

### Common error

Consider the following (non-)proof of the proposition \( \forall n \in \mathbb{Z}, n^2 \geq 0 \).

> Let \( n \) be an arbitrary integer, say \( n = 17 \). Then \( 17^2 = 289 \geq 0 \), so the statement is true.

The error made here is that the *writer* has picked an arbitrary value of \( n \), not the *reader*. (In fact, the above argument actually proves \( \exists n \in \mathbb{Z}, n^2 \geq 0 \).)

The proof should make no assumptions about the value of \( n \) other than that it is an integer. Here is a correct proof:
</markdown><markdown>
Let \( n \) be an arbitrary integer. Either \( n \geq 0 \) or \( n < 0 \). If \( n \geq 0 \) then \( n^2 \geq 0 \), since the product of two nonnegative numbers is nonnegative; if \( n < 0 \) then \( n^2 \geq 0 \), since the product of two negative numbers is positive.

The strategy suggested by the elimination rule for the universal quantifier is one that we use almost without thinking about it.

### Strategy 1.2.16 (Assuming universally quantified statements)
If an assumption in a proof has the form \(\forall x \in X, \, p(x)\), then we may assume that \( p(a) \) is true whenever \( a \) is an element of \( X \).

### Existential quantification (‚Äòthere exists‚Äô, \(\exists\))

#### Definition 1.2.17
The existential quantifier is the quantifier \(\exists\) (LaTeX code: `\exists`) if \( p(x) \) is a logical formula with free variable \( x \) with range \( X \), then \(\exists x \in X, \, p(x)\) is the logical formula defined according to the following rules:

- (\(\exists\)I) If \( a \in X \) and \( p(a) \) is true, then \(\exists x \in X, \, p(x)\);
- (\(\exists\)E) If \(\exists x \in X, \, p(x)\) is true, and \( q \) can be derived from the assumption that \( p(a) \) is true for some fixed \( a \in X \), then \( q \) is true.

The expression \(\exists x \in X, \, p(x)\) represents ‚Äòthere exists \( x \in X \) such that \( p(x) \)‚Äô.

\[
\begin{array}{c}
[a \in X], [p(a)] \\
\hline
\exists x \in X, \, p(x) \\
q
\end{array}
\]

### Strategy 1.2.18 (Proving existentially quantified statements)
To prove a proposition of the form \(\exists x \in X, \, p(x)\), it suffices to prove \( p(a) \) for some specific element \( a \in X \), which should be explicitly defined.

#### Example 1.2.19
We prove that there is a natural number that is a perfect square and is one more than a perfect cube. That is, we prove

\[
\exists n \in \mathbb{N}, \, \left( \exists k \in \mathbb{Z}, \, n = k^2 \right) \land \left( \exists \ell \in \mathbb{Z}, \, n = \ell^3 + 1 \right)
\]
</markdown><markdown>
So define \( n = 9 \). Then \( n = 3^2 \) and \( n = 2^3 + 1 \), so that \( n \) is a perfect square and is one more than a perfect cube, as required.

The following proposition involves an existentially quantified statement‚Äîindeed, to say that a polynomial \( f(x) \) has a real root is to say \(\exists x \in \mathbb{R}, f(x) = 0\).

### Proposition 1.2.20
Fix \( a \in \mathbb{R} \). The cubic polynomial \( x^3 + (1-a^2)x - a \) has a real root.

**Proof**  
Let \( f(x) = x^3 + (1-a^2)x - a \). Define \( x = a \); then

\[
f(x) = f(a) = a^3 + (1-a^2)a - a = a^3 + a - a^3 - a = 0
\]

Hence \( a \) is a root of \( f(x) \). Since \( a \) is real, \( f(x) \) has a real root.

The following exercises require you to prove existentially quantified statements.

### Exercise 1.2.21
Prove that there is a real number which is irrational but whose square is rational.

### Exercise 1.2.22
Prove that there is an integer which is divisible by zero.

### Example 1.2.23
Prove that, for all \( x, y \in \mathbb{Q} \), if \( x < y \) then there is some \( z \in \mathbb{Q} \) with \( x < z < y \).

The elimination rule for the existential quantifier gives rise to the following proof strategy.

### Strategy 1.2.24 (Assuming existentially quantified statements)
If an assumption in the proof has the form \(\exists x \in X, p(x)\), then we may introduce a new variable \( a \in X \) and assume that \( p(a) \) is true.

It ought to be said that when using existential elimination in a proof, the variable \( a \) used to denote a particular element of \( X \) for which \( p(a) \) is true should not already be in use earlier in the proof.

Strategy 1.2.24 is very useful in proofs of divisibility, since the expression ‚Äò\( a \) divides \( b \)‚Äô is an existentially quantified statement‚Äîthis was Exercise 1.2.8(a).

### Proposition 1.2.25
Let \( n \in \mathbb{Z} \). If \( n^3 \) is divisible by 3, then \( (n+1)^3 - 1 \) is divisible by 3.

**Proof**
</markdown><markdown>
Suppose \( n^3 \) is divisible by 3. Take \( q \in \mathbb{Z} \) such that \( n^3 = 3q \). Then

\[
\begin{align*}
(n+1)^3 - 1 &= (n^3 + 3n^2 + 3n + 1) - 1 & \text{expanding} \\
&= n^3 + 3n^2 + 3n & \text{simplifying} \\
&= 3q + 3n^2 + 3n & \text{since } n^3 = 3q \\
&= 3(q + n^2 + n) & \text{factorising}
\end{align*}
\]

Since \( q + n^2 + n \in \mathbb{Z} \), we have proved that \( (n+1)^3 - 1 \) is divisible by 3, as required. ‚ñ°

### Uniqueness

The concept of uniqueness arises whenever we want to use the word ‚Äòthe‚Äô. For example, in [Definition 0.6](#) we defined the base-\( b \) expansion of a natural number \( n \) to be the string \( d_r d_{r-1} \ldots d_1 d_0 \) satisfying some properties. The issue with the word ‚Äòthe‚Äô here is that we don‚Äôt know ahead of time whether a natural number \( n \) may have base-\( b \) expansions other than \( d_r d_{r-1} \ldots d_1 d_0 \)‚Äîthis fact actually requires proof. To prove this fact, we would need to assume that \( e_s e_{s-1} \ldots e_1 e_0 \) were another base-\( b \) expansion of \( n \), and prove that the strings \( d_r d_{r-1} \ldots d_1 d_0 \) and \( e_s e_{s-1} \ldots e_1 e_0 \) are the same‚Äîthis is done in [Theorem 7.3.51](#).

Uniqueness is typically coupled with existence, since we usually want to know if there is exactly one object satisfying a property. This motivates the definition of the unique existential quantifier, which encodes what we mean when we say ‚Äòthere is exactly one \( x \in X \) such that \( p(x) \) is true‚Äô. The ‚Äòexistence‚Äô part ensures that at least one \( x \in X \) makes \( p(x) \) true; the ‚Äòuniqueness‚Äô part ensures that \( x \) is the only element of \( X \) making \( p(x) \) true.

‚ú¶ **Definition 1.2.26**

The **unique existential quantifier** is the quantifier \(\exists!\) ([\(\LaTeX\) code: `\exists!`]) defined such that \(\exists! x \in X, p(x)\) is shorthand for

\[
(\exists x \in X, p(x)) \land (\forall a \in X, \forall b \in X, [(p(a) \land p(b)) \Rightarrow a = b])
\]

‚úê **Example 1.2.27**

Every positive real number has a unique positive square root. We can write this symbolically as

\[
\forall a \in \mathbb{R}, (a > 0 \Rightarrow \exists! b \in \mathbb{R}, (b > 0 \land b^2 = a))
\]

Reading this from left to right, this says: for every real number \( a \), if \( a \) is positive, then there exists a unique real number \( b \), which is positive and whose square is \( a \). ‚óÅ
</markdown><markdown>
### Discussion 1.2.28

Explain why [Definition 1.2.26](#) captures the notion of there being ‚Äòexactly one‚Äô element \( x \in X \) making \( p(x) \) true. Can you think of any other ways that \( \exists! x \in X, p(x) \) could be defined?

### Strategy 1.2.29 (Proving unique-existentially quantified statements)

A proof of a statement of the form \( \exists! x \in X, p(x) \), consists of two parts:

- **Existence** ‚Äî prove that \( \exists x \in X, p(x) \) is true (e.g. using [Strategy 1.2.18](#));
- **Uniqueness** ‚Äî let \( a, b \in X \), assume that \( p(a) \) and \( p(b) \) are true, and derive \( a = b \).

Alternatively, prove existence to obtain a fixed \( a \in X \) such that \( p(a) \) is true, and then prove \( \forall x \in X, [p(x) \Rightarrow x = a] \).

### Example 1.2.30

We prove [Example 1.2.27](#), namely that for each real \( a > 0 \) there is a unique \( b > 0 \) such that \( b^2 = a \). So first fix \( a > 0 \).

- **(Existence)** The real number \( \sqrt{a} \) is positive and satisfies \( (\sqrt{a})^2 = a \) by definition. Its existence will be deferred to a later time, but an informal argument for its existence could be provided using ‚Äònumber line‚Äô arguments as in Chapter 0.

- **(Uniqueness)** Let \( y, z > 0 \) be real numbers such that \( y^2 = a \) and \( z^2 = a \). Then \( y^2 = z^2 \). Rearranging and factorising yields

  \[
  (y - z)(y + z) = 0
  \]

  so either \( y - z = 0 \) or \( y + z = 0 \). If \( y + z = 0 \) then \( z = -y \), and since \( y > 0 \), this means that \( z < 0 \). But this contradicts the assumption that \( z > 0 \). As such, it must be the case that \( y - z = 0 \), and hence \( y = z \), as required.

### Exercise 1.2.31

For each of the propositions, write it out as a logical formula involving the \( \exists! \) quantifier and then prove it, using the structure of the logical formula as a guide.

(a) For each real number \( a \), the equation \( x^2 + 2ax + a^2 = 0 \) has exactly one real solution \( x \).

(b) There is a unique real number \( a \) for which the equation \( x^2 + a^2 = 0 \) has a real solution \( x \).

(c) There is a unique natural number with exactly one positive divisor.
</markdown><markdown>
The unique existential quantifier will play a large role when we study functions in [Section 3.1](#).

## Quantifier alternation

Compare the following two statements:

(i) For every door, there is a key that can unlock it.

(ii) There is a key that can unlock every door.

Letting the variables \( x \) and \( y \) refer to doors and keys, respectively, and letting \( p(x,y) \) be the statement ‚Äòdoor \( x \) can be unlocked by key \( y \)‚Äô, we can formulate these statements as:

(i) \(\forall x, \exists y, p(x,y)\)

(ii) \(\exists y, \forall x, p(x,y)\)

This is a typical ‚Äòreal-world‚Äô example of what is known as **quantifier alternation**‚Äîthe two statements differ only by the order of the front-loaded quantifiers, and yet they say very different things. Statement (i) requires every door to be unlockable, but the keys might be different for different doors; statement (ii), however, implies the existence of some kind of ‚Äòmaster key‚Äô that can unlock all the doors.

Here‚Äôs another example with a more mathematical nature:

### Exercise 1.2.32
Let \( p(x,y) \) be the statement ‚Äò\( x+y \) is even‚Äô.

- Prove that \(\forall x \in \mathbb{Z}, \exists y \in \mathbb{Z}, p(x,y)\) is true.

- Prove that \(\exists y \in \mathbb{Z}, \forall x \in \mathbb{Z}, p(x,y)\) is false.

In both of the foregoing examples, you might have noticed that the ‚Äò\(\forall \exists\)‚Äô statement says something *weaker* than the ‚Äò\(\exists \forall\)‚Äô statement‚Äîin some sense, it is easier to make a \(\forall \exists\) statement true than it is to make an \(\exists \forall\) statement true.

This idea is formalised in [Theorem 1.2.33](#) below, which despite its abstract nature, has an extremely simple proof.
</markdown><markdown>
### Theorem 1.2.33

Let \( p(x,y) \) be a logical formula with free variables \( x \in X \) and \( y \in Y \). Then

\[
\exists y \in Y, \forall x \in X, p(x,y) \implies \forall x \in X, \exists y \in Y, p(x,y)
\]

**Proof**

Suppose \(\exists y \in Y, \forall x \in X, p(x,y)\) is true. We need to prove \(\forall x \in X, \exists y \in Y, p(x,y)\), so fix \( a \in X \)‚Äîour goal is now to prove \(\exists y \in Y, p(a,y)\).

Using our assumption \(\exists y \in Y, \forall x \in X, p(x,y)\), we may choose \( b \in Y \) such that \(\forall x, p(x,b)\) is true. But then \( p(a,b) \) is true, so we have proved \(\exists y \in Y, p(a,y)\), as required. ‚ñ°

Statements of the form \(\exists y \in Y, \forall x \in X, p(x,y)\) imply some kind of *uniformity*: a value of \( y \) making \(\forall x \in X, p(x,y)\) true can be thought of as a ‚Äòone size fits all‚Äô solution to the problem of proving \( p(x,y) \) for a given \( x \in X \). Later in your studies, it is likely that you will encounter the word ‚Äòuniform‚Äô many times‚Äîit is precisely this notion of quantifier alternation that the word ‚Äòuniform‚Äô refers to.
</markdown><markdown>
## TL;DR ‚Äî summary of [Section 1.2](#)

### Variables and logical formulae

**1.2.1** A variable is *free* if a value can be substituted for it; otherwise it is *bound*.

**1.2.2** A *predicate* \( p(x, y, z, \ldots) \) represents a statement involving some free variables \( x, y, z, \ldots \) that becomes a proposition when values for the variables are substituted.

**1.2.4** A *logical formula* is an expression built using predicates, logical operators and quantifiers.

### Quantifiers

**1.2.9** The *universal quantifier* (\(\forall\)) represents ‚Äòfor all‚Äô. We prove \(\forall x \in X, p(x)\) by introducing a variable \( x \in X \) and, assuming nothing about \( x \) other than that it is an element of \( X \), deriving \( p(x) \); we can use an assumption of the form \(\forall x \in X, p(x)\) by deducing \( p(a) \) whenever we know that \( a \in X \).

**1.2.17** The *existential quantifier* (\(\exists\)) represents ‚Äòthere exists... such that...‚Äô. We prove \(\exists x \in X, p(x)\) by finding (with proof) an element \( a \in X \) for which \( p(a) \) is true; we can use an assumption of the form \(\exists x \in X, p(x)\) by introducing a variable \( a \in X \) and assuming that \( p(a) \) is true.

**1.2.26** The *unique existential quantifier* (\(\exists!\)) represents ‚Äòthere exists a unique... such that...‚Äô. We prove \(\exists! x \in X, p(x)\) in two parts: (1) Prove \(\exists x \in X, p(x)\); and (2) Let \( a, b \in X \), assume that \( p(a) \) and \( p(b) \) are true, and derive \( a = b \).

### Quantifier alternation

**1.2.33** For any logical formula \( p(x, y) \), we have that \(\exists y \in Y, \forall x \in X, p(x, y)\) implies \(\forall x \in X, \exists y \in Y, p(x, y)\), but not necessarily vice versa.
</markdown><markdown>
# Section 1.3

## Logical equivalence

We motivate the content of this section with an example.

### Example 1.3.1

Consider the following two logical formulae, where \( P \) denotes the set of all prime numbers.

1. \(\forall n \in P, (n > 2 \Rightarrow \exists k \in \mathbb{Z}, n = 2k + 1)\);

2. \(\neg \exists n \in P, (n > 2 \land \exists k \in \mathbb{Z}, n = 2k)\).

The logical formula (1) translates to ‚Äòevery prime number greater than two is odd‚Äô, and the logical formula (2) translates to ‚Äòthere does not exist an even prime number greater than two‚Äô. These statements are evidently equivalent‚Äîthey mean the same thing‚Äîbut they suggest different proof strategies:

1. Fix a prime number \( n \), assume that \( n > 2 \), and then prove that \( n = 2k + 1 \) for some \( k \in \mathbb{Z} \).

2. Assume that there is some prime number \( n \) such that \( n > 2 \) and \( n = 2k \) for some \( k \in \mathbb{Z} \), and derive a contradiction.

While statement (1) more directly translates the plain English statement ‚Äòevery prime number greater than two is odd‚Äô, it is the proof strategy suggested by (2) that is easier to use. Indeed, if \( n \) is a prime number such that \( n > 2 \) and \( n = 2k \) for some \( k \in \mathbb{Z} \), then 2 is a divisor of \( n \) other than 1 and \( n \) (since \( 1 < 2 < n \)), contradicting the assumption that \( n \) is prime.

The notion of **logical equivalence**, captures precisely the sense in which the logical formulae in (1) and (2) in Example 1.3.1 ‚Äòmean the same thing‚Äô. Being able to transform a logical formula into a different (but equivalent) form allows us to identify a wider range of feasible proof strategies.

### Definition 1.3.2

Let \( p \) and \( q \) be logical formulae. We say that \( p \) and \( q \) are **logically equivalent**, and write \( p \equiv q \) (\LaTeX{} code: `\equiv`), if \( q \) can be derived from \( p \) and \( p \) can be derived from \( q \).
</markdown><markdown>
## Logical equivalence of propositional formulae

While [Definition 1.3.2](#) defines logical equivalence between arbitrary logical formulae, we will start by focusing our attention on logical equivalence between propositional formulae, like those we saw in [Section 1.1](#).

First, let‚Äôs look at a couple of examples of what proofs of logical equivalence might look like. Be warned‚Äîthey‚Äôre not very nice to read! But there is light at the end of the tunnel. After struggling through [Examples 1.3.3](#) and [1.3.4](#) and [Exercise 1.3.5](#), we will introduce a very quick and easy tool for proving propositional formulae are logically equivalent.

### Example 1.3.3
We demonstrate that \( p \land (q \lor r) \equiv (p \land q) \lor (p \land r) \), where \( p, q \) and \( r \) are propositional variables.

- First assume that \( p \land (q \lor r) \) is true. Then \( p \) is true and \( q \lor r \) is true by definition of conjunction. By definition of disjunction, either \( q \) is true or \( r \) is true.
  - If \( q \) is true, then \( p \land q \) is true by definition of conjunction.
  - If \( r \) is true, then \( p \land r \) is true by definition of conjunction.

In both cases we have that \( (p \land q) \lor (p \land r) \) is true by definition of disjunction.

- Now assume that \( (p \land q) \lor (p \land r) \) is true. Then either \( p \land q \) is true or \( p \land r \) is true, by definition of disjunction.
  - If \( p \land q \) is true, then \( p \) is true and \( q \) is true by definition of conjunction.
  - If \( p \land r \) is true, then \( p \) is true and \( r \) is true by definition of conjunction.

In both cases we have that \( p \) is true, and that \( q \lor r \) is true by definition of disjunction. Hence \( p \land (q \lor r) \) is true by definition of conjunction.

Since we can derive \( (p \land q) \lor (p \land r) \) from \( p \land (q \lor r) \) and vice versa, it follows that

\[
p \land (q \lor r) \equiv (p \land q) \lor (p \land r)
\]

as required.

### Example 1.3.4
We prove that \( p \Rightarrow q \equiv (\neg p) \lor q \), where \( p, q \) and \( r \) are propositional variables.

- First assume that \( p \Rightarrow q \) is true. By the law of excluded middle ([Axiom 1.1.44](#)), either \( p \) is true or \( \neg p \) is true‚Äîwe derive \( (\neg p) \lor q \) in each case.
  - If \( p \) is true, then since \( p \Rightarrow q \) is true, it follows from (\(\Rightarrow\)E) that \( q \) is true, and so \( (\neg p) \lor q \) is true by (‚à®I2);
  - If \( \neg p \) is true, then \( (\neg p) \lor q \) is true by (‚à®I1).
</markdown><markdown>
In both cases, we see that \((\neg p) \lor q\) is true.

- Now assume that \((\neg p) \lor q\) is true. To prove that \(p \Rightarrow q\) is true, it suffices by \((\Rightarrow I)\) to assume that \(p\) is true and derive \(q\). So assume \(p\) is true. Since \((\neg p) \lor q\) is true, we have that either \(\neg p\) is true or \(q\) is true.
  - If \(\neg p\) is true, then we obtain a contradiction from the assumption that \(p\) is true, and so \(q\) is true by the principle of explosion (Axiom 1.1.49).
  - If \(q\) is true... well, then \(q\) is true‚Äîthere is nothing more to prove!

In both cases we have that \(q\) is true. Hence \(p \Rightarrow q\) is true.

We have derived \((\neg p) \lor q\) from \(p \Rightarrow q\) and vice versa, and so the two formulae are logically equivalent.

**Exercise 1.3.5**  
Let \(p, q\) and \(r\) be propositional variables. Prove that the propositional formula \((p \lor q) \Rightarrow r\) is logically equivalent to \((p \Rightarrow r) \land (q \Rightarrow r)\).

Working through the derivations each time we want to prove logical equivalence can become cumbersome even for small examples like Examples 1.3.3 and 1.3.4 and Exercise 1.3.5.

The following theorem reduces the problem of proving logical equivalence between propositional formulae to the purely algorithmic task of checking when the formulae are true and when they are false in a (relatively) small list of cases. We will streamline this process even further using truth tables (Definition 1.3.7).

**Theorem 1.3.6**  
Two propositional formulae are logically equivalent if and only if their truth values are the same under any assignment of truth values to their constituent propositional variables.

**Idea of proof**  
A formal proof of this fact is slightly beyond our reach at this point, although we will be able to prove it formally by structural induction, introduced in Section 12.2.

The idea of the proof is that, since propositional formulae are built up from simpler propositional formulae using logical operators, the truth value of a more complex propositional formula is determined by the truth values of its simpler subformulae. If we keep ‚Äòchasing‚Äô these subformulae, we end up with just propositional variables.

For example, the truth value of \((p \Rightarrow r) \land (q \Rightarrow r)\) is determined by the truth values of \(p \Rightarrow r\) and \(q \Rightarrow r\) according to the rules for the conjunction operator \(\land\). In turn, the truth value of \(p \Rightarrow r\) is determined by the truth values of \(p\) and \(r\) according to the implication.
</markdown><markdown>
The truth value of \( q \Rightarrow r \) is determined by the truth values of \( q \) and \( r \) according to the implication operator again. It follows that the truth value of the whole propositional formula \( (p \Rightarrow r) \land (q \Rightarrow r) \) is determined by the truth values of \( p, q, r \) according to the rules for \(\land\) and \(\Rightarrow\).

If some assignment of truth values to propositional variables makes one propositional formula true but another false, then it must be impossible to derive one from the other‚Äîotherwise we‚Äôd obtain a contradiction. Hence both propositional formulae must have the same truth values no matter what assignment of truth values is given to their constituent propositional variables.

We now develop a systematic way of checking the truth values of a propositional formula under each assignment of truth values to its constituent propositional variables.

‚ú¶ **Definition 1.3.7**  
The **truth table** of a propositional formula is the table with one row for each possible assignment of truth values to its constituent propositional variables, and one column for each subformula (including the propositional variables and the propositional formula itself). The entries of the truth table are the truth values of the subformulae.

‚úê **Example 1.3.8**  
The following are the truth tables for \(\neg p\), \(p \land q\), \(p \lor q\) and \(p \Rightarrow q\).

\[
\begin{array}{cccc|ccc|ccc}
p & \neg p & p & q & p \land q & p & q & p \lor q & p & q & p \Rightarrow q \\
\hline
\checkmark & \times & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\times & \checkmark & \checkmark & \times & \times & \checkmark & \times & \checkmark & \checkmark & \times & \times \\
\checkmark & \times & \times & \checkmark & \times & \times & \checkmark & \checkmark & \times & \checkmark & \checkmark \\
\times & \checkmark & \times & \times & \times & \times & \times & \times & \times & \times & \checkmark \\
\end{array}
\]

In Example 1.3.8 we have used the symbol \(\checkmark\) (LaTeX code: \texttt{\textbackslash checkmark}) to mean ‚Äòtrue‚Äô and \(\times\) (LaTeX code: \texttt{\textbackslash times}) to mean ‚Äòfalse‚Äô. Some authors adopt other conventions, such as \(T, F\) or \(\top, \bot\) (LaTeX code: \texttt{\textbackslash top, \textbackslash bot}) or 1, 0 or 0, 1‚Äîthe possibilities are endless!

‚úê **Exercise 1.3.9**  
Use the definitions of \(\land\), \(\lor\) and \(\Rightarrow\) to justify the truth tables in Example 1.3.8.

The next example shows how the truth tables for the individual logical operators (as in Example 1.3.8) may be combined to form a truth table for a more complicated propositional formula that involves three propositional variables.
</markdown><markdown>
### Example 1.3.10

The following is the truth table for \((p \land q) \lor (p \land r)\).

\[
\begin{array}{cccccc}
p & q & r & p \land q & p \land r & (p \land q) \lor (p \land r) \\
\hline
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\checkmark & \checkmark & \times & \checkmark & \times & \checkmark \\
\checkmark & \times & \checkmark & \times & \checkmark & \checkmark \\
\checkmark & \times & \times & \times & \times & \times \\
\times & \checkmark & \checkmark & \times & \times & \times \\
\times & \checkmark & \times & \times & \times & \times \\
\times & \times & \checkmark & \times & \times & \times \\
\times & \times & \times & \times & \times & \times \\
\end{array}
\]

- **Propositional variables**: \(p, q, r\)
- **Intermediate subformulae**: \(p \land q, p \land r\)
- **Main formula**: \((p \land q) \lor (p \land r)\)

Some comments about the construction of this truth table are pertinent:

- The propositional variables appear first. Since there are three of them, there are \(2^3 = 8\) rows. The column for \(p\) contains four \(\checkmark\)s followed by four \(\times\)s; the column for \(q\) contains two \(\checkmark\)s, two \(\times\)s, and then repeats; and the column for \(r\) contains one \(\checkmark\), one \(\times\), and then repeats.

- The next group of columns are the next-most complicated subformulae. Each is constructed by looking at the relevant columns further to the left and comparing with the truth table for conjunction.

- The final column is the main formula itself, which again is constructed by looking at the relevant columns further to the left and comparing with the truth table for disjunction.

Our choices of where to put the vertical bars and what order to put the rows in were not the only choices that could have been made, but when constructing truth tables for more complex logical formulae, it is useful to develop a system and stick to it.

Returning to Theorem 1.3.6, we obtain the following strategy for proving that two propositional formulae are logically equivalent.

### Strategy 1.3.11 (Logical equivalence using truth tables)

In order to prove that propositional formulae are logically equivalent, it suffices to show that they have identical columns in a truth table.

### Example 1.3.12

In Example 1.3.3 we proved that \(p \land (q \lor r) \equiv (p \land q) \lor (p \land r)\). We prove this again using truth tables. First we construct the truth table for \(p \land (q \lor r)\):
</markdown><markdown>
| \( p \) | \( q \) | \( r \) | \( q \lor r \) | \( p \land (q \lor r) \) |
|---------|---------|---------|---------|------------------|
| ‚úì       | ‚úì       | ‚úì       | ‚úì       | ‚úì                |
| ‚úì       | ‚úì       | ‚úó       | ‚úì       | ‚úì                |
| ‚úì       | ‚úó       | ‚úì       | ‚úì       | ‚úì                |
| ‚úì       | ‚úó       | ‚úó       | ‚úó       | ‚úó                |
| ‚úó       | ‚úì       | ‚úì       | ‚úì       | ‚úó                |
| ‚úó       | ‚úì       | ‚úó       | ‚úì       | ‚úó                |
| ‚úó       | ‚úó       | ‚úì       | ‚úì       | ‚úó                |
| ‚úó       | ‚úó       | ‚úó       | ‚úó       | ‚úó                |

Note that the column for \( p \land (q \lor r) \) is identical to that of \( (p \land q) \lor (p \land r) \) in Example 1.3.10. Hence the two formulae are logically equivalent.

To avoid having to write out two truth tables, it can be helpful to combine them into one. For example, the following truth table exhibits that \( p \land (q \lor r) \) is logically equivalent to \( (p \land q) \lor (p \land r) \):

| \( p \) | \( q \) | \( r \) | \( q \lor r \) | \( p \land (q \lor r) \) | \( p \land q \) | \( p \land r \) | \( (p \land q) \lor (p \land r) \) |
|---------|---------|---------|---------|------------------|---------|---------|--------------------------|
| ‚úì       | ‚úì       | ‚úì       | ‚úì       | ‚úì                | ‚úì       | ‚úì       | ‚úì                        |
| ‚úì       | ‚úì       | ‚úó       | ‚úì       | ‚úì                | ‚úì       | ‚úó       | ‚úì                        |
| ‚úì       | ‚úó       | ‚úì       | ‚úì       | ‚úì                | ‚úó       | ‚úì       | ‚úì                        |
| ‚úì       | ‚úó       | ‚úó       | ‚úó       | ‚úó                | ‚úó       | ‚úó       | ‚úó                        |
| ‚úó       | ‚úì       | ‚úì       | ‚úì       | ‚úó                | ‚úó       | ‚úó       | ‚úó                        |
| ‚úó       | ‚úì       | ‚úó       | ‚úì       | ‚úó                | ‚úó       | ‚úó       | ‚úó                        |
| ‚úó       | ‚úó       | ‚úì       | ‚úì       | ‚úó                | ‚úó       | ‚úó       | ‚úó                        |
| ‚úó       | ‚úó       | ‚úó       | ‚úó       | ‚úó                | ‚úó       | ‚úó       | ‚úó                        |

In the following exercises, we use truth tables to repeat the proofs of logical equivalence from Example 1.3.4 and Exercise 1.3.5.

### Exercise 1.3.13
Use a truth table to prove that \( p \Rightarrow q \equiv (\neg p) \lor q \).

### Exercise 1.3.14
Let \( p, q \) and \( r \) be propositional variables. Use a truth table to prove that the propositional formula \( (p \lor q) \Rightarrow r \) is logically equivalent to \( (p \Rightarrow r) \land (q \Rightarrow r) \).

## Some proof strategies

We are now in good shape to use logical equivalence to derive some more sophisticated proof strategies.
</markdown><markdown>
### Theorem 1.3.15 (Law of double negation)

Let \( p \) be a propositional variable. Then \( p \equiv \neg\neg p \).

**Proof**

The proof is almost trivialised using truth tables. Indeed, consider the following truth table.

\[
\begin{array}{c|c|c}
p & \neg p & \neg\neg p \\
\hline
\checkmark & \times & \checkmark \\
\times & \checkmark & \times \\
\end{array}
\]

The columns for \( p \) and \( \neg\neg p \) are identical, and so \( p \equiv \neg\neg p \).

The law of double negation is important because it suggests a second way that we can prove statements by contradiction. Indeed, it says that proving a proposition \( p \) is equivalent to proving \( \neg\neg p \), which amounts to assuming \( \neg p \) and deriving a contradiction.

### Strategy 1.3.16 (Proof by contradiction (indirect version))

In order to prove a proposition \( p \) is true, it suffices to assume that \( p \) is false and derive a contradiction.

At first sight, Strategy 1.3.16 looks very similar to Strategy 1.1.38, which we also termed proof by contradiction. But there is an important difference between the two:

- **Strategy 1.1.38** says that to prove that a proposition is false, it suffices to assume that it is true and derive a contradiction;
- **Strategy 1.3.16** says that to prove that a proposition is true, it suffices to assume that it is false and derive a contradiction.

The former is a direct proof technique, since it arises directly from the definition of the negation operator; the latter is an indirect proof technique, since it arises from a logical equivalence, namely the law of double negation.

### Example 1.3.17

We prove that if \( a, b \) and \( c \) are non-negative real numbers satisfying \( a^2 + b^2 = c^2 \), then \( a + b \geq c \).

Indeed, let \( a, b, c \in \mathbb{R} \) with \( a, b, c \geq 0 \), and assume that \( a^2 + b^2 = c^2 \). Towards a contradiction, assume that it is not the case that \( a + b \geq c \). Then we must have \( a + b < c \). But then

\[
(a + b)^2 = (a + b)(a + b) < (a + b)c < c \cdot c = c^2
\]
</markdown><markdown>
and so

\[ c^2 > (a+b)^2 = a^2 + 2ab + b^2 = c^2 + 2ab \geq c^2 \]

This implies that \( c^2 > c^2 \), which is a contradiction. So it must be the case that \( a+b \geq c \), as required.

The next proof strategy we derive concerns proving implications.

### Definition 1.3.18
The **contrapositive** of a proposition of the form \( p \Rightarrow q \) is the proposition \(\neg q \Rightarrow \neg p\).

### Theorem 1.3.19 (Law of contraposition)
Let \( p \) and \( q \) be propositional variables. Then \( p \Rightarrow q \equiv (\neg q) \Rightarrow (\neg p) \).

**Proof**  
We build the truth tables for \( p \Rightarrow q \) and \((\neg q) \Rightarrow (\neg p)\).

\[
\begin{array}{cccccc}
p & q & p \Rightarrow q & \neg q & \neg p & (\neg q) \Rightarrow (\neg p) \\
\hline
\text{T} & \text{T} & \text{T} & \text{F} & \text{F} & \text{T} \\
\text{T} & \text{F} & \text{F} & \text{T} & \text{F} & \text{F} \\
\text{F} & \text{T} & \text{T} & \text{F} & \text{T} & \text{T} \\
\text{F} & \text{F} & \text{T} & \text{T} & \text{T} & \text{T} \\
\end{array}
\]

The columns for \( p \Rightarrow q \) and \((\neg q) \Rightarrow (\neg p)\) are identical, so they are logically equivalent.

Theorem 1.3.19 suggests the following proof strategy.

### Strategy 1.3.20 (Proof by contraposition)
In order to prove a proposition of the form \( p \Rightarrow q \), it suffices to assume that \( q \) is false and derive that \( p \) is false.

### Example 1.3.21
Fix two natural numbers \( m \) and \( n \). We will prove that if \( mn > 64 \), then either \( m > 8 \) or \( n > 8 \).

By contraposition, it suffices to assume that it is not the case that \( m > 8 \) or \( n > 8 \), and derive that it is not the case that \( mn > 64 \).

So assume that neither \( m > 8 \) nor \( n > 8 \). Then \( m \leq 8 \) and \( n \leq 8 \), so that \( mn \leq 64 \), as required.
</markdown><markdown>
### Exercise 1.3.22

Use the law of contraposition to prove that \( p \leftrightarrow q \equiv (p \Rightarrow q) \land ((\neg p) \Rightarrow (\neg q)) \), and use the proof technique that this equivalence suggests to prove that an integer is even if and only if its square is even.

It feels good to invoke impressive-sounding results like proof by contraposition, but in practice, the logical equivalence between any two different propositional formulae suggests a new proof technique, and not all of these techniques have names. And indeed, the proof strategy in the following exercise, while useful, has no slick-sounding name‚Äîat least, not one that would be widely understood.

### Exercise 1.3.23

Prove that \( p \lor q \equiv (\neg p) \Rightarrow q \). Use this logical equivalence to suggest a new strategy for proving propositions of the form \( p \lor q \), and use this strategy to prove that if two integers sum to an even number, then either both integers are even or both are odd.

### Negation

In pure mathematics it is common to ask whether or not a certain property holds of a mathematical object. For example, in [Section 9.2](#), we will look at convergence of sequences of real numbers: to say that a sequence \( x_0, x_1, x_2, \ldots \) of real numbers converges ([Definition 9.2.15](#)) is to say

\[
\exists a \in \mathbb{R}, \forall \varepsilon > 0 \Rightarrow \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n \geq N \Rightarrow |x_n - a| < \varepsilon]
\]

This is already a relatively complicated logical formula. But what if we wanted to prove that a sequence does not converge? Simply assuming the logical formula above and deriving a contradiction might work sometimes, but it is not particularly enlightening.

Our next goal is to develop a systematic method for negating complicated logical formulae. With this done, we will be able to negate the logical formula expressing ‚Äòthe sequence \( x_0, x_1, x_2, \ldots \) converges‚Äô as follows

\[
\forall a \in \mathbb{R}, \exists \varepsilon > 0 \land \forall N \in \mathbb{N}, \exists n \in \mathbb{N}, [n \geq N \land |x_n - a| \geq \varepsilon]
\]

Granted, this is still a complicated expression, but when broken down element by element, it provides useful information about how it may be proved.

The rules for negating conjunctions and disjunctions are instances of de Morgan‚Äôs laws, which exhibit a kind of duality between \(\land\) and \(\lor\).
</markdown><markdown>
### Theorem 1.3.24 (de Morgan‚Äôs laws for logical operators)

Let \( p \) and \( q \) be logical formulae. Then:

(a) \(\neg(p \land q) \equiv (\neg p) \lor (\neg q)\); and

(b) \(\neg(p \lor q) \equiv (\neg p) \land (\neg q)\).

**Proof of (a)**  
Consider the following truth table.

\[
\begin{array}{c|c|c|c|c|c|c}
p & q & p \land q & \neg(p \land q) & \neg p & \neg q & (\neg p) \lor (\neg q) \\
\hline
\checkmark & \checkmark & \checkmark & \times & \times & \times & \times \\
\checkmark & \times & \times & \checkmark & \times & \checkmark & \checkmark \\
\times & \checkmark & \times & \checkmark & \checkmark & \times & \checkmark \\
\times & \times & \times & \checkmark & \checkmark & \checkmark & \checkmark \\
\end{array}
\]

The columns for \(\neg(p \land q)\) and \((\neg p) \lor (\neg q)\) are identical, so they are logically equivalent.

### Exercise 1.3.25

Prove Theorem 1.3.24(b) thrice: once using the definition of logical equivalence directly (like we did in Examples 1.3.3 and 1.3.4 and Exercise 1.3.5), once using a truth table, and once using part (a) together with the law of double negation.

### Example 1.3.26

We often use de Morgan‚Äôs laws for logical operators without thinking about it. For example to say that ‚Äòneither 3 nor 7 is even‚Äô is equivalent to saying ‚Äò3 is odd and 7 is odd‚Äô. The former statement translates to

\[
\neg[(3 \text{ is even}) \lor (7 \text{ is even})]
\]

while the second statement translates to

\[
[\neg(3 \text{ is even})] \land [\neg(7 \text{ is even})]
\]

### Exercise 1.3.27

Prove that \(\neg(p \Rightarrow q) \equiv p \land (\neg q)\) twice, once using a truth table, and once using Exercise 1.3.13 together with de Morgan‚Äôs laws and the law of double negation.

De Morgan‚Äôs laws for logical operators generalise to statements about quantifiers, expressing a similar duality between \(\forall\) and \(\exists\) as we have between \(\land\) and \(\lor\).
</markdown><markdown>
### Theorem 1.3.28 (de Morgan‚Äôs laws for quantifiers)

Let \( p(x) \) be a logical formula with free variable \( x \) ranging over a set \( X \). Then:

(a) \(\neg \forall x \in X, \, p(x) \equiv \exists x \in X, \, \neg p(x)\); and

(b) \(\neg \exists x \in X, \, p(x) \equiv \forall x \in X, \, \neg p(x)\).

**Proof**

Unfortunately, since these logical formulae involve quantifiers, we do not have truth tables at our disposal, so we must assume each formula and derive the other.

We start by proving the equivalence in part (b), and then we derive (a) as a consequence.

- Assume \(\neg \exists x \in X, \, p(x)\). To prove \(\forall x \in X, \, \neg p(x)\), fix some \( x \in X \). If \( p(x) \) were true, then we‚Äôd have \(\exists x \in X, \, p(x)\), which contradicts our main assumption; so we have \(\neg p(x)\). But then \(\forall x \in X, \, \neg p(x)\) is true.

- Assume \(\forall x \in X, \, \neg p(x)\). For the sake of contradiction, assume \(\exists x \in X, \, p(x)\) were true. Then we obtain some \( a \in X \) for which \( p(a) \) is true. But \(\neg p(a)\) is true by the assumption that \(\forall x \in X, \, \neg p(a)\), so we obtain a contradiction. Hence \(\neg \exists x \in X, \, p(x)\) is true.

This proves that \(\neg \exists x \in X, \, p(x) \equiv \forall x \in X, \, \neg p(x)\).

Now (a) follows from (b) using the law of double negation ([Theorem 1.3.15](#)):

\[
\exists x \in X, \, \neg p(x) \equiv \neg \neg \exists x \in X, \, \neg p(x) \overset{(b)}{\equiv} \neg \forall x \in X, \, \neg \neg p(x) \equiv \neg \forall x \in X, \, p(x)
\]

as required.

The proof strategy suggested by the logical equivalence in Theorem 1.3.28(b) is so important that it has its own name.

### Strategy 1.3.29 (Proof by counterexample)

To prove that a proposition of the form \(\forall x \in X, \, p(x)\) is false, it suffices to find a single element \( a \in X \) such that \( p(a) \) is false. The element \( a \) is called a **counterexample** to the proposition \(\forall x \in X, \, p(x)\).

### Example 1.3.30

We prove by counterexample that not every integer is divisible by a prime number. Indeed, let \( x = 1 \). The only integral factors of 1 are 1 and \(-1\), neither of which are prime, so that 1 is not divisible by any primes.

### Exercise 1.3.31

Prove by counterexample that not every rational number can be expressed as \(\frac{a}{b}\) where \( a \in \mathbb{Z} \) is even and \( b \in \mathbb{Z} \) is odd.
</markdown><markdown>
We have now seen how to negate the logical operators ¬¨, ‚àß, ‚à® and ‚áí, as well as the quantifiers ‚àÄ and ‚àÉ.

### Definition 1.3.32
A logical formula is **maximally negated** if the only instances of the negation operator ¬¨ appear immediately before a predicate (or other proposition not involving logical operators or quantifiers).

### Example 1.3.33
The following propositional formula is maximally negated:

\[
[p \land (q \Rightarrow (\neg r))] \Leftrightarrow (s \land (\neg t))
\]

Indeed, all instances of ¬¨ appear immediately before propositional variables.

However the following propositional formula is **not** maximally negated:

\[
(\neg \neg q) \Rightarrow q
\]

Here the subformula ¬¨¬¨q contains a negation operator immediately before another negation operator (¬¨¬¨q). However by the law of double negation, this is equivalent to \( q \Rightarrow q \), which is maximally negated trivially since there are no negation operators to speak of.

### Exercise 1.3.34
Determine which of the following logical formulae are maximally negated.

(a) \(\forall x \in X, (\neg p(x)) \Rightarrow \forall y \in X, \neg (r(x,y) \land s(x,y))\);

(b) \(\forall x \in X, (\neg p(x)) \Rightarrow \exists y \in X, (\neg r(x,y)) \lor (\neg s(x,y))\);

(c) \(\forall x \in \mathbb{R}, [x > 1 \Rightarrow (\exists y \in \mathbb{R}, [x < y \land \neg (x^2 \leq y)])]\);

(d) \(\neg \exists x \in \mathbb{R}, [x > 1 \land (\forall y \in \mathbb{R}, [x < y \Rightarrow x^2 \leq y])]\).

The following theorem allows us to replace logical formulae by maximally negated ones, which in turn suggests proof strategies that we can use for proving that complicated-looking propositions are **false**.

### Theorem 1.3.35
Every logical formula (built using only the logical operators and quantifiers we have seen so far) is logically equivalent to a maximally negated logical formula.
</markdown><markdown>
## Section 1.3. Logical equivalence

### Idea of proof

Much like [Theorem 1.3.6](#), a precise proof of [Theorem 1.3.35](#) requires some form of induction argument, so instead we will give an idea of the proof.

Every logical formula we have seen so far is built from predicates using the logical operators \(\land, \lor, \Rightarrow\) and \(\lnot\) and the quantifiers \(\forall\) and \(\exists\)‚Äîindeed, the logical operator \(\Leftrightarrow\) was defined in terms of \(\land\) and \(\Rightarrow\), and the quantifier \(\exists\) was defined in terms of the quantifiers \(\forall\) and \(\land\) and the logical operators \(\land\) and \(\Rightarrow\).

But the results in this section allow us to push negations ‚Äòinside‚Äô each of these logical operators and quantifiers, as summarised in the following table.

| Negation outside | Negation inside | Proof              |
|------------------|-----------------|--------------------|
| \(\lnot(p \land q)\) | \((\lnot p) \lor (\lnot q)\) | Theorem 1.3.24(a) |
| \(\lnot(p \lor q)\)  | \((\lnot p) \land (\lnot q)\) | Theorem 1.3.24(b) |
| \(\lnot(p \Rightarrow q)\) | \(p \land (\lnot q)\) | Exercise 1.3.27   |
| \(\lnot(\lnot p)\) | \(p\) | Theorem 1.3.15       |
| \(\lnot \forall x \in X, p(x)\) | \(\exists x \in X, \lnot p(x)\) | Theorem 1.3.28(a) |
| \(\lnot \exists x \in X, p(x)\) | \(\forall x \in X, \lnot p(x)\) | Theorem 1.3.28(b) |

Repeatedly applying these rules to a logical formula eventually yields a logically equivalent, maximally negated logical formula.

### Example 1.3.36

Recall the logical formula from page 71 expressing the assertion that a sequence \(x_0, x_1, x_2, \ldots\) of real numbers converges:

\[
\exists a \in \mathbb{R}, \forall \varepsilon \in \mathbb{R}, (\varepsilon > 0 \Rightarrow \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n \geq N \Rightarrow |x_n - a| < \varepsilon])
\]

We will maximally negate this to obtain a logical formula expressing the assertion that the sequence does not converge.

Let‚Äôs start at the beginning. The negation of the formula we started with is:

\[
\lnot \exists a \in \mathbb{R}, \forall \varepsilon \in \mathbb{R}, (\varepsilon > 0 \Rightarrow \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n \geq N \Rightarrow |x_n - a| < \varepsilon])
\]

The key to maximally negating a logical formula is to ignore information that is not immediately relevant. Here, the expression that we are negating takes the form \(\lnot \exists a \in \mathbb{R}, (\text{stuff})\). It doesn‚Äôt matter what the ‚Äòstuff‚Äô is just yet; all that matters is that we are negating an existentially quantified statement, and so de Morgan‚Äôs laws for quantifiers tells us that this is logically equivalent to \(\forall a \in \mathbb{R}, \lnot(\text{stuff})\). We apply this rule and just re-write the ‚Äòstuff‚Äô, to obtain:

\[
\forall a \in \mathbb{R}, \lnot \forall \varepsilon \in \mathbb{R}, (\varepsilon > 0 \Rightarrow \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n \geq N \Rightarrow |x_n - a| < \varepsilon])
\]
</markdown><markdown>
Now we are negating a universally quantified statement, ¬¨‚àÄùëé ‚àà ‚Ñù, (stuff) which, by de Morgan‚Äôs laws for quantifiers, is equivalent to ‚àÉùëé ‚àà ‚Ñù, ¬¨(stuff):

\[
\forall a \in \mathbb{R}, \exists \varepsilon > 0, \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n > N \implies |x_n - a| < \varepsilon]
\]

At this point, the statement being negated is of the form (stuff) ‚áí (junk), which by Exercise 1.3.27 negates to (stuff) ‚àß ¬¨(junk). Here, ‚Äòstuff‚Äô is ùúñ > 0 and ‚Äòjunk‚Äô is ‚àÉùëÅ ‚àà ‚Ñï, ‚àÄùëõ ‚àà ‚Ñï, [ùëõ > ùëÅ ‚áí |ùë•‚Çô ‚àí ùëé| < ùúñ]. So performing this negation yields:

\[
\forall a \in \mathbb{R}, \exists \varepsilon \in \mathbb{R}, (\varepsilon > 0 \land \neg \exists N \in \mathbb{N}, \forall n \in \mathbb{N}, [n > N \implies |x_n - a| < \varepsilon])
\]

Now we are negating an existentially quantified formula again, so using de Morgan‚Äôs laws for quantifiers gives:

\[
\forall a \in \mathbb{R}, \exists \varepsilon \in \mathbb{R}, (\varepsilon > 0 \land \forall N \in \mathbb{N}, \neg \forall n \in \mathbb{N}, [n > N \implies |x_n - a| < \varepsilon])
\]

The formula being negated here is universally quantified, so using de Morgan‚Äôs laws for quantifiers again gives:

\[
\forall a \in \mathbb{R}, \exists \varepsilon \in \mathbb{R}, (\varepsilon > 0 \land \forall N \in \mathbb{N}, \exists n \in \mathbb{N}, \neg [n > N \implies |x_n - a| < \varepsilon])
\]

We‚Äôre almost there! The statement being negated here is an implication, so applying the rule ¬¨(ùëù ‚áí ùëû) ‚â° ùëù ‚àß (¬¨ùëû) again yields:

\[
\forall a \in \mathbb{R}, \exists \varepsilon \in \mathbb{R}, (\varepsilon > 0 \land \forall N \in \mathbb{N}, \exists n \in \mathbb{N}, [n > N \land \neg (|x_n - a| < \varepsilon)])
\]

At this point, strictly speaking, the formula is maximally negated, since the statement being negated does not involve any other logical operators or quantifiers. However, since ¬¨(|ùë•‚Çô ‚àí ùëé| < ùúñ) is equivalent to |ùë•‚Çô ‚àí ùëé| ‚â• ùúñ, we can go one step further to obtain:

\[
\forall a \in \mathbb{R}, \exists \varepsilon \in \mathbb{R}, (\varepsilon > 0 \land \forall N \in \mathbb{N}, \exists n \in \mathbb{N}, [n > N \land |x_n - a| \geq \varepsilon])
\]

This is as negated as we could ever dream of, and so we stop here.

### Exercise 1.3.37
Find a maximally negated propositional formula that is logically equivalent to ¬¨(ùëù ‚áî ùëû).

### Exercise 1.3.38
Maximally negate the following logical formula, then prove that it is true or prove that it is false.

\[
\exists x \in \mathbb{R}, [x > 1 \land (\forall y \in \mathbb{R}, [x < y \implies x^2 \leq y])]
\]
</markdown><markdown>
## Tautologies

The final concept that we introduce in this chapter is that of a **tautology**, which can be thought of as the opposite of a contradiction. The word ‚Äòtautology‚Äô has other implications when used colloquially, but in the context of symbolic logic it has a precise definition.

### Definition 1.3.39
A **tautology** is a proposition or logical formula that is true, no matter how truth values are assigned to its component propositional variables and predicates.

The reason we are interested in tautologies is that tautologies can be used as assumptions at any point in a proof, for any reason.

### Strategy 1.3.40 (Assuming tautologies)
Let \( p \) be a proposition. Any tautology may be assumed in any proof of \( p \).

### Example 1.3.41
The law of excluded middle (Axiom 1.1.44) says precisely that \( p \lor (\neg p) \) is a tautology. This means that when proving any result, we may split into cases based on whether a proposition is true or false, just as we did in Proposition 1.1.46.

### Example 1.3.42
The formula \( p \Rightarrow (q \Rightarrow p) \) is a tautology.

A direct proof of this fact is as follows. In order to prove \( p \Rightarrow (q \Rightarrow p) \) is true, it suffices to assume \( p \) and derive \( q \Rightarrow p \). So assume \( p \). Now in order to prove \( q \Rightarrow p \), it suffices to assume \( q \) and derive \( p \). So assume \( q \). But we‚Äôre already assuming that \( p \) is true! So \( q \Rightarrow p \) is true, and hence \( p \Rightarrow (q \Rightarrow p) \) is true.

A proof using truth tables is as follows:

\[
\begin{array}{c|c|c|c}
p & q & q \Rightarrow p & p \Rightarrow (q \Rightarrow p) \\
\hline
\text{T} & \text{T} & \text{T} & \text{T} \\
\text{T} & \text{F} & \text{T} & \text{T} \\
\text{F} & \text{T} & \text{F} & \text{T} \\
\text{F} & \text{F} & \text{T} & \text{T} \\
\end{array}
\]

We see that \( p \Rightarrow (q \Rightarrow p) \) is true regardless of the truth values of \( p \) and \( q \).

### Exercise 1.3.43
Prove that each of the following is a tautology:
</markdown><markdown>
(a) \([(p \Rightarrow q) \land (q \Rightarrow r)] \Rightarrow (p \Rightarrow r)\);

(b) \([p \Rightarrow (q \Rightarrow r)] \Rightarrow [(p \Rightarrow q) \Rightarrow (p \Rightarrow r)]\);

(c) \(\exists y \in Y, \forall x \in X, p(x,y) \Rightarrow \forall x \in X, \exists y \in Y, p(x,y)\);

(d) \([\neg (p \land q)] \Leftrightarrow [(\neg p) \lor (\neg q)]\);

(e) \((\neg \forall x \in X, p(x)) \Leftrightarrow (\exists x \in X, \neg p(x))\).

For each, try to interpret what it means, and how it might be useful in a proof.

You may have noticed parallels between de Morgan‚Äôs laws for logical operators and quantifiers, and parts (d) and (e) of Exercise 1.3.43, respectively. They almost seem to say the same thing, except that in Exercise 1.3.43 we used ‚Äò\(\Leftrightarrow\)‚Äô and in Theorems 1.3.24 and 1.3.28 we used ‚Äò\(\equiv\)‚Äô. There is an important difference, though: if \(p\) and \(q\) are logical formulae, then \(p \Rightarrow q\) is itself a logical formula, which we may study as a mathematical object in its own right. However, \(p \equiv q\) is not a logical formula: it is an assertion about logical formulae, namely that the logical formulae \(p\) and \(q\) are equivalent.

There is, nonetheless, a close relationship between \(\Leftrightarrow\) and \(\equiv\)‚Äîthis relationship is summarised in the following theorem.

### Theorem 1.3.44
Let \(p\) and \(q\) be logical formulae.

(a) \(q\) can be derived from \(p\) if and only if \(p \Rightarrow q\) is a tautology;

(b) \(p \equiv q\) if and only if \(p \Leftrightarrow q\) is a tautology.

**Proof**

For (a), note that a derivation of \(q\) from \(p\) is sufficient to establish the truth of \(p \Rightarrow q\) by the introduction rule for implication (\(\Rightarrow I\)), and so if \(q\) can be derived from \(p\), then \(p \Rightarrow q\) is a tautology. Conversely, if \(p \Rightarrow q\) is a tautology, then \(q\) can be derived from \(p\) using the elimination rule for implication (\(\Rightarrow E\)) together with the (tautological) assumption that \(p \Rightarrow q\) is true.

Now (b) follows from (a), since logical equivalence is defined in terms of derivation in each direction, and \(\Leftrightarrow\) is simply the conjunction of two implications.
</markdown><markdown>
## TL;DR ‚Äî summary of Section 1.3

### Logical equivalence

**1.3.2** Two logical formulae are *logically equivalent* (written ‚â°) if each can be derived from the other.

**1.3.7** The *truth table* of a propositional formula is a tabulation of its truth values under all assignments of truth values to its constituent propositional variables. Two propositional formulae are logically equivalent if and only if they have identical columns in a truth table.

### Some specific logical equivalences

**1.3.15** The double-negation of a proposition is equivalent to the original proposition. This gives rise to ‚Äòindirect‚Äô proof by contradiction: in order to prove that a proposition \( p \) is *true*, assume \( p \) is *false* and derive a contradiction.

**1.3.18** The *contrapositive* of an implication \( p \Rightarrow q \) is the implication \( (\neg q) \Rightarrow (\neg p) \). Every implication is equivalent to its contrapositive, so in order to prove \( p \Rightarrow q \), it suffices to assume that \( q \) is *false*, and derive that \( p \) is *false*.

**1.3.24** De Morgan‚Äôs laws for logical operators say that \( \neg (p \land q) \equiv (\neg p) \lor (\neg q) \) and \( \neg (p \lor q) \equiv (\neg p) \land (\neg q) \).

**1.3.27** The negation of \( p \Rightarrow q \) is logically equivalent to \( p \land (\neg q) \), so in order to prove that \( p \Rightarrow q \) is *false*, it suffices to prove that \( p \) is *true* but \( q \) is *false*.

**1.3.28** De Morgan‚Äôs laws for quantifiers say that \( \neg \forall x \in X, p(x) \equiv \exists x \in X, \neg p(x) \) and \( \neg \exists x \in X, p(x) \equiv \forall x \in X, \neg p(x) \). The first of these suggests the strategy of *proof by counterexample*.

### Maximal negation

**1.3.32** A logical formula is *maximally negated* if it contains no negation operators (except, perhaps, immediately before a propositional variable or predicate).

**1.3.35** Every logical formula built using the logical operators and quantifiers in this section is logically equivalent to a maximally negated one.
</markdown><markdown>
## Tautologies

1.3.39 A *tautology* is a logical formula that is true no matter what assignment of truth values is given to its constituent propositional variables or what values are substituted for its free variables. Tautologies can be invoked as assumptions at any point in a proof.

1.3.44 A proposition \( p \) can be derived from a proposition \( q \) if and only if \( p \Rightarrow q \) is a tautology; and \( p \equiv q \) if and only if \( p \leftrightarrow q \) is a tautology.
</markdown><markdown>
# Section 1.E

## Chapter 1 exercises

1.1. For fixed \( n \in \mathbb{N} \), let \( p \) represent the proposition ‚Äò\( n \) is even‚Äô, let \( q \) represent the proposition ‚Äò\( n \) is prime‚Äô and let \( r \) represent the proposition ‚Äò\( n = 2 \)‚Äô. For each of the following propositional formulae, translate it into plain English and determine whether it is true for all \( n \in \mathbb{N} \), true for some values of \( n \) and false for some values of \( n \), or false for all \( n \in \mathbb{N} \).

(a) \( (p \land q) \Rightarrow r \)

(b) \( q \land (\lnot r) \Rightarrow (\lnot p) \)

(c) \( ((\lnot p) \lor (\lnot q)) \lor (\lnot r) \)

(d) \( (p \land q) \land (\lnot r) \)

1.2. For each of the following plain English statements, translate it into a symbolic propositional formula. The propositional variables in your formulae should represent the simplest propositions that they can.

(a) Guinea pigs are quiet, but they‚Äôre loud when they‚Äôre hungry.

(b) It doesn‚Äôt matter that 2 is even, it‚Äôs still a prime number.

(c) \( \sqrt{2} \) can‚Äôt be an integer because it is an irrational number.

1.3. Let \( p \) and \( q \) be propositions, and assume that \( p \Rightarrow (\lnot q) \) is true and that \( (\lnot q) \Rightarrow p \) is false. Which of the following are true, and which are false?

(a) \( q \) being false is necessary for \( p \) to be true.

(b) \( q \) being false is sufficient for \( p \) to be true.

(c) \( p \) being true is necessary for \( q \) to be false.

(d) \( p \) being true is sufficient for \( p \) to be false.

In [Questions 1.4 to 1.7](#), use the definitions of the logical operators in Section 1.1 to describe what steps should be followed in order to prove the propositional formula in the question; the letters \( p, q, r \) and \( s \) are propositional variables.

1.4. \( (p \land q) \Rightarrow (\lnot r) \)

1.5. \( (p \lor q) \Rightarrow (r \Rightarrow s) \)

1.6. \( (p \Rightarrow q) \Leftrightarrow (\lnot p \Rightarrow \lnot q) \)
</markdown><markdown>
1.7. \((p \land \lnot q) \lor (q \land \lnot p)\)

1.8. Find a statement in plain English, involving no variables at all, that is equivalent to the logical formula \(\forall a \in \mathbb{Q}, \forall b \in \mathbb{Q}, (a < b \Rightarrow \exists c \in \mathbb{R}, [a < c < b \land \lnot (c \in \mathbb{Q})])\). Then prove this statement, using the structure of the logical formula as a guide.

1.9. Find a purely symbolic logical formula that is equivalent to the following statement, and then prove it: ‚ÄúNo matter which integer you may choose, there will be an integer greater than it.‚Äù

1.10. Let \(X\) be a set and let \(p(x)\) be a predicate. Find a logical formula representing the statement ‚Äòthere are exactly two elements \(x \in X\) such that \(p(x)\) is true‚Äô. Use the structure of this logical formula to describe how a proof should be structured, and use this structure to prove that there are exactly two real numbers \(x\) such that \(x^2 = 1\).

1.11. Prove that

\[
p \iff q \equiv (p \Rightarrow q) \land ((\lnot p) \Rightarrow (\lnot q))
\]

How might this logical equivalence help you to prove statements of the form ‚Äò\(p\) if and only if \(q\)‚Äô?

1.12. Prove using truth tables that \(p \Rightarrow q \not\equiv q \Rightarrow p\). Give an example of propositions \(p\) and \(q\) such that \(p \Rightarrow q\) is true but \(q \Rightarrow p\) is false.

In Questions 1.13 to 1.16, find a logical formula whose column in a truth table is as shown.

1.13.

\[
\begin{array}{cc|c}
p & q & \\
\hline
\checkmark & \checkmark & \times \\
\checkmark & \times & \checkmark \\
\times & \checkmark & \checkmark \\
\times & \times & \times \\
\end{array}
\]

1.14.

\[
\begin{array}{cc|c}
p & q & \\
\hline
\checkmark & \checkmark & \checkmark \\
\checkmark & \times & \times \\
\times & \checkmark & \times \\
\times & \times & \checkmark \\
\end{array}
\]

1.15.

\[
\begin{array}{ccc|c}
p & q & r & \\
\hline
\checkmark & \checkmark & \checkmark & \checkmark \\
\checkmark & \checkmark & \times & \times \\
\checkmark & \times & \checkmark & \times \\
\checkmark & \times & \times & \checkmark \\
\times & \checkmark & \checkmark & \times \\
\times & \checkmark & \times & \checkmark \\
\times & \times & \checkmark & \checkmark \\
\times & \times & \times & \checkmark \\
\end{array}
\]
</markdown><markdown>
### Section 1.E. Chapter 1 exercises

#### 1.16.

| \( p \) | \( q \) | \( r \) |
|--------|--------|--------|
| ‚úì      | ‚úì      | ‚úì      |
| ‚úì      | ‚úó      | ‚úó      |
| ‚úó      | ‚úì      | ‚úó      |
| ‚úó      | ‚úó      | ‚úó      |
| ‚úó      | ‚úì      | ‚úì      |
| ‚úì      | ‚úó      | ‚úó      |
| ‚úó      | ‚úì      | ‚úì      |
| ‚úó      | ‚úó      | ‚úó      |

#### 1.17.

A new logical operator \( \uparrow \) is defined by the following rules:

(i) If a contradiction can be derived from the assumption that \( p \) is true, then \( p \uparrow q \) is true;

(ii) If a contradiction can be derived from the assumption that \( q \) is true, then \( p \uparrow q \) is true;

(iii) If \( r \) is any proposition, and if \( p \uparrow q \), \( p \) and \( q \) are all true, then \( r \) is true.

This question explores this curious new logical operator.

(a) Prove that \( p \uparrow p \equiv \neg p \), and deduce that \( ((p \uparrow p) \uparrow (p \uparrow p)) \equiv p \).

(b) Prove that \( p \lor q \equiv (p \uparrow p) \uparrow (q \uparrow q) \) and \( p \land q \equiv (p \uparrow q) \uparrow (p \uparrow q) \).

(c) Find a propositional formula using only the logical operator \( \uparrow \) that is equivalent to \( p \Rightarrow q \).

#### 1.18.

Let \( X \) be \( \mathbb{Z} \) or \( \mathbb{Q} \), and define a logical formula \( p \) by:

\[
\forall x \in X, \exists y \in X, (x < y \land [\forall z \in X, \neg (x < z \land z < y)])
\]

Write out \( \neg p \) as a maximally negated logical formula. Prove that \( p \) is true when \( X = \mathbb{Z} \), and \( p \) is false when \( X = \mathbb{Q} \).

#### 1.19.

Use Definition 1.2.26 to write out a maximally negated logical formula that is equivalent to \( \neg \exists! x \in X, p(x) \). Describe the strategy that this equivalence suggests for proving that there is not a unique \( x \in X \) such that \( p(x) \) is true, and use this strategy to prove that, for all \( a \in \mathbb{R} \), if \( a \neq -1 \) then there is not a unique \( x \in \mathbb{R} \) such that \( x^4 - 2ax^2 + a^2 - 1 = 0 \).

#### 1.20.

Define a new quantifier \( \exists! \) such that de Morgan‚Äôs laws for quantifiers (Theorem 1.3.28) hold with \( \forall \) and \( \exists \) replaced by \( \exists! \) and \( \exists! \), respectively.

### True‚ÄìFalse questions

In [Questions 1.21 to 1.30](#), determine (with proof) whether the statement is true or false.
</markdown><markdown>
1.21. Every implication is logically equivalent to its contrapositive.

1.22. Every implication is logically equivalent to its converse.

1.23. Every propositional formula whose only logical operators are conjunctions and negations is logically equivalent to a propositional formula whose only logical operators are disjunctions and negations.

1.24. Every propositional formula whose only logical operators are conjunctions is logically equivalent to a propositional formula whose only logical operators are disjunctions.

1.25. The formulae \( p \land (q \lor r) \) and \( (p \land q) \lor r \) are logically equivalent.

1.26. The formulae \( p \lor (q \lor r) \) and \( (p \lor q) \land (p \lor r) \) are logically equivalent.

1.27. The logical formulae \( \neg \forall x \in X, \forall y \in Y, p(x,y) \) and \( \exists x \in X, \forall y \in Y, p(x,y) \) are logically equivalent.

1.28. \( \neg \forall x \geq 0, \exists y \in \mathbb{R}, y^2 = x \) is logically equivalent to \( \forall x < 0, \exists y \notin \mathbb{R}, y^2 \neq x \).

1.29. \( \neg \forall x \geq 0, \exists y \in \mathbb{R}, y^2 = x \) is logically equivalent to \( \exists x \geq 0, \forall y \in \mathbb{R}, y^2 \neq x \).

1.30. \( \neg \forall x \geq 0, \exists y \in \mathbb{R}, y^2 = x \) is logically equivalent to \( \exists x < 0, \forall y \in \mathbb{R}, y^2 = x \).

### Always‚ÄìSometimes‚ÄìNever questions

In [Questions 1.31 to 1.35](#), determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

1.31. Let \( p \) and \( q \) be propositions and assume that \( p \) is true. Then \( p \Rightarrow q \) is true.

1.32. Let \( p \) and \( q \) be propositions and assume that \( p \) is false. Then \( p \Rightarrow q \) is true.

1.33. Let \( X \) and \( Y \) be sets and let \( p(x,y) \) be a predicate with free variables \( x \in X \) and \( y \in Y \). Then the logical formulae \( \forall x \in X, \forall y \in Y, p(x,y) \) and \( \forall y \in Y, \forall x \in X, p(x,y) \) are logically equivalent.

1.34. Let \( X \) and \( Y \) be sets and let \( p(x,y) \) be a predicate with free variables \( x \in X \) and \( y \in Y \). Then the logical formulae \( \forall x \in X, \exists y \in Y, p(x,y) \) and \( \exists y \in Y, \forall x \in X, p(x,y) \) are logically equivalent.

1.35. Let \( X \) and \( Y \) be sets and let \( p(x,y) \) be a predicate with free variables \( x \in X \) and \( y \in Y \). Then the logical formulae \( \forall x \in X, \forall y \in Y, p(x,y) \) and \( \exists x \in X, \exists y \in Y, \neg p(x,y) \) are logically equivalent.
</markdown><markdown>
# Chapter 2

# Sets

We saw an informal definition of a *set* in [Chapter 0](#), but so far the only sets that we have seen are the number sets (\(\mathbb{N}\), \(\mathbb{Z}\) and so on).

In [Section 2.1](#), we will study sets in the abstract‚Äîin particular, the sets that we study are arbitrary collections of mathematical objects, not just numbers. This is essential for further study in pure mathematics, since most (if not all) areas of pure mathematics concern certain kinds of sets!

In [Section 2.2](#), we will then define some operations that allow us to form new sets out of old sets, and prove some identities of an algebraic nature that are closely related to the rules governing logical operators and quantifiers that we saw in [Chapter 1](#).
</markdown><markdown>
## Section 2.1

# Sets

We begin by redefining the notion of a **set** with a notch more precision than we provided in Chapter 0. At their core, sets seem extremely simple‚Äîsets are just collections of objects‚Äîexcept that if not kept in check, this characterisation of a set leads to logical inconsistencies, such as the infamous *Russell‚Äôs paradox*.

These logical paradoxes can be overcome by restricting ourselves to working inside a universe \(\mathcal{U}\), which we consider to be a set which is so big that it contains all of the mathematical objects that we want to talk about. This is a subtle issue, which is well beyond the scope of this section, but is discussed further in Section B.1.

### Definition 2.1.1

A **set** is a collection of **elements** from a specified **universe of discourse**. The collection of everything in the universe of discourse is called the **universal set**, denoted by \(\mathcal{U}\) (LaTeX code: `\mathcal{U}`).

The expression \(x \in X\) (LaTeX code: `\in`) denotes the statement that \(x\) is an element of \(X\); we write \(x \notin X\) (LaTeX code: `\not\in`) to mean \(\neg (x \in X)\), that is that \(x\) is not an element of \(X\).

### Example 2.1.2

In Chapter 0, we introduced five sets: the set \(\mathbb{N}\) of natural numbers, the set \(\mathbb{Z}\) of integers, the set \(\mathbb{Q}\) of rational numbers, the set \(\mathbb{R}\) of real numbers and the set \(\mathbb{C}\) of complex numbers.

### Exercise 2.1.3

Which of the following propositions are true, and which are false?

- \(\frac{1}{2} \in \mathbb{Z}\)
- \(\frac{1}{2} \in \mathbb{Q}\)
- \(Z \in \mathbb{Q}\)
- \(Z \in \mathcal{U}\)
- \(\frac{1}{2} \in \mathcal{U}\)

We will avoid referring explicitly to the universal set \(\mathcal{U}\) whenever possible, but it will always be there in the background. This is convenient because we no longer need to worry about the domain of discourse of free variables (as we did in Definition 1.2.2), so that we can abbreviate ‚Äò\(\forall x \in \mathcal{U}, p(x)\)‚Äô by ‚Äò\(\forall x, p(x)\)‚Äô, and ‚Äò\(\exists x \in \mathcal{U}, p(x)\)‚Äô by ‚Äò\(\exists x, p(x)\)‚Äô.

Note that under this convention:

- \(\forall x \in X, p(x)\) is logically equivalent to \(\forall x, (x \in X \Rightarrow p(x))\); and
- \(\exists x \in X, p(x)\) is logically equivalent to \(\exists x, (x \in X \land p(x))\).
</markdown><markdown>
## Specifying a set

One way of defining a set is simply to describe it in words, like we have done up to now. There are other, more concise ways of specifying sets, which also remove such ambiguity from the process.

**Lists.** One way is simply to provide a list of the elements of the set. To specify that the list denotes a set, we enclose the list with {curly brackets} (LaTeX code: `\{, \}`). For example, the following is a specification of a set \( X \), whose elements are the natural numbers between 0 and 5 (inclusive):

\[ 
X = \{0, 1, 2, 3, 4, 5\} 
\]

**Implied lists.** Sometimes a list might be too long to write out‚Äîmaybe even infinite‚Äîor the length of the list might depend on a variable. In these cases it will be convenient to use an implied list, in which some elements of the list are written, and the rest are left implicit by writing an ellipsis ‚Äò‚Ä¶‚Äô (LaTeX code: `\dots`). For example, the statement

\[ 
X = \{1, 4, 9, \dots, n^2\} 
\]

means that \( X \) is the set whose elements are all the square numbers from 1 to \( n^2 \), where \( n \) is some number. Implied lists can be ambiguous, since they rely on the reader‚Äôs ability to infer the pattern being followed, so use with caution!

**Set-builder notation.** In general, implied lists can be ambiguous, so in practice they are avoided unless the implied list is very simple, such as a set of consecutive numbers like \(\{3, 4, \dots, 9\}\). In fact, many sets can‚Äôt even be listed in this way.

To get around this, we can use **set-builder notation**, which is a means of specifying a set in terms of the properties its elements satisfy. Given a set \( X \), the set of elements of \( X \) satisfying some property \( p(x) \) is denoted

\[ 
\{x \in X \mid p(x)\} 
\]

The bar ‚Äò\(\mid\)‚Äô (LaTeX code: `\mid`) separates the variable name from the formula that they make true‚Äîsome authors use a colon instead (as in \(\{x \in X : p(x)\}\)).

The set \(\{x \in X \mid p(x)\}\) is read aloud as ‚Äòthe set of \( x \in X \) such that \( p(x) \)‚Äô, but beware‚Äîneither the bar ‚Äò\(\mid\)‚Äô nor the colon ‚Äò:‚Äô mean ‚Äòsuch that‚Äô in other contexts.

‚úé **Example 2.1.4**  
The set of all even integers can be written in set-builder notation as

\[ 
\{n \in \mathbb{Z} \mid n \text{ is even}\} 
\]
</markdown><markdown>
For comparison, the set of all even natural numbers can be written as

\[
\{ n \in \mathbb{N} \mid n \text{ is even} \} = \{ 0, 2, 4, 6, \ldots \}
\]

Note that \(-6\) is an element of the former set but not of the latter set, since \(-6\) is an integer but is not a natural number.

Note moreover that the expression

\[
\{ n \in \mathbb{Q} \mid n \text{ is even} \}
\]

is meaningless, since we have not defined a notion of ‚Äòevenness‚Äô for rational numbers.

### Strategy 2.1.5
Let \( X \) be a set and let \( p(x) \) be a logical formula with free variable \( x \in X \). In order to prove \( a \in \{ x \in X \mid p(x) \} \), it suffices to prove \( a \in X \) and that \( p(a) \) is true.

### Exercise 2.1.6
A **dyadic rational** is a rational number that can be expressed as an integer divided by a power of 2. Express the set of all dyadic rationals using set-builder notation.

An alternate form of set-builder notation uses an expression involving one or more variables to the left of the vertical bar, and the range of the variable(s) to the right. The elements of the set are then the values of the expression as the variable(s) vary as indicated‚Äîthat is:

\[
\{ \text{expr}(x) \mid x \in X \} \text{ is defined to mean } \{ y \mid \exists x \in X, y = \text{expr}(x) \}
\]

where expr(x) is the expression in question.

### Example 2.1.7
The expression \(\{ 3k + 2 \mid k \in \mathbb{Z} \}\) denotes the set of all integers of the form \(3k + 2\), where \(k \in \mathbb{Z}\). It is shorthand for \(\{ n \in \mathbb{Z} \mid \exists k \in \mathbb{Z}, n = 3k + 2 \}\). In implied list notation, we could write this set as \(\{ \ldots, -4, -1, 2, 5, 8, \ldots \}\).

### Exercise 2.1.8
Express the set of dyadic rationals (defined in Exercise 2.1.6) in this alternate form of set-builder notation.

Set-builder notation is useful for defining sets based on the properties they satisfy, as in Definitions 2.1.9 and 2.1.11 below.

### Definition 2.1.9
Let \( n \in \mathbb{N} \). The set \([n]\) is defined by \([n] = \{ k \in \mathbb{N} \mid 1 \leq k \leq n \}\).
</markdown><markdown>
### Example 2.1.10

In implied list notation, \([n] = \{1, 2, \ldots, n\}\). For example, \([4] = \{1, 2, 3, 4\}\). Note that \([0]\) has no elements (it is empty‚Äîsee Definition 2.1.27), since there are no natural numbers \(k\) satisfying the inequality \(1 \leq k \leq 0\).

While not particularly interesting yet, sets of the form \([n]\) will be fundamental throughout Chapter 8, as they are used to define the notion of a finite set, as well as the size of a finite set.

Intervals are particular subsets of \(\mathbb{R}\) that are ubiquitous in mathematics, particularly in analysis and topology.

### Definition 2.1.11 (Intervals of the real line)

Let \(a, b \in \mathbb{R}\). The open interval \((a, b)\), the closed interval \([a, b]\), and the half-open intervals \([a, b)\) and \((a, b]\) from \(a\) to \(b\) are defined by

\[
(a, b) = \{x \in \mathbb{R} \mid a < x < b\} \quad (a, b] = \{x \in \mathbb{R} \mid a < x \leq b\}
\]
\[
[a, b) = \{x \in \mathbb{R} \mid a \leq x < b\} \quad [a, b] = \{x \in \mathbb{R} \mid a \leq x \leq b\}
\]

We further define the unbounded intervals \((-\infty, a)\), \((-\infty, a]\), \([a, \infty)\) and \((a, \infty)\) (\(\LaTeX\) code: \texttt{\textbackslash infty}) by

\[
(-\infty, a) = \{x \in \mathbb{R} \mid x < a\} \quad (a, \infty) = \{x \in \mathbb{R} \mid x > a\}
\]
\[
(-\infty, a] = \{x \in \mathbb{R} \mid x \leq a\} \quad [a, \infty) = \{x \in \mathbb{R} \mid x \geq a\}
\]

### Example 2.1.12

The following illustration depicts the open interval \((-2, 5)\).

The hollow circles \(\circ\) indicate that the endpoints are not included in the interval.

Be warned that the use of the symbol \(\infty\) is misleading, since it suggests that the symbol \(\infty\) on its own has a specific meaning (or, worse, that it refers to a real number). It doesn‚Äôt‚Äîit is just a symbol that suggests unboundedness of the interval in question. A less misleading way of writing \([a, \infty)\), for instance, might be \([a, \rightarrow)\) or \(\mathbb{R}_{\geq a}\); however, \([a, \infty)\) is standard, so it is what we will write.

### Exercise 2.1.13

For each of the following illustrations, find the interval that it depicts. A filled circle \(\bullet\) indicates that an end-point is included in the interval, whereas a hollow circle \(\circ\) indicates that an end-point is not included in the interval.
</markdown><markdown>
## Subsets

It is often the case that everything that is also an element of one set is an element of another set. For example, every integer is a rational number; that is

\[
\forall n \in \mathbb{Z}, n \in \mathbb{Q}
\]

We can say this more concisely by saying that \(\mathbb{Z}\) is a subset of \(\mathbb{Q}\).

### Definition 2.1.14

Let \(X\) be a set. A **subset** of \(X\) is a set \(U\) such that

\[
\forall a, (a \in U \Rightarrow a \in X)
\]

We write \(U \subseteq X\) (LaTeX code: `\subseteq`) for the assertion that \(U\) is a subset of \(X\).

Additionally, the notation \(U \nsubseteq X\) (LaTeX code: `\nsubseteq`) means that \(U\) is not a subset of \(X\), and the notation \(U \subsetneq X\) (LaTeX code: `\subsetneq`) means that \(U\) is a **proper subset** of \(X\), that is a subset of \(X\) that is not equal to \(X\).

### Strategy 2.1.15 (Proving a subset containment)

In order to prove that a set \(U\) is a subset of a set \(X\), it suffices to take an arbitrary element \(a \in U\) and prove that \(a \in X\).

### Example 2.1.16

Every set is a subset of itself‚Äîthat is, \(X \subseteq X\) for all sets \(X\). The proof of this is extremely simple: we must prove \(\forall x \in X, x \in X\). But then this is trivial: let \(x \in X\), then \(x \in X\) by assumption. Done!
</markdown><markdown>
### Example 2.1.17

Let \( a, b, c, d \in \mathbb{R} \) with \( a < c < d < b \). Then \([c, d] \subseteq (a, b)\). Indeed, let \( x \in [c, d] \). Then \( c \leq x \leq d \). But then

\[
a < c \leq x \leq d < b \quad \Rightarrow \quad a < x < b
\]

so that \([c, d] \subseteq (a, b)\), as required.

### Exercise 2.1.18

Let \( a, b, c, d \in \mathbb{R} \) with \( a < b \) and \( c < d \). Prove that \([a, b) \subseteq (c, d]\) if and only if \( a > c \) and \( b \leq d \).

### Example 2.1.19

The number sets from [Chapter 0](#) are related by the following chain of subset inclusions.

\[
\mathbb{N} \subseteq \mathbb{Z} \subseteq \mathbb{Q} \subseteq \mathbb{R} \subseteq \mathbb{C}
\]

The following proposition proves a property of subsethood known as *transitivity*‚Äîwe‚Äôll revisit this property in [Section 5.1](#).

### Proposition 2.1.20

Let \( X, Y, Z \) be sets. If \( X \subseteq Y \) and \( Y \subseteq Z \), then \( X \subseteq Z \).

**Proof**

Suppose that \( X \subseteq Y \) and \( Y \subseteq Z \). We need to prove \( X \subseteq Z \).

So let \( a \in X \). Since \( X \subseteq Y \), it follows from [Definition 2.1.14](#) that \( a \in Y \); and since \( Y \subseteq Z \), it follows again from [Definition 2.1.14](#) that \( a \in Z \).

Hence \( X \subseteq Z \), as required.

## Set equality

This section is all about defining sets, comparing sets, and building new sets from old, and so to make much more progress, we first need to establish what we mean when we say that two sets are *equal*.

### Discussion 2.1.21

Let \( X \) and \( Y \) be sets. What should it mean to say that \( X \) and \( Y \) are equal? Try to provide a precise definition of equality of sets before reading on.

There are different possible notions of ‚Äòsameness‚Äô for sets: we might want to say that two sets \( X \) and \( Y \) are equal when they have quite literally the same definition; or we might want to say that \( X \) and \( Y \) are equal when they contain the same objects as elements. For instance, suppose \( X \) is ‚Äòthe set of all odd natural numbers‚Äô and \( Y \) is ‚Äòthe set of
</markdown><markdown>
of all integers that are differences of consecutive perfect squares'‚Äîin this case, the first of these characterisations of equality might lead us to say \(X \neq Y\), whereas the second would lead us to say \(X = Y\).

Clearly, we have to state our terms at some point. And that point is now.

### Axiom 2.1.22 (Set extensionality)
Let \(X\) and \(Y\) be sets. Then \(X = Y\) if and only if \(\forall a, (a \in X \iff a \in Y)\), or equivalently, if \(X \subseteq Y\) and \(Y \subseteq X\).

This characterisation of set equality suggests the following strategy for proving that two sets are equal.

### Strategy 2.1.23 (Proof by double containment)
In order to prove that a set \(X\) is equal to a set \(Y\), it suffices to:

- Prove \(X \subseteq Y\), i.e. let \(a \in X\) be an arbitrary element, and derive \(a \in Y\); and then
- Prove \(X \supseteq Y\), i.e. let \(a \in Y\) be an arbitrary element, and derive \(a \in X\).

We often write ‚Äò\((\subseteq)\)‚Äô and ‚Äò\((\supseteq)\)‚Äô to indicate the direction of the containment being proved.

### Example 2.1.24
We prove that \(\{x \in \mathbb{R} \mid x^2 \leq 1\} = [-1, 1]\) by double containment.

- \((\subseteq)\) Let \(a \in \{x \in \mathbb{R} \mid x^2 \leq 1\}\). Then \(a \in \mathbb{R}\) and \(a^2 \leq 1\), so that \((1 - a)(1 + a) = 1 - a^2 \geq 0\). It follows that either:
  - \(1 - a \geq 0\) and \(1 + a \geq 0\), in which case \(a \leq 1\) and \(a \geq -1\), so that \(a \in [-1, 1]\).
  - \(1 - a \leq 0\) and \(1 + a \leq 0\), in which case \(a \geq 1\) and \(a \leq -1\), which is a contradiction since \(-1 < 1\).

  So we must have \(a \in [-1, 1]\), as required.

- \((\supseteq)\) Let \(a \in [-1, 1]\). Then \(-1 \leq a \leq 1\), so \(|a| \leq 1\), and hence \(a^2 = |a|^2 \leq 1\), so that \(a \in \{x \in \mathbb{R} \mid x^2 \leq 1\}\), as required.

### Exercise 2.1.25
Prove that \(\{x \in \mathbb{R} \mid x^2 < x\} = (0, 1)\).

The set extensionality axiom has the consequence that sets are independent of the order in which their elements appear in list notation, and they are independent of how many
</markdown><markdown>
Exercise 2.1.26  
Prove by double containment that \(\{0, 1\} = \{1, 0\}\) and \(\{0, 0\} = \{0\}\).

## Inhabitation and emptiness

Another fundamental example of a set is the **empty set**, which is the set with no elements. But we have to be slightly careful about how we use the word ‚Äòthe‚Äô, since it implies uniqueness, and we don‚Äôt know (yet) that two sets with no elements are necessarily equal. So first we will define what it means for a set to be empty, and then we‚Äôll show that there is exactly one empty set.

### Definition 2.1.27
A set \(X\) is **inhabited** (or **nonempty**) if it has at least one element; otherwise, it is **empty**.

The assertion that \(X\) is inhabited is equivalent to the logical formula \(\exists a, a \in X\), and the assertion that \(X\) is empty is equivalent to the logical formula \(\neg \exists a, a \in X\). This suggests the following strategy for proving that a set is inhabited, or that it is empty.

### Strategy 2.1.28 (Proving that a set is inhabited or empty)
In order to prove a set \(X\) is inhabited, it suffices to exhibit an element. In order to prove a set \(X\) is empty, assume that \(X\) is inhabited‚Äîthat is, that there is some element \(a \in X\)‚Äîand derive a contradiction.

In other texts, the term **nonempty** is more common than **inhabited**, but there are reasons to prefer the latter. Indeed, the statement ‚Äò\(X\) is non-empty‚Äô translates more directly to \(\neg (\neg \exists a, a \in X)\), which has an unnecessary double-negative and suggests a proof of inhabitation by contradiction. For this reason, we use the term **inhabited** in this book.

Emptiness may seem like a trivial condition‚Äîand it is‚Äîbut owing to its canonicity, it arises all over the place.

### Example 2.1.29
The set \(\{x \in \mathbb{R} \mid x^2 = 2\}\) is inhabited since, for example, \(\sqrt{2} \in \mathbb{R}\) and \(\sqrt{2}^2 = 2\). However, the set \(\{x \in \mathbb{Q} \mid x^2 = 2\}\) is empty since, if it were inhabited, then there would be a rational number \(x\) such that \(x^2 = 2\), contrary to Proposition 0.28.

### Example 2.1.30
We observed in Example 2.1.10 that the set \([0]\) is empty; here‚Äôs a more formal proof.
</markdown><markdown>
Towards a contradiction, suppose \([0]\) is inhabited. Then there is some \(k \in \mathbb{N}\) such that \(1 \leq k \leq 0\). It follows that \(1 \leq 0\), which contradicts the fact that \(0 < 1\). Hence \([0]\) is empty, after all.

### Exercise 2.1.31
Let \(a, b \in \mathbb{R}\). Prove that \([a, b]\) is empty if and only if \(a > b\), and that \((a, b)\) is empty if and only if \(a \geq b\).

The next exercise is a logical technicality, which is counterintuitive for the same reason that makes the principle of explosion (Axiom 1.1.49) difficult to grasp. However, it is extremely useful for proving facts about the empty set, as we will see soon in Theorem 2.1.33.

### Exercise 2.1.32
Let \(E\) be an empty set and let \(p(x)\) be a predicate with one free variable \(x\) with domain of discourse \(E\). Show that the proposition \(\forall x \in E, p(x)\) is true, and that the proposition \(\exists x \in E, p(x)\) is false. What does the proposition \(\forall x \in E, x \neq x\) mean in English? Is it true?

Thanks to the axiom of extensionality (Axiom 2.1.22), any two empty sets must be equal since they both contain the same elements‚Äînamely, no elements at all! This is made formal in the following theorem.

### Theorem 2.1.33
Let \(E\) and \(E'\) be sets. If \(E\) and \(E'\) are empty, then \(E = E'\).

**Proof.** Suppose that \(E\) and \(E'\) are empty. The assertion that \(E = E'\) is equivalent to

\[
(\forall a \in E, a \in E') \land (\forall a \in E', a \in E)
\]

But \(\forall a \in E, a \in E'\) and \(\forall a \in E', a \in E\) are both true by Exercise 2.1.32 since \(E\) and \(E'\) are empty. So \(E = E'\), as claimed. ‚ñ°

Knowing that there is one and only one empty set means that we may now make the following definition, without worrying about whether the word ‚Äòthe‚Äô is problematic.

### Definition 2.1.34
The **empty set** (also known as the **null set**) is the set with no elements, and is denoted by \(\emptyset\) (LaTeX code: `\varnothing`).

Some authors write \(\{\}\) instead of \(\emptyset\), since \(\{\}\) is simply the empty set expressed in list notation.
</markdown><markdown>
## Exercise 2.1.35
Let \( X \) be a set. Prove that \( \emptyset \subseteq X \).

## Power sets

### Definition 2.1.36
Let \( X \) be a set. The **power set** of \( X \), written \( \mathcal{P}(X) \) (\(\LaTeX\) code: `\mathcal{P}`), is the set of all subsets of \( X \).

### Example 2.1.37
There are four subsets of \(\{1,2\}\), namely

\[
\emptyset, \quad \{1\}, \quad \{2\}, \quad \{1,2\}
\]

so \( \mathcal{P}(X) = \{\emptyset, \{1\}, \{2\}, \{1,2\}\} \).

## Exercise 2.1.38
Write out the elements of \( \mathcal{P}(\{1,2,3\}) \).

## Exercise 2.1.39
Let \( X \) be a set. Show that \( \emptyset \in \mathcal{P}(X) \) and \( X \in \mathcal{P}(X) \).

## Exercise 2.1.40
Write out the elements of \( \mathcal{P}(\emptyset) \), \( \mathcal{P}(\mathcal{P}(\emptyset)) \) and \( \mathcal{P}(\mathcal{P}(\mathcal{P}(\emptyset))) \).

Power sets are often a point of confusion because they bring the property of being a **subset** of one set to that of being an **element** of another, in the sense that for all sets \( U \) and \( X \) we have

\[
U \subseteq X \quad \iff \quad U \in \mathcal{P}(X)
\]

This distinction looks easy to grasp, but when the sets \( U \) and \( X \) look alike, it‚Äôs easy to fall into various traps. Here‚Äôs a simple example.

### Example 2.1.41
It is true that \( \emptyset \subseteq \emptyset \), but false that \( \emptyset \in \emptyset \). Indeed,

- \( \emptyset \subseteq \emptyset \) means \(\forall x \in \emptyset, x \in \emptyset\); but propositions of the form \(\forall x \in \emptyset, p(x)\) are always true, as discussed in Exercise 2.1.32.

- The empty set has no elements; if \( \emptyset \in \emptyset \) were true, it would mean that \( \emptyset \) had an element (that element being \( \emptyset \)). So it must be the case that \( \emptyset \notin \emptyset \).
</markdown><markdown>
The following exercise is intended to help you overcome similar potential kinds of confusion by means of practice. Try to think precisely about what the definitions involved are.

### Exercise 2.1.42
Determine, with proof, whether or not each of the following statements is true.

(a) \( \mathcal{P}(\emptyset) \in \mathcal{P}(\mathcal{P}(\emptyset)) \);

(b) \( \emptyset \in \{\{\emptyset\}\} \);

(c) \(\{\emptyset\} \in \{\{\emptyset\}\} \);

(d) \( \mathcal{P}(\mathcal{P}(\emptyset)) \in \{\emptyset, \{\emptyset, \{\emptyset\}\}\} \).

Repeat the exercise with all instances of ‚Äò\(\in\)‚Äô replaced by ‚Äò\(\subseteq\)‚Äô.
</markdown><markdown>
## TL;DR ‚Äî summary of Section 2.1

### Sets and subsets

**2.1.1** A **set** is a collection of objects (called *elements*) from a fixed universe of discourse $\mathcal{U}$; we write $x \in X$ to mean that $x$ is an element of the set $X$. Sets can be specified by (explicitly or implicitly) listing their elements $\{x_1, x_2, \ldots, x_n, \ldots \}$, or by using *set-builder notation* $\{x \in X \mid p(x)\}$.

**2.1.14** A set $U$ is a **subset** of a set $X$, written $U \subseteq X$, if $\forall a, (a \in U \Rightarrow a \in X)$. To prove that $U \subseteq X$, it suffices to introduce a variable $a$, assume that $a \in U$, and derive $a \in X$.

**2.1.22** Sets $X$ and $Y$ are equal if and only if $\forall a, (a \in X \Leftrightarrow a \in Y)$. In order to prove that $X = Y$, it suffices to prove separately that $X \subseteq Y$ and $Y \subseteq X$‚Äîthis method is called *double containment*.

### Inhabitation and emptiness

**2.1.27** A set $X$ is **inhabited** if it has at least one element; otherwise, it is **empty**.

**2.1.33** There is a unique empty set, denoted by $\varnothing$ or $\{\}$.

### Power sets

**2.1.36** The **power set** of a set $X$ is the set of all subsets of $X$.
</markdown><markdown>
## Section 2.2

# Set operations

In [Example 2.1.24](#) we noted that \([0, \infty)\) is the set of all non-negative real numbers. What if we wanted to talk about the set of all non-negative rational numbers instead? It would be nice if there was some expression in terms of \([0, \infty)\) and \(\mathbb{Q}\) to denote this set.

This is where **set operations** come in‚Äîthey allow us to use previously defined sets to introduce new sets.

### Intersection (\(\cap\))

The **intersection** of two sets is the set of things which are elements of both sets.

#### Definition 2.2.1
Let \(X\) and \(Y\) be sets. The (pairwise) intersection of \(X\) and \(Y\), denoted \(X \cap Y\) (LaTeX code: `\cap`), is defined by

\[ 
X \cap Y = \{ a \mid a \in X \land a \in Y \} 
\]

#### Example 2.2.2
By definition of intersection, we have \(x \in [0, \infty) \cap \mathbb{Q}\) if and only if \(x \in [0, \infty)\) and \(x \in \mathbb{Q}\). Since \(x \in [0, \infty)\) if and only if \(x\) is a non-negative real number (see [Example 2.1.24](#)), it follows that \([0, \infty) \cap \mathbb{Q}\) is the set of all non-negative rational numbers.

#### Exercise 2.2.3
Prove that \([0, \infty) \cap \mathbb{Z} = \mathbb{N}\).

#### Exercise 2.2.4
Write down the elements of the set

\[
\{0, 1, 4, 7\} \cap \{1, 2, 3, 4, 5\}
\]

#### Exercise 2.2.5
Express \([-2, 5) \cap [4, 7)\) as a single interval.

#### Proposition 2.2.6
Let \(X\) and \(Y\) be sets. Prove that \(X \subseteq Y\) if and only if \(X \cap Y = X\).

**Proof**  
Suppose that \(X \subseteq Y\). We prove \(X \cap Y = X\) by double containment.
</markdown><markdown>
- (‚äÜ) Suppose \( a \in X \cap Y \). Then \( a \in X \) and \( a \in Y \) by definition of intersection, so in particular we have \( a \in X \).

- (‚äá) Suppose \( a \in X \). Then \( a \in Y \) since \( X \subseteq Y \), so that \( a \in X \cap Y \) by definition of intersection.

Conversely, suppose that \( X \cap Y = X \). To prove that \( X \subseteq Y \), let \( a \in X \). Then \( a \in X \cap Y \) since \( X = X \cap Y \), so that \( a \in Y \) by definition of intersection, as required.

‚úê **Exercise 2.2.7**  
Let \( X \) be a set. Prove that \( X \cap \emptyset = \emptyset \).

‚ú¶ **Definition 2.2.8**  
Let \( X \) and \( Y \) be sets. We say \( X \) and \( Y \) are **disjoint** if \( X \cap Y \) is empty.

‚úé **Example 2.2.9**  
The sets \(\{0, 2, 4\}\) and \(\{1, 3, 5\}\) are disjoint, since they have no elements in common.

‚úê **Exercise 2.2.10**  
Let \( a, b, c, d \in \mathbb{R} \) with \( a < b \) and \( c < d \). Prove that the open intervals \((a, b)\) and \((c, d)\) are disjoint if and only if \( b < c \) or \( d < a \).

**Union (‚à™)**

The **union** of two sets is the set of things which are elements of at least one of the sets.

‚ú¶ **Definition 2.2.11**  
Let \( X \) and \( Y \) be sets. The (pairwise) **union** of \( X \) and \( Y \), denoted \( X \cup Y \) (LaTeX code: \(\cup\)), is defined by

\[
X \cup Y = \{ a \mid a \in X \lor a \in Y \}
\]

‚úé **Example 2.2.12**  
Let \( E \) be the set of even integers and \( O \) be the set of odd integers. Since every integer is either even or odd, \( E \cup O = \mathbb{Z} \). Note that \( E \cap O = \emptyset \), thus \(\{E, O\}\) is an example of a partition of \(\mathbb{Z}\)‚Äîsee Definition 5.2.21.

‚úê **Exercise 2.2.13**  
Write down the elements of the set

\[
\{0, 1, 4, 7\} \cup \{1, 2, 3, 4, 5\}
\]

‚úê **Exercise 2.2.14**  
Express \([-2, 5) \cup [4, 7)\) as a single interval.
</markdown><markdown>
The union operation allows us to define the following class of sets that will be particularly useful for us when studying counting principles in [Section 8.1](#).

**Exercise 2.2.15**  
Let \( X \) and \( Y \) be sets. Prove that \( X \subseteq Y \) if and only if \( X \cup Y = Y \).

**Example 2.2.16**  
Let \( X, Y, Z \) be sets. We prove that \( X \cap (Y \cup Z) = (X \cap Y) \cup (X \cap Z) \).

- (\(\subseteq\)) Let \( x \in X \cap (Y \cup Z) \). Then \( x \in X \), and either \( x \in Y \) or \( x \in Z \). If \( x \in Y \) then \( x \in X \cap Y \), and if \( x \in Z \) then \( x \in X \cap Z \). In either case, we have \( x \in (X \cap Y) \cup (X \cap Z) \).

- (\(\supseteq\)) Let \( x \in (X \cap Y) \cup (X \cap Z) \). Then either \( x \in X \cap Y \) or \( x \in X \cap Z \). In both cases we have \( x \in X \) by definition of intersection. In the first case we have \( x \in Y \), and in the second case we have \( x \in Z \); in either case, we have \( x \in Y \cup Z \), so that \( x \in X \cap (Y \cup Z) \).

**Exercise 2.2.17**  
Let \( X, Y, Z \) be sets. Prove that \( X \cup (Y \cap Z) = (X \cup Y) \cap (X \cup Z) \).

### Indexed families of sets

We will often have occasion to take the intersection or union not of just two sets, but of an arbitrary collection of sets (even of infinitely many sets). For example, we might want to know which real numbers are elements of \([0, 1 + \frac{1}{n}]\) for each \( n \geq 1 \), and which real numbers are elements of at least one of such sets.

Our task now is therefore to generalise our pairwise notions of intersection and union to arbitrary collections of sets, called **indexed families of sets**.

**Definition 2.2.18**  
An (indexed) family of sets is a specification of a set \( X_i \) for each element \( i \) of some indexing set \( I \). We write \(\{X_i \mid i \in I\}\) for the indexed family of sets.

**Example 2.2.19**  
The sets \([0, 1 + \frac{1}{n}]\) mentioned above assemble into an indexed family of sets, whose indexing set is \(\{n \in \mathbb{N} \mid n \geq 1\}\). We can abbreviate this family of sets by

\[
\{[0, 1 + \frac{1}{n}] \mid n \geq 1\}
\]

Observe that we have left implicit the fact that the variable \( n \) is ranging over the natural numbers and just written ‚Äò\( n \geq 1 \)‚Äô on the right of the vertical bar, rather than separately defining \( I = \{n \in \mathbb{N} \mid n \geq 1\} \) and writing \(\{[0, 1 + \frac{1}{n}] \mid n \in I\}\).
</markdown><markdown>
### Definition 2.2.20

The (indexed) intersection of an indexed family \(\{X_i \mid i \in I\}\) is defined by

\[
\bigcap_{i \in I} X_i = \{a \mid \forall i \in I, a \in X_i\} \quad (\LaTeX \text{ code: } \backslash bigcap\_ {i \in I})
\]

The (indexed) union of \(\{X_i \mid i \in I\}\) is defined by

\[
\bigcup_{i \in I} X_i = \{a \mid \exists i \in I, a \in X_i\} \quad (\LaTeX \text{ code: } \backslash bigcup\_ {i \in I})
\]

### Example 2.2.21

We prove that the intersection of the half-open intervals \([0, 1 + \frac{1}{n})\) for \(n \geq 1\) is \([0, 1]\). We will use the notation \(\bigcap_{n \geq 1}\) as shorthand for \(\bigcap_{n \in \{x \in \mathbb{N} \mid x \geq 1\}}\).

- (‚äÜ) Let \(x \in \bigcap_{n \geq 1} [0, 1 + \frac{1}{n})\).

  Then \(x \in [0, 1 + \frac{1}{n})\) for all \(n \geq 1\). In particular, \(x \geq 0\).

  To see that \(x \leq 1\), assume that \(x > 1\)‚Äîwe will derive a contradiction. Since \(x > 1\), we have \(x - 1 > 0\). Let \(N \geq 1\) be some natural number greater or equal to \(\frac{1}{x-1}\), so that \(\frac{1}{N} \leq x - 1\). Then \(x \geq 1 + \frac{1}{N}\), and hence \(x \notin [0, 1 + \frac{1}{N})\), contradicting the assumption that \(x \in [0, 1 + \frac{1}{n})\) for all \(n \geq 1\).

  So we must have \(x \leq 1\) after all, and hence \(x \in [0, 1]\).

- (‚äá) Let \(x \in [0, 1]\).

  To prove that \(x \in \bigcap_{n \geq 1} [0, 1 + \frac{1}{n})\), we need to show that \(x \in [0, 1 + \frac{1}{n})\) for all \(n \geq 1\). So fix \(n \geq 1\). Since \(x \in [0, 1]\), we have \(x \geq 0\) and \(x \leq 1 < 1 + \frac{1}{n}\), so that \(x \in [0, 1 + \frac{1}{n})\), as required.

Hence \(\bigcap_{n \geq 1} [0, 1 + \frac{1}{n}) = [0, 1]\) by double containment.

### Exercise 2.2.22

Express \(\bigcup_{n \geq 1} [0, 1 - \frac{1}{n})\) as an interval.

### Exercise 2.2.23

Prove that \(\bigcap_{n \in \mathbb{N}} [n] = \emptyset\) and \(\bigcup_{n \in \mathbb{N}} [n] = \{k \in \mathbb{N} \mid k \geq 1\}\).

Indexed intersections and unions generalise their pairwise counterparts, as the following exercise proves.
</markdown><markdown>
### Exercise 2.2.24
Let \( X_1 \) and \( X_2 \) be sets. Prove that

\[
X_1 \cap X_2 = \bigcap_{k \in [2]} X_k \quad \text{and} \quad X_1 \cup X_2 = \bigcup_{k \in [2]} X_k
\]

### Exercise 2.2.25
Find a family of sets \(\{X_n \mid n \in \mathbb{N}\}\) such that:

(i) \(\bigcup_{n \in \mathbb{N}} X_n = \mathbb{N}\);

(ii) \(\bigcap_{n \in \mathbb{N}} X_n = \emptyset\); and

(iii) \(X_i \cap X_j \neq \emptyset\) for all \(i, j \in \mathbb{N}\).

### Relative complement (\(\setminus\))

#### Definition 2.2.26
Let \(X\) and \(Y\) be sets. The **relative complement** of \(Y\) in \(X\), denoted \(X \setminus Y\) (LaTeX code: `\setminus`), is defined by

\[
X \setminus Y = \{x \in X \mid x \notin Y\}
\]

The operation \(\setminus\) is also known as the **set difference** operation. Some authors write \(Y - X\) instead of \(Y \setminus X\).

#### Example 2.2.27
Let \(E\) be the set of all even integers. Then \(n \in \mathbb{Z} \setminus E\) if and only if \(n\) is an integer and \(n\) is not an even integer; that is, if and only if \(n\) is odd. Thus \(\mathbb{Z} \setminus E\) is the set of all odd integers.

Moreover, \(n \in \mathbb{N} \setminus E\) if and only if \(n\) is a natural number and \(n\) is not an even integer. Since the even integers which are natural numbers are precisely the even natural numbers, \(\mathbb{N} \setminus E\) is precisely the set of all odd natural numbers.

### Exercise 2.2.28
Write down the elements of the set

\[
\{0, 1, 4, 7\} \setminus \{1, 2, 3, 4, 5\}
\]
</markdown><markdown>
### Exercise 2.2.29
Express \([-2,5) \setminus [4,7)\) and \([4,7) \setminus [-2,5)\) as intervals.

### Exercise 2.2.30
Let \(X\) and \(Y\) be sets. Prove that \(Y \setminus (Y \setminus X) = X \cap Y\), and deduce that \(X \subseteq Y\) if and only if \(Y \setminus (Y \setminus X) = X\).

### Comparison with logical operators and quantifiers

The astute reader will have noticed some similarities between set operations and the logical operators and quantifiers that we saw in Chapter 1.

Indeed, this can be summarised in the following table. In each row, the expressions in both columns are equivalent, where \(p\) denotes ‚Äò\(a \in X\)‚Äô, \(q\) denotes ‚Äò\(a \in Y\)‚Äô, and \(r(i)\) denotes ‚Äò\(a \in X_i\)‚Äô.

| sets                | logic         |
|---------------------|---------------|
| \(a \notin X\)      | \(\neg p\)    |
| \(a \in X \cap Y\)  | \(p \land q\) |
| \(a \in X \cup Y\)  | \(p \lor q\)  |
| \(a \in \bigcap_{i \in I} X_i\) | \(\forall i \in I, r(i)\) |
| \(a \in \bigcup_{i \in I} X_i\) | \(\exists i \in I, r(i)\) |
| \(a \in X \setminus Y\) | \(p \land (\neg q)\) |

This translation between logic and set theory does not stop there; in fact, as the following theorem shows, De Morgan‚Äôs laws for the logical operators (Theorem 1.3.24) and for quantifiers (Theorem 1.3.28) also carry over to the set operations of union and intersection.

### Theorem 2.2.31 (De Morgan‚Äôs laws for sets)
Given sets \(A, X, Y\) and a family \(\{X_i \mid i \in I\}\), we have

(a) \(A \setminus (X \cup Y) = (A \setminus X) \cap (A \setminus Y)\);

(b) \(A \setminus (X \cap Y) = (A \setminus X) \cup (A \setminus Y)\);

(c) \(A \setminus \bigcup_{i \in I} X_i = \bigcap_{i \in I} (A \setminus X_i)\);

(d) \(A \setminus \bigcap_{i \in I} X_i = \bigcup_{i \in I} (A \setminus X_i)\).
</markdown><markdown>
**Proof of (a)**  
Let \( a \) be arbitrary. By definition of union and relative complement, the assertion that \( a \in A \setminus (X \cup Y) \) is equivalent to the logical formula

\[ a \in A \land \neg (a \in X \lor a \in Y) \]

By de Morgan‚Äôs laws for logical operators, this is equivalent to

\[ a \in A \land (a \notin X \land a \notin Y) \]

which, in turn, is equivalent to

\[ a \in A \land a \notin X \land (a \in A \land a \notin Y) \]

But then by definition of intersection and relative complement, this is equivalent to

\[ a \in (A \setminus X) \cap (A \setminus Y) \]

Hence \( A \setminus (X \cup Y) = (A \setminus X) \cap (A \setminus Y) \), as required. ‚ñ°

**Exercise 2.2.32**  
Complete the proof of de Morgan‚Äôs laws for sets.

**Product (\(\times\))**

‚ú¶ **Definition 2.2.33**  
Let \( X \) and \( Y \) be sets. The (pairwise) cartesian product of \( X \) and \( Y \) is the set \( X \times Y \) (LaTeX code: \times) defined by

\[ X \times Y = \{(a, b) \mid a \in X \land b \in Y\} \]

The elements \((a, b) \in X \times Y\) are called **ordered pairs**, whose defining property is that, for all \( a, x \in X \) and all \( b, y \in Y \), we have \((a, b) = (x, y)\) if and only if \( a = x \) and \( b = y \).

‚úê **Example 2.2.34**  
If you have ever taken calculus, you will probably be familiar with the set \(\mathbb{R} \times \mathbb{R}\).

\[ \mathbb{R} \times \mathbb{R} = \{(x, y) \mid x, y \in \mathbb{R}\} \]

Formally, this is the set of ordered pairs of real numbers. Geometrically, if we interpret \(\mathbb{R}\) as an infinite line, the set \(\mathbb{R} \times \mathbb{R}\) is the (real) plane: an element \((x, y) \in \mathbb{R} \times \mathbb{R}\) describes the point in the plane with coordinates \((x, y)\).

We can investigate this further. For example, the following set:

\[ \mathbb{R} \times \{0\} = \{(x, 0) \mid x \in \mathbb{R}\} \]
</markdown><markdown>
is precisely the x-axis. We can describe graphs as subsets of \(\mathbb{R} \times \mathbb{R}\). Indeed, the graph of \(y = x^2\) is given by

\[ 
G = \{ (x, y) \in \mathbb{R} \times \mathbb{R} \mid y = x^2 \} = \{ (x, x^2) \mid x \in \mathbb{R} \} \subseteq \mathbb{R} \times \mathbb{R} 
\]

### Exercise 2.2.35
Write down the elements of the set \(\{1, 2\} \times \{3, 4, 5\}\).

### Exercise 2.2.36
Let \(X\) be a set. Prove that \(X \times \emptyset = \emptyset\).

### Exercise 2.2.37
Let \(X, Y\) and \(Z\) be sets. Under what conditions is it true that \(X \times Y = Y \times X\)? Under what conditions is it true that \((X \times Y) \times Z = X \times (Y \times Z)\)?

We might have occasion to take cartesian products of more than two sets. For example, whatever the set \(\mathbb{R} \times \mathbb{R} \times \mathbb{R}\) is, its elements should be ordered triples \((a, b, c)\) consisting of elements \(a, b, c \in \mathbb{R}\). This is where the following definition comes in handy.

### Definition 2.2.38
Let \(n \in \mathbb{N}\) and let \(X_1, X_2, \ldots, X_n\) be sets. The (n-fold) cartesian product of \(X_1, X_2, \ldots, X_n\) is the set \(\prod_{k=1}^n X_k\) defined by

\[
\prod_{k=1}^n X_k = \{ (a_1, a_2, \ldots, a_n) \mid a_k \in X_k \text{ for all } 1 \leq k \leq n \}
\]

The elements \((a_1, a_2, \ldots, a_n) \in \prod_{k=1}^n X_k\) are called ordered k-tuples, whose defining property is that, for all \(1 \leq k \leq n\) and all \(a_k, b_k \in X_k\), we have \((a_1, a_2, \ldots, a_n) = (b_1, b_2, \ldots, b_n)\) if and only if \(a_k = b_k\) for all \(1 \leq k \leq n\).

Given a set \(X\), write \(X^n\) to denote the set \(\prod_{k=1}^n X\). We might on occasion also write

\[
X_1 \times X_2 \times \cdots \times X_n = \prod_{k=1}^n X_k
\]

### Example 2.2.39
In Exercise 2.2.37 you might have noticed that the sets \((X \times Y) \times Z\) and \(X \times (Y \times Z)\) are not always equal‚ÄîDefinition 2.2.38 introduces a third potentially non-equal cartesian product of \(X, Y\) and \(Z\). For example, consider when \(X = Y = Z = \mathbb{R}\). Then
</markdown><markdown>
- The elements of \( (\mathbb{R} \times \mathbb{R}) \times \mathbb{R} \) are ordered pairs \(((a, b), c)\), where \((a, b)\) is itself an ordered pair of real numbers and \(c\) is a real number.

- The elements of \(\mathbb{R} \times (\mathbb{R} \times \mathbb{R})\) are ordered pairs \((a, (b, c))\), where \(a\) is a real number and \((b, c)\) is an ordered pair of real numbers.

- The elements of \(\mathbb{R} \times \mathbb{R} \times \mathbb{R} ( = \mathbb{R}^3)\) are ordered triples \((a, b, c)\), where \(a, b\) and \(c\) are real numbers.

So, although these three sets appear to be the same, zooming in closely on the definitions reveals that there are subtle differences between them. A sense in which they are the same is that there are bijections between them‚Äîthe notion of a bijection will be introduced in [Section 3.2](#).
</markdown><markdown>
## TL;DR ‚Äî summary of Section 2.2

### Binary set operations

**2.2.1** The *intersection* of sets \( X \) and \( Y \) is \( X \cap Y = \{ a \mid a \in X \land a \in Y \} \).

**2.2.11** The *union* of sets \( X \) and \( Y \) is \( X \cup Y = \{ a \mid a \in X \lor a \in Y \} \).

**2.2.26** The *relative complement* of a set \( X \) in a set \( Y \) is \( Y \setminus X = \{ a \mid a \in X \land a \notin Y \} \).

**2.2.33** The *cartesian product* of sets \( X \) and \( Y \) is \( X \times Y = \{ (a, b) \mid a \in X \land b \in Y \} \), where \( (a, b) \) is an *ordered pair*.

### Indexed set operations

**2.2.18** An *indexed family* of sets \( \{ X_i \mid i \in I \} \) is a specification of a set \( X_i \) for each element \( i \) of some *indexing set* \( I \).

**2.2.20** The *indexed intersection* of a family \( \{ X_i \mid i \in I \} \) is \( \bigcap_{i \in I} X_i = \{ a \mid \forall i \in I, a \in X_i \} \);  
the *indexed union* of the family is \( \bigcup_{i \in I} X_i = \{ a \mid \exists i \in I, a \in X_i \} \).

**2.2.38** The *n-fold cartesian product* of an \([n]\)-indexed family \( \{ X_i \mid i \in [n] \} \) is the set  
\[
\prod_{k=1}^{n} X_k = \{ (a_1, \ldots, a_n) \mid a_k \in X_k \text{ for all } k \in [n] \},
\]
where \( (a_1, \ldots, a_n) \) is an *ordered n-tuple*.
</markdown><markdown>
## Chapter 2 exercises

### Sets

**2.1.** Express the following sets in the indicated form of notation.

(a) \(\{ n \in \mathbb{Z} \mid n^2 < 20 \}\) in list notation;

(b) \(\{ 4k + 3 \mid k \in \mathbb{N} \}\) in implied list notation;

(c) The set of all odd multiples of six in set-builder notation;

(d) The set \(\{ 1, 2, 5, 10, 17, \ldots, n^2 + 1, \ldots \}\) in set-builder notation.

**2.2.** Find sets \(X_n\) for each \(n \in \mathbb{N}\) such that \(X_{n+1} \nsubseteq X_n\) for all \(n \in \mathbb{N}\). Can any of the sets \(X_n\) be empty?

**2.3.** Express the set \(\mathcal{P}(\{\emptyset, \{\emptyset\}, \{\{\emptyset\}\}\})\) in list notation.

**2.4.** Let \(X\) be a set and let \(U, V \subseteq X\). Prove that \(U\) and \(V\) are disjoint if and only if \(U \subseteq X \setminus V\).

**2.5.** For each of the following statements, determine whether or not it is true for all sets \(A\) and \(X\), and prove your claim.

(a) If \(X \setminus A = \emptyset\), then \(X = A\).

(b) If \(X \setminus A = X\), then \(A = \emptyset\).

(c) If \(X \setminus A = A\), then \(A = \emptyset\).

(d) \(X \setminus (X \setminus A) = A\).

**2.6.** For each of the following statements, determine whether it is true for all sets \(X, Y\), false for all sets \(X, Y\), or true for some choices of \(X\) and \(Y\) and false for others.

(a) \(\mathcal{P}(X \cup Y) = \mathcal{P}(X) \cup \mathcal{P}(Y)\)

(b) \(\mathcal{P}(X \cap Y) = \mathcal{P}(X) \cap \mathcal{P}(Y)\)

(c) \(\mathcal{P}(X \times Y) = \mathcal{P}(X) \times \mathcal{P}(Y)\)

(d) \(\mathcal{P}(X \setminus Y) = \mathcal{P}(X) \setminus \mathcal{P}(Y)\)

**2.7.** Let \(F\) be a set whose elements are all sets. Prove that if \(\forall A \in F, \, \forall x \in A, \, x \in F\), then \(F \subseteq \mathcal{P}(F)\).

Questions 2.8 to 2.13 concern the *symmetric difference* of sets, defined below.
</markdown><markdown>
### Definition 2.E.1

The **symmetric difference** of sets \( X \) and \( Y \) is the set \( X \triangle Y \) (LaTeX code: \texttt{\textbackslash triangle}) defined by

\[
X \triangle Y = \{ a \mid a \in X \text{ or } a \in Y \text{ but not both} \}
\]

2.8. Prove that \( X \triangle Y = (X \setminus Y) \cup (Y \setminus X) = (X \cup Y) \setminus (X \cap Y) \) for all sets \( X \) and \( Y \).

2.9. Let \( X \) be a set. Prove that \( X \triangle X = \emptyset \) and \( X \triangle \emptyset = X \).

2.10. Let \( X \) and \( Y \) be sets. Prove that \( X = Y \) if and only if \( X \triangle Y = \emptyset \).

2.11. Prove that sets \( X \) and \( Y \) are disjoint if and only if \( X \triangle Y = X \cup Y \).

2.12. Prove that \( X \triangle (Y \triangle Z) = (X \triangle Y) \triangle Z \) for all sets \( X, Y \) and \( Z \).

2.13. Prove that \( X \cap (Y \triangle Z) = (X \cap Y) \triangle (X \cap Z) \) for all sets \( X, Y \) and \( Z \).

### Definition 2.E.2

A subset \( U \subseteq \mathbb{R} \) is **open** if, for all \( a \in U \), there exists \( \delta > 0 \) such that \( (a - \delta, a + \delta) \subseteq U \).

In Questions 2.14 to 2.17 you will prove some elementary facts about open subsets of \( \mathbb{R} \).

2.14. For each of the following subsets of \( \mathbb{R} \), determine (with proof) whether it is open:

- (a) \( \emptyset \);
- (b) \( (0, 1) \);
- (c) \( [0, 1] \);
- (d) \( \mathbb{Z} \);
- (e) \( \mathbb{R} \setminus \mathbb{Z} \);
- (f) \( \mathbb{Q} \).

2.15. Prove that a subset \( U \subseteq \mathbb{R} \) is open if and only if, for all \( a \in U \), there exist \( u, v \in \mathbb{R} \) such that \( u < a < v \) and \( (u, v) \subseteq U \).

2.16. In this question you will prove that the intersection of finitely many open sets is open, but the intersection of infinitely many open sets might not be open.

- (a) Let \( n \geq 1 \) and suppose \( U_1, U_2, \ldots, U_n \) are open subsets of \( \mathbb{R} \). Prove that the intersection \( U_1 \cap U_2 \cap \cdots \cap U_n \) is open.

- (b) Prove that \( (0, 1 + \frac{1}{n}) \) is open for all \( n \geq 1 \), but that \( \bigcap_{n \geq 1} (0, 1 + \frac{1}{n}) \) is not open.

2.17. Prove that a subset \( U \subseteq \mathbb{R} \) is open if and only if it can be expressed as a union of open intervals‚Äîmore precisely, \( U \subseteq \mathbb{R} \) is open if and only if, for some indexing set \( I \), there exist real numbers \( a_i, b_i \) for each \( i \in I \), such that \( U = \bigcup_{i \in I} (a_i, b_i) \).

2.18. Let \( \{ A_n \mid n \in \mathbb{N} \} \) and \( \{ B_n \mid n \in \mathbb{N} \} \) be families of sets such that, for all \( i \in \mathbb{N} \), there exists some \( j \geq i \) such that \( B_j \subseteq A_i \). Prove that \( \bigcap_{n \in \mathbb{N}} A_n = \bigcap_{n \in \mathbb{N}} B_n \).
</markdown><markdown>
### True‚ÄìFalse questions

In [Questions 2.19 to 2.24](#), determine (with proof) whether the statement is true or false.

2.19. If \( E \) is a set and \(\neg \forall x, x \in E\), then \( E = \emptyset \).

2.20. If \( E \) is a set and \(\forall x, \neg x \in E\), then \( E = \emptyset \).

2.21. \(\{1, 2, 3\} = \{1, 2, 1, 3, 2, 1\}\)

2.22. \(\emptyset \in \emptyset\)

2.23. \(\emptyset \in \{\emptyset\}\)

2.24. \(\emptyset \in \{\{\emptyset\}\}\).

### Always‚ÄìSometimes‚ÄìNever questions

In [Questions 2.25 to 2.40](#), determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

2.25. Let \( a \) and \( b \) be arbitrary objects. Then \(\{a, b\} = \{b, a\}\).

2.26. Let \( a, b \) and \( c \) be arbitrary objects. Then \(\{a, b\} = \{a, c\}\).

2.27. Let \( X \) and \( Y \) be sets and suppose there is some \( a \) such that \( a \in X \iff a \in Y \). Then \( X = Y \).

2.28. Let \( X \) and \( Y \) be sets and suppose there is some \( a \) such that \( a \in X \) but \( a \notin Y \). Then \( X = Y \).

2.29. Let \( X \) and \( Y \) be sets with \( X \neq Y \). Then \(\forall a, a \in X \Rightarrow a \notin Y\).

2.30. Let \( X \) and \( Y \) be sets with \( X \cap Y = \emptyset \). Then \(\forall a, a \in X \Rightarrow a \notin Y\).

2.31. Let \( X \) and \( Y \) be sets with \( X \neq Y \). Then \(\exists a, a \in X \land a \notin Y\).

2.32. Let \( X \) and \( Y \) be sets of sets and suppose that \( X \subseteq Y \). Then \( X \subseteq Y \).

2.33. Let \( X \) be a set. Then \(\emptyset \in \mathcal{P}(X)\).

2.34. Let \( X \) be a set. Then \(\{\emptyset\} \in \mathcal{P}(X)\).

2.35. Let \( X \) be a set. Then \(\{\emptyset, X\} \subseteq \mathcal{P}(X)\).

2.36. Let \( X \) and \( Y \) be sets such that \(\mathcal{P}(X) = \mathcal{P}(Y)\). Then \( X = Y \).

2.37. Let \( X \) and \( Y \) be sets such that \( X \setminus Y = \emptyset \). Then \( X = Y \).

2.38. Let \( X, Y \) and \( Z \) be sets such that \( X \setminus Y = Z \) and \( Y \subseteq Z \). Then \( X = Y \cup Z \).

2.39. Let \( X \) and \( Y \) be sets. Then \( X \cup Y = X \cap Y \).
</markdown><markdown>
2.40. Let \( X \) be a set and let \( A \subseteq X \). Then \( A \in X \).
</markdown>The page appears to be blank, so there is no text to extract.<markdown>
# Chapter 3

## Functions

If sets are the bread of mathematics, then functions are the butter. As we will see in subsequent chapters of this book, functions are central in almost every area of mathematical study: they allow us to transform elements of one set into elements of another, to define ‚Äòinfinity‚Äô, and to capture real-world notions such as probability and randomness in the abstract.

It is likely (but not assumed) that you have seen functions before, such as real-valued functions in calculus or linear transformations in linear algebra. However, we will study functions in the abstract; the ‚Äòinputs‚Äô and ‚Äòoutputs‚Äô of our functions need not be numbers, vectors or points in space; they can be anything at all‚Äîin fact, the inputs or outputs to our functions might themselves be functions!

We introduce the notion of a function abstractly in Section 3.1. Much of our time will be spent developing basic notions involving functions, including graphs, composition, images and preimages.

We will zoom in on two properties in particular in Section 3.2, namely injectivity and surjectivity. These properties allow us to compare the sizes of sets we will use them extensively in Chapters 8 and 10 for doing exactly that.
</markdown><markdown>
## Section 3.1

# Functions

One way of studying interactions between sets is by studying *functions* between them, which we will define informally in [Definition 3.1.1](#). Functions are mathematical objects which assign to each element of one set exactly one element of another. Almost every branch of mathematics studies functions, be it directly or indirectly, and almost every application of mathematics arises from a translation of the abstract notion of a function to the real world. Just one example of this is the theory of computation‚Äîfunctions provide precisely the language necessary to describe the deterministic input-output behaviour of algorithms.

You might have come across the notion of a function before now. In schools, functions are often introduced as being like *machines*‚Äîthey have inputs and outputs, and on a given input they always return the same output. For instance, there is a function which takes integers as inputs and gives integers as outputs, which on the input \( x \) returns the integer \( x + 3 \).

This characterisation of functions, however, is clearly not precise enough for the purposes of mathematical proof. A next approximation to a precise definition of a function might look something like this:

### Definition 3.1.1

A function \( f \) from a set \( X \) to a set \( Y \) is a specification of elements \( f(x) \in Y \) for \( x \in X \), such that

\[
\forall x \in X, \exists! y \in Y, y = f(x)
\]

Given \( x \in X \), the (unique!) element \( f(x) \in Y \) is called the *value* of \( f \) at \( x \).

The set \( X \) is called the *domain* (or *source*) of \( f \), and \( Y \) is called the *codomain* (or *target*) of \( f \). We write \( f : X \rightarrow Y \) (\LaTeX{} code: \( f : X \to Y \)) to denote the assertion that \( f \) is a function with domain \( X \) and codomain \( Y \).

This is better‚Äîwe‚Äôre now talking about sets, and not mysterious ‚Äòmachines‚Äô.

Moreover, this definition establishes a close relationship between functions and the \(\exists!\) quantifier: indeed, to say that \( f \) assigns to each element of \( X \) a unique element of \( Y \) is to say precisely that

\[
\forall x \in X, \exists! y \in Y, y = f(x)
\]

Conversely, any true proposition of the form \(\forall x \in X, \exists! y \in Y, p(x,y)\) defines a function.
</markdown><markdown>
### Section 3.1. Functions

\( f : X \to Y \): the function \( f \) assigns to each \( x \in X \) the unique \( y \in Y \) such that \( p(x,y) \) is true. In other words, \(\forall x \in X, p(x, f(x))\) is true!

We can use this to generate some examples of functions.

#### Example 3.1.2
Example 1.2.27 said that every positive real number has a unique positive square root; we proved this in Example 1.2.30. What this means is that there is a function

\[
r : \mathbb{R}^{>0} \to \mathbb{R}^{>0} \quad \text{where} \quad \mathbb{R}^{>0} = \{x \in \mathbb{R} \mid x > 0\}
\]

defined by letting \( r(x) \) be the (unique) positive square root of \( x \), for each \( x \in \mathbb{R}^{>0} \). That is, we have a function \( r \) defined by \( r(x) = \sqrt{x} \).

#### Exercise 3.1.3
Recall Exercise 1.2.31. Which of the statements (a), (b) or (c) is of the form \(\forall x \in X, \exists! y \in Y, p(x,y)\)? For each statement of this form, determine the domain and codomain of the corresponding function, and write an expression defining this function.

### Specifying a function

Just like with sets, there are many ways to specify a function \( f : X \to Y \), but when we do so, we must be careful that what we write really *does* define a function!

This correctness of specification is known as *well-definedness*, and ultimately amounts to verifying that the condition \(\forall x \in X, \exists! y \in Y, f(x) = y\) holds for the specification of \( f \). Namely *totality*, *existence* and *uniqueness*:

- **Totality.** A value \( f(x) \) should be specified for each \( x \in X \)‚Äîthis corresponds to the ‚Äò\(\forall x \in X\)‚Äô quantifier in the definition of functions.

- **Existence.** For each \( x \in X \), the specified value \( f(x) \) should actually exist, and should be an element of \( Y \)‚Äîthis corresponds to the *existence* part of the ‚Äò\(\exists! y \in Y\)‚Äô quantifier in the definition of functions.

- **Uniqueness.** For each \( x \in X \), the specified value \( f(x) \) should refer to only one element of \( Y \)‚Äîthis corresponds to the *uniqueness* part of the ‚Äò\(\exists! y \in Y\)‚Äô quantifier in the definition of functions.

When specifying a function, you should justify each of these components of well-definedness unless they are extremely obvious. You will probably find that, in most cases, the only component in need of justification is uniqueness, but keep all three in mind.
</markdown><markdown>
### Lists

If \( X \) is finite, then we can specify a function \( f : X \to Y \) by simply listing the values of \( f \) at all possible elements \( x \in X \). For example, we can define a function

\[
f : \{1, 2, 3\} \to \{\text{red, yellow, green, blue, purple}\}
\]

by declaring

\[
f(1) = \text{red}, \quad f(2) = \text{purple}, \quad f(3) = \text{green}
\]

Note that the function is at this point completely specified: we know its values at all elements of the domain \(\{1, 2, 3\}\). It doesn‚Äôt matter that some of the elements of the codomain (yellow and blue) are unaccounted for‚Äîall that matters is that each element of the domain is associated with exactly one element of the codomain.

Unfortunately, most of the sets that we work with will be infinite, or of an unspecified finite size; in these cases, simply writing a list of values isn‚Äôt sufficient. Fortunately for us, there are other ways of specifying functions.

### Formulae

In many cases, particularly when the domain \( X \) and codomain \( Y \) are number sets, we can define a function by giving a formula for the value of \( f(x) \) for each \( x \in X \). For example, we can define a function \( f : \mathbb{R} \to \mathbb{R} \) by letting

\[
f(x) = x^2 + 3 \quad \text{for all } x \in \mathbb{R}
\]

### By cases

It will at times be convenient to define a function using different specifications for different elements of the domain. A very simple example is the absolute value function \(|\cdot| : \mathbb{R} \to \mathbb{R}\), defined for \( x \in \mathbb{R} \)

\[
|x| = 
\begin{cases} 
x & \text{if } x \geq 0 \\
-x & \text{if } x \leq 0 
\end{cases}
\]

Here we have split into two cases based on the conditions \( x \geq 0 \) and \( x \leq 0 \).

When specifying a function \( f : X \to Y \) by cases, it is important that the conditions be:

- **exhaustive**: given \( x \in X \), at least one of the conditions on \( X \) must hold; and
- **compatible**: if any \( x \in X \) satisfies more than one condition, the specified value must be the same no matter which condition is picked.

For the absolute value function defined above, these conditions are satisfied. Indeed, for \( x \in \mathbb{R} \), it is certainly the case that \( x \geq 0 \) or \( x \leq 0 \), so the conditions are exhaustive. Moreover, given \( x \in \mathbb{R} \), if both \( x \geq 0 \) and \( x \leq 0 \), then \( x = 0 \)‚Äîso we need to check that the specification yields the same value when \( x = 0 \) regardless of which condition we pick. The \( x \geq 0 \) condition yields the value 0, and the \( x \leq 0 \) condition yields the value \(-0\), which is equal to 0‚Äîso the conditions are compatible. We could have used
</markdown><markdown>
### Section 3.1. Functions

\( x < 0 \) instead of \( x \leq 0 \); in this case the conditions are *mutually exclusive*, so certainly compatible because they do not overlap.

**Algorithms.** You might, on first exposure to functions, have been taught to think of a function as a *machine* which, when given an *input*, produces an *output*. This 'machine' is defined by saying what the possible inputs and outputs are, and then providing a list of instructions (an *algorithm*) for the machine to follow, which on any input produces an output‚Äîand, moreover, if fed the same input, the machine always produces the same output.

For example, we might instruct a machine to take rational numbers as inputs and give rational numbers as outputs, and to follow the following sequence of steps on a given input

multiply by 2 \(\rightarrow\) add 5 \(\rightarrow\) square the result \(\rightarrow\) divide by 6

This 'machine' defines a function \( M : \mathbb{Q} \rightarrow \mathbb{Q} \) which, in equation form, is specified by

\[
M(x) = \frac{(2x + 5)^2}{6} \quad \text{for all } x \in \mathbb{Q}
\]

In our more formal set-up, therefore, we can define a function \( M : I \rightarrow O \) by specifying:

- a set \( I \) of all *inputs*;
- a set \( O \) of potential *outputs*; and
- a deterministic[^a] algorithm which describes how an input \( x \in I \) is transformed into an output \( M(x) \in O \).

That is, the domain is the set \( I \) of all possible 'inputs', the codomain is a set \( O \) containing all the possible 'outputs', and the function \( M \) is a rule specifying how an input is associated with the corresponding output.

For now, we will use algorithmic specifications of functions only sparingly‚Äîthis is because it is much harder to make formal what is meant by an 'algorithm', and it is important to check that a given algorithm is deterministic.

### Function equality

In [Section 2.1](#) we discussed how there may be many different possible ways of characterising equality of sets. This matter was resolved by declaring that two sets are equal if and only if they have the same elements (this was [Axiom 2.1.22](#)).

[^a]: The word 'deterministic' just means that the algorithm always produces the same output on a single input.
</markdown><markdown>
A similar matter arises for functions. For example, consider the function \( f : \mathbb{R} \to \mathbb{R} \) defined by \( f(x) = 2x \) for all \( x \in \mathbb{R} \), and the function \( g : \mathbb{R} \to \mathbb{R} \), defined by letting \( g(x) \) be the result of taking \( x \), multiplying it by three, dividing the result by four, dividing the result by six, and then multiplying the result by sixteen. It so happens that \( g(x) = 2x \) for all \( x \in \mathbb{R} \) as well, but that is not how \( g \) is defined; moreover, if \( f \) and \( g \) were implemented as algorithms, then it would take longer to compute the values of \( g \) than it would take to compute the values of \( f \).

Should we consider \( f \) and \( g \) to be *equal*? If we are only interested in whether \( f \) and \( g \) have the same values on each argument, then the answer should be ‚Äòyes‚Äô; if we are interested in the algorithmic behaviour of \( f \) and \( g \), then the answer should be ‚Äòno‚Äô.

We resolve this dilemma with the following axiom. By adopting this axiom, we are stating that the functions \( f \) and \( g \) discussed above are equal.

### Axiom 3.1.4 (Function extensionality)
Let \( f : X \to Y \) and \( g : A \to B \) be functions. Then \( f = g \) if and only if the following conditions hold:

(i) \( X = A \) and \( Y = B \); and

(ii) \( f(x) = g(x) \) for all \( x \in X \).

### Strategy 3.1.5 (Proving two functions are equal)
Given functions \( f, g : X \to Y \) with the same domain and codomain, in order to prove that \( f = g \), it suffices to prove that \( f(x) = g(x) \) for all \( x \in X \).

A consequence of Axiom 3.1.4 is that, for fixed sets \( X \) and \( Y \), a function \( X \to Y \) is uniquely determined by its input-output pairs. This set is called the *graph* of the function; the proof of the equivalence between functions and their graphs is the content of Theorem 3.1.9.

### Definition 3.1.6
Let \( f : X \to Y \) be a function. The *graph* of \( f \) is the subset \( \mathrm{Gr}(f) \subseteq X \times Y \) defined by

\[
\mathrm{Gr}(f) = \{ (x, f(x)) \mid x \in X \} = \{ (x, y) \in X \times Y \mid y = f(x) \}
\]

### Example 3.1.7
Given a (sufficiently well-behaved) function \( f : \mathbb{R} \to \mathbb{R} \), we can represent \( \mathrm{Gr}(f) \subseteq \mathbb{R} \times \mathbb{R} \) by plotting it on a pair of axes using Cartesian coordinates in the usual way. For
</markdown><markdown>
example, if \( f \) is defined by \( f(x) = \frac{x}{2} \) for all \( x \in \mathbb{R} \), then its graph

\[
\text{Gr}(f) = \left\{ \left( x, \frac{x}{2} \right) \mid x \in \mathbb{R} \right\}
\]

can be represented by graph plot in Figure 3.1.

![Graph of the function \( f : \mathbb{R} \to \mathbb{R} \) defined by \( f(x) = \frac{x}{2} \) for all \( x \in \mathbb{R} \)](image.png)

**Exercise 3.1.8**  
Find a function \( f : \mathbb{Z} \to \mathbb{Z} \) whose graph is equal to the set

\[
\{ \ldots, (-2, -5), (-1, -2), (0, 1), (1, 4), (2, 7), (3, 10), \ldots \}
\]

Theorem 3.1.9 below provides a way of verifying that a function is well-defined by characterising their graphs.

**Theorem 3.1.9**  
Let \( X \) and \( Y \) be sets. A subset \( G \subseteq X \times Y \) is the graph of a function if and only if

\[
\forall x \in X, \exists! y \in Y, (x, y) \in G
\]

**Proof**  
(\(\Rightarrow\)). Suppose \( G \subseteq X \times Y \) is the graph of a function, say \( G = \text{Gr}(f) \) for some \( f : X \to Y \).
</markdown><markdown>
Then for each \( x \in X \), it follows from well-definedness of \( f \) that \( f(x) \) is the unique element \( y \in Y \) for which \( (x,y) \in G \). That is, \( (x, f(x)) \in G \), and if \( y \in Y \) with \( (x,y) \in G \), then \( y = f(x) \).

(\(\Leftarrow\)). Suppose \( G \subseteq X \times Y \) satisfies \(\forall x \in X, \exists! y \in Y, (x,y) \in G\). Define a function \( f : X \to Y \) by, for each \( x \in X \), defining the value \( f(x) \) to be the unique element \( y \in Y \) for which \( (x,y) \in G \). Well-definedness of \( f \) is then immediate from our assumption of the existence and uniqueness of such a value of \( y \) for each \( x \in X \).

‚úèÔ∏è **Example 3.1.10**  
The set \( G \) defined by

\[ G = \{(1, \text{red}), (2, \text{red}), (3, \text{green})\} \]

is the graph of a function \( f : \{1,2,3\} \to \{\text{red, green, blue}\} \). The function \( f \) is defined by

\[ f(1) = \text{red}, \quad f(2) = \text{red}, \quad f(3) = \text{green} \]

However, \( G \) is not the graph of a function \( \{1,2,3,4\} \to \{\text{red, green, blue}\} \), since \( G \) contains no elements of the form \( (4,y) \) for \( y \in \{\text{red, green, blue}\} \). Moreover, the set \( G' \) defined by

\[ G' = \{(1, \text{red}), (2, \text{red}), (2, \text{blue}), (3, \text{green})\} \]

does not define the graph of a function \( \{1,2,3\} \to \{\text{red, green, blue}\} \), since there is not a *unique* element of the form \( (2,y) \) in \( G' \)‚Äîrather, there are two of them!

‚úèÔ∏è **Exercise 3.1.11**  
For each of the following specifications of sets \( X, Y, G \), determine whether or not \( G \) is the graph of a function from \( X \) to \( Y \).

(a) \( X = \mathbb{R}, Y = \mathbb{R}, G = \{(a, a^2) \mid a \in \mathbb{R}\} \);

(b) \( X = \mathbb{R}, Y = \mathbb{R}, G = \{(a^2, a) \mid a \in \mathbb{R}\} \);

(c) \( X = \mathbb{R}^{\geq 0}, Y = \mathbb{R}^{\geq 0}, G = \{(a^2, a) \mid a \in \mathbb{R}^{\geq 0}\}, \text{ where } \mathbb{R}^{\geq 0} = \{x \in \mathbb{R} \mid x \geq 0\} \);

(d) \( X = \mathbb{Q}, Y = \mathbb{Q}, G = \{(x,y) \in \mathbb{Q} \times \mathbb{Q} \mid xy = 1\} \).

(e) \( X = \mathbb{Q}, Y = \mathbb{Q}, G = \{(a,a) \mid a \in \mathbb{Z}\} \);

‚ùñ **Aside**  
In light of Theorem 3.1.9, some people choose to define functions \( X \to Y \) as particular subsets of \( X \times Y \)‚Äîthat is, they identify functions with their graphs. This is particularly useful when studying the logical foundations of mathematics. We avoid this practice here, because it is not conceptually necessary, and it would preclude other possible ways of encoding functions.

We will now look at some more examples (and non-examples) of functions.
</markdown><markdown>
### Example 3.1.12

Example 1.2.27 gives a prime example of a function: it says that for every positive real number \( a \) there is a unique positive real number \( b \) such that \( b^2 = a \). This unique \( b \) is precisely the positive square root \(\sqrt{a}\) of \( a \). Writing \(\mathbb{R}^{>0}\) for the set of positive real numbers, we have thus established that taking the positive square root defines a function \(\mathbb{R}^{>0} \to \mathbb{R}^{>0}\).

There is a class of functions called **identity functions** that, despite being very simple, are so important that we will give them a numbered definition!

### Definition 3.1.13

Let \( X \) be a set. The **identity function** on \( X \) is the function \(\mathrm{id}_X : X \to X\) defined by \(\mathrm{id}_X(x) = x\) for all \( x \in X \).

You should convince yourself that the specification of \(\mathrm{id}_X\) given in Definition 3.1.13 is well-defined.

Another interesting example of a function is the **empty function**, which is useful in coming up with counterexamples and proving combinatorial identities (see Section 8.1).

### Definition 3.1.14

Let \( X \) be a set. The **empty function** with codomain \( X \) is the (unique!) function \(\emptyset \to X\). It has no values, since there are no elements of its domain.

Again, you should convince yourself that this specification is well-defined. Conceptually, convincing yourself of this is not easy; but writing down the proof of well-definedness is extremely easy‚Äîyou will find that there is simply nothing to prove!

### Example 3.1.15

Define \( f : \mathbb{R} \to \mathbb{R} \) by the equation \( f(x)^2 = x \) for all \( x \in \mathbb{R} \). This is not well-defined for a few reasons. First, if \( x < 0 \) then there is no real number \( y \) such that \( y^2 = x \), so for \( x < 0 \) there are no possible values of \( f(x) \) in the codomain of \( f \), so **existence** fails. Second, if \( x > 0 \) then there are in fact two real numbers \( y \) such that \( y^2 = x \), namely the positive square root \(\sqrt{x}\) and the negative square root \(-\sqrt{x}\). The specification of \( f \) does not indicate which of these values to take, so **uniqueness** fails.

Notice that the function \( r : \mathbb{R}^{>0} \to \mathbb{R}^{>0} \) from Example 3.1.2 is (well-)defined by the equation \( r(x)^2 = x \) for all \( x \in \mathbb{R}^{>0} \). This illustrates why it is very important to specify the domain and codomain when defining a function.

### Exercise 3.1.16

Which of the following specifications of functions are well-defined?
</markdown><markdown>
(a) \( g : \mathbb{Q} \to \mathbb{Q} \) defined by the equation \((x+1)g(x) = 1\) for all \( x \in \mathbb{Q} \);

(b) \( h : \mathbb{N} \to \mathbb{Q} \) defined by \((x+1)h(x) = 1\) for all \( x \in \mathbb{N} \);

(c) \( k : \mathbb{N} \to \mathbb{N} \) defined by \((x+1)k(x) = 1\) for all \( x \in \mathbb{N} \);

(d) \( \ell : \mathbb{N} \to \mathbb{N} \) defined by \(\ell(x) = \ell(x)\) for all \( x \in \mathbb{N} \).

### Exercise 3.1.17
Find a condition on sets \( X \) and \( Y \) such that the specification of a function \( i : X \cup Y \to \{0, 1\} \) given by

\[
i(z) = 
\begin{cases} 
0 & \text{if } z \in X \\
1 & \text{if } z \in Y 
\end{cases}
\]

to be well-defined.

### Composition of functions

In our section on sets, we talked about various operations that can be performed on sets‚Äîunion, intersection, and so on. There are also operations on functions, by far the most important of which is **composition**. To understand how composition works, let‚Äôs revisit the algorithmically defined function \( M : \mathbb{Q} \to \mathbb{Q} \) from page 117:

multiply by 2 \(\to\) add 5 \(\to\) square the result \(\to\) divide by 6

The function \( M \) is, in some sense, a **sequence** of functions, performed one-by-one until the desired result is reached. This is precisely **composition of functions**.

### Definition 3.1.18
Given functions \( f : X \to Y \) and \( g : Y \to Z \), their **composite** \( g \circ f \) (LaTeX code: `g \circ f`) (read ‚Äò\( g \) composed with \( f \)‚Äô or ‚Äò\( g \) after \( f \)‚Äô or even just ‚Äò\( g \) \( f \)‚Äô) is the function \( g \circ f : X \to Z \) defined by

\[
(g \circ f)(x) = g(f(x)) \text{ for all } x \in X
\]

Intuitively, \( g \circ f \) is the function resulting from first applying \( f \), and then applying \( g \), to the given input.

### Common error
Function composition is in some sense written ‚Äòbackwards‚Äô: in the expression \( g \circ f \), the function which is applied **first** is written **last**‚Äîthere is a good reason for this: the
</markdown><markdown>
### Example 3.1.19

The function \( M \) from page 117 can be defined as the composite

\[ M = ((k \circ h \circ g) \circ f \]

where

- \( f : \mathbb{Q} \to \mathbb{Q} \) is defined by \( f(x) = 2x \) for all \( x \in \mathbb{Q} \);
- \( g : \mathbb{Q} \to \mathbb{Q} \) is defined by \( g(x) = x + 5 \) for all \( x \in \mathbb{Q} \);
- \( h : \mathbb{Q} \to \mathbb{Q} \) is defined by \( h(x) = x^2 \) for all \( x \in \mathbb{Q} \);
- \( k : \mathbb{Q} \to \mathbb{Q} \) is defined by \( k(x) = \frac{x}{6} \) for all \( x \in \mathbb{Q} \).

### Exercise 3.1.20

Let \( f, g, h, k : \mathbb{Q} \to \mathbb{Q} \) be as in Example 3.1.19. Compute equations defining the following composites:

(a) \( f \circ g \);

(b) \( g \circ f \);

(c) \( ((f \circ g) \circ h) \circ k \);

(d) \( f \circ (g \circ (h \circ k)) \);

(e) \( (g \circ g) \circ (g \circ g) \).

### Example 3.1.21

Let \( f : X \to Y \) be any function. Then

\[ \text{id}_Y \circ f = f = f \circ \text{id}_X \]

To see this, let \( x \in X \). Then

\[
\begin{align*}
(\text{id}_Y \circ f)(x) &= \text{id}_Y(f(x)) & \text{by definition of composition} \\
&= f(x) & \text{by definition of } \text{id}_Y \\
&= f(\text{id}_X(x)) & \text{by definition of } \text{id}_X \\
&= (f \circ \text{id}_X)(x) & \text{by definition of composition}
\end{align*}
\]

Equality of the three functions in question follows.
</markdown><markdown>
### Exercise 3.1.22

Prove that composition of functions is **associative**, that is, if \( f : X \to Y \), \( g : Y \to Z \) and \( h : Z \to W \) are functions, then

\[
h \circ (g \circ f) = (h \circ g) \circ f : X \to W
\]

As a consequence of associativity, when we want to compose more than two functions, it doesn‚Äôt matter what order we compose the functions in. As such, we can just write \( h \circ g \circ f \).

### Exercise 3.1.23

Let \( f : X \to Y \) and \( g : Z \to W \) be functions, and suppose that \( Y \subseteq Z \). Note that there is a function \( h : X \to W \) defined by \( h(x) = g(f(x)) \) for all \( x \in X \). Write \( h \) as a composite of functions involving \( f \) and \( g \).

## Characteristic functions

A class of functions that are particularly useful for proving results about sets are **characteristic functions**.

### Definition 3.1.24

Let \( X \) be a set and let \( U \subseteq X \). The **characteristic function** of \( U \) in \( X \) is the function \( \chi_U : X \to \{0, 1\} \) (LaTeX code: `\chi_{U}`) defined by

\[
\chi_U(a) = 
\begin{cases} 
1 & \text{if } a \in U \\
0 & \text{if } a \notin U 
\end{cases}
\]

### Example 3.1.25

Consider the subset \( U = \{1, 3, 5\} \subseteq [6] \). Then the values of the characteristic function \( \chi_U : [6] \to \{0, 1\} \) are given by

\[
\begin{align*}
\chi_U(1) &= 1 \\
\chi_U(2) &= 0 \\
\chi_U(3) &= 1 \\
\chi_U(4) &= 0 \\
\chi_U(5) &= 1 \\
\chi_U(6) &= 0 \\
\end{align*}
\]

### Theorem 3.1.26

Let \( X \) be a set and let \( U, V \subseteq X \). Then \( U = V \) if and only if \( \chi_U = \chi_V \).

**Proof**
</markdown><markdown>
### Section 3.1. Functions

- (‚áí) Assume \( U = V \) and let \( a \in X \). Then

  \[
  \chi_U(a) = 1 \Leftrightarrow a \in U \quad \text{by definition of } \chi_U
  \]
  \[
  \Leftrightarrow a \in V \quad \text{since } U = V
  \]
  \[
  \Leftrightarrow \chi_V(a) = 1 \quad \text{by definition of } \chi_V
  \]

  Likewise \(\chi_U(a) = 0\) if and only if \(\chi_V(a) = 0\), so that \(\chi_U = \chi_V\) by function extensionality.

- (‚áê) Assume \(\chi_U = \chi_V\) and let \( a \in X \). Then

  \[
  a \in U \Leftrightarrow \chi_U(a) = 1 \quad \text{by definition of } \chi_U
  \]
  \[
  \Leftrightarrow \chi_V(a) = 1 \quad \text{since } \chi_U = \chi_V
  \]
  \[
  \Leftrightarrow a \in V \quad \text{by definition of } \chi_V
  \]

  so \( U = V \) by set extensionality.

---

### Strategy 3.1.27 (Proving set identities using characteristic functions)

In order to prove that two subsets \( U \) and \( V \) of a set \( X \) are equal, it suffices to prove that \(\chi_U = \chi_V\).

---

### Theorem 3.1.28

Let \( X \) be a set and let \( U, V \subseteq X \). Then

(a) \(\chi_{U \cap V}(a) = \chi_U(a)\chi_V(a)\) for all \( a \in X \);

(b) \(\chi_{U \cup V}(a) = \chi_U(a) + \chi_V(a) - \chi_U(a)\chi_V(a)\) for all \( a \in X \);

(c) \(\chi_{X \setminus V}(a) = 1 - \chi_V(a)\) for all \( a \in X \).

**Proof of (a)**

Let \( a \in X \). Since the only values that \(\chi_U(a)\) and \(\chi_V(a)\) can take are 0 and 1, we have

\[
\chi_U(a)\chi_V(a) = 
\begin{cases} 
1 & \text{if } \chi_U(a) = 1 \text{ and } \chi_V(a) = 1 \\ 
0 & \text{otherwise} 
\end{cases}
\]

But \(\chi_U(a) = 1\) if and only if \( a \in U \) and \(\chi_V(a) = 1\) if and only if \( a \in V \), so that

\[
\chi_U(a)\chi_V(a) = 
\begin{cases} 
1 & \text{if } a \in U \cap V \\ 
0 & \text{if } a \notin U \cap V 
\end{cases}
\]

This is exactly to say that \(\chi_U(a)\chi_V(a) = \chi_{U \cap V}(a)\), as required.
</markdown><markdown>
### Exercise 3.1.29

Prove parts (b) and (c) of [Theorem 3.1.28](#).

Theorem 3.1.28 can be used in conjunction with [Strategy 3.1.27](#) to prove set theoretic identities using their characteristic functions.

### Example 3.1.30

In [Example 2.2.16](#) we proved that \( X \cap (Y \cup Z) = (X \cap Y) \cup (X \cap Z) \) for all sets \( X, Y \) and \( Z \). We prove this again using characteristic functions, considering \( X, Y \) and \( Z \) as subsets of a universal set \( U \).

So let \( a \in U \). Then

\[
\begin{align*}
\chi_{X \cap (Y \cup Z)}(a) &= \chi_X(a) \chi_{Y \cup Z}(a) & \text{by Theorem 3.1.28(a)} \\
&= \chi_X(a) (\chi_Y(a) + \chi_Z(a) - \chi_Y(a) \chi_Z(a)) & \text{by Theorem 3.1.28(b)} \\
&= \chi_X(a) \chi_Y(a) + \chi_X(a) \chi_Z(a) - \chi_X(a) \chi_Y(a) \chi_Z(a) & \text{rearranging} \\
&= \chi_X(a) \chi_Y(a) + \chi_X(a) \chi_Z(a) - \chi_X(a)^2 \chi_Y(a) \chi_Z(a) & \text{since } \chi_X(a)^2 = \chi_X(a) \\
&= \chi_{X \cap Y}(a) + \chi_{X \cap Z}(a) - \chi_{X \cap Y}(a) \chi_{X \cap Z}(a) & \text{by Theorem 3.1.28(a)} \\
&= \chi_{(X \cap Y) \cup (X \cap Z)}(a) & \text{by Theorem 3.1.28(b)}
\end{align*}
\]

Using [Strategy 3.1.27](#), it follows that \( X \cap (Y \cup Z) = (X \cap Y) \cup (X \cap Z) \).

### Exercise 3.1.31

Use characteristic functions to prove de Morgan‚Äôs laws for pairwise unions and intersections ([Theorem 2.2.31](#)).

### Images and Preimages

#### Definition 3.1.32

Let \( f : X \to Y \) be a function and let \( U \subseteq X \). The **image of** \( U \) **under** \( f \) is the subset \( f[U] \subseteq Y \) (also written \( f_*(U) \) or even just \( f(U) \)) is defined by

\[
f[U] = \{ f(x) \mid x \in U \} = \{ y \in Y \mid \exists x \in U, y = f(x) \}
\]

That is, \( f[U] \) is the set of values that the function \( f \) takes when given inputs from \( U \).

The **image of** \( f \) is the image of the entire domain, i.e. the set \( f[X] \).

#### Example 3.1.33

Let \( f : \mathbb{R} \to \mathbb{R} \) be defined by \( f(x) = x^2 \). The image of \( f \) is the set \( \mathbb{R}^{\geq 0} \) of all nonnegative real numbers. Let‚Äôs prove this:
</markdown><markdown>
- \( f[\mathbb{R}] \subseteq \mathbb{R}^{\geq 0} \). Let \( y \in f[\mathbb{R}] \). Then \( y = x^2 \) for some \( x \in \mathbb{R} \). But \( x^2 \geq 0 \), so we must have \( y \in \mathbb{R}^{\geq 0} \), as required.

- \( \mathbb{R}^{\geq 0} \subseteq f[\mathbb{R}] \). Let \( y \in \mathbb{R}^{\geq 0} \). Then \(\sqrt{y} \in \mathbb{R}\), and \( y = (\sqrt{y})^2 = f(\sqrt{y}) \). Hence \( y \in f[\mathbb{R}] \), as required.

We have shown by double containment that \( f[\mathbb{R}] = \mathbb{R}^{\geq 0} \).

### Exercise 3.1.34
For each of the following functions \( f \) and subsets \( U \) of their domain, describe the image \( f[U] \).

(a) \( f : \mathbb{Z} \to \mathbb{Z} \) defined by \( f(n) = 3n \), with \( U = \mathbb{N} \);

(b) \( f : X \to X \times X \times X \) (where \( X \) is any set) defined by \( f(x) = (x, x) \) with \( U = X \);

(c) \( f : \{a, b, c\} \to \{1, 2, 3\} \) defined by \( f(a) = 1 \), \( f(b) = 3 \) and \( f(c) = 1 \), with \( U = \{a, b, c\} \).

### Exercise 3.1.35
Prove that \( f[\emptyset] = \emptyset \) for all functions \( f \).

### Example 3.1.36
Let \( f : X \to Y \) be a function and let \( U, V \subseteq X \). Then \( f[U \cap V] \subseteq f[U] \cap f[V] \). To see this, let \( y \in f[U \cap V] \). Then \( y = f(x) \) for some \( x \in U \cap V \). By definition of intersection, \( x \in U \) and \( x \in V \). Since \( x \in U \) and \( y = f(x) \), we have \( y \in f[U] \); likewise, since \( x \in V \), we have \( y \in f[V] \). But then by definition of intersection, we have \( y \in f[U] \cap f[V] \).

### Exercise 3.1.37
Let \( f : X \to Y \) be a function and let \( U, V \subseteq X \). We saw in Example 3.1.36 that \( f[U \cap V] \subseteq f[U] \cap f[V] \). Determine which of the following is true, and for each, provide a proof of its truth or falsity:

(a) \( f[U] \cap f[V] \subseteq f[U \cap V] \);

(b) \( f[U \cup V] \subseteq f[U] \cup f[V] \);

(c) \( f[U] \cup f[V] \subseteq f[U \cup V] \).
</markdown><markdown>
### Definition 3.1.38

Let \( f : X \to Y \) be a function and let \( V \subseteq Y \). The **preimage of \( V \) under \( f \)** is the subset \( f^{-1}[V] \) (LaTeX code: `f^{-1}`) (also written \( f^*(V) \) (LaTeX code: `f^*`), or just \( f^{-1}(V) \)) is defined by

\[
f^{-1}[V] = \{ x \in X \mid f(x) \in V \} = \{ x \in X \mid \exists y \in V, f(x) = y \}
\]

That is, \( f^{-1}[V] \) is the set of all the elements of its domain \( X \) that the function \( f \) sends to elements of \( V \).

### Example 3.1.39

Let \( f : \mathbb{Z} \to \mathbb{Z} \) be the function defined by \( f(x) = x^2 \) for all \( x \in X \). Then

- \( f^{-1}[\{1, 4, 9\}] = \{-3, -2, -1, 1, 2, 3\} \);
- \( f^{-1}[\{1, 2, 3, 4, 5, 6, 7, 8, 9\}] = \{-3, -2, -1, 1, 2, 3\} \) too, since the other elements of \([9]\) are not perfect squares, and hence not of the form \( f(x) \) for \( x \in \mathbb{Z} \);
- \( f^{-1}[\mathbb{N}] = \mathbb{Z} \), since for any \( x \in \mathbb{Z} \) we have \( f(x) \geq 0 \), so that \( f(x) \in \mathbb{N} \).

### Example 3.1.40

Let \( f : X \to Y \) be a function, let \( U \subseteq X \) and let \( V \subseteq Y \). Then \( f[U] \subseteq V \) if and only if \( U \subseteq f^{-1}[V] \). The proof is as follows.

(‚áí). Suppose \( f[U] \subseteq V \); we‚Äôll prove \( U \subseteq f^{-1}[V] \). So fix \( x \in U \). Then \( f(x) \in f[U] \) by definition of image. But then \( f(x) \in V \) by our assumption that \( f[U] \subseteq V \), and so \( x \in f^{-1}[V] \) by definition of preimage. Since \( x \) was arbitrarily chosen from \( U \), it follows that \( U \subseteq f^{-1}[V] \).

(‚áê). Suppose \( U \subseteq f^{-1}[V] \); we‚Äôll prove \( f[U] \subseteq V \). So fix \( y \in f[U] \). Then \( y = f(x) \) for some \( x \in U \) by definition of image. But then \( x \in f^{-1}[V] \) by our assumption that \( U \subseteq f^{-1}[V] \), and so \( f(x) \in V \) by definition of preimage. But \( y = f(x) \), so \( y \in V \), and since \( y \) was arbitrarily chosen, it follows that \( f[U] \subseteq V \).

The following exercise demonstrates that preimages interact very nicely with the basic set operations (intersection, union and relative complement):

### Exercise 3.1.41

Let \( f : X \to Y \) be a function. Prove that \( f^{-1}[\emptyset] = \emptyset \) and \( f^{-1}[Y] = X \).

### Exercise 3.1.42

Let \( X \) be a set. Prove that every function \( f : X \to \{0, 1\} \) is the characteristic function of the subset \( f^{-1}[\{1\}] \subseteq X \).
</markdown><markdown>
## TL;DR ‚Äî summary of Section 3.1

### Functions and graphs

**3.1.1** A *function* \( f \) from a set \( X \) (called the *domain*) to a set \( Y \) (called the *codomain*) assigns to each \( x \in X \) a unique element \( f(x) \in Y \); we write \( f : X \to Y \) to mean that \( f \) is a function from \( X \) to \( Y \).

**3.1.4** Two functions are equal if and only if they have the same domain and codomain and the same values at all arguments.

**3.1.6** The *graph* of a function \( f : X \to Y \) is \( \text{Gr}(f) = \{(x, f(x)) \mid x \in X\} \subseteq X \times Y \). Two functions \( f, g : X \to Y \) are equal if and only if they have the same graph.

### Composition of functions

**3.1.18** The *composite* of \( f : X \to Y \) and \( g : Y \to Z \) is the function \( g \circ f : X \to Z \) defined by \( (g \circ f)(x) = g(f(x)) \) for all \( x \in X \).

### Characteristic functions

**3.1.24** The *characteristic function* of a subset \( U \subseteq X \) is the function \( \chi_U : X \to \{0, 1\} \) defined by, for each \( a \in X \), letting \( \chi_U(a) = 1 \) if \( a \in U \), and \( \chi_U(a) = 0 \) otherwise.

**3.1.26** Two subsets of a fixed set are equal if and only if they have equal characteristic functions.

**3.1.28** The characteristic functions of intersections, unions and relative complements of subsets \( U, V \subseteq X \) can be expressed in terms of \( \chi_U \) and \( \chi_V \) using arithmetic operations; this gives us another way of proving set identities.

### Images and preimages

**3.1.32** The *image* of a subset \( U \subseteq X \) under a function \( f : X \to Y \) is the subset \( f[U] = \{f(x) \mid x \in U\} \subseteq Y \). The *image* of \( f \) is the subset \( f[X] \subseteq Y \).

**3.1.38** The *preimage* of a subset \( V \subseteq Y \) under a function \( f : X \to Y \) is the subset \( f^{-1}[V] = \{x \in X \mid f(x) \in Y\} \subseteq X \).
</markdown><markdown>
## Section 3.2
### Injections and surjections

To motivate some of the definitions to come, look at the dots (‚óè) and stars (‚òÖ) below. Are there more dots or more stars?

```
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ
```

Pause for a second and think about how you knew the answer to this question.

Indeed, there are more dots than stars. There are a couple of ways to arrive at this conclusion:

(i) You could count the number of dots, count the number of stars, and then compare the two numbers; or

(ii) You could notice that the dots and the stars are evenly spaced, but that the line of dots is longer than the line of stars.

It is likely that you chose method (ii). In fact, it is likely that you haven‚Äôt even counted the number of dots or the number of stars yet‚Äîand you don‚Äôt need to! We can conclude that there are more dots than stars by simply pairing up dots with stars‚Äîwe eventually run out of stars, and there are still dots left over, so there must have been more dots than stars.

### Injectivity

One way of formalising this act of pairing up stars with dots mathematically is to define a function \( f : S \to D \) from the set \( S \) of stars to the set \( D \) of dots, where the value of \( f \) at each star is the dot that it is paired with. We of course must do this in such a way that each dot is paired with at most one star:

```
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë ‚Üë
‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ
```
</markdown><markdown>
It is a property of this function‚Äîcalled *injectivity*‚Äîthat allows us to deduce that there are more dots than stars.

Intuitively, a function \( f : X \to Y \) is injective if it puts the elements of \( X \) in one-to-one correspondence with the elements of a subset of \( Y \)‚Äîjust like how the stars are in one-to-one correspondence with a subset of the dots in the example above.

### Definition 3.2.1
A function \( f : X \to Y \) is **injective** (or one-to-one) if

\[
\forall a, b \in X, f(a) = f(b) \Rightarrow a = b
\]

An injective function is said to be an **injection**.

### Strategy 3.2.2 (Proving a function is injective)
In order to prove that a function \( f : X \to Y \) is injective, it suffices to fix \( a, b \in X \), assume that \( f(a) = f(b) \), and then derive \( a = b \).

By contraposition, \( f : X \to Y \) being injective is equivalent to saying, for all \( a, b \in X \), if \( a \neq b \), then \( f(a) \neq f(b) \). This is usually less useful for proving that a function is injective, but it does provide a good intuition‚Äîit says that \( f \) sends distinct inputs to distinct outputs.

The following is a very simple example from elementary arithmetic:

### Example 3.2.3
Define \( f : \mathbb{Z} \to \mathbb{Z} \) by letting \( f(x) = 2n + 1 \) for all \( n \in \mathbb{Z} \). We‚Äôll prove that \( f \) is injective. Fix \( m, n \in \mathbb{Z} \), and assume that \( f(m) = f(n) \). By definition of \( f \), we have \( 2m + 1 = 2n + 1 \). Subtracting 1 yields \( 2m = 2n \), and dividing by 2 yields \( m = n \). Hence \( f \) is injective. 

The following example is slightly more sophisticated.

### Proposition 3.2.4
Let \( f : X \to Y \) and \( g : Y \to Z \) be functions. If \( f \) and \( g \) are injective, then \( g \circ f \) is injective.

**Proof**

Suppose that \( f \) and \( g \) are injective and let \( a, b \in X \). We need to prove that

\[
(g \circ f)(a) = (g \circ f)(b) \Rightarrow a = b
\]

So assume \( (g \circ f)(a) = (g \circ f)(b) \). By definition of function composition, this implies that \( g(f(a)) = g(f(b)) \). By injectivity of \( g \), we have \( f(a) = f(b) \); and by injectivity of \( f \), we have \( a = b \).
\end{proof}
</markdown><markdown>
### Exercise 3.2.5
Let \( f : X \to Y \) and \( g : Y \to Z \) be functions. Prove that if \( g \circ f \) is injective, then \( f \) is injective.

### Exercise 3.2.6
Write out what it means to say a function \( f : X \to Y \) is not injective, and say how you would prove that a given function is not injective. Give an example of a function which is not injective, and use your proof technique to write a proof that it is not injective.

### Exercise 3.2.7
For each of the following functions, determine whether it is injective or not injective.

- \( f : \mathbb{N} \to \mathbb{Z} \), defined by \( f(n) = n^2 \) for all \( n \in \mathbb{N} \).
- \( g : \mathbb{Z} \to \mathbb{N} \), defined by \( g(n) = n^2 \) for all \( n \in \mathbb{Z} \).
- \( h : \mathbb{N} \times \mathbb{N} \times \mathbb{N} \to \mathbb{N} \), defined by \( h(x,y,z) = 2^x \cdot 3^y \cdot 5^z \) for all \( x,y,z \in \mathbb{N} \).

### Exercise 3.2.8
Let \( a, b \in \mathbb{R} \) with \( b \neq 0 \), and define \( f : \mathbb{R} \to \mathbb{R} \) by \( f(t) = a + bt \) for all \( t \in \mathbb{R} \). Prove that \( f \) is injective.

## Surjectivity

Let‚Äôs revisit the rows of dots and stars that we saw earlier. Beforehand, we made our idea that there are more dots than stars formal by proving the existence of an injection \( f : S \to D \) from the set \( S \) of stars to the set \( D \) of dots.

However, we could have drawn the same conclusion instead from defining a function \( D \to S \), which in some sense covers the stars with dots‚Äîthat is, every star is paired up with at least one dot.

This property is called surjectivity‚Äîa function \( f : X \to Y \) is surjective if every element of \( Y \) is a value of \( f \). This is made precise in [Definition 3.2.9](#).
</markdown><markdown>
### Definition 3.2.9
A function \( f : X \to Y \) is **surjective** (or onto) if

\[
\forall y \in Y, \ \exists x \in X, \ f(x) = y
\]

A surjective function is said to be a **surjection**.

### Strategy 3.2.10
To prove that a function \( f : X \to Y \) is surjective, it suffices to take an arbitrary element \( y \in Y \) and, in terms of \( y \), find an element \( x \in X \) such that \( f(x) = y \).

In order to find \( x \), it is often useful to start from the equation \( f(x) = y \) and derive some possible values of \( x \). But be careful‚Äîin order to complete the proof, it is necessary to verify that the equation \( f(x) = y \) is true for the chosen value of \( x \).

### Example 3.2.11
Fix \( n \in \mathbb{N} \) with \( n > 0 \), and define a function \( r : \mathbb{Z} \to \{0, 1, \ldots, n-1\} \) by letting \( r(a) \) be the remainder of \( a \) when divided by \( n \) (see Theorem 0.18). This function is surjective, since for each \( k \in \{0, 1, \ldots, n-1\} \) we have \( r(k) = k \).

### Exercise 3.2.12
For each of the following pairs of sets \( (X, Y) \), determine whether the function \( f : X \to Y \) defined by \( f(x) = 2x + 1 \) is surjective.

(a) \( X = \mathbb{Z} \) and \( Y = \{x \in \mathbb{Z} \mid x \text{ is odd}\} \);

(b) \( X = \mathbb{Z} \) and \( Y = \mathbb{Z} \);

(c) \( X = \mathbb{Q} \) and \( Y = \mathbb{Q} \);

(d) \( X = \mathbb{R} \) and \( Y = \mathbb{R} \).

### Exercise 3.2.13
Let \( f : X \to Y \) be a function. Find a subset \( V \subseteq Y \) and a surjection \( g : X \to V \) agreeing with \( f \) (that is, such that \( g(x) = f(x) \) for all \( x \in X \)).

### Exercise 3.2.14
Let \( f : X \to Y \) be a function. Prove that \( f \) is surjective if and only if \( Y = f[X] \).

### Exercise 3.2.15
Let \( f : X \to Y \) be a function. Prove that there is a set \( Z \) and functions

\[
p : X \to Z \quad \text{and} \quad i : Z \to Y
\]

such that \( p \) is surjective, \( i \) is injective, and \( f = i \circ p \).
</markdown><markdown>
### Exercise 3.2.16
Let \( f : X \to \mathcal{P}(X) \) be a function. By considering the set \( B = \{ x \in X \mid x \not\in f(x) \} \), prove that \( f \) is not surjective.

## Bijectivity

Bijective functions formalise the idea of putting sets into one-to-one correspondence‚Äîeach element of one set is paired with exactly one element of another.

‚ú¶ **Definition 3.2.17**  
A function \( f : X \to Y \) is **bijective** if it is injective and surjective. A bijective function is said to be a **bijection**.

‚ùñ **Proof tip**  
To prove that a function \( f \) is bijective, prove that it is injective and surjective.

‚úê **Example 3.2.18**  
Let \( D \subseteq \mathbb{Q} \) be the set of **dyadic rational numbers**, that is

\[
D = \left\{ x \in \mathbb{Q} \mid x = \frac{a}{2^n} \text{ for some } a \in \mathbb{Z} \text{ and } n \in \mathbb{N} \right\}
\]

Let \( k \in \mathbb{N} \), and define \( f : D \to D \) by \( f(x) = \frac{x}{2^k} \). We will prove that \( f \) is a bijection.

- **(Injectivity)** Fix \( x, y \in D \) and suppose that \( f(x) = f(y) \). Then \( \frac{x}{2^k} = \frac{y}{2^k} \), so that \( x = y \), as required.

- **(Surjectivity)** Fix \( y \in D \). We need to find \( x \in D \) such that \( f(x) = y \). Well certainly if \( 2^k y \in D \) then we have

  \[
  f(2^k y) = \frac{2^k y}{2^k} = y
  \]

  so it suffices to prove that \( 2^k y \in D \). Since \( y \in D \), we must have \( y = \frac{a}{2^n} \) for some \( n \in \mathbb{N} \).

  - If \( k \leq n \) then \( n - k \in \mathbb{N} \) and so \( 2^k y = \frac{a}{2^{n-k}} \in D \).
  - If \( k > n \) then \( k - n > 0 \) and \( 2^k y = 2^{k-n} a \in \mathbb{Z} \); but \( \mathbb{Z} \subseteq D \) since if \( a \in \mathbb{Z} \) then \( a = \frac{a}{2^0} \).

  So again we have \( 2^k y \in D \).

In both cases we have \( 2^k y \in D \); and \( f(2^k y) = y \), so that \( f \) is surjective.

Since \( f \) is both injective and surjective, it is bijective.

‚úé **Exercise 3.2.19**  
Let \( X \) be a set. Prove that the identity function \( \text{id}_X : X \to X \) is a bijection.

‚úé **Exercise 3.2.20**  
Let \( f : X \to Y \) and \( g : Y \to Z \) be bijections. Prove that \( g \circ f \) is a bijection.
</markdown><markdown>
## Inverses

Our next goal is to characterise injections, surjections and bijections in terms of other functions, called inverses.

Recall [Definition 3.2.1](#), which says that a function \( f : X \to Y \) is injective if, for all \( a, b \in X \), if \( f(a) = f(b) \) then \( a = b \).

### Exercise 3.2.21
Let \( f : X \to Y \) be a function. Prove that \( f \) is injective if and only if

\[
\forall y \in f[X], \exists! x \in X, y = f(x)
\]

Thinking back to [Section 3.1](#), you might notice that this means that the logical formula ‚Äò\( y = f(x) \)‚Äô defines a function \( f[X] \to X \)‚Äîspecifically, if \( f \) is injective then there is a function \( g : f[X] \to X \) which is (well-)defined by specifying \( x = g(f(x)) \) for all \( x \in X \). Thinking of \( f \) as an encoding function, we then have that \( g \) is the corresponding decoding function‚Äîdecoding is possible by injectivity of \( f \). (If \( f \) were not injective then distinct elements of \( X \) might have the same encoding, in which case we‚Äôre stuck if we try to decode them!)

### Exercise 3.2.22
Define a function \( e : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) by \( e(m,n) = 2^m \cdot 3^n \). Prove that \( e \) is injective. We can think of \( e \) as encoding pairs of natural numbers as single natural numbers‚Äîfor example, the pair \( (4, 1) \) is encoded as \( 2^4 \cdot 3^1 = 48 \). For each of the following natural numbers \( k \), find the pairs of natural numbers encoded by \( e \) as \( k \):

\[
1 \quad 24 \quad 7776 \quad 59049 \quad 396718580736
\]

In [Exercise 3.2.22](#), we were able to decode any natural number of the form \( 2^m \cdot 3^n \) for \( m, n \in \mathbb{N} \). This process of decoding yields a function

\[
d : \{k \in \mathbb{N} \mid k = 2^m \cdot 3^n \text{ for some } m, n \in \mathbb{N}\} \to \mathbb{N} \times \mathbb{N}
\]

What would happen if we tried to decode a natural number not of the form \( 2^m \cdot 3^n \) for \( m, n \in \mathbb{N} \), say 5 or 100? Well‚Ä¶ it doesn‚Äôt really matter! All we need to be true is that \( d(e(m,n)) = (m,n) \) for all \( (m,n) \in \mathbb{N} \times \mathbb{N} \); the value of \( d \) on other natural numbers is irrelevant.

### Definition 3.2.23
Let \( f : X \to Y \) be a function. A left inverse (or post-inverse) for \( f \) is a function \( g : Y \to X \) such that \( g \circ f = \text{id}_X \).
</markdown><markdown>
### Example 3.2.24

Let \( e : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) be as in Exercise 3.2.22. Define a function \( d : \mathbb{N} \to \mathbb{N} \times \mathbb{N} \) by

\[
d(k) = 
\begin{cases} 
(m, n) & \text{if } k = 2^m \cdot 3^n \text{ for some } m, n \in \mathbb{N} \\
(0, 0) & \text{otherwise}
\end{cases}
\]

Note that \( d \) is well-defined by the fundamental theorem of arithmetic (Theorem 7.2.12). Moreover, given \( m, n \in \mathbb{N} \), we have

\[
d(e(m, n)) = d(2^m \cdot 3^n) = (m, n)
\]

and so \( d \) is a left inverse for \( e \).

### Exercise 3.2.25

Let \( f : X \to Y \) be a function. Prove that if \( f \) has a left inverse, then \( f \) is injective.

Exercise 3.2.25 gives us a new strategy for proving that a function is injective.

### Strategy 3.2.26 (Proving a function is injective by finding a left inverse)

In order to prove that a function \( f : X \to Y \) is injective, it suffices to find a function \( g : Y \to X \) such that \( g(f(x)) = x \) for all \( x \in X \).

It would be convenient if the converse to Exercise 3.2.25 were true‚Äîand it is, provided that we impose the condition that the domain of the function be inhabited.

### Proposition 3.2.27

Let \( f : X \to Y \) be a function. If \( f \) is injective and \( X \) is inhabited, then \( f \) has a left inverse.

**Proof**

Suppose that \( f \) is injective and \( X \) is inhabited. Fix \( x_0 \in X \)‚Äînote that this element exists since \( X \) is inhabited‚Äîand define \( g : Y \to X \) as follows.

\[
g(y) = 
\begin{cases} 
x & \text{if } y = f(x) \text{ for some } x \in X \\
x_0 & \text{otherwise}
\end{cases}
\]

The only part of the specification of \( g \) that might cause it to fail to be well-defined is the case when \( y = f(x) \) for some \( x \in X \). The reason why \( g \) is well-defined is precisely injectivity of \( f \): if \( y = f(x) \) for some \( x \in X \), then the value of \( x \in X \) for which \( y = f(x) \) is unique. (Indeed, if \( a \in X \) satisfied \( y = f(a) \), then we‚Äôd have \( a = x \) by injectivity of \( f \).)

So \( g \) is indeed well-defined. To see that \( g \) is a left inverse for \( f \), let \( x \in X \). Letting \( y = f(x) \), we see that \( y \) falls into the first case in the specification of \( g \), so that \( g(f(x)) = g(y) = a \) for the value of \( a \in X \) for which \( y = f(a) \)‚Äîbut as noted above, we have \( a = x \) by injectivity of \( f \).
\end{proof}
</markdown><markdown>
### Exercise 3.2.28

Let \( f : X \to Y \) be a function with left inverse \( g : Y \to X \). Prove that \( g \) is a surjection.

We established a relationship between injections and left inverses in Exercise 3.2.25 and proposition 3.2.27, so it might come as no surprise that there is a relationship between surjections and **right inverses**.

### Definition 3.2.29

Let \( f : X \to Y \) be a function. A **right inverse** (or **pre-inverse**) for \( f \) is a function \( g : Y \to X \) such that \( f \circ g = \text{id}_Y \).

### Example 3.2.30

Define \( f : \mathbb{R} \to \mathbb{R}^{\geq 0} \) by \( f(x) = x^2 \). Note that \( f \) is surjective, since for each \( y \in \mathbb{R}^{\geq 0} \) we have \( \sqrt{y} \in \mathbb{R} \) and \( f(\sqrt{y}) = y \). However \( f \) is not injective; for instance

\[
f(-1) = 1 = f(1)
\]

Here are three right inverses for \( f \):

- The positive square root function \( g : \mathbb{R}^{\geq 0} \to \mathbb{R} \) defined by \( g(y) = \sqrt{y} \) for all \( y \in \mathbb{R}^{\geq 0} \). Indeed, for each \( y \in \mathbb{R}^{\geq 0} \) we have

  \[
  f(g(y)) = f(\sqrt{y}) = (\sqrt{y})^2 = y
  \]

- The negative square root function \( h : \mathbb{R}^{\geq 0} \to \mathbb{R} \) defined by \( h(y) = -\sqrt{y} \) for all \( y \in \mathbb{R}^{\geq 0} \). Indeed, for each \( y \in \mathbb{R}^{\geq 0} \) we have

  \[
  f(h(y)) = f(-\sqrt{y}) = (-\sqrt{y})^2 = y
  \]

- The function \( k : \mathbb{R}^{\geq 0} \to \mathbb{R} \) defined by

  \[
  k(y) = 
  \begin{cases} 
  \sqrt{y} & \text{if } 2n \leq y < 2n + 1 \text{ for some } n \in \mathbb{N} \\
  -\sqrt{y} & \text{otherwise}
  \end{cases}
  \]

  Note that \( k \) is well-defined, and again \( f(k(y)) = y \) for all \( y \in \mathbb{R}^{\geq 0} \) since no matter what value \( k(y) \) takes, it is equal to either \( \sqrt{y} \) or \(-\sqrt{y}\).

There are many more right inverses for \( f \)‚Äîin fact, there are infinitely many more!

### Exercise 3.2.31

Let \( f : X \to Y \) be a function. Prove that if \( f \) has a right inverse, then \( f \) is surjective.

### Strategy 3.2.32 (Proving a function is surjective by finding a right inverse)

In order to prove that a function \( f : X \to Y \) is surjective, it suffices to find a function \( g : Y \to X \) such that \( f(g(y)) = y \) for all \( y \in Y \).
</markdown><markdown>
## Interlude: the axiom of choice

It would be convenient if the converse to Exercise 3.2.31 were true‚Äîthat is, if \( f : X \to Y \) is surjective, then it has a right inverse. Let‚Äôs examine what a proof of this fact would entail. The fact that \( f : X \to Y \) is surjective can be expressed as

\[
\forall y \in Y, \exists x \in X, f(x) = y
\]

A right inverse would be a function \( g : Y \to X \), so by Definition 3.1.1, it must satisfy the following condition

\[
\forall y \in Y, \exists! x \in X, g(y) = x
\]

The temptation is therefore to construct \( g : Y \to X \) as follows. Let \( y \in Y \). By definition of surjectivity, there exists some \( x \in X \) such that \( f(x) = y \)‚Äîdefine \( g(y) \) to be such an element \( x \). Then we have \( f(g(y)) = f(x) = y \), as required.

There is an extremely subtle‚Äîbut important‚Äîissue with this construction.

By choosing \( g(y) \) to be a fixed element of \( X \) such that \( f(x) = y \), we are assuming ahead of time that there is a mechanism for choosing, for each \( y \in Y \), a unique element of \( f^{-1}[\{y\}] \) to be the value of \( g(y) \). In other words we are assuming that some statement \( R(x,y) \) satisfies the property

\[
\forall y \in Y, \exists! x \in X, [x \in f^{-1}[\{y\}] \land R(x,y)]
\]

But by Definition 3.1.1, this assumption is saying exactly that there exists a function \( Y \to X \) that associates to each \( y \in Y \) an element \( x \in X \) such that \( f(x) = y \).

To state this in plainer terms: we tried to prove that there exists a right inverse for \( f \) by assuming that there exists a right inverse for \( f \). Evidently, this is not a valid proof strategy.

Surprisingly, it turns out that neither the assumption that every surjection has a right inverse, nor the assumption that there exists a surjection with no right inverse, leads to a contradiction. As such, the assertion that every surjection has a right inverse is *provably unprovable*, although the proof that it is unprovable is far beyond the scope of this textbook.

Nonetheless, the construction of a right inverse that we gave above didn‚Äôt *feel* like we were abusing the fabric of mathematics and logic.

The essence of the proof is that if a statement of the form \( \forall a \in A, \exists b \in B, p(a,b) \) is true, then we should be able to define a function \( h : A \to B \) such that \( p(a,h(a)) \) is true for all \( a \in A \): the function \( h \) ‚Äòchooses‚Äô for each \( a \in A \) a particular element \( b = h(a) \in B \) such that \( p(a,b) \) is true.
</markdown><markdown>
What makes this possible is to **axiom of choice**, stated precisely below.

### Axiom 3.2.33 (Axiom of choice)
Let \(\{X_i \mid i \in I\}\) be a family of inhabited sets. Then there is a function \(h : I \to \bigcup_{i \in I} X_i\) such that \(h(i) \in X_i\) for each \(i \in I\).

There are reasons to keep track of the axiom of choice:

- The axiom of choice is perhaps the strangest assumption that we make‚Äîmost of the other axioms that we have stated have been ‚Äòevidently true‚Äô, but this is not the case for the axiom of choice;
- There are fields of mathematics which require the translation of results about sets into results about other kinds of objects‚Äîknowing whether the axiom of choice is necessary to prove a result tells us whether this is possible;
- The axiom of choice is highly non-constructive: if a proof of a result that does not use the axiom of choice is available, it usually provides more information than a proof of the same result that does use the axiom of choice.

With this in mind, when we need to invoke the axiom of choice to prove a result, we will mark the result with the letters AC. This can be freely ignored on first reading, but readers may find it useful when using this book as a reference at a later date.

### Proposition \(^{AC}\) 3.2.34
Let \(X\) and \(Y\) be sets and let \(p(x,y)\) be a logical formula with free variables \(x \in X\) and \(y \in Y\). If \(\forall x \in X, \, \forall y \in Y, \, p(x,y)\) is true, then there exists a function \(h : X \to Y\) such that \(\forall x \in X, \, p(x,h(x))\) is true.

**Proof**  
For each \(a \in X\), define \(Y_a = \{b \in Y \mid p(a,b)\}\). Note that \(Y_a\) is inhabited for each \(a \in X\) by the assumption that \(\forall x \in X, \, \exists y \in Y, \, p(x,y)\) is true. Since \(Y_a \subseteq Y\) for each \(a \in X\), by the axiom of choice there exists a function \(h : X \to Y\) such that \(h(a) \in Y_a\) for all \(a \in X\). But then \(p(a,h(a))\) is true for each \(a \in X\) by definition of the sets \(Y_a\).

In light of Proposition 3.2.34, the axiom of choice manifests itself in proofs as the following proof strategy.

### Strategy \(^{AC}\) 3.2.35 (Making choices)
If an assumption in a proof has the form \(\forall x \in X, \, \exists y \in Y, \, p(x,y)\), then we may make a choice, for each \(a \in A\), of a particular element \(b = b_a \in B\) for which \(p(a,b)\) is true.
</markdown><markdown>
## Back to inverses

We now return to the converse of Exercise 3.2.31.

### Proposition 3.2.36
Every surjection has a right inverse.

**Proof**  
Let \( f : X \to Y \) be a surjection, and define \( g : Y \to X \) as follows. Given \( y \in Y \), define \( g(y) \) to be a particular choice of \( x \in X \) such that \( f(x) = y \)‚Äînote that there exists such an element \( x \in X \) since \( f \) is surjective, so \( g \) exists by Strategy 3.2.35. Then by definition of \( g \) we have \( f(g(y)) = y \) for all \( y \in Y \), so that \( g \) is a surjection. ‚ñ°

It seems logical that we might be able to classify bijections as being those functions which have a left inverse and a right inverse. We can actually say something stronger‚Äîthe left and right inverse can be taken to be the same function! (In fact, Proposition 3.2.42 establishes that they are necessarily the same function.)

### Definition 3.2.37
Let \( f : X \to Y \) be a function. A (two-sided) inverse for \( f \) is a function \( g : Y \to X \) which is both a left inverse and a right inverse for \( f \).

It is customary to simply say ‚Äòinverse‚Äô rather than ‚Äòtwo-sided inverse‚Äô.

### Example 3.2.38
Let \( D \) be the set of dyadic rational numbers, as defined in Example 3.2.18. There, we defined a function \( f : D \to D \) defined by \( f(x) = \frac{x}{2^k} \) for all \( x \in D \), where \( k \) is some fixed natural number. We find an inverse for \( f \).

Define \( g : D \to D \) by \( g(x) = 2^k x \). Then

- \( g \) is a left inverse for \( f \). To see this, note that for all \( x \in D \) we have
  \[
  g(f(x)) = g\left(\frac{x}{2^k}\right) = 2^k \cdot \frac{x}{2^k} = x
  \]

- \( g \) is a right inverse for \( f \). To see this, note that for all \( y \in D \) we have
  \[
  f(g(y)) = f(2^k y) = \frac{2^k y}{2^k} = y
  \]

Since \( g \) is a left inverse for \( f \) and a right inverse for \( f \), it is a two-sided inverse for \( f \).
</markdown><markdown>
### Exercise 3.2.39

The following functions have two-sided inverses. For each, find its inverse and prove that it is indeed an inverse.

(a) \( f : \mathbb{R} \to \mathbb{R} \) defined by \( f(x) = \frac{2x+1}{3} \).

(b) \( g : \mathcal{P}(\mathbb{N}) \to \mathcal{P}(\mathbb{N}) \) defined by \( g(X) = \mathbb{N} \setminus X \).

(c) \( h : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) defined by \( h(m, n) = 2^m(2n+1) - 1 \) for all \( m, n \in \mathbb{N} \).

In light of the correspondences between injections and left inverses, and surjections and right inverses, it may be unsurprising that there is a correspondence between bijections and two-sided inverses.

### Exercise 3.2.40

Let \( f : X \to Y \) be a function. Then \( f \) is bijective if and only if \( f \) has an inverse.

### Strategy 3.2.41 (Proving a function is bijective by finding an inverse)

In order to prove that a function \( f : X \to Y \) is bijective, it suffices to find a function \( g : Y \to X \) such that \( g(f(x)) = x \) for all \( x \in X \) and \( f(g(y)) = y \) for all \( y \in Y \).

When proving a function \( f : X \to Y \) is bijective by finding an inverse \( g : Y \to X \), it is important to check that \( g \) is both a left inverse and a right inverse for \( f \). If you only prove that \( g \) is a left inverse for \( f \), for example, then you have only proved that \( f \) is injective!

It turns out that if a function has both a left and a right inverse, then they must be equal. This is the content of the following proposition.

### Proposition 3.2.42

Let \( f : X \to Y \) be a function and suppose \( \ell : Y \to X \) is a left inverse for \( f \) and \( r : Y \to X \) is a right inverse for \( f \). Then \( \ell = r \).

**Proof**

The proof is deceptively simple:

\[
\begin{align*}
\ell &= \ell \circ \text{id}_Y & \text{by definition of identity functions} \\
&= \ell \circ (f \circ r) & \text{since } r \text{ is a right inverse for } f \\
&= (\ell \circ f) \circ r & \text{by Exercise 3.1.22} \\
&= \text{id}_X \circ r & \text{since } \ell \text{ is a left inverse for } f \\
&= r & \text{by definition of identity functions}
\end{align*}
\]
</markdown><markdown>
There is some intuition behind why the left and right inverses of a function \( f : X \to Y \) should be equal if they both exist.

- A left inverse \(\ell : Y \to X\) exists only if \( f \) is injective. It looks at each element \( y \in Y \) and, if it is in the image of \( f \), returns the (unique) value \( x \in X \) for which \( f(x) = y \).

- A right inverse \( r : Y \to X \) exists only if \( f \) is surjective. It looks at each element \( y \in Y \) and picks out one of the (possibly many) values \( x \in X \) for which \( f(x) = y \).

When \( f \) is a bijection, every element of \( Y \) is in the image of \( f \) (by surjectivity), and is a value of \( f \) at a unique element of \( X \) (by injectivity), and so the left and right inverses are forced to return the same value on each input‚Äîhence they are equal.

It follows from [Proposition 3.2.42](#) that, for any function \( f : X \to Y \), any two inverses for \( f \) are equal‚Äîthat is, every bijective function has a **unique** inverse!

‚ú¶ **Notation 3.2.43**  
Let \( f : X \to Y \) be a function. Write \( f^{-1} : Y \to X \) to denote the (unique) inverse for \( f \), if it exists.

‚ú£ **Proposition 3.2.44**  
Let \( f : X \to Y \) be a bijection. A function \( g : Y \to X \) is a left inverse for \( f \) if and only if it is a right inverse for \( f \).

**Proof**  
We will prove the two directions separately.

- (\( \Rightarrow \)) Suppose \( g : Y \to X \) is a left inverse for \( f \)‚Äîthat is, \( g(f(x)) = x \) for all \( x \in X \). We prove that \( f(g(y)) = y \) for all \( y \in Y \), thus establishing that \( g \) is a right inverse for \( f \). So let \( y \in Y \). Since \( f \) is a bijection, it is in particular a surjection, so there exists \( x \in X \) such that \( y = f(x) \). But then
  \[
  \begin{align*}
  f(g(y)) &= f(g(f(x))) & \text{since } y = f(x) \\
  &= f(x) & \text{since } g(f(x)) = x \\
  &= y & \text{since } y = f(x)
  \end{align*}
  \]
  So indeed \( g \) is a right inverse for \( f \).

- (\( \Leftarrow \)) Suppose \( g : Y \to X \) is a right inverse for \( f \)‚Äîthat is, \( f(g(y)) = y \) for all \( y \in Y \). We prove that \( g(f(x)) = x \) for all \( x \in X \), thus establishing that \( g \) is a left inverse for \( f \). So let \( x \in X \). Letting \( y = f(x) \), we have \( f(g(y)) = y \) since \( g \) is a right inverse for \( f \). This says precisely that \( f(g(f(x))) = f(x) \), since \( y = f(x) \). By injectivity of \( f \), we have \( g(f(x)) = x \), as required.

‚úé **Exercise 3.2.45**  
Let \( f : X \to Y \) be a bijection. Prove that \( f^{-1} : Y \to X \) is a bijection.
</markdown><markdown>
### Exercise 3.2.46

Let \( f : X \to Y \) and \( g : Y \to Z \) be bijections. Prove that \( g \circ f : X \to Z \) is a bijection, and write an expression for its inverse in terms of \( f^{-1} \) and \( g^{-1} \).

### Exercise 3.2.47

Let \( f : X \to A \) and \( g : Y \to B \) be bijections. Prove that there is a bijection \( X \times Y \to A \times B \), and describe its inverse.

At the beginning of this section we motivated the definitions of injections, surjections and bijections by using them to compare two quantities (of dots and stars)‚Äîhowever, as you might have noticed, we have not yet actually proved that this intuition aligns with reality. For example, how do we know that if there is an injection \( f : X \to Y \) then \( Y \) has at least as many elements as \( X \)?

Answering this seemingly simple question is surprisingly difficult and has different answers depending on whether the sets involved are finite or infinite. In fact, the words ‚Äòfinite‚Äô, ‚Äòinfinite‚Äô and ‚Äòsize‚Äô are themselves defined in terms of injections, surjections and bijections! We therefore leave this task to future sections.

In [Section 6.1](#), we define what it means for a set to be finite and what the size of a finite set is ([Definition 6.1.1](#)), and then prove that the sizes of finite sets can be compared by finding an injection, surjection or bijection between them [Theorem 6.1.7](#).

Comparing the sizes of infinite sets, and even defining what ‚Äòsize‚Äô means for infinite sets, is another can of worms entirely and leads to some fascinating mathematics. For example, we can prove some counterintuitive results, such as the set \(\mathbb{N}\) of natural numbers and the set \(\mathbb{Q}\) of rational numbers have the same size. The journey down this rabbit hole begins in [Chapter 10](#).
</markdown><markdown>
## TL;DR ‚Äî summary of [Section 3.2](#)

### Injections, surjections and bijections

**3.2.1** A function \( f : X \to Y \) is **injective** if, for all \( a, b \in X \), if \( f(a) = f(b) \), then \( a = b \); this is equivalent to saying that distinct elements of \( X \) are mapped by \( f \) to distinct elements of \( Y \).

**3.2.9** A function \( f : X \to Y \) is **surjective** if, for all \( y \in Y \), there exists \( x \in X \) such that \( f(x) = y \); this is equivalent to saying that the image of \( f \) is its entire codomain.

**3.2.17** A function \( f : X \to Y \) is **bijective** if it is both injective and surjective.

### Inverses

**3.2.23** A **left inverse** for a function \( f : X \to Y \) is a function \( g : Y \to X \) such that \( g \circ f = \text{id}_X \). If \( f \) has a left inverse, then it is injective; conversely, if \( f \) is injected and \( X \) is inhabited, then \( f \) has a left inverse.

**3.2.29** A **right inverse** for a function \( f : X \to Y \) is a function \( g : Y \to X \) such that \( f \circ g = \text{id}_Y \). If \( f \) has a right inverse, then it is surjective; conversely, assuming the axiom of choice, if \( f \) is surjective, then it has a right inverse.

**3.2.37** An **inverse** for a function \( f : X \to Y \) is a function \( g : Y \to X \) that is both a left inverse and a right inverse for \( f \). A function \( f \) is bijective if and only if it has an inverse.

**3.2.42** If a function \( f : X \to Y \) has both a left and a right inverse, then they are equal; in particular, every bijection has a unique inverse, denoted by \( f^{-1} : Y \to X \).
</markdown><markdown>
## Section 3.E
# Chapter 3 exercises

3.1. For each of the following equations, determine whether there exists a function \( f : \mathbb{R} \to \mathbb{R} \) such that, for all \( x, y \in \mathbb{R} \), the equation holds if and only if \( y = f(x) \).

(a) \( x + y = 1 \)  
(b) \( x^2 + y^2 = 1 \)  
(c) \( x = 0 \)  
(d) \( y = 0 \)  
(e) \( (1 + x^2)y = 1 \)  
(f) \( (1 - x^2)y = 0 \)

3.2. Let \( X \) be a set. Prove that

\[
\forall a \in X, \exists! U \in \mathcal{P}(X), (a \in U \land \exists! x \in X, x \in U)
\]

Give an explicit description of the function \( X \to \mathcal{P}(X) \) that is suggested by this logical formula.

3.3. Show that there is only one function whose codomain is empty. What is its domain?

‚ú¶ **Definition 3.E.1**  
A function \( f : \mathbb{R} \to \mathbb{R} \) is even if \( f(-x) = f(x) \) for all \( x \in \mathbb{R} \), and it is odd if \( f(-x) = -f(x) \) for all \( x \in \mathbb{R} \).

3.4. Let \( n \in \mathbb{N} \). Prove that the function \( f : \mathbb{R} \to \mathbb{R} \) defined by \( f(x) = x^n \) for all \( x \in \mathbb{R} \) is even if and only if \( n \) is even, and odd if and only if \( n \) is odd.

3.5. Prove that there is a unique function \( f : \mathbb{R} \to \mathbb{R} \) that is both even and odd.

3.6. Let \( U \subseteq \mathbb{R} \), and let \( \chi_U : \mathbb{R} \to \{0, 1\} \) be the indicator function of \( U \).

(a) Prove that \( \chi_U \) is an even function if and only if \( U = \{-u \mid u \in U\} \);

(b) Prove that \( \chi_U \) is an odd function if and only if \( \mathbb{R} \setminus U = \{-u \mid u \in U\} \).

3.7. Prove that for every function \( f : \mathbb{R} \to \mathbb{R} \), there is a unique even function \( g : \mathbb{R} \to \mathbb{R} \) and a unique odd function \( h : \mathbb{R} \to \mathbb{R} \) such that \( f(x) = g(x) + h(x) \) for all \( x \in \mathbb{R} \).

3.8. Let \( \{\theta_n : [n] \to [n] \mid n \in \mathbb{N}\} \) be a family of functions such that \( f \circ \theta_m = \theta_n \circ f \) for all \( f : [m] \to [n] \). Prove that \( \theta_n = \text{id}_{[n]} \) for all \( n \in \mathbb{N} \).

3.9. Let \( X \) be a set and let \( U, V \subseteq X \). Describe the indicator function \( \chi_{U \Delta V} \) of the symmetric difference of \( U \) and \( V \) (see Definition 2.E.1) in terms of \( \chi_U \) and \( \chi_V \).

3.10. [This question assumes some familiarity with integral calculus.] A lie that is commonly told to calculus students is that

\[
\int \frac{1}{x} \, dx = \log (|x|) + c
\]
</markdown><markdown>
where log denotes the natural logarithm function and \( c \) is an arbitrary real constant. Prove that it is actually the case that

\[
\int \frac{1}{x} \, dx = \log(|x|) + a \chi_U(x) + b \chi_V(x)
\]

where \( a \) and \( b \) are arbitrary real constants, and \( U \) and \( V \) are particular (inhabited) subsets of \( \mathbb{R} \setminus \{0\} \) that you should determine.

## Images and preimages

In Questions 3.11 to 3.14, find the image \( f[U] \) of the subset \( U \) of the domain of the function \( f \) described in the question.

3.11. \( f : \mathbb{R} \to \mathbb{R}; \ f(x) = \sqrt{1 + x^2} \) for all \( x \in \mathbb{R}; \ U = \mathbb{R}. \)

3.12. \( f : \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}; \ f(a, b) = a + 2b \) for all \( (a, b) \in \mathbb{Z} \times \mathbb{Z}; \ U = \{1\} \times \mathbb{Z}. \)

3.13. \( f : \mathbb{N} \to \mathcal{P}(\mathbb{N}); \ f(0) = \emptyset \) and \( f(n + 1) = f(n) \cup \{n\} \) for all \( n \in \mathbb{N}; \ U = \mathbb{N}. \)

3.14. \( f : \mathbb{R}^R \to \mathbb{R}^R \) (where \( \mathbb{R}^R \) is the set of all functions \( \mathbb{R} \to \mathbb{R} \); \( f(h)(x) = h(|x|) \) for all \( h \in \mathbb{R}^R \) and all \( x \in \mathbb{R}; \ U = \mathbb{R}^R. \)

In Questions 3.15 to 3.17, find the preimage \( f^{-1}[V] \) of the subset \( V \) of the codomain of the function \( f \) described in the question.

3.15. \( f : \mathbb{R} \to \mathbb{R}; \ f(x) = \sqrt{1 + x^2} \) for all \( x \in \mathbb{R}; \ V = (-5, 5]. \)

3.16. \( f : \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}; \ f(a, b) = a + 2b \) for all \( (a, b) \in \mathbb{Z} \times \mathbb{Z}; \ V = \{n \in \mathbb{Z} \mid n \text{ is odd}\}. \)

3.17. \( f : \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}) \to \mathcal{P}(\mathbb{N}); \ f(A, B) = A \cap B \) for all \( (A, B) \in \mathcal{P}(\mathbb{N}) \times \mathcal{P}(\mathbb{N}); \ V = \{\emptyset\}. \)

3.18. Let \( f : X \to Y \) be a function. For each of the following statements, either prove it is true or find a counterexample.

(a) \( U \subseteq f^{-1}[f[U]] \) for all \( U \subseteq X; \)

(b) \( f^{-1}[f[U]] \subseteq U \) for all \( U \subseteq X; \)

(c) \( V \subseteq f[f^{-1}[V]] \) for all \( V \subseteq Y; \)

(d) \( f[f^{-1}[V]] \subseteq V \) for all \( V \subseteq Y. \)

3.19. Let \( f : X \to Y \) be a function, let \( A \) be a set, and let \( p : X \to A \) and \( i : A \to Y \) be functions such that the following conditions hold:

(i) \( i \) is injective;

(ii) \( i \circ p = f; \) and

(iii) If \( q : X \to B \) and \( j : B \to Y \) are functions such that \( j \) is injective and \( j \circ q = f, \) then there is a unique function \( u : A \to B \) such that \( j \circ u = i. \)
</markdown><markdown>
Prove that there is a unique bijection \( \nu : A \to f[X] \) such that \( i(a) = \nu(a) \) for all \( a \in f[X] \).

### 3.20.
Let \( f : X \to Y \) be a function and let \( U, V \subseteq Y \). Prove that:

(a) \( f^{-1}[U \cap V] = f^{-1}[U] \cap f^{-1}[V] \);

(b) \( f^{-1}[U \cup V] = f^{-1}[U] \cup f^{-1}[V] \); and

(c) \( f^{-1}[Y \setminus U] = X \setminus f^{-1}[U] \).

Thus preimages preserve the basic set operations.

### 3.21.
Let \( f : X \to Y \) and \( g : Y \to Z \) be functions.

(a) Prove that \( (g \circ f)[U] = g[f[U]] \) for all \( U \subseteq X \);

(b) Prove that \( (g \circ f)^{-1}[W] = f^{-1}[g^{-1}[W]] \) for all \( W \subseteq Z \).

## Injections, surjections and bijections

### 3.22.
(a) Prove that, for all functions \( f : X \to Y \) and \( g : Y \to Z \), if \( g \circ f \) is bijective, then \( f \) is injective and \( g \) is surjective.

(b) Find an example of a function \( f : X \to Y \) and a function \( g : Y \to Z \) such that \( g \circ f \) is bijective, \( f \) is not surjective and \( g \) is not injective.

### 3.23.
For each of the following pairs \( (U, V) \) of subsets of \( \mathbb{R} \), determine whether the specification ‚Äò\( f(x) = x^2 - 4x + 7 \) for all \( x \in U \)‚Äô defines a function \( f : U \to V \) and, if it does, determine whether \( f \) is injective and whether \( f \) is surjective.

(a) \( U = \mathbb{R} \) and \( V = \mathbb{R} \);

(b) \( U = (1, 4) \) and \( V = [3, 7) \);

(c) \( U = [3, 4) \) and \( V = [4, 7) \);

(d) \( U = (3, 4] \) and \( V = [4, 7) \);

(e) \( U = [2, \infty) \) and \( V = [3, \infty) \);

(f) \( U = [2, \infty) \) and \( V = \mathbb{R} \).

### 3.24.
For each of the following pairs of sets \( X \) and \( Y \), find (with proof) a bijection \( f : X \to Y \).

(a) \( X = \mathbb{Z} \) and \( Y = \mathbb{N} \);

(b) \( X = \mathbb{R} \) and \( Y = (-1, 1) \);

(c) \( X = [0, 1] \) and \( Y = (0, 1) \);

(d) \( X = [a, b] \) and \( Y = (c, d) \), where \( a, b, c, d \in \mathbb{R} \) with \( a < b \) and \( c < d \).
</markdown><markdown>
3.25. Prove that the function \( f : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) defined by \( f(a,b) = \left( \frac{a + b + 1}{2} \right) + b \) for all \( (a,b) \in \mathbb{N} \times \mathbb{N} \) is a bijection.

3.26. Let \( e : X \to X \) be a function such that \( e \circ e = e \). Prove that there exist a set \( Y \) and functions \( f : X \to Y \) and \( g : Y \to X \) such that \( g \circ f = e \) and \( f \circ g = \text{id}_Y \).

**True‚ÄìFalse questions**

In Questions 3.27 to 3.34, determine (with proof) whether the statement is true or false.

3.27. The set \( G = \{(x,y) \in \mathbb{R} \times \mathbb{R} \mid x^2 = y^2\} \) is the graph of a function.

3.28. The set \( G = \{(x,y) \in \mathbb{Z} \times \mathbb{N} \mid x^2 = y^2\} \) is the graph of a function.

3.29. Every function with empty domain has an empty codomain.

3.30. Every function with empty codomain has an empty domain.

3.31. The image of a function is a subset of its domain.

3.32. Given a function \( f : X \to Y \), the assignment \( V \mapsto f^{-1}[V] \) defines a function \( \mathcal{P}(Y) \to \mathcal{P}(X) \).

3.33. Let \( f, g \) and \( h \) be composable functions. If \( h \circ g \circ f \) is injective, then \( g \) is injective.

3.34. Every left inverse is surjective and every right inverse is injective.

**Always‚ÄìSometimes‚ÄìNever questions**

In Questions 3.35 to 3.46, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

3.35. Let \( f : X \to Y \) be a function and let \( U \subseteq X \). Then there is a function \( g : U \to Y \) defined by \( g(x) = f(x) \) for all \( x \in U \).

3.36. Let \( f : X \to Y \) be a function and let \( V \subseteq Y \). Then there is a function \( g : X \to V \) defined by \( g(x) = f(x) \) for all \( x \in X \).

3.37. Let \( X \) be a set. Then there is a unique function \( X \to \{0\} \).

3.38. Let \( X \) be a set. Then there is a unique function \( X \to \emptyset \).

3.39. Let \( X \) and \( Y \) be sets and let \( G \subseteq X \times Y \). Then \( G \) is the graph of a function \( f : X \to Y \).

3.40. Let \( f : \{1, 2, 3\} \to \{1, 2\} \), let \( G = \text{Gr}(f) \) and let \( G' = \{(y,x) \mid (x,y) \in G\} \). Then \( G' \) is the graph of a function \( \{1, 2\} \to \{1, 2, 3\} \).
</markdown><markdown>
3.41. Let \( f : X \to Y \) be a function and let \( U \subseteq X \) be inhabited. Then \( f[U] \) is inhabited.

3.42. Let \( f : X \to Y \) be a function and let \( V \subseteq Y \) be inhabited. Then \( f^{-1}[V] \) is inhabited.

3.43. Let \( f : \{1, 2\} \to \{1, 2, 3\} \) be a function. Then \( f \) is injective.

3.44. Let \( f : \{1, 2\} \to \{1, 2, 3\} \) be a function. Then \( f \) is surjective.

3.45. Let \( a, b, c, d \in \mathbb{R} \) and define \( f : \mathbb{R}^2 \to \mathbb{R}^2 \) by \( f(x, y) = (ax + by, cx + dy) \) for all \( (x, y) \in \mathbb{R}^2 \). Then \( f \) is injective if and only if \( f \) is surjective.

3.46. Let \( U, V \subseteq \mathbb{R} \) and suppose that \( x^2 \in V \) for all \( x \in U \). The function \( f : U \to V \) defined by \( f(x) = x^2 \) is injective.
</markdown>It seems the page is blank. There is no text to extract.<markdown>
# Chapter 4

## Mathematical induction
</markdown><markdown>
## Section 4.1

# Peano‚Äôs axioms

The purpose of this section is to forget everything we think we know about the natural numbers, and reconstruct our former knowledge (and more!) using the following fundamental property:

> Every natural number can be obtained in a unique way by  
> starting from zero and adding one some finite number of times.

This is slightly imprecise‚Äîit is not clear what is meant by ‚Äòadding one some finite number of times‚Äô, for example. Worse still, we are going to define what ‚Äòfinite‚Äô means in terms of natural numbers in Section 6.1, so we‚Äôd better not refer to finiteness in our definition of natural numbers!

The following definition captures precisely the properties that we need in order to characterise the idea of \(\mathbb{N}\) that we have in our minds. To begin with, \(\mathbb{N}\) should be a set. Whatever the elements of this set \(\mathbb{N}\) actually are, we will think about them as being natural numbers. One of the elements, in particular, should play the role of the natural number 0‚Äîthis will be the zero element \(z \in \mathbb{N}\); and there should be a notion of ‚Äòadding one‚Äô‚Äîthis will be the successor function \(s : \mathbb{N} \rightarrow \mathbb{N}\). Thus given an element \(n \in \mathbb{N}\), though of as a natural number, we think about the element \(s(n)\) as the natural number ‚Äò\(n + 1\)‚Äô. Note that this is strictly for the purposes of intuition: we will define ‚Äò+‚Äô and ‚Äò1‚Äô in terms of \(z\) and \(s\), not vice versa.

### Definition 4.1.1

A notion of natural numbers is a set \(\mathbb{N}\), together with an element \(z \in \mathbb{N}\), called a zero element, and a function \(s : \mathbb{N} \rightarrow \mathbb{N}\) called a successor function, satisfying the following properties:

(i) \(z \not\in s[\mathbb{N}]\); that is, \(z \neq s(n)\) for any \(n \in \mathbb{N}\).

(ii) \(s\) is injective; that is, for all \(m, n \in \mathbb{N}\), if \(s(m) = s(n)\), then \(m = n\).

(iii) \(\mathbb{N}\) is generated by \(z\) and \(s\); that is, for all sets \(X\), if \(z \in X\) and for all \(n \in \mathbb{N}\) we have \(n \in X \Rightarrow s(n) \in X\), then \(\mathbb{N} \subseteq X\).

The properties (i), (ii) and (iii) are called Peano‚Äôs axioms.

Note that Definition 4.1.1 does not specify what \(\mathbb{N}\), \(z\) and \(s\) actually are; it just specifies the properties that they must satisfy. It turns out that it doesn‚Äôt really matter what notion
</markdown><markdown>
of natural numbers we use, since any two notions are essentially the same. We will not worry about the specifics here‚Äîthat task is left to Section B.2: a particular notion of natural numbers is defined in Construction B.2.5, and the fact that all notions of natural numbers are ‚Äòessentially the same‚Äô is made precise and proved in Theorem B.2.8.

We can define all the concepts involving natural numbers that we are familiar with, and prove all the properties that we take for granted, just from the element \( z \in \mathbb{N} \) and the successor function \( s : \mathbb{N} \to \mathbb{N} \).

For instance, we define ‚Äò0‚Äô to mean \( z \), define ‚Äò1‚Äô to mean \( s(z) \), define ‚Äò2‚Äô to mean \( s(s(z)) \), and so on. Thus ‚Äò12‚Äô is defined to mean

\[
s(s(s(s(s(s(s(s(s(s(s(s(z))))))))))))
\]

From now on, then, let‚Äôs write 0 instead of \( z \) for the zero element of \( \mathbb{N} \). It would be nice if we could write ‚Äò\( n + 1 \)‚Äô instead of \( s(n) \), but we must first define what ‚Äò\(+\)‚Äô means. In order to do this, we need a way of defining expressions involving natural numbers; this is what the recursion theorem allows us to do.

### Theorem 4.1.2 (Recursion theorem)
Let \( X \) be a set. For all \( a \in X \) and all \( h : \mathbb{N} \times X \to X \), there is a unique function \( f : \mathbb{N} \to X \) such that \( f(0) = a \) and \( f(s(n)) = h(n, f(n)) \) for all \( n \in \mathbb{N} \).

**Proof**  
Let \( a \in X \) and \( h : \mathbb{N} \times X \to X \). We prove existence and uniqueness of \( f \) separately.

- Define \( f : \mathbb{N} \to X \) by specifying \( f(0) = a \) and \( f(s(n)) = h(n, f(n)) \). Since \( h \) is a function and \( s \) is injective, existence and uniqueness of \( x \in X \) such that \( f(n) = x \) is guaranteed, provided that \( f(n) \) is defined, so we need only verify totality.

So let \( D = \{ n \in \mathbb{N} \mid f(n) \text{ is defined} \} \). Then:

- \( 0 \in D \), since \( f(0) \) is defined to be equal to \( a \).
- Let \( n \in \mathbb{N} \) and suppose \( n \in D \). Then \( f(n) \) is defined and \( f(s(n)) = h(n, f(n)) \), so that \( f(s(n)) \) is defined, and hence \( s(n) \in D \).

By condition (iii) of Definition 4.1.1, we have \( \mathbb{N} \subseteq D \), so that \( f(n) \) is defined for all \( n \in \mathbb{N} \), as required.

- To see that \( f \) is unique, suppose \( g : \mathbb{N} \to X \) were another function such that \( g(0) = a \) and \( g(s(n)) = h(n, g(n)) \) for all \( n \in \mathbb{N} \).

To see that \( f = g \), let \( E = \{ n \in \mathbb{N} \mid f(n) = g(n) \} \). Then

- \( 0 \in E \), since \( f(0) = a = g(0) \).
</markdown><markdown>
‚ãÑ Let \( n \in \mathbb{N} \) and suppose that \( n \in E \). Then \( f(n) = g(n) \), and so

\[
f(s(n)) = h(n, f(n)) = h(n, g(n)) = g(s(n))
\]

and so \( s(n) \in E \).

Again, condition (iii) of [Definition 4.1.1](#) is satisfied, so that \( \mathbb{N} \subseteq E \). It follows that \( f(n) = g(n) \) for all \( n \in \mathbb{N} \), and so \( f = g \).

Thus we have established the existence and uniqueness of a function \( f : \mathbb{N} \to X \) such that \( f(0) = a \) and \( f(s(n)) = h(n, f(n)) \) for all \( n \in \mathbb{N} \).

The recursion theorem allows us to define expressions involving natural numbers by recursion; this is [Strategy 4.1.3](#).

‚ú∂ **Strategy 4.1.3 (Definition by recursion)**

In order to specify a function \( f : \mathbb{N} \to X \), it suffices to define \( f(0) \) and, for given \( n \in \mathbb{N} \), assume that \( f(n) \) has been defined, and define \( f(s(n)) \) in terms of \( n \) and \( f(n) \).

‚úé **Example 4.1.4**

We can use recursion to define addition on the natural numbers as follows.

For fixed \( m \in \mathbb{N} \), we can define a function \( \text{add}_m : \mathbb{N} \to \mathbb{N} \) by recursion by:

\[
\text{add}_m(0) = m \quad \text{and} \quad \text{add}_m(s(n)) = s(\text{add}_m(n)) \quad \text{for all } n \in \mathbb{N}
\]

In more familiar notation, for \( m, n \in \mathbb{N} \), define the expression ‚Äò\( m + n \)‚Äô to mean \( \text{add}_m(n) \). Another way of expressing the recursive definition of \( \text{add}_m(n) \) is to say that, for each \( m \in \mathbb{N} \), we are defining \( m + n \) by recursion on \( n \) as follows:

\[
m + 0 = m \quad \text{and} \quad m + s(n) = s(m + n) \quad \text{for all } n \in \mathbb{N}
\]

We can use the recursive definition of addition to prove familiar equations between numbers. The following proposition is a proof that \( 2 + 2 = 4 \). This may seem silly, but notice that the expression ‚Äò\( 2 + 2 = 4 \)‚Äô is actually shorthand for the following:

\[
\text{add}_{s(s(0))}(s(s(0))) = s(s(s(s(0))))
\]

We must therefore be careful to apply the definitions in its proof.

‚ú∂ **Proposition 4.1.5**

\( 2 + 2 = 4 \)
</markdown><markdown>
**Proof**

We use the recursive definition of addition.

\[
\begin{align*}
2 + 2 &= 2 + s(1) & \text{since } 2 = s(1) \\
&= s(2 + 1) & \text{by definition of } + \\
&= s(2 + s(0)) & \text{since } 1 = s(0) \\
&= s(s(2 + 0)) & \text{by definition of } + \\
&= s(s(2)) & \text{by definition of } + \\
&= s(3) & \text{since } 3 = s(2) \\
&= 4 & \text{since } 4 = s(3)
\end{align*}
\]

as required.

The following result allows us to drop the notation ‚Äò\(s(n)\)‚Äô and just write ‚Äò\(n + 1\)‚Äô instead.

**Proposition 4.1.6**

For all \(n \in \mathbb{N}\), we have \(s(n) = n + 1\).

**Proof**

Let \(n \in \mathbb{N}\). Then by the recursive definition of addition we have

\[
n + 1 = n + s(0) = s(n + 0) = s(n)
\]

as required.

In light of Proposition 4.1.6, we will now abandon the notation \(s(n)\), and write \(n + 1\) instead.

We can define the arithmetic operations of multiplication and exponentiation by recursion, too.

**Example 4.1.7**

Fix \(m \in \mathbb{N}\). Define \(m \cdot n\) for all \(n \in \mathbb{N}\) by recursion on \(n\) as follows:

\[
m \cdot 0 = 0 \quad \text{and} \quad m \cdot (n + 1) = (m \cdot n) + m \text{ for all } n \in \mathbb{N}
\]

Formally, what we have done is define a function \(\text{mult}_m : \mathbb{N} \to \mathbb{N}\) recursively by \(\text{mult}_m(z) = z\) and \(\text{mult}_m(s(n)) = \text{add}_{\text{mult}_m(n)}(m)\) for all \(n \in \mathbb{N}\). But the definition we provided is easier to understand.

**Proposition 4.1.8**

\(2 \cdot 2 = 4\)

**Proof**
</markdown><markdown>
We use the recursive definitions of addition and recursion.

\[
\begin{align*}
2 \cdot 2 &= 2 \cdot (1 + 1) & \text{since } 2 = 1 + 1 \\
&= (2 \cdot 1) + 2 & \text{by definition of } \cdot \\
&= (2 \cdot (0 + 1)) + 2 & \text{since } 1 = 0 + 1 \\
&= ((2 \cdot 0) + 2) + 2 & \text{by definition of } \cdot \\
&= (0 + 2) + 2 & \text{by definition of } \cdot \\
&= (0 + (1 + 1)) + 2 & \text{since } 2 = 1 + 1 \\
&= ((0 + 1) + 1) + 2 & \text{by definition of } + \\
&= (1 + 1) + 2 & \text{since } 1 = 0 + 1 \\
&= 2 + 2 & \text{since } 2 = 1 + 1 \\
&= 4 & \text{by Proposition 4.1.5}
\end{align*}
\]

as required.

### Exercise 4.1.9

Given \( m \in \mathbb{N} \), define \( m^n \) for all \( n \in \mathbb{N} \) by recursion on \( n \), and prove that \( 2^2 = 4 \) using the recursive definitions of exponentiation, multiplication and addition.

We could spend the rest of our lives doing long computations involving recursively defined arithmetic operations, so at this point we will stop, and return to taking for granted the facts that we know about arithmetic operations.

There are, however, a few more notions that we need to define by recursion so that we can use them in our proofs later.

### Definition 4.1.10

Given \( n \in \mathbb{N} \), the **sum** of \( n \) real numbers \( a_1, a_2, \ldots, a_n \) is the real number \( \sum_{k=1}^{n} a_k \) defined by recursion on \( n \in \mathbb{N} \) by

\[
\sum_{k=1}^{0} a_k = 0 \quad \text{and} \quad \sum_{k=1}^{n+1} a_k = \left( \sum_{k=1}^{n} a_k \right) + a_{n+1} \quad \text{for all } n \in \mathbb{N}
\]

### Definition 4.1.11

Given \( n \in \mathbb{N} \), the **product** of \( n \) real numbers \( a_1, a_2, \ldots, a_n \) is the real number \( \prod_{k=1}^{n} a_k \) defined by recursion on \( n \in \mathbb{N} \) by

\[
\prod_{k=1}^{0} a_k = 1 \quad \text{and} \quad \prod_{k=1}^{n+1} a_k = \left( \prod_{k=1}^{n} a_k \right) \cdot a_{n+1} \quad \text{for all } n \in \mathbb{N}
\]
</markdown><markdown>
### Example 4.1.12

Let \( x_i = i^2 \) for each \( i \in \mathbb{N} \). Then

\[
\sum_{i=1}^{5} x_i = 1 + 4 + 9 + 16 + 25 = 55
\]

and

\[
\prod_{i=1}^{5} x_i = 1 \cdot 4 \cdot 9 \cdot 16 \cdot 25 = 14400
\]

### Exercise 4.1.13

Let \( x_1, x_2 \in \mathbb{R} \). Working strictly from the definitions of indexed sum and indexed product, prove that

\[
\sum_{i=1}^{2} x_i = x_1 + x_2 \quad \text{and} \quad \prod_{i=1}^{2} x_i = x_1 \cdot x_2
\]

## Binomials and factorials

### Definition 4.1.14 (to be redefined in Definition 8.1.10)

Let \( n \in \mathbb{N} \). The **factorial** of \( n \), written \( n! \), is defined recursively by

\[
0! = 1 \quad \text{and} \quad (n+1)! = (n+1) \cdot n! \quad \text{for all } n \geq 0
\]

Put another way, we have

\[
n! = \prod_{i=1}^{n} i
\]

for all \( n \in \mathbb{N} \)‚Äîrecall Definition 4.1.11 to see why these definitions are really just two ways of wording the same thing.

### Definition 4.1.15 (to be redefined in Definition 8.1.4)

Let \( n, k \in \mathbb{N} \). The **binomial coefficient** \(\binom{n}{k}\) (LaTeX code: `\binom{n}{k}`) (read ‚Äòn choose k‚Äô) is defined by recursion on \( n \) and on \( k \) by

\[
\binom{n}{0} = 1, \quad \binom{0}{k+1} = 0, \quad \binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}
\]

This definition gives rise to an algorithm for computing binomial coefficients: they fit into a diagram known as **Pascal‚Äôs triangle**, with each binomial coefficient computed as the sum of the two lying above it (with zeroes omitted):
</markdown><markdown>
\[
\begin{array}{cccccc}
& & & \binom{0}{0} & & \\
& & \binom{1}{0} & & \binom{1}{1} & \\
& \binom{2}{0} & & \binom{2}{1} & & \binom{2}{2} \\
\binom{3}{0} & & \binom{3}{1} & & \binom{3}{2} & & \binom{3}{3} \\
& \binom{4}{0} & & \binom{4}{1} & & \binom{4}{2} & & \binom{4}{3} & & \binom{4}{4} \\
& & \binom{5}{0} & & \binom{5}{1} & & \binom{5}{2} & & \binom{5}{3} & & \binom{5}{4} & & \binom{5}{5} \\
& & \vdots & & \vdots & & \vdots & & \vdots & & \vdots & & \vdots \\
\end{array}
\]

\[
\begin{array}{cccccc}
1 & & & & & \\
1 & 1 & & & & \\
1 & 2 & 1 & & & \\
1 & 3 & 3 & 1 & & \\
1 & 4 & 6 & 4 & 1 & \\
1 & 5 & 10 & 10 & 5 & 1 \\
\vdots & & \vdots & & \vdots & & \vdots \\
\end{array}
\]

**Exercise 4.1.16**  
Write down the next two rows of Pascal‚Äôs triangle.
</markdown><markdown>
## Section 4.2
### Weak induction

Just as recursion exploited the structure of the natural numbers to *define expressions* involving natural numbers, induction exploits the very same structure to *prove results* about natural numbers.

#### Theorem 4.2.1 (Weak induction principle)
Let \( p(n) \) be a logical formula with free variable \( n \in \mathbb{N} \), and let \( n_0 \in \mathbb{N} \). If

(i) \( p(n_0) \) is true; and

(ii) For all \( n \geq n_0 \), if \( p(n) \) is true, then \( p(n+1) \) is true;

then \( p(n) \) is true for all \( n \geq n_0 \).

**Proof**

Define \( X = \{ n \in \mathbb{N} \mid p(n_0 + n) \text{ is true} \} \); that is, given a natural number \( n \), we have \( n \in X \) if and only if \( p(n_0 + n) \) is true. Then

- \( 0 \in X \), since \( n_0 + 0 = n_0 \) and \( p(n_0) \) is true by (i).

- Let \( n \in \mathbb{N} \) and assume \( n \in X \). Then \( p(n_0 + n) \) is true. Since \( n_0 + n \geq n_0 \) and \( p(n_0 + n) \) is true, we have \( p(n_0 + n + 1) \) is true by (ii). But then \( n + 1 \in X \).

So by Definition 4.1.1(iii) we have \( \mathbb{N} \subseteq X \). Hence \( p(n_0 + n) \) is true for all \( n \in \mathbb{N} \). But this is equivalent to saying that \( p(n) \) is true for all \( n \geq n_0 \).

#### Strategy 4.2.2 (Proof by (weak) induction)
In order to prove a proposition of the form \( \forall n \geq n_0, p(n) \), it suffices to prove that:

- \( p(n_0) \) is true; and

- For all \( n \geq n_0 \), if \( p(n) \) is true, then \( p(n+1) \) is true.

Some terminology has evolved for proofs by induction:

- The proof of \( p(n_0) \) is called the **base case**;

- The proof of \( \forall n \geq n_0, (p(n) \Rightarrow p(n+1)) \) is called the **induction step**;

- In the induction step, the assumption \( p(n) \) is called the **induction hypothesis**;

- In the induction step, the proposition \( p(n+1) \) is called the **induction goal**.
</markdown><markdown>
The following diagram illustrates the weak induction principle.

To interpret this diagram:

- The shaded diamond represents the base case \( p(n_0) \);
- The square represents the induction hypothesis \( p(n) \);
- The dashed circle represents the induction goal \( p(n+1) \);
- The arrow represents the implication we must prove in the induction step.

We will use analogous diagrams to illustrate the other induction principles in this section.

### Proposition 4.2.3
Let \( n \in \mathbb{N} \). Then 

\[
\sum_{k=1}^{n} k = \frac{n(n+1)}{2}
\]

**Proof**

We proceed by induction on \( n \geq 0 \).

- **(Base case)** We need to prove 

\[
\sum_{k=1}^{0} k = \frac{0(0+1)}{2}.
\]

This is true, since 

\[
\frac{0(0+1)}{2} = 0,
\]

and 

\[
\sum_{k=1}^{0} k = 0
\]

by Definition 4.1.10.

- **(Induction step)** Let \( n \geq 0 \) and suppose that 

\[
\sum_{k=1}^{n} k = \frac{n(n+1)}{2};
\]

this is the induction hypothesis.

We need to prove that 

\[
\sum_{k=1}^{n+1} k = \frac{(n+1)(n+2)}{2};
\]

this is the induction goal.
</markdown><markdown>
We proceed by calculation:

\[
\sum_{k=1}^{n+1} k = \left( \sum_{k=1}^{n} k \right) + (n+1) \quad \text{by Definition 4.1.10}
\]

\[
= \frac{n(n+1)}{2} + (n+1) \quad \text{by induction hypothesis}
\]

\[
= (n+1) \left( \frac{n}{2} + 1 \right) \quad \text{factorising}
\]

\[
= \frac{(n+1)(n+2)}{2} \quad \text{rearranging}
\]

The result follows by induction. ‚ñ°

Before moving on, let‚Äôs reflect on the proof of Proposition 4.2.3 to highlight some effective ways of writing a proof by induction.

- We began the proof by indicating that it was a proof by induction. While it is clear in this section that most proofs will be by induction, that will not always be the case, so it is good practice to indicate the proof strategy at hand.

- The base case and induction step are clearly labelled in the proof. This is not strictly necessary from a mathematical perspective, but it helps the reader to navigate the proof and to identify what the goal is at each step.

- We began the induction step by writing, ‚ÄòLet \( n \geq n_0 \) and suppose that [‚Ä¶ induction hypothesis goes here‚Ä¶]‚Äô. This is typically how your induction step should begin, since the proposition being proved in the induction step is of the form \( \forall n \geq n_0, (p(n) \Rightarrow \cdots) \).

- Before proving anything in the base case or induction step, we wrote out what it was that we were trying to prove in that part of the proof. This is helpful because it helps to remind us (and the person reading the proof) what we are aiming to achieve.

Look out for these features in the proof of the next proposition, which is also by induction on \( n \geq 0 \).

### Proposition 4.2.4
The natural number \( n^3 - n \) is divisible by 3 for all \( n \in \mathbb{N} \).

**Proof**  
We proceed by induction on \( n \geq 0 \).

- **(Base case)** We need to prove that \( 0^3 - 0 \) is divisible by 3. Well

  \[
  0^3 - 0 = 0 = 3 \times 0
  \]

  so \( 0^3 - 0 \) is divisible by 3.
</markdown><markdown>
- **(Induction step)** Let \( n \in \mathbb{N} \) and suppose that \( n^3 - n \) is divisible by 3. Then \( n^3 - n = 3k \) for some \( k \in \mathbb{Z} \).

We need to prove that \( (n+1)^3 - (n+1) \) is divisible by 3; in other words, we need to find some natural number \( \ell \) such that

\[
(n+1)^3 - (n+1) = 3\ell
\]

We proceed by computation.

\[
\begin{align*}
(n+1)^3 - (n+1) &= (n^3 + 3n^2 + 3n + 1) - n - 1 & \text{expand brackets} \\
&= n^3 - n + 3n^2 + 3n + 1 - 1 & \text{rearrange terms} \\
&= n^3 - n + 3n^2 + 3n & \text{since } 1 - 1 = 0 \\
&= 3k + 3n^2 + 3n & \text{by induction hypothesis} \\
&= 3(k + n^2 + n) & \text{factorise}
\end{align*}
\]

Thus we have expressed \( (n+1)^3 - (n+1) \) in the form \( 3\ell \) for some \( \ell \in \mathbb{Z} \); specifically, \( \ell = k + n^2 + n \).

The result follows by induction.

üìå **Exercise 4.2.5**

Prove by induction that

\[
\sum_{k=0}^{n} 2^k = 2^{n+1} - 1
\]

for all \( n \in \mathbb{N} \).

The following proposition has a proof by induction in which the base case is not zero.

‚ú£ **Proposition 4.2.6**

For all \( n \geq 4 \), we have \( 3n < 2^n \).

**Proof**

We proceed by induction on \( n \geq 4 \).

- **(Base case)** \( p(4) \) is the statement \( 3 \cdot 4 < 2^4 \). This is true, since \( 12 < 16 \).

- **(Induction step)** Suppose \( n \geq 4 \) and that \( 3n < 2^n \). We want to prove \( 3(n+1) < 2^{n+1} \).

  Well,

  \[
  \begin{align*}
  3(n+1) &= 3n + 3 & \text{expanding} \\
  &< 2^n + 3 & \text{by induction hypothesis} \\
  &< 2^n + 2^4 & \text{since } 3 < 16 = 2^4 \\
  &\leq 2^n + 2^n & \text{since } n \geq 4 \\
  &= 2 \cdot 2^n & \text{simplifying} \\
  &= 2^{n+1} & \text{simplifying}
  \end{align*}
  \]
</markdown><markdown>
So we have proved \(3(n+1) < 2^{n+1}\), as required.

The result follows by induction.

Note that the proof in Proposition 4.2.6 says nothing about the truth or falsity of \(p(n)\) for \(n = 0, 1, 2, 3\). In order to assert that these cases are false, you need to show them individually; indeed:

- \(3 \times 0 = 0\) and \(2^0 = 1\), hence \(p(0)\) is true;
- \(3 \times 1 = 3\) and \(2^1 = 2\), hence \(p(1)\) is false;
- \(3 \times 2 = 6\) and \(2^2 = 4\), hence \(p(2)\) is false;
- \(3 \times 3 = 9\) and \(2^3 = 8\), hence \(p(3)\) is false.

So we deduce that \(p(n)\) is true when \(n = 0\) or \(n \geq 4\), and false when \(n \in \{1, 2, 3\}\).

Sometimes a ‚Äòproof‚Äô by induction might appear to be complete nonsense. The following is a classic example of a ‚Äòfail by induction‚Äô:

### Example 4.2.7

The following argument supposedly proves that every horse is the same colour.

- **(Base case)** Suppose there is just one horse. This horse is the same colour as itself, so the base case is immediate.

- **(Induction step)** Suppose that every collection of \(n\) horses is the same colour. Let \(X\) be a set of \(n+1\) horses. Removing the first horse from \(X\), we see that the last \(n\) horses are the same colour by the induction hypothesis. Removing the last horse from \(X\), we see that the first \(n\) horses are the same colour. Hence all the horses in \(X\) are the same colour.

By induction, we‚Äôre done.

### Exercise 4.2.8

Write down the statement \(p(n)\) that Example 4.2.7 attempted to prove for all \(n \geq 1\). Convince yourself that the proof of the base case is correct, then write down‚Äîwith quantifiers‚Äîexactly the proposition that the induction step is meant to prove. Explain why the argument in the induction step failed to prove this proposition.

There are several ways to avoid situations like that of Example 4.2.7 by simply putting more thought into writing the proof. Some tips are:

- State \(p(n)\) explicitly. In the statement ‚Äòall horses are the same colour‚Äô it is not clear exactly what the induction variable is. However, we could have said:
</markdown><markdown>
Let \( p(n) \) be the statement ‚Äòevery set of \( n \) horses has the same colour‚Äô.

- Refer explicitly to the base case \( n_0 \) in the induction step. In Example 4.2.7, our induction hypothesis simply stated ‚Äòassume every set of \( n \) horses has the same colour‚Äô. Had we instead said:

  Let \( n \geq 1 \) and assume every set of \( n \) horses has the same colour.

  We may have spotted the error in what was to come.

What follows are a couple more examples of proofs by weak induction.

### Proposition 4.2.9

For all \( n \in \mathbb{N} \), we have

\[
\sum_{k=0}^{n} k^3 = \left( \sum_{k=0}^{n} k \right)^2.
\]

**Proof**

We proved in Proposition 4.2.6 that

\[
\sum_{k=0}^{n} k = \frac{n(n+1)}{2}
\]

for all \( n \in \mathbb{N} \), thus it suffices to prove that

\[
\sum_{k=0}^{n} k^3 = \frac{n^2(n+1)^2}{4}
\]

for all \( n \in \mathbb{N} \).

We proceed by induction on \( n \geq 0 \).

- **(Base case)** We need to prove that \( 0^3 = \frac{0^2(0+1)^2}{4} \). This is true since both sides of the equation are equal to 0.

- **(Induction step)** Fix \( n \in \mathbb{N} \) and suppose that

  \[
  \sum_{k=0}^{n} k^3 = \frac{n^2(n+1)^2}{4}.
  \]

  We need to prove
</markdown><markdown>
that

\[
\sum_{k=0}^{n+1} k^3 = \frac{(n+1)^2(n+2)^2}{4}
\]

This is true since:

\[
\begin{align*}
\sum_{i=0}^{n+1} i^3 &= \sum_{i=0}^{n} k^3 + (n+1)^3 & \text{by definition of sum} \\
&= \frac{n^2(n+1)^2}{4} + (n+1)^3 & \text{by induction hypothesis} \\
&= \frac{n^2(n+1)^2 + 4(n+1)^3}{4} & \text{(algebra)} \\
&= \frac{(n+1)^2(n^2 + 4(n+1))}{4} & \text{(algebra)} \\
&= \frac{(n+1)^2(n+2)^2}{4} & \text{(algebra)}
\end{align*}
\]

By induction, the result follows.

In the next proposition, we prove the correctness of a well-known formula for the sum of an arithmetic progression of real numbers.

### Proposition 4.2.10

Let \( a, d \in \mathbb{R} \). Then

\[
\sum_{k=0}^{n} (a + kd) = \frac{(n+1)(2a + nd)}{2}
\]

for all \( n \in \mathbb{N} \).

**Proof**

We proceed by induction on \( n \geq 0 \).

- **(Base case)** We need to prove that

\[
\sum_{k=0}^{0} (a + kd) = \frac{(0+1)(2a + 0d)}{2}
\]

This is true, since

\[
\sum_{k=0}^{0} (a + kd) = a + 0d = a = \frac{2a}{2} = \frac{1 \cdot (2a)}{2} = \frac{(0+1)(2a + 0d)}{2}
\]

- **(Induction step)** Fix \( n \in \mathbb{N} \) and suppose that

\[
\sum_{k=0}^{n} (a + kd) = \frac{(n+1)(2a + nd)}{2}
\]

We need to prove:

\[
\sum_{k=0}^{n+1} (a + kd) = \frac{(n+2)(2a + (n+1)d)}{2}
\]
</markdown><markdown>
This is true, since

\[
\sum_{k=0}^{n+1} (a + kd)
\]

\[
= \sum_{k=0}^{n} (a + kd) + (a + (n+1)d) \quad \text{by definition of sum}
\]

\[
= \frac{(n+1)(2a + nd)}{2} + (a + (n+1)d) \quad \text{by induction hypothesis}
\]

\[
= \frac{(n+1)(2a + nd) + 2a + 2(n+1)d}{2} \quad \text{(algebra)}
\]

\[
= \frac{(n+1) \cdot 2a + (n+1) \cdot nd + 2a + 2(n+1)d}{2} \quad \text{(algebra)}
\]

\[
= \frac{2a(n+1+1) + (n+1)(nd + 2d)}{2} \quad \text{(algebra)}
\]

\[
= \frac{2a(n+2) + (n+1)(n+2)d}{2} \quad \text{(algebra)}
\]

\[
= \frac{(n+2)(2a + (n+1)d)}{2} \quad \text{(algebra)}
\]

By induction, the result follows.

The following exercises generalise Exercise 4.2.5 to prove the correctness of a formula for the sum of a geometric progression of real numbers.

**Exercise 4.2.11**  
Let \( a, r \in \mathbb{R} \) with \( r \neq 1 \). Then

\[
\sum_{k=0}^{n} ar^k = \frac{a(1 - r^{n+1})}{1 - r}
\]

for all \( n \in \mathbb{N} \).

So far in this section most of the results that we have proved by induction have concerned numbers, and particularly identities involving natural numbers. But the scope of proof by induction is not limited to such results‚Äîthe next exercise, for example, concerns sets and bijections.

**Exercise 4.2.12**  
Let \( n \in \mathbb{N} \) and let \( \{X_k \mid 1 \leq k \leq n\} \) be a family of sets. Prove by induction on \( n \) that there is a bijection

\[
\prod_{k=1}^{n+1} X_k \to \left( \prod_{k=1}^{n} X_k \right) \times X_n.
\]
</markdown><markdown>
## Binomials and factorials

Proof by induction turns out to be a very useful way of proving facts about binomial coefficients \(\binom{n}{k}\) and factorials \(n!\).

### Example 4.2.13

We prove that

\[
\sum_{i=0}^{n} \binom{n}{i} = 2^n
\]

by induction on \(n\).

- **(Base case)** We need to prove \(\binom{0}{0} = 1\) and \(2^0 = 1\). These are both true by the definitions of binomial coefficients and exponents.

- **(Induction step)** Fix \(n \geq 0\) and suppose that

\[
\sum_{i=0}^{n} \binom{n}{i} = 2^n
\]

We need to prove

\[
\sum_{i=0}^{n+1} \binom{n+1}{i} = 2^{n+1}
\]

This is true, since

\[
\sum_{i=0}^{n+1} \binom{n+1}{i} = \binom{n+1}{0} + \sum_{i=1}^{n+1} \binom{n+1}{i} \quad \text{splitting the sum}
\]

\[
= 1 + \sum_{j=0}^{n} \binom{n+1}{j+1} \quad \text{letting } j = i - 1
\]

\[
= 1 + \sum_{j=0}^{n} \left( \binom{n}{j} + \binom{n}{j+1} \right) \quad \text{by Definition 4.1.15}
\]

\[
= 1 + \sum_{j=0}^{n} \binom{n}{j} + \sum_{j=0}^{n} \binom{n}{j+1} \quad \text{separating the sums}
\]

Now \(\sum_{j=0}^{n} \binom{n}{j} = 2^n\) by the induction hypothesis. Moreover, reindexing the sum using \(k = j + 1\) yields

\[
\sum_{j=0}^{n} \binom{n}{j+1} = \sum_{k=1}^{n+1} \binom{n}{k} = \sum_{k=1}^{n} \binom{n}{k} + \binom{n}{n+1}
\]
</markdown><markdown>
By the induction hypothesis, we have

\[
\sum_{k=1}^{n} \binom{n}{k} = \sum_{k=0}^{n} \binom{n}{k} - \binom{n}{0} = 2^n - 1
\]

and \(\binom{n}{n+1} = 0\), so that

\[
\sum_{j=0}^{n} \binom{n}{j+1} = 2^n - 1.
\]

Putting this together, we have

\[
1 + \sum_{j=0}^{n} \binom{n}{j} + \sum_{j=0}^{n} \binom{n}{j+1} = 1 + 2^n + (2^n - 1)
\]

\[
= 2 \cdot 2^n
\]

\[
= 2^{n+1}
\]

so the induction step is finished.

By induction, we‚Äôre done.

---

### Theorem 4.2.14

Let \( n, k \in \mathbb{N} \). Then

\[
\binom{n}{k} = 
\begin{cases} 
\frac{n!}{k!(n-k)!} & \text{if } k \leq n \\ 
0 & \text{if } k > n 
\end{cases}
\]

**Proof**

We proceed by induction on \( n \).

- **(Base case)** When \( n = 0 \), we need to prove that \(\binom{0}{k} = \frac{0!}{k!(0-k)!}\) for all \( k \leq 0 \), and that \(\binom{0}{k} = 0\) for all \( k > 0 \).

  If \( k \leq 0 \) then \( k = 0 \), since \( k \in \mathbb{N} \). Hence we need to prove

  \[
  \binom{0}{0} = \frac{0!}{0!0!}
  \]

  But this is true since \(\binom{0}{0} = 1\) and \(\frac{0!}{0!0!} = \frac{1}{1 \times 1} = 1\).

  If \( k > 0 \) then \(\binom{0}{k} = 0\) by Definition 4.1.15.

- **(Induction step)** Fix \( n \in \mathbb{N} \) and suppose that \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) for all \( k \leq n \) and \(\binom{n}{k} = 0\) for all \( k > n \).

  We need to prove that, for all \( k \leq n+1 \), we have

  \[
  \binom{n+1}{k} = \frac{(n+1)!}{k!(n+1-k)!}
  \]
</markdown><markdown>
and that \( \binom{n+1}{k} = 0 \) for all \( k > n+1 \).

So fix \( k \in \mathbb{N} \). There are four possible cases: either (i) \( k = 0 \), or (ii) \( 0 < k \leq n \), or (iii) \( k = n+1 \), or (iv) \( k > n+1 \). In cases (i), (ii) and (iii), we need to prove the factorial formula for \( \binom{n+1}{k} \); in case (iv), we need to prove that \( \binom{n+1}{k} = 0 \).

(i) Suppose \( k = 0 \). Then \( \binom{n+1}{0} = 1 \) by Definition 4.1.15, and

\[
\frac{(n+1)!}{k!(n+1-k)!} = \frac{(n+1)!}{0!(n+1)!} = 1
\]

since \( 0! = 1 \). So \( \binom{n+1}{0} = \frac{(n+1)!}{0!(n+1)!} \).

(ii) If \( 0 < k \leq n \) then \( k = \ell + 1 \) for some natural number \( \ell < n \). Then \( \ell + 1 \leq n \), so we can use the induction hypothesis to apply factorial formula to both \( \binom{n}{\ell} \) and \( \binom{n}{\ell+1} \). Hence

\[
\binom{n+1}{k} = \binom{n+1}{\ell+1} = \binom{n}{\ell} + \binom{n}{\ell+1}
\]

since \( k = \ell + 1 \)

\[
= \frac{n!}{\ell!(n-\ell)!} + \frac{n!}{(\ell+1)!(n-\ell-1)!}
\]

by Definition 4.1.15

\[
= \frac{n!}{\ell!(n-\ell)!} + \frac{n!}{(\ell+1)!(n-\ell-1)!}
\]

by induction hypothesis

Now note that

\[
\frac{n!}{\ell!(n-\ell)!} = \frac{n!}{\ell!(n-\ell)!} \cdot \frac{\ell+1}{\ell+1} = \frac{n!}{(\ell+1)!(n-\ell)!} \cdot (\ell+1)
\]

and

\[
\frac{n!}{(\ell+1)!(n-\ell-1)!} = \frac{n!}{(\ell+1)!(n-\ell-1)!} \cdot \frac{n-\ell}{n-\ell} = \frac{n!}{(\ell+1)!(n-\ell)!} \cdot (n-\ell)
\]

Piecing this together, we have

\[
\frac{n!}{\ell!(n-\ell)!} + \frac{n!}{(\ell+1)!(n-\ell-1)!}
\]

\[
= \frac{n!}{(\ell+1)!(n-\ell)!} \cdot [(\ell+1) + (n-\ell)]
\]

\[
= \frac{n!(n+1)}{(\ell+1)!(n-\ell)!}
\]

\[
= \frac{(n+1)!}{(\ell+1)!(n-\ell)!}
\]
</markdown><markdown>
so that 

\[
\binom{n+1}{\ell+1} = \frac{(n+1)!}{(\ell+1)!(n-\ell)!}.
\]

Now we‚Äôre done; indeed,

\[
\frac{(n+1)!}{(\ell+1)!(n-\ell)!} = \frac{(n+1)!}{k!(n+1-k)!}
\]

since \( k = \ell + 1 \).

(iii) If \( k = n+1 \), then

\[
\binom{n+1}{k} = \binom{n+1}{n+1} \quad \text{since } k = n+1
\]

\[
= \binom{n}{n} + \binom{n}{n+1} \quad \text{by Definition 4.1.15}
\]

\[
= \frac{n!}{n!0!} + 0 \quad \text{by induction hypothesis}
\]

\[
= 1
\]

and 

\[
\frac{(n+1)!}{(n+1)!0!} = 1,
\]

so again the two quantities are equal.

(iv) If \( k > n+1 \), then \( k = \ell + 1 \) for some \( \ell > n \), and so by Definition 4.1.15 and the induction hypothesis we have

\[
\binom{n+1}{k} = \binom{n+1}{\ell+1} \overset{\text{IH}}{=} \binom{n}{\ell} + \binom{n}{\ell+1} = 0 + 0 = 0
\]

On first reading, this proof is long and confusing, especially in the induction step where we are required to split into four cases. We will give a much simpler proof in Section 8.1 (see Theorem 8.1.42), where we prove the statement combinatorially by putting the elements of two sets in one-to-one correspondence.

We can use Theorem 4.2.14 to prove useful identities involving binomial coefficients.

‚úèÔ∏è **Example 4.2.15**

Let \( n, k, \ell \in \mathbb{N} \) with \( \ell \leq k \leq n \) then

\[
\binom{n}{k} \binom{k}{\ell} = \binom{n}{\ell} \binom{n-\ell}{k-\ell}
\]
</markdown><markdown>
Indeed:

\[
\binom{n}{k} \binom{k}{\ell} = \frac{n!}{k!(n-k)!} \cdot \frac{k!}{\ell!(k-\ell)!} \quad \text{by Theorem 4.2.14}
\]

\[
= \frac{n!k!}{k!\ell!(n-k)!(k-\ell)!} \quad \text{combine fractions}
\]

\[
= \frac{n!}{\ell!(n-k)!(k-\ell)!} \quad \text{cancel } k!
\]

\[
= \frac{n!(n-\ell)!}{\ell!(n-k)!(k-\ell)!(n-\ell)!} \quad \text{multiply by } \frac{(n-\ell)!}{(n-\ell)!}
\]

\[
= \frac{n!}{\ell!(n-\ell)!} \cdot \frac{(n-\ell)!}{(k-\ell)!(n-k)!} \quad \text{separate fractions}
\]

\[
= \frac{n!}{\ell!(n-\ell)!} \cdot \frac{(n-\ell)!}{(k-\ell)!((n-\ell)-(k-\ell))!} \quad \text{rearranging}
\]

\[
= \binom{n}{\ell} \binom{n-\ell}{k-\ell} \quad \text{by Theorem 4.2.14}
\]

**Exercise 4.2.16**

Prove that \(\binom{n}{k} = \binom{n}{n-k}\) for all \(n, k \in \mathbb{N}\) with \(k \leq n\).

A very useful application of binomial coefficients in elementary algebra is the binomial theorem.

**Theorem 4.2.17 (Binomial theorem)**

Let \(n \in \mathbb{N}\) and \(x, y \in \mathbb{R}\). Then

\[
(x+y)^n = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k}
\]

*Proof*

In the case when \(y = 0\) we have \(y^{n-k} = 0\) for all \(k < n\), and so the equation reduces to

\[
x^n = x^n y^{n-n}
\]

which is true, since \(y^0 = 1\). So for the rest of the proof, we will assume that \(y \neq 0\).

We will now reduce to the case when \(y = 1\); and extend to arbitrary \(y \neq 0\) afterwards.

We prove \((1+x)^n = \sum_{k=0}^{n} \binom{n}{k} x^k\) by induction on \(n\).
</markdown><markdown>
- **(Base case)** \((1+x)^0 = 1\) and \(\binom{0}{0}x^0 = 1 \cdot 1 = 1\), so the statement is true when \(n = 0\).
- **(Induction step)** Fix \(n \in \mathbb{N}\) and suppose that

  \[
  (1+x)^n = \sum_{k=0}^{n} \binom{n}{k} x^k
  \]

  We need to show that \((1+x)^{n+1} = \sum_{k=0}^{n+1} \binom{n+1}{k} x^k\). Well,

  \[
  \begin{align*}
  (1+x)^{n+1} &= (1+x)(1+x)^n & \text{by laws of indices} \\
  &= (1+x) \cdot \sum_{k=0}^{n} \binom{n}{k} x^k & \text{by induction hypothesis} \\
  &= \sum_{k=0}^{n} \binom{n}{k} x^k + x \cdot \sum_{k=0}^{n} \binom{n}{k} x^k & \text{by expanding } (x+1) \\
  &= \sum_{k=0}^{n} \binom{n}{k} x^k + \sum_{k=0}^{n} \binom{n}{k} x^{k+1} & \text{distributing } x \\
  &= \sum_{k=0}^{n} \binom{n}{k} x^k + \sum_{k=1}^{n+1} \binom{n}{k-1} x^k & k \to k-1 \text{ in second sum} \\
  &= \left( \binom{n}{0} x^0 + \sum_{k=1}^{n} \left( \binom{n}{k} + \binom{n}{k-1} \right) x^k + \binom{n}{n} x^{n+1} \right) & \text{splitting the sums} \\
  &= \left( \binom{n}{0} x^0 + \sum_{k=1}^{n} \binom{n+1}{k} x^k + \binom{n}{n} x^{n+1} \right) & \text{by Definition 4.1.15} \\
  &= \left( \binom{n+1}{0} x^0 + \sum_{k=1}^{n} \binom{n+1}{k} x^k + \binom{n+1}{n+1} x^{n+1} \right) & \text{see } (\ast) \text{ below} \\
  &= \sum_{k=0}^{n+1} \binom{n+1}{k} x^k
  \end{align*}
  \]

  The step labelled \((\ast)\) holds because

  \[
  \binom{n}{0} = 1 = \binom{n+1}{0} \quad \text{and} \quad \binom{n}{n} = 1 = \binom{n+1}{n+1}
  \]

  By induction, we‚Äôve shown that \((1+x)^n = \sum_{i=0}^{n} \binom{n}{k} x^k\) for all \(n \in \mathbb{N}\).

  When \(y \neq 0\) is not necessarily equal to 1, we have that

  \[
  (x+y)^n = y^n \cdot \left( 1 + \frac{x}{y} \right)^n = y^n \cdot \sum_{k=0}^{n} \binom{n}{k} \left( \frac{x}{y} \right)^k = \sum_{k=0}^{n} \binom{n}{k} x^k y^{n-k}
  \]
</markdown><markdown>
The middle equation follows by what we just proved; the leftmost and rightmost equations are simple algebraic rearrangements.

### Example 4.2.18

In Example 4.2.13 we saw that

\[
\sum_{k=0}^{n} \binom{n}{k} = 2^n
\]

This follows quickly from the binomial theorem, since

\[
2^n = (1+1)^n = \sum_{k=0}^{n} \binom{n}{k} \cdot 1^k \cdot 1^{n-k} = \sum_{k=0}^{n} \binom{n}{k}
\]

Likewise, in Exercise 4.2.19 you proved that the alternating sum of binomial coefficients is zero; that is, for \( n \in \mathbb{N} \), we have

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = 0
\]

The proof is greatly simplified by applying the binomial theorem. Indeed, by the binomial theorem, we have

\[
0 = 0^n = (-1+1)^n = \sum_{k=0}^{n} \binom{n}{k} (-1)^k 1^{n-k} = \sum_{k=0}^{n} (-1)^k \binom{n}{k}
\]

Both of these identities can be proved much more elegantly, quickly and easily using enumerative combinatorics. This will be the topic covered in Section 8.1.

### Exercise 4.2.19

Use the binomial theorem to prove that

\[
\sum_{i=0}^{n} (-1)^i \binom{n}{i} = 0
\]
</markdown><markdown>
## Section 4.3
# Strong induction

Consider the following example, which we will attempt to prove by induction.

### Example 4.3.1
Define a sequence recursively by

\[
b_0 = 1 \quad \text{and} \quad b_{n+1} = 1 + \sum_{k=0}^{n} b_k \quad \text{for all } n \in \mathbb{N}
\]

We will attempt to prove by induction that \( b_n = 2^n \) for all \( n \in \mathbb{N} \).

- **(Base case)** By definition of the sequence we have \( b_0 = 1 = 2^0 \). So far so good.

- **(Induction step)** Fix \( n \in \mathbb{N} \), and suppose that \( b_n = 2^n \). We need to show that \( b_{n+1} = 2^{n+1} \).

  Well, 

  \[
  b_{n+1} = 1 + \sum_{k=0}^{n} b_k = \ldots \text{ uh oh.}
  \]

Here‚Äôs what went wrong. If we could replace each \( b_k \) by \( 2^k \) in the sum, then we‚Äôd be able to complete the proof. However we cannot justify this substitution: our induction hypothesis only gives us information about \( b_n \), not about a general term \( b_k \) for \( k < n \).

The strong induction principle looks much like the weak induction principle, except that its induction hypothesis is more powerful. Despite its name, strong induction is no stronger than weak induction; the two principles are equivalent. In fact, we‚Äôll prove the strong induction principle by *weak induction*!

### Theorem 4.3.2 (Strong induction principle)
Let \( p(n) \) be a logical formula with free variable \( n \in \mathbb{N} \) and let \( n_0 \in \mathbb{N} \). If

(i) \( p(n_0) \) is true; and

(ii) For all \( n \geq n_0 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then \( p(n+1) \) is true;

then \( p(n) \) is true for all \( n \geq n_0 \).

**Proof**

For each \( n \geq n_0 \), let \( q(n) \) be the assertion that \( p(k) \) is true for all \( n_0 \leq k \leq n \).

Notice that \( q(n) \) implies \( p(n) \) for all \( n \geq n_0 \), since given \( n \geq n_0 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then in particular \( p(k) \) is true when \( k = n \).
</markdown><markdown>
So it suffices to prove \( q(n) \) is true for all \( n \geq n_0 \). We do so by weak induction.

- **(Base case)** \( q(n_0) \) is equivalent to \( p(n_0) \), since the only natural number \( k \) with \( n_0 \leq k \leq n_0 \) is \( n_0 \) itself; hence \( q(n_0) \) is true by condition (i).

- **(Induction step)** Let \( n \geq n_0 \) and suppose \( q(n) \) is true. Then \( p(k) \) is true for all \( n_0 \leq k \leq n \).

  We need to prove that \( q(n+1) \) is true‚Äîthat is, that \( p(k) \) is true for all \( n_0 \leq k \leq n+1 \).
  But we know \( p(k) \) is true for all \( n_0 \leq k \leq n \)‚Äîthis is the induction hypothesis‚Äîand then \( p(n+1) \) is true by condition (ii). So we have that \( p(k) \) is true for all \( n_0 \leq k \leq n+1 \) after all.

By induction, \( q(n) \) is true for all \( n \geq n_0 \). Hence \( p(n) \) is true for all \( n \geq n_0 \).

‚ùñ **Strategy 4.3.3 (Proof by strong induction)**

In order to prove a proposition of the form \( \forall n \geq n_0, p(n) \), it suffices to prove that:

- **(Base case)** \( p(n_0) \) is true; and

- **(Induction step)** For all \( n \geq n_0 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then \( p(n+1) \) is true.

Like with weak induction, we can illustrate how strong induction works diagrammatically. The induction hypothesis, represented by the large square, now encompasses \( p(k) \) for all \( n_0 \leq k \leq n \), where \( p(n_0) \) is the base case.

Observe that the only difference from weak induction is the induction hypothesis.

- **Weak induction step:** Fix \( n \geq n_0 \), \(\boxed{\text{assume } p(n) \text{ is true}}\), derive \( p(n+1) \);

- **Strong induction step:** Fix \( n \geq n_0 \), \(\boxed{\text{assume } p(k) \text{ is true for all } n_0 \leq k \leq n}\), derive \( p(n+1) \).

We now use strong induction to complete the proof of Example 4.3.1.
</markdown><markdown>
### Example 4.3.4 (Example 4.3.1 revisited)

Define a sequence recursively by

\[ 
b_0 = 1 \quad \text{and} \quad b_{n+1} = 1 + \sum_{k=0}^{n} b_k \quad \text{for all } n \in \mathbb{N} 
\]

We will prove by strong induction that \( b_n = 2^n \) for all \( n \in \mathbb{N} \).

- **(Base case)** By definition of the sequence we have \( b_0 = 1 = 2^0 \).

- **(Induction step)** Fix \( n \in \mathbb{N} \), and suppose that \( b_k = 2^k \) for all \( k \leq n \). We need to show that \( b_{n+1} = 2^{n+1} \). This is true, since

  \[
  b_{n+1} = 1 + \sum_{k=0}^{n} b_k \quad \text{by the recursive formula for } b_{n+1}
  \]

  \[
  = 1 + \sum_{k=0}^{n} 2^k \quad \text{by the induction hypothesis}
  \]

  \[
  = 1 + (2^{n+1} - 1) \quad \text{by Exercise 4.2.5}
  \]

  \[
  = 2^{n+1}
  \]

By induction, it follows that \( b_n = 2^n \) for all \( n \in \mathbb{N} \).

The following theorem adapts the strong induction principle to proofs where we need to refer to a *fixed* number of previous steps in our induction step.

### Theorem 4.3.5 (Strong induction principle (multiple base cases))

Let \( p(n) \) be a logical formula with free variable \( n \in \mathbb{N} \) and let \( n_0 < n_1 \in \mathbb{N} \). If

(i) \( p(n_0), p(n_0 + 1), \ldots, p(n_1) \) are all true; and

(ii) For all \( n \geq n_1 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then \( p(n+1) \) is true;

then \( p(n) \) is true for all \( n \geq n_0 \).

**Proof**

The fact that \( p(n) \) is true for all \( n \geq n_0 \) follows from strong induction. Indeed:

- **(Base case)** \( p(n_0) \) is true by (i);

- **(Induction step)** Fix \( n \geq n_0 \) and assume \( p(k) \) is true for all \( n_0 \leq k \leq n \). Then:

  - If \( n < n_1 \), then \( n + 1 \leq n_1 \), so that \( p(n) \) is true by (i);
</markdown><markdown>
If \( n \geq n_1 \), then \( p(n+1) \) is true by (ii).

In both cases we see that \( p(n+1) \) is true, as required.

Thus by strong induction, we have that \( p(n) \) is true for all \( n \geq n_0 \).

### Strategy 4.3.6 (Proof by strong induction with multiple base cases)
In order to prove a statement of the form \(\forall n \geq n_0, p(n)\), it suffices to prove that:

- **(Base cases)** \( p(k) \) for all \( k \in \{n_0, n_0 + 1, \ldots, n_1\} \), where \( n_1 > n_0 \); and
- **(Induction step)** For all \( n \geq n_1 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then \( p(n+1) \) is true.

This kind of strong induction differs from the usual kind in two main ways:

- There are multiple base cases \( p(n_0), p(n_0 + 1), \ldots, p(n_1) \), not just one.

- The induction step refers to both the least base case \( n_0 \) and the greatest base case \( n_1 \): the variable \( n \) in the induction step is taken to be greater than or equal to \( n_1 \), while the induction hypothesis assumes \( p(k) \) for all \( n_0 \leq k \leq n \).

The following diagram illustrates how strong induction with multiple base cases works.

Note the difference in quantification of variables in the induction step between regular strong induction and strong induction with multiple base cases:

- **One base case.** Fix \( n \geq n_0 \) and assume \( p(k) \) is true for all \( n_0 \leq k \leq n \).

- **Multiple base cases.** Fix \( n \geq n_1 \) and assume \( p(k) \) is true for all \( n_0 \leq k \leq n \).

Getting the quantification of the variables \( n \) and \( k \) in the strong induction step is crucial, since the quantification affects what may be assumed about \( n \) and \( k \).

The need for multiple base cases often arises when proving results about recursively defined sequences, where the definition of a general term depends on the values of a fixed number of previous terms.
</markdown><markdown>
### Example 4.3.7

Define the sequence

\[ a_0 = 0, \quad a_1 = 1, \quad a_n = 3a_{n-1} - 2a_{n-2} \text{ for all } n \geq 2 \]

We find and prove a general formula for \( a_n \) in terms of \( n \). Writing out the first few terms in the sequence establishes a pattern that we might attempt to prove:

\[
\begin{array}{c|cccccccc}
n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
a_n & 0 & 1 & 3 & 7 & 15 & 31 & 63 & 127 & 255 \\
\end{array}
\]

It appears that \( a_n = 2^n - 1 \) for all \( n \geq 0 \). We prove this by strong induction, taking the cases \( n = 0 \) and \( n = 1 \) as our base cases.

- **(Base cases)** By definition of the sequence, we have:
  - \( a_0 = 0 = 2^0 - 1 \); and
  - \( a_1 = 1 = 2^1 - 1 \);

  so the claim is true when \( n = 0 \) and \( n = 1 \).

- **(Induction step)** Fix \( n \geq 1 \) and assume that \( a_k = 2^k - 1 \) for all \( 0 \leq k \leq n \). We need to prove that \( a_{n+1} = 2^{n+1} - 1 \).

  Well since \( n \geq 1 \), we have \( n+1 \geq 2 \), so we can apply the recursive formula to \( a_{n+1} \). Thus

  \[
  \begin{align*}
  a_{n+1} &= 3a_n - 2a_{n-1} & \text{by definition of } a_{n+1} \\
  &= 3(2^n - 1) - 2(2^{n-1} - 1) & \text{by induction hypothesis} \\
  &= 3 \cdot 2^n - 3 - 2 \cdot 2^{n-1} + 2 & \text{expanding} \\
  &= 3 \cdot 2^n - 3 - 2^n + 2 & \text{using laws of indices} \\
  &= 2 \cdot 2^n - 1 & \text{simplifying} \\
  &= 2^{n+1} - 1 & \text{using laws of indices}
  \end{align*}
  \]

So the result follows by induction.

The following exercises have proofs by strong induction with multiple base cases.

### Exercise 4.3.8

Define a sequence recursively by \( a_0 = 4, \, a_1 = 9 \) and \( a_n = 5a_{n-1} - 6a_{n-2} \) for all \( n \geq 2 \). Prove that \( a_n = 3 \cdot 2^n + 3^n \) for all \( n \in \mathbb{N} \).

### Exercise 4.3.9

The **Tribonacci sequence** is the sequence \( t_0, t_1, t_2, \ldots \) defined by

\[ t_0 = 0, \quad t_1 = 0, \quad t_2 = 1, \quad t_n = t_{n-1} + t_{n-2} + t_{n-3} \text{ for all } n \geq 3 \]
</markdown><markdown>
## Section 4.3. Strong induction

Prove that \( t_n \leq 2^{n-3} \) for all \( n \geq 3 \).

### Exercise 4.3.10

The Frobenius coin problem asks when a given amount of money can be obtained from coins of given denominations. For example, a value of 7 dubloons cannot be obtained using only 3 dubloon and 5 dubloon coins, but for all \( n \geq 8 \), a value of \( n \) dubloons can be obtained using only 3 dubloon and 5 dubloon coins. Prove this.

## Well-ordering principle

The underlying reason why we can perform induction and recursion on the natural numbers is because of the way they are ordered. Specifically, the natural numbers satisfy the **well-ordering principle**: if a set of natural numbers has at least one element, then it has a least element. This sets the natural numbers apart from the other number sets; for example, \(\mathbb{Z}\) has no least element, since if \( a \in \mathbb{Z} \) then \( a - 1 \in \mathbb{Z} \) and \( a - 1 < a \).

### Theorem 4.3.11 (Well-ordering principle)

Let \( X \) be a set of natural numbers. If \( X \) is inhabited, then \( X \) has a least element.

**Idea of proof**

Under the assumption that \( X \) is a set of natural numbers, the proposition we want to prove has the form \( p \Rightarrow q \). Namely

\[ 
X \text{ is inhabited } \Rightarrow X \text{ has a least element} 
\]

Assuming \( X \) is inhabited doesn‚Äôt really give us much to work with, so let‚Äôs try the contrapositive:

\[ 
X \text{ has no least element } \Rightarrow X \text{ is empty} 
\]

The assumption that \( X \) has no least element *does* give us something to work with. Under this assumption we need to deduce that \( X \) is empty.

We will do this by ‚Äòforcing \( X \) up‚Äô by strong induction. Certainly \( 0 \notin X \), otherwise it would be the least element. If none of the numbers \( 0, 1, \ldots, n \) are elements of \( X \) then neither can \( n + 1 \) be, since if it were then it would be the least element of \( X \). Let‚Äôs make this argument formal.

**Proof**

Let \( X \) be a set of natural numbers containing no least element. We prove by strong induction that \( n \notin X \) for all \( n \in \mathbb{N} \).

- **(Base case)** \( 0 \notin X \) since if \( 0 \in X \) then 0 must be the least element of \( X \), as it is the least natural number.
</markdown><markdown>
- **(Induction step)** Suppose \( k \notin X \) for all \( 0 \leq k \leq n \). If \( n + 1 \in X \) then \( n + 1 \) is the least element of \( X \); indeed, if \( \ell < n + 1 \) then \( 0 \leq \ell \leq n \), so \( \ell \notin X \) by the induction hypothesis. This contradicts the assumption that \( X \) has no least element, so \( n + 1 \notin X \).

By strong induction, \( n \notin X \) for each \( n \in \mathbb{N} \). Since \( X \) is a set of natural numbers, and it contains no natural numbers, it follows that \( X \) is empty.

The following proof that \( \sqrt{2} \) is irrational is a classic application of the well-ordering principle.

‚ú£ **Proposition 4.3.12**  
The number \( \sqrt{2} \) is irrational.

To prove **Proposition 4.3.12** we will use the following lemma, which uses the well-ordering principle to prove that fractions can be ‚Äòcancelled to lowest terms‚Äô.

‚ú£ **Lemma 4.3.13**  
Let \( q \) be a positive rational number. There is a pair of nonzero natural numbers \( a, b \) such that \( q = \frac{a}{b} \) and such that the only natural number which divides both \( a \) and \( b \) is 1.

*Proof*  
First note that we can express \( q \) as the ratio of two nonzero natural numbers, since \( q \) is a positive rational number. By the well-ordering principle, there is a *least* natural number \( a \) such that \( q = \frac{a}{b} \) for some positive \( b \in \mathbb{N} \).

Suppose that some natural number \( d \) other than 1 divides both \( a \) and \( b \). Note that \( d \neq 0 \), since if \( d = 0 \) then that would imply \( a = 0 \). Since \( d \neq 1 \), it follows that \( d \geq 2 \).

Since \( d \) divides \( a \) and \( b \), there exist natural numbers \( a', b' \) such that \( a = a'd \) and \( b = b'd \). Moreover, \( a', b' > 0 \) since \( a, b, d > 0 \). It follows that

\[
q = \frac{a}{b} = \frac{a'd}{b'd} = \frac{a'}{b'}
\]

But \( d \geq 2 \), and hence

\[
a' = \frac{a}{d} \leq \frac{a}{2} < a
\]

contradicting minimality of \( a \). Hence our assumption that some natural number \( d \) other than 1 divides both \( a \) and \( b \) was false‚Äîit follows that the only natural number dividing both \( a \) and \( b \) is 1.

We are now ready to prove that \( \sqrt{2} \) is irrational.

*Proof of Proposition 4.3.12*  
Suppose \( \sqrt{2} \) is rational. Since \( \sqrt{2} > 0 \), this means that we can write

\[
\sqrt{2} = \frac{a}{b}
\]
</markdown><markdown>
where \( a \) and \( b \) are both positive natural numbers. By Lemma 4.3.13, we may assume that the only natural number dividing \( a \) and \( b \) is 1.

Multiplying the equation \(\sqrt{2} = \frac{a}{b}\) and squaring yields

\[ a^2 = 2b^2 \]

Hence \( a^2 \) is even. By Proposition 1.1.46, \( a \) is even, so we can write \( a = 2c \) for some \( c > 0 \). Then \( a^2 = (2c)^2 = 4c^2 \), and hence

\[ 4c^2 = 2b^2 \]

Dividing by 2 yields

\[ 2c^2 = b^2 \]

and hence \( b^2 \) is even. By Proposition 1.1.46 again, \( b \) is even.

But if \( a \) and \( b \) are both even, the natural number 2 divides both \( a \) and \( b \). This contradicts the fact that the only natural number dividing both \( a \) and \( b \) is 1. Hence our assumption that \(\sqrt{2}\) is rational is incorrect, and \(\sqrt{2}\) is irrational.

‚ùñ **Writing tip**

In the proof of Proposition 4.3.12 we could have separately proved that \( a^2 \) being even implies \( a \) is even, and that \( b^2 \) being even implies \( b \) is even. This would have required us to repeat the same proof twice, which is somewhat tedious! Proving auxiliary results separately (as in Lemma 4.3.13) and then quoting them in later theorems can improve the readability of the main proof, particularly when the auxiliary results are particularly technical. Doing so also helps emphasise the important steps.

‚úé **Exercise 4.3.14**

What goes wrong in the proof of Proposition 4.3.12 if we try instead to prove that \(\sqrt{4}\) is irrational?

‚úé **Exercise 4.3.15**

Prove that \(\sqrt{3}\) is irrational.
</markdown><markdown>
## Chapter 4 exercises

### Recursive definitions

In Questions 4.1 to 4.5, use the recursive definitions of addition, multiplication and exponentiation directly to prove the desired equation.

4.1. \(1 + 3 = 4\)

4.2. \(0 + 5 = 5\)

4.3. \(2 \cdot 3 = 6\)

4.4. \(0 \cdot 5 = 0\)

4.5. \(2^3 = 8\)

4.6. Give a recursive definition of new quantifiers \(\exists^n\) for \(n \in \mathbb{N}\), where given a set \(X\) and a predicate \(p(x)\), the logical formula \(\exists^n x \in X, \, p(x)\) means ‚Äòthere are exactly \(n\) elements \(x \in X\) such that \(p(x)\) is true‚Äô. That is, define \(\exists^0\), and then define \(\exists^{n+1}\) in terms of \(\exists^n\).

4.7. Use the recursive definition of binomial coefficients ([Definition 4.1.15](#)) to prove directly that \(\binom{4}{2} = 6\).

4.8. (a) Find the number of trailing 0s in the decimal expansion of \(41!\).

(b) Find the number of trailing 0s in the binary expansion of \(41!\).

4.9. Let \(N\) be a set, let \(z \in N\) and let \(s : N \to N\). Prove that \((N, z, s)\) is a notion of natural numbers (in the sense of [Definition 4.1.1](#)) if and only if, for every set \(X\), every element \(a \in X\) and every function \(f : X \to X\), there is a unique function \(h : N \to X\) such that \(h(z) = a\) and \(h \circ f = s \circ h\).

### Proofs by induction

4.10. Find all natural numbers \(n\) such that \(n^5 < 5^n\).

4.11. Prove that \((1 + x)^{123 \, 456 \, 789} \geq 1 + 123 \, 456 \, 789 \cdot x\) for all real \(x \geq -1\).

4.12. Let \(a \in \mathbb{N}\) and assume that the last digit in the decimal expansion of \(a\) is 6. Prove that the last digit in the decimal expansion of \(a^n\) is 6 for all \(n \geq 1\).
</markdown><markdown>
### Section 4.E. Chapter 4 exercises

**4.13.** Let \( a, b \in \mathbb{R} \), and let \( a_0, a_1, a_2, \ldots \) be a sequence such that \( a_0 = a, a_1 = b \) and 

\[
a_n = \frac{a_{n-1} + a_{n+1}}{2}
\]

for all \( n \geq 1 \). Prove that \( a_n = a + (b-a)n \) for all \( n \in \mathbb{N} \).

**4.14.** Let \( f : \mathbb{R} \to \mathbb{R} \) be a function such that \( f(x+y) = f(x) + f(y) \) for all \( x, y \in \mathbb{R} \).

(a) Prove by induction that there is a real number \( a \) such that \( f(n) = an \) for all \( n \in \mathbb{N} \).

(b) Deduce that \( f(n) = an \) for all \( n \in \mathbb{Z} \).

(c) Deduce further that \( f(x) = ax \) for all \( x \in \mathbb{Q} \).

**4.15.** Let \( f : \mathbb{R} \to \mathbb{R} \) be a function such that \( f(0) > 0 \) and \( f(x+y) = f(x)f(y) \) for all \( x, y \in \mathbb{R} \). Prove that there is some positive real number \( a \) such that \( f(x) = a^x \) for all rational numbers \( x \).

**4.16.** Let \( a, b \in \mathbb{Z} \). Prove that \( b-a \) divides \( b^n - a^n \) for all \( n \in \mathbb{N} \).

**4.17.** Prove by induction that \( 7^n - 2 \cdot 4^n + 1 \) is divisible by 18 for all \( n \in \mathbb{N} \).

### Some examples from calculus

Questions **4.18** to **4.23** assume familiarity with the basic techniques of differential and integral calculus.

**4.18.** Prove that 

\[
\frac{d^n}{dx^n} (xe^x) = (n+x)e^x
\]

for all \( n \in \mathbb{N} \).

**4.19.** Find and prove an expression for 

\[
\frac{d^n}{dx^n} (x^2e^x)
\]

for all \( n \in \mathbb{N} \).

**4.20.** Let \( f \) and \( g \) be differentiable functions. Prove that 

\[
\frac{d^n}{dx^n} (f(x)g(x)) = \sum_{k=0}^{n} \binom{n}{k} f^{(k)}(x)g^{(n-k)}(x)
\]

for all \( n \in \mathbb{N} \), where the notation \( h^{(r)} \) denotes the \( r \)th derivative of \( h \).

**4.21.** Find and prove an expression for the \( n \)th derivative of \( \log(x) \) for all \( n \in \mathbb{N} \).

**4.22.** Prove that \( (\cos \theta + i \sin \theta)^n = \cos(n\theta) + i \sin(n\theta) \) for all \( \theta \in \mathbb{R} \) and all \( n \in \mathbb{N} \).

**4.23.** (a) Prove that 

\[
\int_0^{\frac{\pi}{2}} \sin^{n+2}(x) \, dx = \frac{n+1}{n+2} \int_0^{\frac{\pi}{2}} \sin^n(x) \, dx
\]

for all \( n \in \mathbb{N} \).

(b) Use induction to prove that 

\[
\int_0^{\frac{\pi}{2}} \sin^{2n}(x) \, dx = \frac{\pi}{2^{2n+1}} \binom{2n}{n}
\]

for all \( n \in \mathbb{N} \).

(c) Find and prove an expression for 

\[
\int_0^{\frac{\pi}{2}} \sin^{2n+1}(x) \, dx
\]

for all \( n \in \mathbb{N} \).
</markdown><markdown>
### True‚ÄìFalse questions

In Questions 4.24 to 4.32, determine (with proof) whether the statement is true or false.

4.24. Every factorial is positive.

4.25. Every binomial coefficient is positive.

4.26. Every proof by strong induction can be turned into a proof by weak induction by changing only the induction hypothesis.

4.27. Every proof by weak induction can be turned into a proof by strong induction by changing only the induction hypothesis.

4.28. Given a set \( X \), a surjection \( f : \mathbb{N} \to X \) and a logical formula \( p(x) \) with free variable \( x \in X \), if \( p(f(0)) \) is true and \( \forall n \in \mathbb{N}, \left( p(f(n)) \Rightarrow p(f(n+1)) \right) \) is true, then \( \forall x \in X, p(x) \) is true.

4.29. Given some logical formula \( p(x) \) with free variable \( x \in \mathbb{N} \), if for all \( x \in \mathbb{N} \) there exists some \( y \leq x \) such that \( p(x) \) is false, then \( p(x) \) is false for all \( x \in \mathbb{N} \).

4.30. Every inhabited subset of the set \( \mathbb{Q}_{\geq 0} \) of nonnegative rational numbers has a least element.

4.31. Every inhabited subset of the set \( \mathbb{Z} \) of integers has a least element.

4.32. Every inhabited subset of the set \( \mathbb{Z}_{< 0} \) of negative integers has a greatest element.

### Always‚ÄìSometimes‚ÄìNever questions

In Questions 4.33 to 4.35, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

4.33. Let \( n, k, \ell \in \mathbb{Z} \). Then 
\[
\binom{n}{k+\ell} = \binom{n}{k} + \binom{n}{\ell}.
\]

4.34. Let \( m, n \geq 2 \). Then \( (m+n)! = m! + n! \).

4.35. Let \( p(x) \) be a logical formula with free variable \( x \in \mathbb{Z} \), and suppose that \( p(0) \) is true and, for all \( n \in \mathbb{Z} \), if \( p(n) \) is true, then \( p(n+1) \) and \( p(n-2) \) are true. Then \( \forall n \in \mathbb{Z}, p(n) \) is true.
</markdown><markdown>
# Chapter 5

## Relations
</markdown><markdown>
## Section 5.1
### Relations

Many interesting results or concepts in mathematics arise from observing how the elements of one set interact with the elements of another set, or how elements of a single set interact with each other. We can make this idea of ‚Äòinteraction‚Äô precise using the notion of a **relation**.

‚ú¶ **Definition 5.1.1**  
Let \( X \) and \( Y \) be sets. A (binary) relation from \( X \) to \( Y \) is a logical formula \( R(x, y) \) with two free variables \( x \in X \) and \( y \in Y \). We call \( X \) the **domain** of \( R \) and \( Y \) the **codomain** of \( R \).

A relation \( R \) is **homogeneous** if it has the same domain and codomain \( X \), in which case we say that \( R \) is a relation on \( X \).

Given \( x \in X \) and \( y \in Y \), if \( R(x, y) \) is true then we say ‚Äò\( x \) is related to \( y \) by \( R \)‚Äô, and write \( x R y \) (\(\LaTeX\) code: \( x \mathrel{R} y \)).

‚úê **Example 5.1.2**  
We have already seen many examples of relations.

- Divisibility (‚Äò\( x \) divides \( y \)‚Äô) is a relation on \( \mathbb{Z} \).
- The inequality relation \(\leq\) is a relation on \( \mathbb{R} \).
- For any set \( X \), equality \( = \) is a relation on \( X \).
- Logical equivalence \(\equiv\) is a relation on the set of all logical formulae.
- For any set \( X \), the subset relation \(\subseteq\) is a relation on \(\mathcal{P}(X)\).

These relations were all homogeneous, but not all relations are:

- For any set \( X \), the elementhood relation \(\in\) is a relation from \( X \) to \(\mathcal{P}(X)\).
- Every function \( f : X \to Y \) induces a relation \( R_f \) from \( X \) to \( Y \), defined by taking \( x R_f y \) to mean \( f(x) = y \).

‚úê **Exercise 5.1.3**  
Give three more examples of relations, not all of which are homogeneous.
</markdown><markdown>
Like with sets and functions, we must determine when to declare that two relations are equal. For example, consider the relation \( R \) on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by letting \( a \, R \, b \) mean \(\exists x \in \mathbb{R}, \, a + x^2 = b\). It so happens that \( a \, R \, b \) if and only if \( a \leq b \)‚Äîwe‚Äôll prove this in Example 5.1.5. So should \( R \) be equal to \(\leq\)? On the one hand you might say ‚Äòyes‚Äô, since \(\leq\) and \( R \) relate the same pairs of real numbers. On the other hand you might say ‚Äòno‚Äô, since the fact that \(\leq\) and \( R \) relate the same pairs of real numbers was not immediate and required proof. In fact, if we were to replace \( \mathbb{R} \) by \( \mathbb{Q} \), it then \(\leq\) and \( R \) would not relate the same pairs of elements, since for instance \( 0 \leq 2 \) but there is no rational number \( x \) such that \( 0 + x^2 = 2 \).

But as with sets and functions, we settle for the extensional notion of equality: just as two sets are equal when they have the same elements (Axiom 2.1.22), and two functions are equal when they have the same values (Axiom 3.1.4), we consider two relations to be equal when they relate the same pairs of elements (Axiom 5.1.4).

### Axiom 5.1.4 (Relation extensionality)
Let \( R \) and \( S \) be relations. Then \( R = S \) if and only if \( R \) and \( S \) have the same domain \( X \) and codomain \( Y \), and

\[
\forall x \in X, \, \forall y \in Y, \, (x \, R \, y \iff x \, S \, y)
\]

That is, two relations with the same domain and codomain are equal precisely when they relate the same pairs of elements.

### Example 5.1.5
Recall the relation \( R \) on \( \mathbb{R} \) that we defined above for \( a, b \in \mathbb{R} \) by letting \( a \, R \, b \) if and only if \( a + x^2 = b \) for some \( x \in \mathbb{R} \). To see that \( R = \leq \), note that \( a + (b - a) = b \), and that \( b - a \) is the square of a real number if and only if \( b - a \geq 0 \), which occurs if and only if \( a \leq b \).

### Exercise 5.1.6
Let \( R \) and \( S \) be relations on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by letting

\[
a \, R \, b \iff b - a \in \mathbb{Q} \quad \text{and} \quad a \, S \, b \iff \exists n \in \mathbb{Z}, \, (n \neq 0) \land n(b - a) \in \mathbb{Z}
\]

Prove that \( R = S \).

The true reason why Axiom 5.1.4 is powerful is that it allows us to reason about relations entirely set theoretically by working with their graphs‚Äîthe sets of pairs of elements that they relate‚Äîrather than with the particular formulae defining the relation.

### Definition 5.1.7
Let \( R \) be a relation from a set \( X \) to a set \( Y \). The **graph** of \( R \) is the set \(\mathrm{Gr}(R)\) of pairs \((x, y) \in X \times Y\) for which \( x \, R \, y \). That is

\[
\mathrm{Gr}(R) = \{(x, y) \in X \times Y \mid x \, R \, y\} \subseteq X \times Y
\]
</markdown><markdown>
### Example 5.1.8

The graph of the relation \(\leq\) on \([3]\) is

\[
\{(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)\}
\]

Likewise, the graph of the relation \(\leq\) viewed as a relation from \([2]\) to \([4]\) is

\[
\{(1,1), (1,2), (1,3), (1,4), (2,2), (2,3), (2,4)\}
\]

This demonstrates that the graph of a relation is sensitive to the domain (and codomain) of the relation.

### Example 5.1.9

Consider the relation \(C\) from \(\mathbb{R}\) to \(\mathbb{R}\) defined by \(x \, C \, y \iff x^2 + y^2 = 1\). Then

\[
\text{Gr}(C) = \{(x, y) \in \mathbb{R} \times \mathbb{R} \mid x^2 + y^2 = 1\}
\]

Plotting \(\text{Gr}(C)\) on a standard pair of axes yields a circle with radius 1 centred at the point \((0,0)\), shown below with a unit grid.

Note that \(\text{Gr}(C)\) is *not* the graph of a function \(f : [0,1] \to \mathbb{R}\), since for example both \((0,1)\) and \((0,-1)\) are elements of \(\text{Gr}(C)\), the value \(f(0)\) would not be uniquely defined.

### Exercise 5.1.10

Let \(R\) be the relation on \(\mathbb{Z}\) defined for \(x, y \in \mathbb{Z}\) by letting \(x \, R \, y\) if and only if \(x^2 = y^2\). Describe its graph \(\text{Gr}(R) \subseteq \mathbb{Z} \times \mathbb{Z}\).
</markdown><markdown>
### Exercise 5.1.11

Let \( f : X \to Y \) be a function, and define the relation \( R_f \) from \( X \) to \( Y \) as in Example 5.1.2. Prove that \(\text{Gr}(R_f) = \text{Gr}(f)\)‚Äîthat is, the graph of the relation \( R_f \) is equal to the graph of the function \( f \).

### Definition 5.1.12

The **discrete relation** from a set \( X \) to a set \( Y \) is the relation \( D_{X,Y} \) defined by letting \( x D_{X,Y} y \) be true for all \( x \in X \) and \( y \in Y \).

The **empty relation** from a set \( X \) to a set \( Y \) is the relation \( \emptyset_{X,Y} \) (\(\LaTeX\) code: `\varnothing`) defined by letting \( x \emptyset_{X,Y} y \) be false for all \( x \in X \) and \( y \in Y \).

### Exercise 5.1.13

Let \( X \) and \( Y \) be sets. Describe the graphs \(\text{Gr}(D_{X,Y})\) and \(\text{Gr}(\emptyset_{X,Y})\).

It turns out that, for fixed sets \( X \) and \( Y \), relations from \( X \) to \( Y \) correspond with subsets of \( X \times Y \)‚Äîsee Theorem 5.1.14 below. This fact is so convenient that many (if not most) authors actually define ‚Äòrelation from \( X \) to \( Y \)‚Äô to mean ‚Äòsubset of \( X \times Y \)‚Äô.

### Theorem 5.1.14

Let \( X \) and \( Y \) be sets. Every subset \( G \subseteq X \times Y \) is the graph of a unique relation \( R \) from \( X \) to \( Y \).

**Proof**

Fix \( G \subseteq X \times Y \). Define a relation \( R \) by

\[
\forall x \in X, \, \forall y \in Y, \, x R y \iff (x, y) \in G
\]

Then certainly \( G = \text{Gr}(R) \), since for all \( x \in X \) and \( y \in Y \) we have

\[
(x, y) \in G \iff x R y \iff (x, y) \in \text{Gr}(R)
\]

Moreover, if \( S \) is a relation from \( X \) to \( Y \) such that \( G = \text{Gr}(S) \), then, for all \( x \in X \) and \( y \in Y \)

\[
x S y \iff (x, y) \in \text{Gr}(S) \iff (x, y) \in G \iff x R y
\]

so \( S = R \).

Hence there is exactly one relation from \( X \) to \( Y \) whose graph is \( G \).

Theorem 5.1.14 suggests that, for the purposes of defining relations and proving that relations are equal, we may work entirely set theoretically with the graphs of the relations.
</markdown><markdown>
### Strategy 5.1.15 (Relations as graphs)

In order to specify a relation \( R \), it suffices to specify its domain \( X \), its codomain \( Y \), and its graph \( \text{Gr}(R) \subseteq X \times Y \). Furthermore, in order to prove that two relations \( R \) and \( S \) are equal, it suffices to prove that they have the same domain and codomain, and that their graphs are equal.

### Example 5.1.16

Consider the set \( G = \{(2m+i, 2n+i) \mid m, n \in \mathbb{Z}, i \in \{0, 1\}\} \). Since \( G \subseteq \mathbb{Z} \times \mathbb{Z} \), it is the graph of a (unique) relation \( R \) on \( \mathbb{Z} \), which is necessarily defined for \( a, b \in \mathbb{Z} \) by letting \( a \, R \, b \) if and only if there are integers \( m \) and \( n \), and \( i \in \{0, 1\} \), such that \( a = 2m+i \) and \( b = 2n+i \). But this says precisely that \( a \) and \( b \) both leave the same remainder (namely \( i \)) when divided by 2, so that \( R \) can be described by saying that, for all \( a, b \in \mathbb{Z} \), we have \( a \, R \, b \) if and only if \( a \) and \( b \) are both even or both odd.

### Definition 5.1.17

Let \( X \) be a set. The **diagonal subset** of \( X \times X \) is the set \( \Delta_X \) (LaTeX code: \(\Delta_X\)) defined by \( \Delta_X = \{(x, x) \mid x \in X\} \).

To see why \( \Delta_X \) is called the ‚Äòdiagonal‚Äô subset, try plotting \( \Delta_{\mathbb{R}} \subseteq \mathbb{R} \times \mathbb{R} \) on a standard pair of axes (like in Example 5.1.9).

### Exercise 5.1.18

Let \( X \) be a set. Prove that \( \Delta_X = \text{Gr}(=) \).

## Properties of homogeneous relations

Most of the relations of interest to us in this book are homogeneous‚Äîthat is, relations on a set. In fact, they broadly fall into one of two categories: **equivalence relations**, which are relations that ‚Äòbehave like \( = \)‚Äô; and **order relations**, which are relations that ‚Äòbehave like \( < \)‚Äô. We will study equivalence relations in Section 5.2 and order relations in Section 12.1, but examples of such relations pop up throughout the book. (In fact, we have already seen several!)

Our task for the rest of this section is to isolate the properties that a relation must satisfy in order to be classified as an equivalence relation or an order relation.

To aid with intuition, we will illustrate these properties with diagrams: given a relation \( R \), the fact that \( a \, R \, b \) will be represented diagrammatically as follows:

\[
a \longrightarrow b
\]
</markdown><markdown>
A **reflexive** relation is one that relates every element of its domain to itself.

### Definition 5.1.19
A relation \( R \) on a set \( X \) is **reflexive** if \( a \, R \, a \) for all \( a \in X \).

### Example 5.1.20
Given any set \( X \), the equality relation \( = \) on \( X \) is reflexive, since \( a = a \) for all \( a \in X \).

### Example 5.1.21
Let \( R \) be the relation on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by \( a \, R \, b \) if and only if \( b - a \in \mathbb{Q} \). Then \( R \) is reflexive, since for all \( a \in \mathbb{R} \), we have \( a - a = 0 \in \mathbb{Q} \), so that \( a \, R \, a \).

### Exercise 5.1.22
Let \( X \) be a set. Prove that \( \subseteq \) is a reflexive relation on \( \mathcal{P}(X) \), but \( \subset \) is not.

### Exercise 5.1.23
Prove that the relation ‚Äò\( x \) divides \( y \)‚Äô on \( \mathbb{Z} \) is reflexive.

The next exercise demonstrates that when determining if a relation is reflexive, we must be careful to specify its domain.

### Exercise 5.1.24
Let \( G = \{(1, 1), (2, 2), (3, 3)\} \). Let \( R \) be the relation on \([3]\) whose graph is \( G \), and let \( S \) be the relation on \([4]\) whose graph is \( G \). Prove that \( R \) is reflexive, but \( S \) is not.

Symmetric relations are those for which the **direction** of the relation doesn‚Äôt matter: two elements are either each related to the other, or not related at all.

### Definition 5.1.25
A relation \( R \) on a set \( X \) is **symmetric** if, for all \( a, b \in X \), if \( a \, R \, b \), then \( b \, R \, a \).

### Example 5.1.26
Given any set \( X \), the equality relation \( = \) on \( X \) is symmetric, since for all \( a, b \in X \), if \( a = b \), then \( b = a \).
</markdown><markdown>
### Example 5.1.27

Let \( R \) be the relation on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by \( a \, R \, b \) if and only if \( b - a \in \mathbb{Q} \). Then \( R \) is symmetric.

To see this, let \( a, b \in \mathbb{R} \) and assume that \( a \, R \, b \). Then \( b - a \in \mathbb{Q} \), so that \( b - a = \frac{p}{q} \) for some \( p, q \in \mathbb{Z} \) with \( q \neq 0 \). But then

\[
a - b = -(b - a) = -\frac{p}{q}
\]

so that \( a - b \in \mathbb{Q} \). Hence \( b \, R \, a \), as required.

### Exercise 5.1.28

Find all subsets \( U \subseteq \mathbb{Z} \) such that the relation ‚Äò\( x \) divides \( y \)‚Äô on \( U \) is symmetric.

We showed in Exercise 5.1.24 that reflexivity of a relation is sensitive to its domain. The next exercise demonstrates that symmetry is not sensitive to the domain‚Äîthat is, it is an intrinsic property of the relation.

### Exercise 5.1.29

Let \( R \) and \( S \) be relations such that \( \text{Gr}(R) = \text{Gr}(S) \). Note that the domain of \( R \) might be different from the domain of \( S \). Prove that \( R \) is symmetric if and only if \( S \) is symmetric.

A condition related to symmetry, but in a sense opposite to it, is antisymmetry. It says that the only way that two elements of a set can each be related to the other is if they are equal.

### Definition 5.1.30

Let \( X \) be a set. A relation \( R \) on \( X \) is **antisymmetric** if, for all \( a, b \in X \), if \( a \, R \, b \) and \( b \, R \, a \), then \( a = b \).

\[
\begin{array}{c}
a \quad \longrightarrow \quad b \\
\quad \longleftarrow \quad
\end{array}
\quad \Rightarrow \quad a = b
\]

A word of warning here is that ‚Äòantisymmetric‚Äô does not mean the same thing as ‚Äònot symmetric‚Äô‚Äîindeed, we will see, equality is both symmetric and antisymmetric, and many relations are neither symmetric nor antisymmetric. [Even more confusingly, there is a notion of asymmetric relation, which also does not mean ‚Äònot symmetric‚Äô.]

### Example 5.1.31

Given any set \( X \), the equality relation \( = \) on \( X \) is antisymmetric, since for all \( a, b \in X \), if \( a = b \) and \( b = a \), then \( a = b \).
</markdown><markdown>
### Example 5.1.32

The order relation \(\leq\) on \(\mathbb{R}\) is antisymmetric, since for all \(a, b \in \mathbb{R}\), if \(a \leq b\) and \(b \leq a\), then \(a = b\).

### Exercise 5.1.33

Prove that the relation ‚Äò\(x\) divides \(y\)‚Äô on \(\mathbb{N}\) is antisymmetric, but not on \(\mathbb{Z}\).

### Exercise 5.1.34

Let \(X\) be a set. Prove that \(\subseteq\) is an antisymmetric relation on \(\mathcal{P}(X)\).

### Exercise 5.1.35

Let \(X\) be a set and let \(R\) be a relation on \(X\). Prove that \(R\) is both symmetric and antisymmetric if and only if \(\text{Gr}(R) \subseteq \Delta_X\), where \(\Delta_X\) is the diagonal subset of \(X \times X\) (see Definition 5.1.17). Deduce that the only reflexive, symmetric and antisymmetric relation on a set \(X\) is the equality relation on \(X\).

The last property we will study in some detail is **transitivity**. Transitive relations are those for which we can skip over intermediate related elements‚Äîfor example, we can deduce \(0 < 3\) from the facts that \(0 < 1\) and \(1 < 2\) and \(2 < 3\).

### Definition 5.1.36

A relation \(R\) on a set \(X\) is **transitive** if, for all \(a, b, c \in X\), if \(a \, R \, b\) and \(b \, R \, c\), then \(a \, R \, c\).

### Example 5.1.37

Given any set \(X\), the equality relation \(=\) on \(X\) is transitive since, for all \(a, b, c \in X\), if \(a = b\) and \(b = c\), then \(a = c\).

### Example 5.1.38

Let \(R\) be the relation on \(\mathbb{R}\) defined for \(a, b \in \mathbb{R}\) by \(a \, R \, b\) if and only if \(b - a \in \mathbb{Q}\). Then \(R\) is transitive.

To see this, let \(a, b, c \in \mathbb{R}\) and assume that \(a \, R \, b\) and \(b \, R \, c\). Then \(b - a \in \mathbb{Q}\) and \(c - b \in \mathbb{Q}\), so there exist \(p, q, r, s \in \mathbb{Z}\) with \(q, s \neq 0\) such that

\[
b - a = \frac{p}{q} \quad \text{and} \quad c - b = \frac{r}{s}
\]

It follows that

\[
c - a \equiv (c - b) + (b - a) = \frac{p}{q} + \frac{r}{s} = \frac{ps + qr}{qs}
\]

so that \(c - a \in \mathbb{Q}\). Hence \(a \, R \, c\), as required.
</markdown><markdown>
### Exercise 5.1.39
Let \( X \) be a set. Prove that \(\subseteq\) is a transitive relation on \(\mathcal{P}(X)\).

### Exercise 5.1.40
Prove that the relation ‚Äò\( x \) divides \( y \)‚Äô on \(\mathbb{Z}\) is transitive.

Like symmetry, transitive is an intrinsic property of relations‚Äîthat is, transitivity is not sensitive to the domain of the relation‚Äîas the next exercise demonstrates.

### Exercise 5.1.41
Let \( R \) and \( S \) be relations such that \(\text{Gr}(R) = \text{Gr}(S)\). Note that the domain of \( R \) might be different from the domain of \( S \). Prove that \( R \) is transitive if and only if \( S \) is transitive.

A fundamental property of transitive relations is that we can prove two elements \( a \) and \( b \) are related by finding a chain of related elements starting at \( a \) and finishing at \( b \). This is the content of the following proposition.

### Proposition 5.1.42
Let \( R \) be a relation on a set \( X \). Then \( R \) is transitive if and only if, for any finite sequence \( x_0, x_1, \ldots, x_n \) of elements of \( X \) such that \( x_{i-1} \, R \, x_i \) for all \( i \in [n] \), we have \( x_0 \, R \, x_n \).

**Proof**  
For the sake of abbreviation, let \( p(n) \) be the assertion that, for any \( n \geq 1 \) and any sequence \( x_0, x_1, \ldots, x_n \) of elements of \( X \) such that \( x_{i-1} \, R \, x_i \) for all \( i \in [n] \), we have \( x_0 \, R \, x_n \).

We prove the two directions of the proposition separately.

- (\(\Rightarrow\)) Suppose \( R \) is transitive. For \( n \geq 1 \). We prove \( p(n) \) is true for all \( n \geq 1 \) by induction.
  - **(Base case)** When \( n = 1 \) this is immediate, since we assume that \( x_0 \, R \, x_1 \).
  - **(Induction step)** Fix \( n \geq 1 \) and suppose \( p(n) \) is true. Let \( x_0, \ldots, x_n, x_{n+1} \) is a sequence such that \( x_{i-1} \, R \, x_i \) for all \( i \in [n+1] \). We need to prove that \( x_0 \, R \, x_{n+1} \). By the induction hypothesis we know that \( x_0 \, R \, x_n \). By definition of the sequence we have \( x_n \, R \, x_{n+1} \). By transitivity, we have \( x_0 \, R \, x_{n+1} \).

So by induction, we have proved the \(\Rightarrow\) direction.

- (\(\Leftarrow\)) Suppose \( p(n) \) is true for all \( n \geq 1 \). Then in particular \( p(2) \) is true, which is precisely the assertion that \( R \) is transitive.

So we‚Äôre done.

That is, **Proposition 5.1.42** states that for a transitive relation \( R \) on a set \( X \), if \( x_0, x_1, \ldots, x_n \in X \), then

\[
x_0 \, R \, x_1 \, R \cdots R \, x_n \quad \Rightarrow \quad x_0 \, R \, x_n
\]
</markdown><markdown>
where ‚Äò\(x_0 \, R \, x_1 \, R \cdots R \, x_n\)‚Äô abbreviates the assertion that \(x_i \, R \, x_{i+1}\) for each \(i < n\).
</markdown><markdown>
## Section 5.2
Equivalence relations and partitions

An equivalence relation on a set \( X \) is a relation on \( X \) that, to a certain extent, behaves like equality. That is, equivalence relations give us a way of saying that two elements of a set are ‚Äòsimilar‚Äô, without having to be equal. As an example, we might be interested in when the base-10 expansions of two natural numbers end in the same digit, or when two finite sets have the same number of elements.

‚ú¶ **Definition 5.2.1**  
A relation \( R \) on a set \( X \) is an **equivalence relation** if it is reflexive, symmetric and transitive.

To denote a relation that we know (or suspect) is an equivalence relation, we will usually use a symbol like ‚Äò\(\sim\)‚Äô (LATEX code: `\sim`) or ‚Äò\(\equiv\)‚Äô (LATEX code: `\equiv`) or ‚Äò\(\approx\)‚Äô (LATEX code: `\approx`) instead of a letter like ‚Äò\( R \)‚Äô or ‚Äò\( S \)‚Äô.

‚úê **Example 5.2.2**  
Given any set \( X \), it follows from Examples 5.1.20, 5.1.26 and 5.1.37 that the equality relation \( = \) is an equivalence relation on \( X \). This is a relief, since we motivated equivalence relations by saying that they are those that behave like equality!

‚úê **Example 5.2.3**  
Let \( R \) be the relation on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by \( a \, R \, b \) if and only if \( b - a \in \mathbb{Q} \). Piecing together Examples 5.1.21, 5.1.27 and 5.1.38, we see that \( R \) is an equivalence relation on \( \mathbb{R} \).

‚úê **Exercise 5.2.4**  
Given a function \( f : X \to Y \), define a relation \(\sim_f\) on \( X \) by

\[
a \sim_f b \iff f(a) = f(b)
\]

for all \( a, b \in X \). Prove that \(\sim_f\) is an equivalence relation on \( X \).

The equivalence relation in the next exercise comes back with a vengeance in Section 10.1, where we will use it to compare the sizes of (finite and) infinite sets.

‚úê **Exercise 5.2.5**  
Let \( \mathcal{S} \) be some set whose elements are all sets. (For example, we could take \( \mathcal{S} = \mathcal{P}(X) \) for some fixed set \( X \).) Define a relation \(\cong\) (LATEX code: `\cong`) on \( \mathcal{S} \) by letting \( U \cong V \) if and only if there exists a bijection \( f : U \to V \), for all \( U, V \in \mathcal{S} \). Prove that \(\cong\) is an equivalence relation on \( \mathcal{S} \).
</markdown><markdown>
## A first look at modular arithmetic

A particularly useful family of equivalence relations is given by **congruence** of integers, which allows us to do **modular arithmetic**‚Äîthis is the topic of Section 7.3. For a fixed integer \( n \), this relation identifies two integers when they have the same remainder upon division by \( n \) (as in Theorem 0.18).

‚ú¶ **Definition 5.2.6**  
Fix \( n \in \mathbb{Z} \). Given integers \( a, b \in \mathbb{Z} \), we say \( a \) is **congruent to** \( b \) **modulo** \( n \), and write

\[ 
a \equiv b \mod n \quad (\text{LaTeX code: } a \equiv b \ \bmod{n})
\]

if \( n \) divides \( a - b \). If \( a \) is not congruent to \( b \) modulo \( n \), write

\[ 
a \not\equiv b \mod n \quad (\text{LaTeX code: } \not\equiv)
\]

The number \( n \) is called the **modulus** of the congruence.

Before we prove that congruence is modulo \( n \) is an equivalence relation for all \( n \in \mathbb{Z} \), it is worthwhile to get a feel for how it works.

‚úèÔ∏è **Example 5.2.7**  
Let \( a, b \in \mathbb{Z} \). Then \( a \equiv b \mod 2 \) if and only if \( a \) and \( b \) are both even or both odd‚Äîthat is, if and only if they have the same **parity**.

Indeed, by the division theorem, we can write \( a = 2k + i \) and \( b = 2\ell + j \) for some \( k, \ell \in \mathbb{Z} \) and \( i, j \in \{0, 1\} \). Then

\[
b - a = (2k + i) - (2\ell + j) = 2(k - \ell) + (i - j)
\]

Note that \( i - j \in \{-1, 0, 1\} \), and so \( a \equiv b \mod 2 \) if and only if \( i = j \). But this occurs if and only if \( i = j = 0 \), in which case \( a \) and \( b \) are both even, or \( i = j = 1 \), in which case \( a \) and \( b \) are both odd.

‚úèÔ∏è **Example 5.2.8**  
Let \( a, b \in \mathbb{N} \). Then \( a \equiv b \mod 10 \) if and only if 10 divides \( b - a \), which occurs if and only if the last digit in the decimal expansion of \( b - a \) is 0. But this implies that the decimal expansions of \( a \) and \( b \) have the same last digit. So the relation of congruence modulo 10 on \( \mathbb{N} \) is the same as the relation of ‚Äòhaving the same last (decimal) digit‚Äô.

‚úèÔ∏è **Exercise 5.2.9**  
Let \( n \in \mathbb{Z} \). Prove that if \( n \neq 0 \), then \( a \equiv b \mod n \) if and only if \( a \) and \( b \) have the same remainder when divided by \( n \).

‚úèÔ∏è **Exercise 5.2.10**  
Let \( a, b \in \mathbb{Z} \). When is it true that \( a \equiv b \mod 0 \)? When is it true that \( a \equiv b \mod 1 \)?
</markdown><markdown>
Having got a better feel for how congruence works, we now prove that, for each \( n \in \mathbb{Z} \), congruence modulo \( n \) is an equivalence relation on \( \mathbb{Z} \).

### Theorem 5.2.11
Let \( n \in \mathbb{Z} \). Then congruence modulo \( n \) is an equivalence relation on \( \mathbb{Z} \). That is:

(a) \( a \equiv a \mod n \) for all \( a \in \mathbb{Z} \);

(b) For all \( a, b \in \mathbb{Z} \), if \( a \equiv b \mod n \), then \( b \equiv a \mod n \);

(c) For all \( a, b, c \in \mathbb{Z} \), if \( a \equiv b \mod n \) and \( b \equiv c \mod n \), then \( a \equiv c \mod n \).

**Proof**

(a) Let \( a \in \mathbb{Z} \). Note that \( a - a = 0 \), which is divisible by \( n \) since \( 0 = 0 \times n \), and hence \( a \equiv a \mod n \). So congruence modulo \( n \) is reflexive.

(b) Let \( a, b \in \mathbb{Z} \) and suppose \( a \equiv b \mod n \). Then \( n \) divides \( a - b \), so that \( a - b = kn \) for some \( k \in \mathbb{Z} \). Hence \( b - a = -kn \), and so \( n \) divides \( b - a \), so that \( b \equiv a \mod n \) as required. So congruence modulo \( n \) is symmetric.

(c) Let \( a, b, c \in \mathbb{Z} \) and suppose that \( a \equiv b \mod n \) and \( b \equiv c \mod n \). Then \( n \) divides both \( a - b \) and \( b - c \), so there exist \( k, \ell \in \mathbb{Z} \) such that

\[
a - b = kn \quad \text{and} \quad b - c = \ell n
\]

Hence \( a - c = (a - b) + (b - c) = (k + \ell)n \), so that \( n \) divides \( a - c \). Hence \( a \equiv c \mod n \), as required. So congruence modulo \( n \) is transitive.

Since congruence modulo \( n \) is reflexive, symmetric and transitive, it is an equivalence relation.

### Equivalence classes

What makes equivalence relations so useful is they give us a way of ignoring information that is irrelevant to the task at hand.

For example, suppose \( a \) and \( b \) are two very large natural numbers, each with several trillion (decimal) digits. We want to know what the last digit of \( ab \) is. To find this out, it would be silly to compute \( ab \) and then look at its last digit. Instead, we can observe that the last digit of a product of two integers depends only on the last digit of each integer‚Äîfor example, \( 1527 \times 9502 \) has the same last digit as \( 7 \times 2 = 14 \). By using the equivalence relation ‚Äòhas the same last digit as‚Äô, we are able to ignore the irrelevant information about \( a \) and \( b \)‚Äîthat is, all but one of their trillions of digits‚Äîand simplify the problem considerably.
</markdown><markdown>
To make this precise, we introduce the notion of an *equivalence class*. For a set \( X \) with an equivalence relation, the equivalence class of an element \( a \in X \) will be the set of elements of \( X \) that \( a \) is equivalent to. By working with the *equivalence classes* of elements of \( X \), rather than the elements of \( X \) themselves, we are able to regard two equivalent elements as being ‚Äòthe same‚Äô.

‚ú¶ **Definition 5.2.12**  
Let \( X \) be a set and let \(\sim\) be an equivalence relation on \( X \). The \(\sim\)-equivalence class of an element \( a \in X \) is the set \([a]_\sim\) (LaTeX code: \texttt{[x]_{\textbackslash sim}}) defined by

\[
[a]_\sim = \{ x \in X \mid a \sim x \}
\]

The *quotient* of \( X \) by \(\sim\) is the set \( X/\sim \) (LaTeX code: \texttt{X/\{\textbackslash sim\}}) of all \(\sim\)-equivalence classes of elements of \( X \); that is

\[
X/\sim = \{ [a]_\sim \mid a \in X \}
\]

‚ú∂ **LaTeX tip**  
Putting {curly brackets} around the command for a symbol like \(\sim\) (\texttt{\textbackslash sim}) tells LaTeX to consider the symbol as a symbol, rather than as a connective. Compare the following:

| LaTeX code       | Output     |
|------------------|------------|
| \texttt{X/\sim = Y} | \( X/\sim = Y \) |
| \texttt{X/\{\textbackslash sim\} = Y} | \( X/\sim = Y \) |

This is because, without braces, LaTeX thinks you‚Äôre saying ‚ÄòX-forward-slash is related to is equal to Y‚Äô, which clearly makes no sense; putting braces around \texttt{\textbackslash sim} signifies to LaTeX that the \(\sim\) symbol is being considered as an object in its own right, rather than as a connective.

‚úë **Example 5.2.13**  
Let \(\sim\) be the relation of congruence modulo 2 on \(\mathbb{Z}\). We showed in Example 5.2.7 that, for all \( a, b \in \mathbb{Z} \) we have \( a \equiv b \mod 2 \) if and only if \( a \) and \( b \) have the same parity. But this means that, for all \([a]_\sim\) is the set of all integers with the same parity as \( a \)‚Äîthat is:

- If \( a \) is even, then \([a]_\sim\) is the set of all even integers; and
- If \( a \) is odd, then \([a]_\sim\) is the set of all odd integers.

It follows that \(\mathbb{Z}/\sim = \{ [0]_\sim, [1]_\sim \} = \{ E, O \} \), where \( E \) is the set of all even integers and \( O \) is the set of all odd integers.

‚úë **Exercise 5.2.14**  
Let \(\approx\) be the relation of congruence modulo 10 on \(\mathbb{N}\). Describe the equivalence classes, and give an explicit expression of the quotient \(\mathbb{N}/\approx\) in list notation.
</markdown><markdown>
### Example 5.2.15

Let \( f : X \to Y \) be a function, and let \(\sim_f\) be the equivalence relation on \(X\) that we defined in Exercise 5.2.4. Given \( a \in X \), we have

\[
[a]_{\sim_f} = \{ x \in X \mid a \sim_f x \} = \{ x \in X \mid f(a) = f(x) \}
\]

Thus we have \([a]_{\sim_f} = f^{-1}[\{f(a)\}]\).

### Exercise 5.2.16

Let \( f : X \to Y \) be a function. Prove that \( f \) is injective if and only if each \(\sim_f\)-equivalence class has a unique element, where \(\sim_f\) is the equivalence relation defined in Exercise 5.2.4.

The next result demonstrates that an equivalence relation \(\sim\) on a set \(X\) ‚Äòdescends‚Äô to the equality relation \(=\) on the quotient \(X/\sim\). This means that if we would rather deal with equality than with the equivalence relation itself, then we may do so by working inside the quotient \(X/\sim\) rather than in the set \(X\).

### Theorem 5.2.17

Let \(\sim\) be an equivalence relation on a set \(X\). Then for all \(a, b \in X\), we have \(a \sim b\) if and only if \([a]_{\sim} = [b]_{\sim}\).

**Proof**

The proof is an exercise in piecing together the properties of equivalence relations.

Fix \(a, b \in X\).

- (\(\Rightarrow\)) Suppose \(a \sim b\). We prove \([a]_{\sim} = [b]_{\sim}\) by double containment.

  - (\(\subseteq\)) Let \(x \in [a]_{\sim}\)‚Äîthen \(a \sim x\). We are assuming that \(a \sim b\), so that \(b \sim a\) by symmetry, and so \(b \sim x\) by transitivity. So \(x \in [b]_{\sim}\).

  - (\(\supseteq\)) Let \(x \in [b]_{\sim}\)‚Äîthen \(b \sim x\). We are assuming that \(a \sim b\), and so \(a \sim x\) by transitivity. So \(x \in [a]_{\sim}\).

  We have shown by double containment that \([a]_{\sim} = [b]_{\sim}\).

- (\(\Leftarrow\)) Assume \([a]_{\sim} = [b]_{\sim}\). We have \(b \sim b\) by reflexivity, and so \(b \in [b]_{\sim}\). But then \(b \in [a]_{\sim}\), so that \(a \sim b\), as required.

So \(a \sim b\) if and only if \([a]_{\sim} = [b]_{\sim}\).

For congruence, special terminology and notation exists for equivalence classes and quotients.
</markdown><markdown>
### Definition 5.2.18

Let \( n \in \mathbb{Z} \). The **congruence class** of an integer \( a \) modulo \( n \) is defined by

\[
[a]_n = [a] \equiv \mod n = \{ x \in \mathbb{Z} \mid a \equiv x \mod n \}
\]

The set of all congruence classes modulo \( n \) is denoted by

\[
\mathbb{Z}/n\mathbb{Z} = \mathbb{Z}/(\equiv \mod n) = \{ [a]_n \mid a \in \mathbb{Z} \}
\]

### Example 5.2.19

Using the terminology of congruence classes, Example 5.2.13 can be rephrased by saying that \(\mathbb{Z}/2\mathbb{Z} = \{ [0]_2, [1]_2 \}\). Moreover, Theorem 5.2.17 gives us a more succinct proof: for all \( a \in \mathbb{Z} \), we have \( a \equiv 0 \mod 2 \) if and only if \( a \) is even, and \( a \equiv 1 \mod 2 \) if and only if \( a \) is odd. Therefore for all \( a \in \mathbb{Z} \), we have \([a]_2 = [0]_2\) or \([a]_2 = [1]_2\), and so

\[
\mathbb{Z}/2\mathbb{Z} = \{ [a]_2 \mid a \in \mathbb{Z} \} = \{ [0]_2, [1]_2 \}
\]

Additionally, \([0]_2\) is the set of all even integers and \([1]_2\) is the set of all odd integers.

The next exercise generalises the previous one, proving that congruence classes correspond with remainders.

### Exercise 5.2.20

Let \( n \in \mathbb{Z} \) with \( n \neq 0 \). Prove that the function

\[
i : \{ 0, 1, \ldots, |n|-1 \} \to \mathbb{Z}/n\mathbb{Z}
\]

defined by \( i(r) = [r]_n \) for all \( 0 \leq r < |n| \) is a bijection.

### Partitions

A partition of a set \( X \) is a way of breaking \( X \) up into mutually disjoint subsets. They will be an immensely useful tool for counting how many elements a finite set has in Chapter 8, and will reappear in Section 10.2 for defining arithmetic operations with cardinal numbers.
</markdown><markdown>
## Definition 5.2.21

A **partition** of a set \( X \) is a collection \( \mathcal{U} = \{ U_i \mid i \in I \} \) of subsets of \( X \) such that the following conditions hold:

(a) For each \( i \in I \), the subset \( U_i \) is inhabited;

(b) The sets \( U_i \) for \( i \in I \) are **pairwise disjoint**‚Äîthat is, \( U_i \cap U_j \) is empty for all \( i, j \in I \) with \( i \neq j \);

(c) \(\bigcup_{i \in I} U_i = X\).

Note that, by contraposition, condition (b) in Exercise 5.2.26 is equivalent to saying that for all \( i, j \in I \), if \( U_i \cap U_j \) is inhabited, then \( i = j \)‚Äîthis is useful for verifying pairwise disjointness in proofs.

## Strategy 5.2.22 (Proving a family of subsets forms a partition)

Let \( X \) be a set. In order to prove a collection \( \mathcal{U} \subseteq \mathcal{P}(X) \) is a partition of \( X \), it suffices to prove:

(a) Each \( U \in \mathcal{U} \) is inhabited;

(b) For all \( U, V \in \mathcal{U} \), if \( U \cap V \) is inhabited, then \( U = V \);

(c) For all \( a \in X \), there is some \( U \in \mathcal{U} \) such that \( a \in U \).

## Example 5.2.23

We can partition \( \mathbb{Z} \) as \( E \cup O \), where \( E \) is the set of all even integers and \( O \) is the set of all odd integers:

(a) \( E \) and \( O \) are inhabited, since \( 0 \in E \) and \( 1 \in O \).

(b) The family \(\{E, O\}\) is pairwise disjoint if and only if \( E \cap O \) is empty; and it is, since no integer can be both even and odd.

(c) \( E \cup O = \mathbb{Z} \) since every integer is either even or odd.

## Example 5.2.24

The sets \(\{2n, 2n + 1\}\) for \( n \in \mathbb{N} \) form a partition of \( \mathbb{N} \):

(a) \( 2n \in \{2n, 2n + 1\} \) for each \( n \in \mathbb{N} \), so the sets are all inhabited.

(b) Suppose that \( m, n \in \mathbb{N} \) and that \(\{2m, 2m + 1\} \cap \{2n, 2n + 1\}\) is inhabited. Note that \( 2m \neq 2n + 1 \) and \( 2n \neq 2m + 1 \) by the division theorem (Theorem 7.1.1), so either...
</markdown><markdown>
Given \( a \in \mathbb{N} \), we have \( a = 2n + i \), where \( n \in \mathbb{N} \) is the quotient of \( a \) when divided by 2, and where \( i \in \{0, 1\} \) is the remainder of \( a \) when divided by 2. But then \( a \in \{2n, 2n + 1\} \). Thus \(\bigcup_{n \in \mathbb{N}} \{2n, 2n + 1\} = \mathbb{N}\).

### Exercise 5.2.25
Let \( f : X \to Y \) be a surjection, and define a collection \(\mathcal{F}\) of subsets of \( X \) by

\[
\mathcal{F} = \{ f^{-1}[\{b\}] \mid b \in Y \}
\]

That is, \(\mathcal{F}\) is the set of subsets of \( X \) given by the preimages of individual elements of \( Y \) under \( f \). Prove that \(\mathcal{F}\) is a partition of \( X \). Where in your proof do you use surjectivity of \( f \)?

### Exercise 5.2.26
Let \( X \) be a set and let \(\mathcal{U} = \{ U_i \mid i \in I \}\) be a family of inhabited subsets of \( X \). Prove that \(\mathcal{U}\) is a partition of \( X \) if and only if for each \( a \in X \), there is a unique set \( U_i \in \mathcal{U} \) with \( a \in U_i \).

### Exercise 5.2.27
If \(\sim\) be an equivalence relation on \( X \), then \( X/\sim \) is a partition \( X \).

In fact, the converse of Exercise 5.2.27 is also true, as we prove next.

### Proposition 5.2.28
Let \( X \) be a set and let \(\mathcal{U}\) be a partition of \( X \). Then \(\mathcal{U} = X/\sim\) for a unique equivalence relation \(\sim\) on \( X \).

**Proof**

Define a relation \(\sim\) by

\[
x \sim y \iff \exists U \in \mathcal{U}, x \in U \text{ and } y \in U
\]

for all \( x, y \in X \). That is, \( x \sim y \) if and only if \( x \) and \( y \) are elements of the same set of the partition. We check that \(\sim\) is an equivalence relation.

- **Reflexivity.** Let \( x \in X \). Then \( x \in U \) for some \( U \in \mathcal{U} \) since \(\bigcup_{U \in \mathcal{U}} U = X\). Hence \( x \sim x \).

- **Symmetry.** Let \( x, y \in X \) and suppose \( x \sim y \). Then there is some \( U \in \mathcal{U} \) with \( x \in U \) and \( y \in U \). But then it is immediate that \( y \sim x \).

- **Transitivity.** Let \( x, y, z \in X \) and suppose that \( x \sim y \) and \( y \sim z \). Then there exist \( U, V \in \mathcal{U} \) with \( x, y \in U \) and \( y, z \in V \). Thus \( y \in U \cap V \). Since \(\mathcal{U}\) is a partition of \( X \), its
</markdown><markdown>
elements are pairwise disjoint; thus if \( U \neq V \) then \( U \cap V = \emptyset \). Hence \( U = V \). Thus \( x \in U \) and \( z \in U \), so \( x \sim z \).

The definition of \(\sim\) makes it immediate that \( X/\sim = \mathcal{U} \).

To prove that \(\sim\) is the only such relation, suppose \(\approx\) is another equivalence relation on \( X \) for which \( X/\approx = \mathcal{U} \). Then, given \( x, y \in X \), we have:

\[
\begin{align*}
x \sim y & \iff \exists U \in \mathcal{U}, x \in U \land y \in U & \text{by definition of } \sim \\
& \iff \exists x \in X, x \in [z]_{\sim} \land y \in [z]_{\sim} & \text{since } \mathcal{U} = X/\approx \\
& \iff \exists z \in X, x \approx z \land y \approx z & \text{by definition of } [z]_{\approx} \\
& \iff x \approx y & \text{by symmetry and transitivity}
\end{align*}
\]

So \(\sim = \approx\).

Exercise 5.2.27 and Proposition 5.2.28 prove that equivalence relations and quotients are essentially the same thing: the quotient of a set by an equivalence relation is a partition of the set, and every partition of a set is the quotient by a unique equivalence relation!

The following lemma can be skipped over without grave consequences‚Äîit is a technical result with an extremely fiddly proof, but we will use it at a couple of points later in the book. It says that, given two partitioned sets, if we can pair up the sets in the partition, and pair up the elements in each pair of paired-up partitions, then we can pair up the elements of each set.

### Lemma 5.2.29
Let \( X \) and \( Y \) be sets, let \(\{ U_i \mid i \in I \}\) be a partition of \( X \) and let \(\{ V_j \mid j \in J \}\) be a partition of \( Y \). If there exists:

- A bijection \( f : I \to J \); and
- For each \( i \in I \), a bijection \( g_i : U_i \to V_{f(i)} \);

then there exists a bijection \( h : X \to Y \).

**Proof**

Given \( a \in X \), let \( i(a) \) be the unique element of \( I \) such that \( a \in X_{i(a)} \). Note that this is valid since \(\{ X_i \mid i \in I \}\) is a partition of \( X \). Likewise, given \( b \in Y \), let \( j(b) \) be the unique element of \( J \) such that \( b \in Y_{j(b)} \).

Define \( h : X \to Y \) by \( h(a) = g_{i(a)}(a) \) for all \( a \in X \). This is well-defined since

\[
h(a) = g_{i(a)}(a) \in Y_{f(i(a))} \subseteq Y
\]

This also shows that \( j(h(a)) = f(i(a)) \).
</markdown><markdown>
Now define \( k : Y \to X \) by \( k(b) = g_{f^{-1}(j(b))}^{-1}(b) \) for all \( b \in Y \). Then \( k \) is well-defined: indeed, \( g_{f^{-1}(j(b))} \) is a function from \( U_{f^{-1}(j(b))} \) to \( V_{j(b)} \), and so

\[
k(b) = g_{f^{-1}(j(b))}^{-1}(b) \in U_{f^{-1}(j(b))} \subseteq X
\]

This also shows that \( i(k(b)) = f^{-1}(j(b)) \).

Then \( k \) is an inverse for \( h \). To see this, let \( a \in X \); then

\[
\begin{align*}
k(h(a)) &= g_{f^{-1}(j(h(a)))}^{-1}(h(a)) & \text{by definition of } k \\
&= g_{f^{-1}(f(i(a)))}^{-1}(h(a)) & \text{since } j(h(a)) = f(i(a)) \\
&= g_{i(a)}^{-1}(h(a)) & \text{since } f^{-1} \circ f = \text{id}_I \\
&= g_{i(a)}^{-1}(g_{i(a)}(a)) & \text{by definition of } h \\
&= a & \text{since } g_{i(a)}^{-1} \circ g_{i(a)} = \text{id}_{X_{i(a)}}
\end{align*}
\]

A similarly tedious computation reveals that \( h(k(b)) = b \) for all \( b \in Y \):

\[
\begin{align*}
h(k(b)) &= g_{i(k(b))}(k(b)) & \text{by definition of } h \\
&= g_{f^{-1}(j(b))}(k(b)) & \text{since } i(k(b)) = f^{-1}(j(b)) \\
&= g_{f^{-1}(j(b))}(g_{f^{-1}(j(b))}^{-1}(b)) & \text{by definition of } k \\
&= b & \text{since } g_{f^{-1}(j(b))} \circ g_{f^{-1}(j(b))}^{-1} = \text{id}_{Y_{j(b)}}
\end{align*}
\]

So \( k \) is an inverse for \( h \), as required.

‚úé **Exercise 5.2.30**  
Let \( X \) and \( Y \) be sets, let \( \sim \) be an equivalence relation on \( X \) and let \( \approx \) be an equivalence relation on \( Y \). Assume that there is a bijection \( p : X / \sim \to Y / \approx \), and for each equivalence class \( E \in X / \sim \) there is a bijection \( h_E : E \to p(E) \). Use Lemma 5.2.29 to prove that there is a bijection \( h : X \to Y \).

## The quotient function

We will now show that equivalence relations on a set \( X \) are essentially the same thing as surjections from \( X \) to another set.

‚ú¶ **Definition 5.2.31**  
Let \( X \) be a set and let \( \sim \) be an equivalence relation on \( X \). The **quotient function** for \( \sim \) is the function \( q_\sim : X \to X / \sim \) defined by \( q(a) = [a]_\sim \) for each \( a \in X \). That is, the quotient function sends each element of \( X \) to its \( \sim \)-equivalence class.
</markdown><markdown>
### Example 5.2.32

Recall that, given \( a \in \mathbb{Z} \), we have \([a]_2 = [0]_2\) if \( a \) is even, and \([a]_2 = [1]_2\) is \( a \) is odd. Thus the quotient function \( q_2 : \mathbb{Z} \to \mathbb{Z}/2\mathbb{Z} \) can be viewed as telling us the parity of an integer.

### Exercise 5.2.33

Let \( n \in \mathbb{Z} \) with \( n \neq 0 \). Describe the quotient function \( q_n : \mathbb{Z} \to \mathbb{Z}/n\mathbb{Z} \) in terms of remainders.

### Exercise 5.2.34

Let \(\sim\) be an equivalence relation on a set \( X \). Prove that the quotient function \( q_\sim : X \to X/\sim \) is surjective.

The theorem we prove next can be viewed as the converse to Exercise 5.2.34. It proves that every surjection ‚Äòis‚Äô a quotient function, in the sense that given any surjection \( p : X \to A \), we can view \( A \) as a quotient of \( X \) by a suitably-defined equivalence relation, and then \( p \) ‚Äòis‚Äô the corresponding quotient function.

### Theorem 5.2.35

Let \( X \) be a set. Then for every set \( A \) and every surjection \( p : X \to A \), there exist a unique equivalence relation \(\sim\) on \( X \) and bijection \( f : X/\sim \to A \) such that \( f([x]) = p(x) \) for all \( x \in X \).

**Proof**

Let \( A \) be a set and \( p : X \to A \) be a surjection.

- **(Existence)** Define a relation \(\sim\) on \( X \) by \( x \sim y \) if and only if \( p(x) = p(y) \). Then \(\sim\) is an equivalence relation by Exercise 5.2.4.

  Moreover, given \( x \in X \), we have

  \[
  [x]_\sim = \{ y \in X \mid p(x) = p(y) \} = p^{-1}(\{ p(x) \})
  \]

  So define \( f : X/\sim \to A \) by letting \( f([x]_\sim) = p(x) \). Then \( f \) is well-defined, since if \([x]_\sim = [y]_\sim\), then \( x \sim y \), so that \( p(x) = p(y) \).

  Furthermore, \( f \) is a bijection:

  - **(Injectivity)** Let \([x]_\sim, [y]_\sim \in X/\sim\) and assume \( f([x]_\sim) = f([y]_\sim) \). Then \( p(x) = p(y) \), so that \( x \sim y \), and hence \([x]_\sim = [y]_\sim\).

  - **(Surjectivity)** Let \( a \in A \). Since \( p \) is a surjection, there is some \( x \in X \) such that \( p(x) = a \). But then \( f([x]_\sim) = p(x) = a \).

  So we have established that there exist an equivalence relation \(\sim\) on \( X \) and a bijection \( f : X/\sim \to A \) such that \( f([x]_\sim) = p(x) \) for all \( x \in X \).

- **(Uniqueness)** Suppose \(\approx\) is another equivalence relation on \( X \) and that \( g : X/\approx \to A \) is a bijection such that \( g([x]_\approx) = p(x) \) for all \( x \in X \). We prove that \(\sim = \approx\), and then that \( g = f \), so that \(\sim\) and \( f \) are unique.
</markdown><markdown>
So let \( x, y \in X \). Then

\[
x \sim y \iff p(x) = p(y) \quad \text{by definition of } \sim
\]
\[
\iff g([x]_{\approx}) = g([y]_{\approx}) \quad \text{by definition of } g
\]
\[
\iff [x]_{\approx} = [y]_{\approx} \quad \text{since } g \text{ is bijective}
\]
\[
\iff x \approx y \quad \text{by Exercise 5.2.27}
\]

It follows that \(\sim = \approx\), and then for all \( x \in X \) we have

\[
f([x]_{\sim}) = p(x) = g([x]_{\approx}) = g([x]_{\sim})
\]

so that \( f = g \), as required.

In light of [Theorem 5.2.35](#), we have now established the equivalence of three notions for a given set \( X \):

\[
\begin{array}{c}
\text{equivalence relations} \\
\text{on } X
\end{array}
\]

\[
\begin{array}{ccc}
& \nearrow & \text{surjections with} \\
\text{partitions of } X & \longleftrightarrow & \text{domain } X
\end{array}
\]

**Exercise 5.2.36**

Give an explicit description of the dashed arrow in the above diagram. That is, describe the correspondence between partitions of a set \( X \) and surjections whose domain is \( X \).
</markdown><markdown>
## Chapter 5 exercises

### Properties of relations

5.1. For each of the eight subsets

\[ P \subseteq \{\text{reflexive, symmetric, transitive}\} \]

find a relation satisfying (only) the properties in \( P \).

5.2. Prove that if \( R \) is a symmetric, antisymmetric relation on a set \( X \), then it is a subrelation of the equality relation‚Äîthat is, \( \text{Gr}(R) \subseteq \text{Gr}(=) \).

5.3. A relation \( R \) on a set \( X \) is **left-total** if for all \( x \in X \), there exists some \( y \in X \) such that \( x R y \). Prove that every left-total, symmetric, transitive relation is reflexive.

### Equivalence relations

‚ú¶ **Definition 5.E.1**  
Let \( R \) be a relation on a set \( X \) and let \( f : X \to Y \) be a function. The **transport** of \( R \) along \( f \) is the relation \( S \) on \( Y \) defined for \( c, d \in Y \) by letting \( c \, S \, d \) if and only if there exist \( a, b \in X \) such that \( f(a) = c, f(b) = d \) and \( a \, R \, b \). That is

\[ \text{Gr}(S) = \{(f(a), f(b)) \mid a, b \in X, \, a \, R \, b \} \]

5.4. Let \( X \) and \( Y \) be sets and let \( f : X \to Y \). Prove that if \( \sim \) is an equivalence relation on \( X \), then the transport of \( \sim \) along \( f \) is an equivalence relation on \( Y \).

‚ú¶ **Definition 5.E.2**  
Let \( R \) be any relation on a set \( X \). The **equivalence relation generated by \( R \)** is the relation \( \sim_R \) on \( X \) defined as follows. Given \( x, y \in X \), say \( x \sim_R y \) if and only if for some \( k \in \mathbb{N} \) there is a sequence \( (a_0, a_1, \ldots, a_k) \) of elements of \( X \) such that \( a_0 = x, a_k = y \), and for all \( 0 \leq i < k \), either \( a_i R a_{i+1} \) or \( a_{i+1} R a_i \).

5.5. Fix \( n \in \mathbb{Z} \) and let \( R \) be the relation on \( \mathbb{Z} \) defined by \( x R y \) if and only if \( y = x + n \). Prove that \( \sim_R \) is the relation of congruence modulo \( n \).

5.6. Let \( X \) be a set and let \( R \) be the subset relation on \( \mathcal{P}(X) \). Prove that \( U \sim_R V \) for all \( U, V \subseteq X \).
</markdown><markdown>
5.7. Let \( X \) be a set, fix two distinct elements \( a, b \in X \), and define a relation \( R \) on \( X \) by declaring \( aRb \) only‚Äîthat is, for all \( x, y \in X \), we have \( xRy \) if and only if \( x = a \) and \( y = b \). Prove that the relation \(\sim_R\) is defined by \( x \sim y \) if and only if either \( x = y \) or \(\{x, y\} = \{a, b\}\).

In Questions 5.8 to 5.11, let \( R \) be a relation on a set \( X \), and let \(\sim_R\) be the equivalence relation generated by \( R \) (as in Definition 5.E.2). In these questions, you will prove that \(\sim_R\) is the ‚Äòsmallest‚Äô equivalence relation extending \( R \).

5.8. Prove that \(\sim_R\) is an equivalence relation on \( X \).

5.9. Prove that \( xRy \Rightarrow x \sim_R y \) for all \( x, y \in X \).

5.10. Prove that if \(\approx\) is any equivalence relation on \( X \) and \( xRy \Rightarrow x \approx y \) for all \( x, y \in X \), then \( x \sim_R y \Rightarrow x \approx y \) for all \( x, y \in X \).

5.11. Prove that if \( R \) is an equivalence relation, then \(\sim_R = R\).

5.12. This question requires some familiarity with concepts from calculus. Let \( \mathcal{C}^1(\mathbb{R}) \) denote the set of all differentiable functions \( F : \mathbb{R} \to \mathbb{R} \) whose derivative \( F' : \mathbb{R} \to \mathbb{R} \) is continuous, and define a relation \(\sim\) on \( \mathcal{C}^1(\mathbb{R}) \) by letting \( F \sim G \) if and only if \( F' = G' \).

(a) Prove that \(\sim\) is an equivalence relation on \( \mathcal{C}^1(\mathbb{R}) \).

(b) Let \( F \in \mathcal{C}^1(\mathbb{R}) \). Prove that

\[
[F]_\sim = \{ G \in \mathcal{C}^1(\mathbb{R}) \mid \exists c \in \mathbb{R}, \forall x \in \mathbb{R}, G(x) = F(x) + c \}
\]

(c) Interpret the notion of an indefinite integral \(\int f(x) \, dx\) of a continuous function \( f : \mathbb{R} \to \mathbb{R} \) in terms of \(\sim\)-equivalence classes.

**True‚ÄìFalse questions**

In Questions 5.13 to 5.19, determine (with proof) whether the statement is true or false.

5.13. Every reflexive relation is symmetric.

5.14. There is a unique relation on \( \mathbb{N} \) that is reflexive, symmetric and antisymmetric.

5.15. Every relation that is not symmetric is antisymmetric.

5.16. Every symmetric, antisymmetric relation is reflexive.

5.17. The empty set is the graph of a relation on \( \mathbb{N} \).

5.18. For all sets \( X \), the graph of a function \( X \to X \) is the graph of a relation on \( X \).

5.19. For all sets \( X \), the graph of a relation on \( X \) is the graph of a function \( X \to X \).
</markdown><markdown>
### Always‚ÄìSometimes‚ÄìNever questions

In Questions 5.20 to 5.21, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

5.20. Let \(\sim\) be an equivalence relation on a set \(X\). Then the relation \(\approx\) on \(\mathcal{P}(X) \setminus \{\emptyset\}\), defined by \(A \approx B\) if and only if \(x \sim y\) for all \(x \in A\) and \(y \in B\), is an equivalence relation.

5.21. Let \(\sim\) be an equivalence relation on a set \(X\). Then the relation \(\approx\) on \(\mathcal{P}(X) \setminus \{\emptyset\}\), defined by \(A \approx B\) if and only if \(x \sim y\) for some \(x \in A\) and \(y \in B\), is an equivalence relation.
</markdown><markdown>
## Chapter 6

# Finite and infinite sets
</markdown><markdown>
## Section 6.1

# Finite sets

As its title suggests, this section is all about exploring the properties of finite sets, and to do this we must first define what we mean by ‚Äòfinite‚Äô. We certainly know a finite set when we see one‚Äîfor example:

- The set \(\{ \text{red, orange, yellow, green, blue, purple} \}\) is finite.
- The set \([0, 1]\) is infinite, but it has finite length.
- The set \([0, \infty)\) is infinite and has infinite length.
- The set \(\mathcal{P}(\mathbb{N})\) is infinite, but has no notion of ‚Äòlength‚Äô to speak of.
- The empty set \(\emptyset\) is finite.

If we are to make a definition of ‚Äòfinite set‚Äô, we must first figure out what the finite sets above have in common but the infinite sets do not.

It is difficult to define ‚Äòfinite‚Äô without being imprecise. A first attempt at a definition might be something like the following:

> A set \(X\) is finite if the elements of \(X\) don‚Äôt go on forever.

This is good intuition, but isn‚Äôt good enough as a mathematical definition, because ‚Äògo on‚Äô and ‚Äòforever‚Äô are not precise terms (unless they themselves are defined). So let‚Äôs try to make this more precise:

> A set \(X\) is finite if the elements of \(X\) can be listed one by one  
> in such a way that the list has both a start and an end.

This is better but is still not entirely precise‚Äîit is not entirely clear what is meant by ‚Äòlisted one by one‚Äô. But we can make this precise: to list the elements of \(X\) one-by-one means that we are specifying a ‚Äòfirst element‚Äô, a ‚Äòsecond element‚Äô, a ‚Äòthird element‚Äô, and so on. To say that this list has an end means that we eventually reach the ‚Äò\(n\)th element‚Äô, for some \(n \in \mathbb{N}\), and there is no ‚Äò\((n + 1)^{st}\) element‚Äô. In other words, for some natural number \(n\), we are pairing up the elements of \(X\) with the natural numbers from 1 to \(n\).

Recall that, for each \(n \in \mathbb{N}\), the set of natural numbers from 1 up to \(n\) has its own notation:
</markdown><markdown>
### Definition 2.1.9
Let \( n \in \mathbb{N} \). The set \([n]\) is defined by \([n] = \{ k \in \mathbb{N} \mid 1 \leq k \leq n \}\).

Since ‚Äòpairing up‚Äô really means ‚Äòfinding a bijection‚Äô, we are now ready to define what it means for a set to be finite.

### Definition 6.1.1
A set \( X \) is **finite** if there exists a bijection \( f : [n] \to X \) for some \( n \in \mathbb{N} \). The function \( f \) is called an **enumeration** of \( X \). If \( X \) is not finite we say it is **infinite**.

This definition suggests the following strategy for proving that a set is finite.

### Strategy 6.1.2 (Proving that a set is finite)
In order to prove that a set \( X \) is finite, it suffices to find a bijection \([n] \to X\) for some \( n \in \mathbb{N} \).

### Example 6.1.3
Let \( X = \{ \text{red, orange, yellow, green, blue, purple} \} \). We said above that \( X \) is finite; now we can prove it. Define \( f : [6] \to X \) by

\[
\begin{align*}
f(1) &= \text{red} \\
f(2) &= \text{orange} \\
f(3) &= \text{yellow} \\
f(4) &= \text{green} \\
f(5) &= \text{blue} \\
f(6) &= \text{purple}
\end{align*}
\]

The function \( f \) is evidently a bijection, since each element of \( X \) can be expressed uniquely as \( f(k) \) for some \( k \in [6] \). So \( X \) is finite.

### Exercise 6.1.4
Prove that \([n]\) is finite for each \( n \in \mathbb{N} \).

Note that Exercise 6.1.4 implies, in particular, that \(\emptyset\) is finite, since \(\emptyset = [0]\).

## The size of a finite set

Whilst it might sometimes be useful just to know that a set is finite, it will be even more useful to know how many elements it has. This quantity is called the **size** of the set. Intuitively, the size of the set should be the length of the list of its elements, but for this to be well-defined, we first need to know that the number of elements in the list is independent of the way we choose to list them.

The ‚Äòlist of elements‚Äô of a finite set \( X \) is the bijection \([n] \to X\) given by Definition 6.1.1, and \( n \) is the length of the list, this means that we need to prove that if \([m] \to X\) and \([n] \to X\) are bijections, then \( m = n \). This will be Theorem 6.1.9.
</markdown><markdown>
To be able to prove this, we must first prove some technical results involving functions that we will use in the proof.

### Lemma 6.1.5
Let \( X \) and \( Y \) be sets, let \( f : X \to Y \) be an injection and let \( a \in X \). Then there is an injection \( f^v : X \setminus \{a\} \to Y \setminus \{f(a)\} \) defined by \( f^v(x) = f(x) \) for all \( x \in X \setminus \{a\} \). Moreover if \( f \) is a bijection \( X \to Y \), then \( f^v \) is a bijection \( X \setminus \{a\} \to Y \setminus \{f(a)\} \).

**Proof**  
Note that \( f^v \) is well-defined by injectivity of \( f \): indeed, if we had \( f(x) = f(a) \) for some \( x \in X \setminus \{a\} \), then that would imply \( x = a \in X \setminus \{a\} \), which is a contradiction.

To see that \( f^v \) is injective, let \( x, y \in X \setminus \{a\} \) and \( f^v(x) = f^v(y) \). Then \( f(x) = f(y) \), so that \( x = y \) by injectivity of \( f \).

Now suppose that \( f \) is a bijection. Then \( f \) has an inverse \( g : Y \to X \), which is itself a bijection (and hence an injection). Repeating the above argument with \( g \) yields an injection \( g^v : Y \setminus \{f(a)\} \to X \setminus \{a\} \) defined by \( g^v(y) = g(y) \) for all \( y \in Y \). But then for all \( x \in X \setminus \{a\} \) and all \( y \in Y \setminus \{f(a)\} \) we have

\[
g^v(f^v(x)) = g(f(x)) = x \quad \text{and} \quad f^v(g^v(y)) = f(g(y))
\]

so that \( g^v \) is an inverse for \( f^v \). Hence if \( f \) is a bijection, then \( f^v \) is a bijection.

A consequence of Lemma 6.1.5 is the next useful result, which we will use in our proof that every set has a unique ‚Äòsize‚Äô.

### Lemma 6.1.6
Let \( X \) be an inhabited set. There is a bijection \( X \setminus \{a\} \to X \setminus \{b\} \) for all \( a, b \in X \).

**Proof**  
Define \( f : X \to X \) be the function that swaps \( a \) and \( b \) and leaves all other elements of \( X \) fixed‚Äîthat is, we define

\[
f(x) = 
\begin{cases} 
b & \text{if } x = a \\ 
a & \text{if } x = b \\ 
x & \text{if } x \notin \{a, b\} 
\end{cases}
\]

Note that \( f(f(x)) = x \) for all \( x \in X \), so \( f \) is a bijection‚Äîit is its own inverse! Since \( f(a) = b \), it follows from Lemma 6.1.5 that the function \( f^v : X \setminus \{a\} \to X \setminus \{b\} \) defined by \( f^v(x) = f(x) \) for all \( x \in X \setminus \{a\} \) is a bijection, as required.
</markdown><markdown>
### Theorem 6.1.7

Let \( m, n \in \mathbb{N} \).

(a) If there exists an injection \( f : [m] \to [n] \), then \( m \leq n \).

(b) If there exists a surjection \( g : [m] \to [n] \), then \( m \geq n \).

(c) If there exists a bijection \( h : [m] \to [n] \), then \( m = n \).

**Proof of (a)**

For fixed \( m \in \mathbb{N} \), let \( p(m) \) be the assertion that, for all \( n \in \mathbb{N} \), if there exists an injection \( [m] \to [n] \), then \( m \leq n \). We prove that \( p(m) \) is true for all \( m \in \mathbb{N} \) by induction.

- **(Base case)** We need to prove that, for all \( n \in \mathbb{N} \) if there exists an injection \( [0] \to [n] \), then \( 0 \leq n \). This is automatically true, since \( 0 \leq n \) for all \( n \in \mathbb{N} \).

- **(Induction step)** Fix \( m \in \mathbb{N} \) and suppose that, for all \( n \in \mathbb{N} \), if there exists an injection \( [m] \to [n] \), then \( m \leq n \).

  Now let \( n \in \mathbb{N} \) and suppose that there is an injection \( f : [m+1] \to [n] \). We need to prove that \( m+1 \leq n \).

  First note that \( n \geq 1 \). Indeed, since \( m+1 \geq 1 \), we have \( 1 \in [m+1] \), and so \( f(1) \in [n] \). This means that \( [n] \) is inhabited, and so \( n \geq 1 \). In particular, \( n-1 \in \mathbb{N} \) and so the set \( [n-1] \) is well-defined. It suffices to prove that \( m \leq n-1 \).

  Let \( a = f(m+1) \in [n] \). Then:

  - By Lemma 6.1.5 there is an injection \( f^\vee : [m] = [m+1] \setminus \{m+1\} \to [n] \setminus \{a\} \).
  - By Lemma 6.1.6 there is a bijection \( g : [n] \setminus \{a\} \to [n] \setminus \{n\} = [n-1] \).

  Composing these two functions gives an injection \( g \circ f^\vee : [m] \to [n-1] \). But then \( m \leq n-1 \) by the induction hypothesis, and so \( m+1 \leq n \) as required.

The result now follows by induction.

‚ú∑

**Exercise 6.1.8**

Prove parts (b) and (c) of Theorem 6.1.7.

Phew! That was fun. With these technical results proved, we can now prove the theorem we needed for the size of a finite set to be well-defined.

### Theorem 6.1.9 (Uniqueness of size)

Let \( X \) be a finite set and let \( f : [m] \to X \) and \( g : [n] \to X \) be enumerations of \( X \), where \( m, n \in \mathbb{N} \). Then \( m = n \).

**Proof**

Since \( f : [m] \to X \) and \( g : [n] \to X \) are bijections, the function \( g^{-1} \circ f : [m] \to [n] \) is a bijection by Exercises 3.2.20 and 3.2.45. Hence \( m = n \) by Theorem 6.1.7(c).

‚ú∑
</markdown><markdown>
As we mentioned above, [Theorem 6.1.9](#) tells us that any two ways of listing (enumerating) the elements of a finite set yield the same number of elements. We may now make the following definition.

‚ú¶ **Definition 6.1.10**  
Let \( X \) be a finite set. The **size** (or **cardinality**) of \( X \), written \( |X| \), is the unique natural number \( n \) for which there exists a bijection \([n] \to X\).

‚úê **Example 6.1.11**  
[Example 6.1.3](#) showed that \(\{ \text{red, orange, yellow, green, blue, purple} \} = 6\), and provided the proof was correct, [Exercise 6.1.4](#) showed that \(|[n]| = n\) for all \( n \in \mathbb{N} \); in particular, \(|\emptyset| = 0\).

‚úê **Example 6.1.12**  
Fix \( n \in \mathbb{N} \) and let \( X = \{ a \in \mathbb{Z} \mid -n \leq a \leq n \} \). There is a bijection \( f : [2n + 1] \to X \) defined by \( f(k) = k - n - 1 \). Indeed:

- \( f \) is well-defined. We need to prove \( f(k) \in X \) for all \( k \in [2n + 1] \). Well given \( k \in [2n + 1] \), we have \( 1 \leq k \leq 2n + 1 \), and so

  \[
  -n = 1 - (n + 1) \leq k - (n + 1) = f(k) \leq (2n + 1) - (n + 1) = n
  \]

  so that \( f(k) \in X \) as claimed.

- \( f \) is injective. Let \( k, \ell \in [2n + 1] \) and assume \( f(k) = f(\ell) \). Then \( k - n - 1 = \ell - n - 1 \), and so \( k = \ell \).

- \( f \) is surjective. Let \( a \in X \) and define \( k = a + n + 1 \). Then

  \[
  1 = (-n) + n + 1 \leq \underbrace{a + n + 1}_{=k} \leq n + n + 1 = 2n + 1
  \]

  and so \( k \in [2n + 1] \), and moreover \( f(k) = (a + n + 1) - n - 1 = a \).

Since \( f \) is a bijection, we have \(|X| = 2n + 1\) by [Definition 6.1.10](#).

‚úê **Exercise 6.1.13**  
Let \( X \) be a finite set with \(|X| = n > 1\). Let \( a \in X \) and let \( b \notin X \). Prove that

(a) \( X \setminus \{a\} \) is finite and \(|X \setminus \{a\}| = n - 1\); and

(b) \( X \cup \{b\} \) is finite and \(|X \cup \{b\}| = n + 1\).

Identify where in your proofs you make use the hypotheses that \( a \in X \) and \( b \notin X \).
</markdown><markdown>
## Comparing the sizes of finite sets

When we used dots and stars to motivate the definitions of injective and surjective functions at the beginning of [Section 3.2](#), we suggested the following intuition:

- If there is an injection \( f : X \to Y \), then \( X \) has ‚Äòat most as many elements as \( Y \)‚Äô; and
- If there is a surjection \( g : X \to Y \), then \( X \) has ‚Äòat least as many elements as \( Y \)‚Äô.

We are now in a position to prove this, at least when \( X \) and \( Y \) are finite. The following theorem is a generalisation of [Theorem 6.1.7](#).

### Theorem 6.1.14
Let \( X \) and \( Y \) be sets.

(a) If \( Y \) is finite and there is an injection \( f : X \to Y \), then \( X \) is finite and \( |X| \leq |Y| \).

(b) If \( X \) is finite and there is a surjection \( f : X \to Y \), then \( Y \) is finite and \( |X| \geq |Y| \).

(c) If one of \( X \) or \( Y \) is finite and there is a bijection \( f : X \to Y \), then \( X \) and \( Y \) are both finite and \( |X| = |Y| \).

**Proof of (a)**

We prove by induction that, for all \( n \in \mathbb{N} \), if \( Y \) is a finite set of size \( n \) and there is an injection \( f : X \to Y \), then \( X \) is finite and \( |X| \leq n \).

- **(Base case)** Let \( Y \) be a finite set of size 0‚Äîthat is, \( Y \) is empty. Suppose there is an injection \( f : X \to Y \). If \( X \) is inhabited, then there exists an element \( a \in X \), so that \( f(a) \in Y \). This contradicts emptiness of \( Y \), so that \( X \) must be empty. Hence \( |X| = 0 \leq 0 \), as required.

- **(Induction step)** Fix \( n \in \mathbb{N} \) and assume that, if \( Z \) is a finite set of size \( n \) and there is an injection \( f : X \to Z \), then \( X \) is finite and \( |X| \leq n \).

  Fix a finite set \( Y \) of size \( n + 1 \) and an injection \( f : X \to Y \). We need to prove that \( X \) is finite and \( |X| \leq n + 1 \).

  If \( X \) is empty, then \( |X| = 0 \leq n + 1 \) as required. So assume that \( X \) is inhabited, and fix an element \( a \in X \).

  By [Lemma 6.1.5](#), there is an injection \( f' : X \setminus \{a\} \to Y \setminus \{f(a)\} \). Moreover, by [Exercise 6.1.13](#), \( Y \setminus \{f(a)\} \) is finite and \( |Y \setminus \{f(a)\}| = (n + 1) - 1 = n \).

  Applying the induction hypothesis with \( Z = Y \setminus \{f(a)\} \) tells us that, \( X \setminus \{a\} \) is finite and \( |X \setminus \{a\}| \leq n \). But \( |X \setminus \{a\}| = |X| - 1 \) by [Exercise 6.1.13](#), and so \( |X| \leq n + 1 \), as required.
</markdown><markdown>
The result now follows by induction.

### Exercise 6.1.15
Prove parts (b) and (c) of Theorem 6.1.14.

Theorem 6.1.14 suggests the following strategies for comparing the sizes of finite sets:

#### Strategy 6.1.16 (Comparing the sizes of finite sets)
Let \( X \) and \( Y \) be finite sets.

(a) In order to prove that \( |X| \leq |Y| \), it suffices to find an injection \( X \to Y \).

(b) In order to prove that \( |X| \geq |Y| \), it suffices to find a surjection \( X \to Y \).

(c) In order to prove that \( |X| = |Y| \), it suffices to find a bijection \( X \to Y \).

Strategy (c) is commonly known as **bijective proof**.

### Closure properties of finite sets

We now use Strategy 6.1.16 to prove some **closure properties** of finite sets‚Äîthat is, operations we can perform on finite sets to ensure that the result remains finite.

### Exercise 6.1.17
Let \( X \) be a finite set. Prove that every subset \( U \subseteq X \) is finite and \( |U| \leq |X| \).

### Exercise 6.1.18
Let \( X \) and \( Y \) be finite sets. Prove that \( X \cap Y \) is finite.

#### Proposition 6.1.19
Let \( X \) and \( Y \) be finite sets. Then \( X \cup Y \) is finite, and moreover

\[
|X \cup Y| = |X| + |Y| - |X \cap Y|
\]

**Proof**

We will prove this in the case when \( X \) and \( Y \) are disjoint. The general case, when \( X \) and \( Y \) are not assumed to be disjoint, will be Exercise 6.1.20.

Let \( m = |X| \) and \( n = |Y| \), and let \( f : [m] \to X \) and \( g : [n] \to Y \) be bijections.

Since \( X \) and \( Y \) are disjoint, we have \( X \cap Y = \emptyset \). Define \( h : [m + n] \to X \cup Y \) as follows; given \( k \in [m + n] \), let

\[
h(k) = 
\begin{cases} 
f(k) & \text{if } k \leq m \\
g(k - m) & \text{if } k > m 
\end{cases}
\]
</markdown><markdown>
Note that \( h \) is well-defined: the cases \( k \leq m \) and \( k > m \) are mutually exclusive, they cover all possible cases, and \( k - m \in [n] \) for all \( m + 1 \leq k \leq n \) so that \( g(k - m) \) is defined. It is then straightforward to check that \( h \) has an inverse \( h^{-1} : X \cup Y \to [m + n] \) defined by

\[
h^{-1}(z) = 
\begin{cases} 
f^{-1}(z) & \text{if } z \in X \\ 
g^{-1}(z) + m & \text{if } z \in Y 
\end{cases}
\]

Well-definedness of \( h^{-1} \) relies fundamentally on the assumption that \( X \cap Y = \emptyset \), as this is what ensures that the cases \( x \in X \) and \( x \in Y \) are mutually exclusive.

Hence \(|X \cup Y| = m + n = |X| + |Y|\), which is as required since \(|X \cap Y| = 0\).

### Exercise 6.1.20
The following steps complete the proof of [Proposition 6.1.19](#):

(a) Given sets \( A \) and \( B \), prove that the sets \( A \times \{0\} \) and \( B \times \{1\} \) are disjoint, and find bijections \( A \to A \times \{0\} \) and \( B \to B \times \{1\} \). Write \( A \sqcup B \) (LaTeX code: `\sqcup`) to denote the set \((A \times \{0\}) \cup (B \times \{1\})\). The set \( A \sqcup B \) is called the **disjoint union** of \( A \) and \( B \).

(b) Prove that, if \( A \) and \( B \) are finite then \( A \sqcup B \) is finite and

\[
|A \sqcup B| = |A| + |B|
\]

(c) Let \( X \) and \( Y \) be sets. Find a bijection

\[
(X \cup Y) \sqcup (X \cap Y) \to X \cup Y
\]

(d) Complete the proof of [Proposition 6.1.19](#)‚Äîthat is, prove that if \( X \) and \( Y \) are finite sets, not necessarily disjoint, then \( X \cup Y \) is finite and

\[
|X \cup Y| = |X| + |Y| - |X \cap Y|
\]

### Exercise 6.1.21
Let \( X \) be a finite set and let \( U \subseteq X \). Prove that \( X \setminus U \) is finite, and moreover \(|X \setminus U| = |X| - |U|\).

### Exercise 6.1.22
Let \( m, n \in \mathbb{N} \). Prove that \([m] \times [n] = mn\).

### Proposition 6.1.23
Let \( X \) and \( Y \) be finite sets. Then \( X \times Y \) is finite, and moreover

\[
|X \times Y| = |X| \cdot |Y|
\]
</markdown><markdown>
Proof

Let \( X \) and \( Y \) be finite sets, let \( m = |X| \) and \( n = |Y| \), and let \( f : [m] \to X \) and \( g : [n] \to Y \) be bijections. Define a function \( h : [m] \times [n] \to X \times Y \) by

\[
h(k, \ell) = (f(k), g(\ell))
\]

for each \( k \in [m] \) and \( \ell \in [n] \). It is easy to see that this is a bijection, with inverse defined by

\[
h^{-1}(x, y) = (f^{-1}(x), g^{-1}(y))
\]

for all \( x \in X \) and \( y \in Y \). By Exercise 6.1.22 there is a bijection \( p : [mn] \to [m] \times [n] \), and by Exercise 3.2.20 the composite \( h \circ p : [mn] \to X \times Y \) is a bijection. Hence \(|X \times Y| = mn\).

In summary, we have proved that the property of finiteness is preserved by taking subsets, pairwise unions, pairwise intersections, pairwise cartesian products, and relative complements.

## Infinite sets

We conclude this section by proving that not all sets are finite‚Äîspecifically, we‚Äôll prove that \( \mathbb{N} \) is infinite. Intuitively this seems extremely easy: of course \( \mathbb{N} \) is infinite! But in mathematical practice, this isn‚Äôt good enough: we need to use our definition of ‚Äòinfinite‚Äô to prove that \( \mathbb{N} \) is infinite. Namely, we need to prove that there is no bijection \([n] \to \mathbb{N}\) for any \( n \in \mathbb{N} \). We will use Lemma 6.1.24 below in our proof.

### Lemma 6.1.24

Every inhabited finite set of natural numbers has a greatest element.

**Proof**

We‚Äôll prove by induction on \( n \geq 1 \) that every subset \( U \subseteq \mathbb{N} \) of size \( n \) has a greatest element.

- **(Base case)** Take \( U \subseteq \mathbb{N} \) with \(|U| = 1\). then \( U = \{m\} \) for some \( m \in \mathbb{N} \). Since \( m \) is the only element of \( U \), it is certainly the greatest element of \( U \)!

- **(Induction step)** Fix \( n \geq 1 \) and suppose that every set of natural numbers of size \( n \) has a greatest element (IH).

  Let \( U \subseteq \mathbb{N} \) with \(|U| = n + 1\). We wish to show that \( U \) has a greatest element.

  Since \(|U| = n + 1\), we may write \( U = \{m_1, m_2, \ldots, m_n, m_{n+1}\} \) for distinct natural numbers \( m_k \). But then \(|U \setminus \{m_{n+1}\}| = n\) by Exercise 6.1.13, and so by the induction hypothesis, \( U \setminus \{m_{n+1}\} \) has a greatest element, say \( m_k \). Now:

  - If \( m_k < m_{n+1} \), then \( m_{n+1} \) is the greatest element of \( U \).
</markdown><markdown>
If \( m_k > m_{n+1} \), then \( m_k \) is the greatest element of \( U \).

In any case, \( U \) has a greatest element. This completes the induction step.

### Theorem 6.1.25
The set \( \mathbb{N} \) is infinite.

**Proof**

We proceed by contradiction. Suppose \( \mathbb{N} \) is finite. Then \( |\mathbb{N}| = n \) for some \( n \in \mathbb{N} \), and hence \( \mathbb{N} \) is either empty (nonsense, since \( 0 \in \mathbb{N} \)) or, by Lemma 6.1.24, it has a greatest element \( g \). But \( g + 1 \in \mathbb{N} \) since every natural number has a successor, and \( g + 1 > g \), so this contradicts maximality of \( g \). Hence \( \mathbb{N} \) is infinite.
</markdown><markdown>
## Section 6.2

# Countable and uncountable sets

In [Section 6.1](#), we defined what it means for a set to be ‚Äòfinite‚Äô in order to capture the idea that its elements can be listed in such a way that the list has a start and an end. We did so by observing that a list of the elements of a finite set is essentially the same thing as a bijection \( f : [n] \to X \) for some \( n \in \mathbb{N} \), with the element \( f(k) \in X \) playing the role of the \( k \)th element of the list.

We are now interested in **infinite sets**. We certainly can‚Äôt expect a list of all the elements of an infinite set to end, so the question now is: can its elements be listed if we allow the list to be infinite? We will call such sets **countable sets**.

It is perhaps surprising that not every set is countable: some sets are ‚Äòtoo big‚Äô for their elements to be listed! We will prove that such **uncountable sets** exist later in this section.

The precise definition of what it means for a set \( X \) to be countable ([Definition 6.2.1](#)) captures the idea that we can list the elements of \( X \) one-by-one such that, even if the list goes on forever, each element of \( X \) appears at some finite stage in the list. The list might or might not be finite; it has a start, but it might not have an end.

To illustrate, consider the following list of the elements of \( \mathbb{N} \):

\[ 0, \ 1, \ 2, \ 3, \ 4, \ \ldots, \ n, \ n+1, \ \ldots \]

The list does not end, since \( \mathbb{N} \) is infinite ([Theorem 6.1.25](#)), but nevertheless every natural number appears at some finite stage along the list.

As another example, consider the set \( \mathbb{Z} \) of all integers. We might wish to list the elements of \( \mathbb{Z} \) in the usual way:

\[ \ldots, \ -(n+1), \ -n, \ \ldots, \ -3, \ -2, \ -1, \ 0, \ 1, \ 2, \ 3, \ \ldots, \ n, \ n+1, \ \ldots \]

This does not fulfil our criterion that the list must have a start: it is infinite in both directions. However, it is still possible to list them, by slotting the negative integers in-between the non-negative integers:

\[ 0, \ -1, \ 1, \ -2, \ 2, \ -3, \ 3, \ \ldots, \ -n, \ n, \ -(n+1), \ n+1, \ \ldots \]

This is not how we would usually think about listing the integers, but we have nonetheless found a way of doing it so that every integer appears at some finite stage on the list.

But specifying a list of the elements of an infinite set \( X \), such that every element of \( X \) appears at some finite stage on the list, is equivalent to specifying a bijection \( f : \mathbb{N} \to X \).
</markdown><markdown>
where the element \( f(k) \in X \) plays the role of the \( k^{th} \) element of the list‚Äîthat is, the elements of \( X \) can be listed as

\[ f(0), \ f(1), \ f(2), \ \ldots, \ f(n), \ f(n+1), \]

This motivates the following definition.

‚ú¶ **Definition 6.2.1**  
A set \( X \) is **countably infinite** if there exists a bijection \( f : \mathbb{N} \to X \). The bijection \( f \) is called an **enumeration** of \( X \). We say \( X \) is **countable** if it is finite or countably infinite.

Some authors prefer to use ‚Äòcountable‚Äô to mean ‚Äòcountably infinite‚Äô, in which case they would say ‚Äòfinite or countable‚Äô to mean ‚Äòcountable‚Äô.

‚úê **Example 6.2.2**  
The set \( \mathbb{N} \) is countably infinite, since by Exercise 3.2.19, the identity function \(\text{id}_{\mathbb{N}} : \mathbb{N} \to \mathbb{N}\) is a bijection. This enumeration yields the usual list of natural numbers

\[ 0, \ 1, \ 2, \ 3, \ \ldots, \ n, \ n+1, \ \ldots \]

‚úê **Example 6.2.3**  
The function \( f : \mathbb{Z} \to \mathbb{N} \) defined for \( x \in \mathbb{Z} \) by

\[
f(x) = 
\begin{cases} 
2x & \text{if } x \geq 0 \\
-(2x+1) & \text{if } x < 0 
\end{cases}
\]

is a bijection. Indeed, it has an inverse given by

\[
f^{-1}(x) = 
\begin{cases} 
\frac{x}{2} & \text{if } x \text{ is even} \\
-\frac{x+1}{2} & \text{if } x \text{ is odd} 
\end{cases}
\]

Hence the set of integers \( \mathbb{Z} \) is countably infinite. The corresponding list of integers is given by

\[ 0, \ -1, \ 1, \ -2, \ 2, \ -3, \ 3, \ -4, \ 4, \ \ldots \]

which is exactly the list we presented earlier! The fact that \( f \) is a bijection ensures that each integer appears on this list exactly once.

‚úê **Exercise 6.2.4**  
Prove that the set of all positive integers is countably infinite.

‚úê **Exercise 6.2.5**  
Prove that the set of all even natural numbers is countably infinite, and that the set of all odd natural numbers is countably infinite.
</markdown><markdown>
Since the inverse of a bijection is a bijection, we may also prove that a set \( X \) is countable by finding a bijection \( X \to \mathbb{N} \).

### Exercise 6.2.6
Prove that the function \( p : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) defined by \( p(x, y) = 2^x(2y + 1) - 1 \) is a bijection. Deduce that \( \mathbb{N} \times \mathbb{N} \) is countable.

## Closure properties of countable sets

Proving that a set is infinite by finding an explicit bijection with the set of natural numbers can be overly burdensome, so we will now develop some **closure properties** of countable sets. This will allow us to prove that a set is countable by relating it to sets that we already know are countable, without having to find a bijection \( f : \mathbb{N} \to X \) every time.

### Proposition 6.2.7
Let \( f : X \to Y \) be a bijection. Then \( X \) is countably infinite if and only if \( Y \) is countably infinite.

**Proof**  
Suppose \( X \) is countably infinite. Then there is a bijection \( g : \mathbb{N} \to X \). But then \( f \circ g : \mathbb{N} \to Y \) is a composite of bijections, so is bijective, meaning that \( Y \) is countably infinite.

Conversely, suppose \( Y \) is countably infinite. Then there is a bijection \( h : \mathbb{N} \to Y \). But then \( f^{-1} \circ h : \mathbb{N} \to X \) is a composite of bijections, so is bijective, meaning that \( X \) is countably infinite.

### Exercise 6.2.8
Prove if \( X \) and \( Y \) are countably infinite sets, then \( X \times Y \) is countably infinite.

Exercise 6.2.8 allows us to prove that the product of finitely many countably infinite sets are countably infinite‚Äîthis is an analogue of (the independent version of) the multiplication principle (Lemma 8.1.12) for countably infinite sets.

### Proposition 6.2.9 (The product of finitely many countable sets is countable)
Let \( n \geq 1 \) and let \( X_1, \ldots, X_n \) be countably infinite sets. Then the product \(\prod_{i=1}^{n} X_i\) is countably infinite.

**Proof**  
We proceed by induction on \( n \geq 1 \).

- **(Base case)** When \( n = 1 \) the assertion is trivial: if \( X_1 \) is countably infinite then \( X_1 \) is countably infinite.
</markdown><markdown>
- **(Induction step)** Fix \( n \geq 1 \) and suppose that for any sets \( X_1, \ldots, X_n \), the product

  \[
  \prod_{i=1}^{n} X_i
  \]

  is countably infinite. Fix sets \( X_1, \ldots, X_{n+1} \). Then 

  \[
  \prod_{i=1}^{n} X_i
  \]

  is countably infinite by the induction hypothesis, and \( X_{n+1} \) is countably infinite by assumption, so by Exercise 6.2.8, the set

  \[
  \left( \prod_{i=1}^{n} X_i \right) \times X_{n+1}
  \]

  is countably infinite. But by Exercise 4.2.12 there is a bijection

  \[
  \prod_{i=1}^{n+1} X_i \to \left( \prod_{i=1}^{n} X_i \right) \times X_{n+1}
  \]

  and so by Exercise 6.2.8 we have that 

  \[
  \prod_{i=1}^{n+1} X_i
  \]

  is countably infinite, as required.

By induction, we‚Äôre done.

We often just want to know whether a set is **countable**, rather than countably infinite. This might be because we only seek an upper bound on how large the set is; or it might be because we already know that the set is infinite, so proving that it is countable suffices.

The following theorem allows us to prove that a set \( X \) is countable‚Äîthat is, finite or countably infinite‚Äîby either surjecting \( \mathbb{N} \) onto \( X \), or injecting \( X \) into \( \mathbb{N} \). Using the intuition of Section 6.1, where we compared the sizes of finite sets using injections and surjections, this says that a set is countable if and only if \( X \) is at most as large as \( \mathbb{N} \).

### Theorem 6.2.10

Let \( X \) be an inhabited set. The following are equivalent:

(i) \( X \) is countable;

(ii) There exists a surjection \( f : \mathbb{N} \to X \);

(iii) There exists an injection \( f : X \to \mathbb{N} \).

**Proof**

We‚Äôll prove (i)\(\iff\)(ii) and (i)\(\iff\)(iii).

- (i)\(\implies\)(ii). Suppose \( X \) is countable. If \( X \) is countably infinite, then there exists a bijection \( f : \mathbb{N} \to X \), which is a surjection. If \( X \) is finite then there exists a bijection \( g : [m] \to X \), where \( m = |X| \geq 1 \). Define \( f : \mathbb{N} \to X \) by

  \[
  f(n) = 
  \begin{cases} 
  g(n) & \text{if } 1 \leq n \leq m \\
  g(1) & \text{if } n = 0 \text{ or } n > m 
  \end{cases}
  \]
</markdown><markdown>
Then \( f \) is surjective: if \( x \in X \) then there exists \( n \in [m] \) such that \( g(n) = x \), and then \( f(n) = g(n) = x \).

- (ii)\(\Rightarrow\)(i). Suppose there exists a surjection \( f : \mathbb{N} \to X \). To prove that \( X \) is countable, it suffices to prove that if \( X \) is infinite then it is countably infinite. So suppose \( X \) is infinite, and define a sequence recursively by
  - \( a_0 = 0 \);
  - Fix \( n \in \mathbb{N} \) and suppose \( a_0, \ldots, a_n \) have been defined. Define \( a_{n+1} \) to be the least natural number for which \( f(a_{n+1}) \not\in \{ f(a_0), f(a_1), \ldots, f(a_n) \} \).

Define \( g : \mathbb{N} \to X \) by \( g(n) = f(a_n) \) for all \( n \in \mathbb{N} \). Then
- \( g \) is injective, since if \( m \leq n \) then \( f(a_m) \neq f(a_n) \) by construction of the sequence \( (a_n)_{n \in \mathbb{N}} \).
- \( g \) is surjective. Indeed, given \( x \in X \), by surjectivity there exists \( m \in \mathbb{N} \) which is least such that \( f(m) = x \), and we must have \( a_n = m \) for some \( n \leq m \) by construction of the sequence \( (a_n)_{n \in \mathbb{N}} \). So \( x = f(a_n) = g(n) \), and hence \( g \) is surjective.

So \( g \) is a bijection, and \( X \) is countable.

- (i)\(\Rightarrow\)(iii). Suppose \( X \) is countable. If \( X \) is countably infinite, then there exists a bijection \( f : \mathbb{N} \to X \), so \( f^{-1} : X \to \mathbb{N} \) is bijective and hence injective. If \( X \) is finite then there exists a bijection \( g : [m] \to X \), where \( m = |X| \geq 1 \). Then \( g^{-1} : X \to [m] \) is injective. Let \( i : [m] \to \mathbb{N} \) be defined by \( i(k) = k \) for all \( k \in [m] \). Then \( i \circ g^{-1} \) is injective; indeed, for \( x, x' \in X \) we have

\[
i(g^{-1}(x)) = i(g^{-1}(x')) \Rightarrow g^{-1}(x) = g^{-1}(x') \Rightarrow x = x'
\]

The first implication is by definition of \( i \), and the second is by injectivity of \( g^{-1} \). So there exists an injection \( X \to \mathbb{N} \).

- (iii)\(\Rightarrow\)(i). Suppose there exists an injection \( f : X \to \mathbb{N} \). To prove that \( X \) is countable, it suffices to prove that if \( X \) is infinite then it is countably infinite. Define a sequence \( (a_n)_{n \in \mathbb{N}} \) recursively as follows:
  - Let \( a_0 \) be the least element of \( f[X] \);
  - Fix \( n \in \mathbb{N} \) and suppose \( a_0, \ldots, a_n \) have been defined. Let \( a_{n+1} \) be the least element of \( f[X] \setminus \{ a_0, \ldots, a_n \} \). This exists since \( f \) is injective, so \( f[X] \) is infinite.

Define \( g : \mathbb{N} \to X \) by, for each \( n \in \mathbb{N} \), letting \( g(n) \) be the unique value of \( x \) for which \( f(x) = a_n \). Then
- \( g \) is injective. By construction \( a_m \neq a_n \) whenever \( m \neq n \). Let \( x, y \in X \) be such that \( f(x) = a_m \) and \( f(y) = a_n \). Since \( f \) is injective, we must have \( x \neq y \), and so \( g(m) = x \neq y = g(n) \).
- \( g \) is surjective. Fix \( x \in X \). Then \( f(x) \in f[X] \), so there exists \( m \in \mathbb{N} \) such that \( f(x) = a_m \). Hence \( g(m) = x \).
</markdown><markdown>
So \( g \) is a bijection, and \( X \) is countably infinite.

Hence the equivalences have been proved.

In fact, we needn‚Äôt even use \( \mathbb{N} \) as the domain of the surjection or the codomain of the injection; we can in fact use any countable set \( C \).

### Exercise 6.2.11
Let \( X \) be an inhabited set. The following are equivalent:

(i) \( X \) is countable;

(ii) There exists an injection \( f : X \to C \) for some countable set \( C \);

(iii) There exists a surjection \( f : C \to X \) for some countable set \( C \).

Exercise 6.2.11 is useful for proving the countability of many other sets: as we build up our repertoire of countable sets, all we need to do in order to prove a set \( X \) is countable is find a surjection from a set we already know is countable to \( X \), or an injection from \( X \) into a set we already know is countable.

This proof technique yields an incredibly short proof of the following counterintuitive result, which can be interpreted to mean that there are exactly as many rational numbers as there are natural numbers.

### Theorem 6.2.12
The set \( \mathbb{Q} \) of rational numbers is countable.

**Proof**  
Define a function \( q : \mathbb{Z} \times (\mathbb{Z} \setminus \{0\}) \to \mathbb{Q} \) by letting \( q(a, b) = \frac{a}{b} \) for all \( a, b \in \mathbb{Z} \) with \( b \neq 0 \).

By Example 6.2.3 and Exercise 6.2.8, the set \( \mathbb{Z} \times (\mathbb{Z} \setminus \{0\}) \) is countable.

The function \( q \) is surjective by definition of \( \mathbb{Q} \)‚Äîindeed, to say \( x \in \mathbb{Q} \) is precisely to say that \( x = \frac{a}{b} = q(a, b) \) for some \( (a, b) \in \mathbb{Z} \times (\mathbb{Z} \setminus \{0\}) \).

By Exercise 6.2.11, it follows that \( \mathbb{Q} \) is countable.

### Exercise 6.2.13
Let \( X \) be a countable set. Prove that \( \binom{X}{k} \) is countable for each \( k \in \mathbb{N} \).
</markdown><markdown>
### Theorem 6.2.14 (The union of countably many countable sets is countable)

Let \(\{X_n \mid n \in \mathbb{N}\}\) be a family of countable sets. Then the set \(X\) defined by

\[
X = \bigcup_{n \in \mathbb{N}} X_n
\]

is countable.

**Proof**

We may assume that the sets \(X_n\) are all inhabited, since the empty set does not contribute to the union.

For each \(n \in \mathbb{N}\) there is a surjection \(f_n : \mathbb{N} \to X_n\). Define \(f : \mathbb{N} \times \mathbb{N} \to X\) by \(f(m, n) = f_m(n)\) for all \(m, n \in \mathbb{N}\). Then \(f\) is surjective: if \(x \in X\) then \(x \in X_m\) for some \(m \in \mathbb{N}\). Since \(f_m\) is surjective, it follows that \(x = f_m(n)\) for some \(n \in \mathbb{N}\). But then \(x = f(m, n)\). Since \(\mathbb{N} \times \mathbb{N}\) is countable, it follows from Exercise 6.2.11 that \(X\) is countable. ‚ñ°

### Example 6.2.15

Let \(X\) be a countable set. The set of all finite subsets of \(X\) is countable. Indeed, the set of all finite subsets of \(X\) is equal to \(\bigcup_{k \in \mathbb{N}} \binom{X}{k}\), which is a union of countably many countable sets by Exercise 6.2.13, so is countable by Theorem 6.2.14. ‚óÅ

### Uncountable sets

We have now seen plenty of examples of countable sets. It is not immediately obvious that not every set is countable. How do we know that there are sets out there whose elements can‚Äôt be listed?

We prove in Theorem 6.2.16 that there exists an uncountable set, namely the power set of the natural numbers. The proof is deceptively simple, but the implications are important.

### Theorem 6.2.16

\(\mathcal{P}(\mathbb{N})\) is uncountable.

**Proof**

We proved in Exercise 3.2.16 that no function \(f : \mathbb{N} \to \mathcal{P}(\mathbb{N})\) is surjective. Specifically, given \(f : \mathbb{N} \to \mathcal{P}(\mathbb{N})\), we can show that the element

\[
B = \{k \in \mathbb{N} \mid k \not\in f(k)\} \in \mathcal{P}(\mathbb{N})
\]

is not equal to \(f(n)\) for any \(n \in \mathbb{N}\). It follows from Exercise 6.2.11(iii) that \(\mathcal{P}(\mathbb{N})\) is uncountable. ‚ñ°
</markdown><markdown>
The argument used in [Theorem 6.2.16](#) is an example of Cantor‚Äôs diagonal argument, and is typical.

### Theorem 6.2.17
Let \( X \) be a set, and assume that for every function \( f : \mathbb{N} \to X \), there is:

(i) A family of logical formulae \( p_n(x) \) with \( x \in X \), one for each \( n \in \mathbb{N} \); and

(ii) An element \( b \in X \);

such that \( \forall n \in \mathbb{N}, \ [p_n(b) \iff \neg p_n(f(n))] \). Then \( X \) is uncountable.

**Proof**

We prove that no function \( f : \mathbb{N} \to X \) is surjective. So let \( f : \mathbb{N} \to X \) be an arbitrary function and let \( p_n(x) \) and \( b \in X \) be as in the statement of the theorem.

To see that \( f \) is not surjective, assume towards a contradiction that \( b = f(k) \) for some \( k \in \mathbb{N} \).

- If \( p_k(b) \) is true, then \( p_k(f(k)) \) is true since \( b = f(k) \), and \( \neg p_k(f(k)) \) is true since \( p_k(b) \iff \neg p_k(f(k)) \). This is a contradiction.

- If \( \neg p_k(b) \) is true, then \( \neg p_k(f(k)) \) is false since \( b = f(k) \), and so \( p_k(b) \) is true since \([ \neg p_k(f(k)) ] \Rightarrow p_k(b) \). This is a contradiction.

In both cases we arrive at a contradiction. So \( b \neq f(k) \) for any \( k \in \mathbb{N} \), and so \( f \) is not surjective.

Hence \( X \) is uncountable by [Exercise 6.2.11(iii)](#).

### Strategy 6.2.18 (Cantor‚Äôs diagonal argument)
In order to prove that a set \( X \) is uncountable, it suffices to prove that no function \( f : \mathbb{N} \to X \) is surjective using the following argument: given a function \( f : \mathbb{N} \to X \), find an element \( b \in X \) that ‚Äòdisagrees‚Äô with each value \( f(n) \) about some statement involving \( n \).

### Example 6.2.19
In the proof of [Theorem 6.2.16](#), we proved that \( \mathcal{P}(\mathbb{N}) \) is uncountable as follows. Given \( f : \mathbb{N} \to \mathcal{P}(\mathbb{N}) \), we needed to find an element \( B \in \mathcal{P}(\mathbb{N}) \)‚Äîthat is, a subset \( B \subseteq \mathbb{N} \)‚Äîthat ‚Äòdisagreed‚Äô with each value \( f(n) \) about something involving \( n \).

Keep in mind that, for each \( n \in \mathbb{N} \), the element \( f(n) \in \mathcal{P}(\mathbb{N}) \) is a subset of \( \mathbb{N} \), so it makes sense to ask whether \( n \) is an element of \( f(n) \) or not.

With this in mind, for each \( n \in \mathbb{N} \), we forced \( B \) to disagree with \( f(n) \) about whether \( n \)
</markdown><markdown>
is an element. That is

\[ n \in B \iff n \notin f(n) \]

In the language of Theorem 6.2.17, the statement \( p_n(U) \) (for \( n \in \mathbb{N} \) and \( U \in \mathcal{P}(\mathbb{N}) \) is the statement ‚Äò\( n \in U \)‚Äô; for each \( n \in \mathbb{N} \), this statement is true with \( U = B \) if and only if it is false with \( U = f(n) \).

The definition \( B = \{ k \in \mathbb{N} \mid k \notin f(k) \} \) forces \( n \in B \iff n \notin f(n) \) to be true for all \( n \in \mathbb{N} \), and so Cantor‚Äôs diagonal argument applies.

Cantor‚Äôs diagonal argument can also be used to prove that \( \mathbb{R} \) is uncountable by considering the decimal expansions of real numbers (Definition 9.3.14).

### Theorem 6.2.20

\(\mathbb{R}\) is uncountable.

**Proof**

Let \( f : \mathbb{N} \to \mathbb{R} \) be an arbitrary function. We prove that \( f \) is not surjective.

For each \( n \in \mathbb{N} \), let the decimal expansion of \( f(n) \in \mathbb{R} \) be \( x^n_0.x^n_1x^n_2x^n_3\ldots \)‚Äîthat is

\[
f(n) = \sum_{i \geq 0} x^n_i \cdot 10^{-i}
\]

[The superscript ‚Äò\( n \)‚Äô here is a label, not an exponent.]

We define \( b \in \mathbb{R} \) by forcing the decimal expansions of \( b \) and \( f(n) \) to differ in the \( n \)th place for each \( n \in \mathbb{N} \). Specifically, for each \( n \in \mathbb{N} \) let

\[
b_n = 
\begin{cases} 
0 & \text{if } x^n_n \neq 0 \\
1 & \text{if } x^n_n = 0 
\end{cases}
\]

and let \( b = b_0.b_1b_2b_3\ldots = \sum_{i \geq 0} b_i \cdot 10^{-i} \).

Note that \( b \) does not have recurring 9s, so it is a valid decimal expansion of a real number. Moreover \( b \neq f(n) \) for any \( n \in \mathbb{N} \) by uniqueness of decimal expansions of real numbers (Theorem 9.3.12), since \( b_n \neq x^n_n \).

Hence \( f \) is not surjective, so that \( \mathbb{R} \) is uncountable.

### Exercise 6.2.21

Use Cantor‚Äôs diagonal argument to prove that the set \( \mathbb{N}^\mathbb{N} \) of all functions \( \mathbb{N} \to \mathbb{N} \) is uncountable.
</markdown><markdown>
Using Cantor‚Äôs diagonal argument every time we want to prove that a set is uncountable can be cumbersome. However, just like with, [Theorem 6.2.10](#), we can prove that sets are uncountable by relating them to other sets that we already know are uncountable.

### Theorem 6.2.22

Let \( X \) be a set. The following are equivalent:

(i) \( X \) is uncountable;

(ii) There exists a surjection \( f : X \to U \) for some uncountable set \( U \);

(iii) There exists an injection \( f : U \to X \) for some uncountable set \( U \).

**Proof**

For (i)\(\Rightarrow\)(ii) and (i)\(\Rightarrow\)(iii), take \( U = X \) and \( f = \text{id}_X \).

For (ii)\(\Rightarrow\)(i), suppose that there is a surjection \( f : X \to U \) for some uncountable set \( U \). If \( X \) were countable, then there would be a surjection \( g : \mathbb{N} \to X \) by [Theorem 6.2.10(ii)](#); but then \( f \circ g : \mathbb{N} \to U \) would be a surjection, contradicting the fact that \( U \) is uncountable.

For (iii)\(\Rightarrow\)(i), suppose there is an injection \( f : U \to X \) for some uncountable set \( U \). If \( X \) were countable, then there would be an injection \( g : X \to \mathbb{N} \) by [Theorem 6.2.10(iii)](#); but then \( g \circ f : U \to \mathbb{N} \) would be an injection, contradicting the fact that \( U \) is uncountable.

### Proposition 6.2.23

The set \(\{0, 1\}^\mathbb{N}\) of all functions \(\mathbb{N} \to \{0, 1\}\) is uncountable.

**Proof**

Define \( F : \mathcal{P}(\mathbb{N}) \to \{0, 1\}^\mathbb{N} \) for \( U \subseteq \mathbb{N} \) by letting \( F(U) = \chi_U : \mathbb{N} \to \{0, 1\} \) be the characteristic function of \( U \) ([Definition 3.1.24](#)).

Given \( U, V \subseteq \mathbb{N} \), we have

\[
F(U) = F(V) \implies \chi_U = \chi_V \implies U = V
\]

by [Theorem 3.1.26](#). But then \( F \) is injective, and \(\mathcal{P}(\mathbb{N})\) is uncountable by Cantor‚Äôs theorem, so that \(\{0, 1\}^\mathbb{N}\) is uncountable by [Theorem 6.2.22](#).

### Exercise 6.2.24

Prove that if a set \( X \) has an uncountable subset, then \( X \) is uncountable.

### Exercise 6.2.25

Let \( X \) be a set. Prove that \(\mathcal{P}(X)\) is either finite or uncountable.
</markdown><markdown>
## Detecting countability

We have now seen several examples of countable and uncountable sets. But being able to prove that a set is countable, or that a set is uncountable, only gets us so far‚Äîit will be beneficial to gain some intuition for how to detect whether a set is countable or uncountable.

We now work towards proving the following heuristic:

* A set is countable if each of its elements has a finite description. *

Before we make this precise, let‚Äôs see this heuristic in action. Consider some of the sets that we have already seen are countable:

- The set \( \mathbb{N} \) is countable; every natural number can be expressed as a finite string of digits, so has a finite description‚Äîfor example, 7 or 15213.

- The set \( \mathbb{Z} \) is countable; every integer has a finite description as a finite string of digits, possibly modified by a minus sign‚Äîfor example, 7 or \(-15213\).

- The set \( \mathbb{Q} \) is countable; every rational number has a finite description as a pair of integers (which themselves have finite descriptions), one atop the other, separated by a horizontal line‚Äîfor example, \( \frac{7}{1} \) or \(\frac{-15213}{21128}\).

- The set of all *finite* subsets of \( \mathbb{N} \) is countable; every finite subset of \( \mathbb{N} \) has a finite expression in list notation by writing an open curly bracket ‚Äò\{‚Äô, followed by a finite number of natural numbers (which themselves have finite expressions) separated by commas, and finally writing a closed curly bracket ‚Äò\}‚Äô‚Äîfor example \(\{15, 0, 281\}\) or \(\{\}\).

Now consider some of the sets that we know (or have been told) are uncountable:

- The set \( \mathcal{P}(\mathbb{N}) \) is uncountable; intuitively, the infinite subsets of \( \mathbb{N} \) cannot be expressed finitely in a uniform way.

- The set \( \mathbb{R} \) is uncountable; intuitively, there is no uniform way of finitely describing real numbers‚Äîfor example, decimal expansions are no good since they are infinite.
</markdown><markdown>
### Definition 6.2.26

Let \(\Sigma\) be a set. A **word** over \(\Sigma\) is an expression of the form \(a_1a_2 \ldots a_n\), where \(n \in \mathbb{N}\) and \(a_i \in \Sigma\) for all \(i \in [n]\). The natural number \(n\) is called the **length** of the word. The unique word of length 0 is called the **empty word**, and is denoted by \(\varepsilon\) (LaTeX code: `\varepsilon`).

The set \(\Sigma\) is called the **alphabet**. The set of all words over \(\Sigma\) is denoted by \(\Sigma^*\) (LaTeX code: `\Sigma^*`); the operation \((-)^*\) is called the **Kleene star**.

Formally, \(\Sigma^*\) is defined to be the union \(\bigcup_{n \in \mathbb{N}} \Sigma^n\), where the sets \(\Sigma^n\) are recursively defined by

\[
\Sigma^0 = \{\varepsilon\} \quad \text{and} \quad \Sigma^{n+1} = \{wa \mid w \in \Sigma^n, a \in \Sigma\} \quad \text{for all } n \geq 0
\]

That is, given \(n \in \mathbb{N}\), the elements of \(\Sigma^n\) are precisely the words over \(\Sigma\) of length \(n\).

### Example 6.2.27

The elements of \(\{0, 1\}^*\) are precisely the finite binary strings:

\[
\{0, 1\}^* = \{\varepsilon, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, \ldots\}
\]

Likewise, the elements of \(\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}^*\) are the finite strings of the base-10 digits. Keep in mind, though, that formally the elements of this string are **words** and not **numbers**, and that two words may correspond with the same natural number‚Äîfor example 123 and 00123 are distinct elements of \(\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}^*\), despite the fact that they both **represent** the same natural number.

### Exercise 6.2.28

Let \(\Sigma = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \div, -\}\). Describe which words over \(\Sigma\) immediately represent rational numbers (in the sense of treating the digit symbols as numbers and the symbols \(\div\) and \(-\) as arithmetic operations). Find some examples of elements of \(\Sigma^*\) that do not represent rational numbers.

### Exercise 6.2.29

Let \(\Sigma\) be an alphabet. Prove that if \(\Sigma\) is countable, then \(\Sigma^*\) is countable.

As hinted by Example 6.2.27, we can use words over sets to make precise what we mean by ‚Äòfinite description‚Äô.

The idea is that finitely describing the elements of a set \(X\) amounts to defining a function \(D : X \to \Sigma^*\) for some set \(\Sigma\); given \(x \in X\), the word \(D(x) \in \Sigma^*\) can be thought of as a finite description of the element \(x \in X\).

But \(D\) cannot be any old function‚Äîif two elements of \(X\) have the same description, then they should be equal. That is

\[
\forall x, y \in X, \, D(x) = D(y) \implies x = y
\]
</markdown><markdown>
But this says exactly that \( D \) must be injective!

‚ú¶ **Definition 6.2.30**  
Let \( X \) be a set and let \( \Sigma \) be an alphabet. A **finite description** of the elements of \( X \) over \( \Sigma \) is an injection \( D : X \to \Sigma^* \).

‚úê **Example 6.2.31**  
Let \( \mathcal{F}(\mathbb{N}) \) be the set of all finite subsets of \( \mathbb{N} \). We already know that this set is countable by Example 6.2.15; now we exhibit a finite description of its elements.

Let \( \Sigma = \{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \boxed{\{ }, \boxed{\} }, \boxed{, } \} \)‚Äîthat is, the elements of the alphabet \( \Sigma \) are the digits 0 through 9, an open curly bracket, a closed curly bracket and a comma. (The curly brackets and comma are boxed to distinguish them from the same symbols used in the list notation describing \( \Sigma \).)

Define \( D : \mathcal{F}(\mathbb{N}) \to \Sigma^* \) by letting \( D(U) \) be the expression of the finite set \( U \subseteq \mathbb{N} \) in list notation, with its elements listed in increasing order. This ensures well-definedness of \( D \)‚Äîfor example

\[
\{1, 2, 3\} = \{2, 3, 1\} \quad \text{but} \quad D(\{1, 2, 3\}) = D(\{2, 3, 1\}) = \{1, 2, 3\}
\]

But then \( D \) is injective: given finite subsets \( U, V \subseteq \mathbb{N} \), if \( D(U) = D(V) \), then \( U \) and \( V \) have identical expressions in list notation‚Äîthis means that they have the same elements, so they are equal!

‚úê **Example 6.2.32**  
Every set has a trivial finite description, taking the set itself to be the alphabet. Indeed, the function \( D : X \to X^* \) defined by \( D(x) = x \) for all \( x \in X \) is evidently an injection!

Example 6.2.32 raises the point that if we want to prove that a set is countable by showing that its elements have finite descriptions, then we must place some restrictions on the alphabets that we may use. For example, the alphabet that we used in Example 6.2.31 is finite‚Äîit has 13 elements.

‚úê **Exercise 6.2.33**  
Give an explicit finite description of the rational numbers over a finite alphabet.

‚úê **Exercise 6.2.34**  
Let \( \mathcal{C}(\mathbb{N}) \) be the set of all cofinite subsets of \( \mathbb{N} \); that is

\[
\mathcal{C}(\mathbb{N}) = \{ U \subseteq \mathbb{N} \mid \mathbb{N} \setminus U \text{ is finite} \}
\]

Give an explicit finite description of the elements of \( \mathcal{C}(\mathbb{N}) \) over a finite alphabet.
</markdown><markdown>
### Theorem 6.2.35

Let \( X \) be a set. Then \( X \) is countable if and only if there is a finite description of the elements of \( X \) over a countable alphabet.

**Proof**

If \( X \) is countable, then we may take \( X \) itself to be our alphabet! The function \( D : X \to X^* \) defined by \( D(x) = x \) for all \( x \in X \) is evidently injective.

Conversely, if there is an injection \( D : X \to \Sigma^* \) for some countable alphabet \( \Sigma \), then \( \Sigma^* \) is countable by Exercise 6.2.29, and so \( X \) is countable by Exercise 6.2.11(ii).

### Example 6.2.36

In light of Theorem 6.2.35, we may deduce from Example 6.2.31 and Exercises 6.2.33 and 6.2.34 that \( \mathcal{P}(\mathbb{N}) \), \( \mathbb{Q} \) and \( \mathcal{C}(\mathbb{N}) \) are all countable sets.

### Exercise 6.2.37

Prove that the set of all polynomials with rational coefficients is countable.

### Exercise 6.2.38

Consider the following argument that claims to be a proof that \( \mathcal{P}(\mathbb{N}) \) is countable. (It isn‚Äôt.)

Let \( \Sigma = \{ \mathbb{N}, x, U, V, \mid, \in, \{, \} \} \)‚Äîagain, we have boxed some symbols to emphasize that they are elements of \( \Sigma \).

Define \( D : \mathcal{P}(\mathbb{N}) \to \Sigma^* \) by

\[
D(U) = \{ x \in \mathbb{N} \mid x \in U \} \quad \text{and} \quad D(V) = \{ x \in \mathbb{N} \mid x \in V \}
\]

for all \( U, V \subseteq \mathbb{N} \). Then \( \Sigma \) is finite, and \( D \) is injective since for all \( U, V \subseteq \mathbb{N} \) we have

\[
D(U) = D(V) \Rightarrow \{ x \in \mathbb{N} \mid x \in U \} = \{ x \in \mathbb{N} \mid x \in V \} \Rightarrow U = V
\]

Since \( D \) is a finite description of the subsets of \( \mathbb{N} \) over a finite alphabet, it follows from Theorem 6.2.35 that \( \mathcal{P}(\mathbb{N}) \) is countable.

This proof cannot be valid, since we know that \( \mathcal{P}(\mathbb{N}) \) is uncountable. Where in the proof did we go wrong?

### Exercise 6.2.39

Suppose we were to attempt to prove that \( \mathcal{P}(\mathbb{N}) \) is countable using the same argument as Example 6.2.31. Where does the proof fail?

### Exercise 6.2.40

Prove that, for every countable set \( X \), there is a finite description of the elements of \( X \) over a finite alphabet.
</markdown><markdown>
## Section 6.E

# Chapter 6 exercises
</markdown><markdown>
# Part II

## Topics in pure mathematics
</markdown>It seems the image is blank. Could you please provide a different image or check if the file is correct?<markdown>
# Chapter 7

# Number theory
</markdown><markdown>
## Section 7.1
# Division

This section introduces the notion of divisibility. As we have already mentioned, it is not always the case that one integer can divide another. As you read through this section, note that we never use fractions; everything we do is internal to \(\mathbb{Z}\), and does not require that we ‚Äòspill over‚Äô to \(\mathbb{Q}\) at any point. This will help you when you study ring theory in the future, and is a good practice to mimic in your own work.

The following theorem, called the division theorem, is the crux of everything that is to follow.

### Theorem 7.1.1 (Division theorem)
Let \(a, b \in \mathbb{Z}\) with \(b \neq 0\). There exist unique \(q, r \in \mathbb{Z}\) such that

\[ 
a = qb + r \quad \text{and} \quad 0 \leq r < |b| 
\]

### Strategy
Let‚Äôs look at the simple case when \(a \geq 0\) and \(b > 0\). We can always find \(q, r\) such that \(a = qb + r\), for example \(q = 0\) and \(r = a\). Moreover, by increasing \(q\) we can reduce \(r\), since

\[ 
qb + r = (q + 1)b + (r - b) 
\]

We will keep doing this until the ‚Äòremainder‚Äô is as small as it can be without being negative. As an example, consider the case when \(a = 14\) and \(b = 5\). This procedure gives

\[
\begin{align*}
14 &= 0 \times 5 + 14 \\
   &= 1 \times 5 + 9 \\
   &= 2 \times 5 + 4 \quad \leftarrow \text{least nonnegative remainder} \\
   &= 3 \times 5 + (-1) \\
   &= \ldots
\end{align*}
\]

This procedure shows that in this case we should take \(q = 2\) and \(r = 4\), since \(14 = 2 \times 5 + 4\) and \(0 \leq 4 < |5|\).

We can show that such a descending sequence of remainders terminates using the well-ordering principle, and then we must argue that the quotient and remainder that we obtain are unique.

### Proof
We may assume that \(b > 0\): if not, replace \(b\) by \(-b\) and \(q\) by \(-q\). We may also assume that \(a \geq 0\). Otherwise, replace \(a\) by \(-a\), \(q\) by \(-(q + 1)\) and \(r\) by \(b - r\).
</markdown><markdown>
Thus, what follows assumes that \( a \geq 0 \) and \( b > 0 \).

- **Existence.** We prove that such integers \( q, r \) exist by the well-ordering principle. Namely, we define a sequence \( (r_n)_{n \in \mathbb{N}} \) such that \( a = nb + r_n \) and \( r_0 > r_1 > r_2 > \cdots \), and use this sequence to find the values of \( q, r \).

  - Let \( r_0 = a \). Then \( a = 0b + r_0 \), as required.
  - Suppose \( r_n \) has been defined, and let \( r_{n+1} = r_n - b \). Then

    \[
    (n+1)b + r_{n+1} = (n+1)b + r_n - b = nb + b + r_n - b = nb + r = a
    \]

    Since \( b > 0 \), we must have \( r_{n+1} < r_n \) for all \( n \).

Let \( R = \bigcap \{ r_n \mid n \in \mathbb{N} \} \). That is, \( R \) is the set of terms of the sequence which are non-negative. Since \( r_0 = a \geq 0 \), we have that \( r_0 \in R \) and hence \( R \) is inhabited. By the well-ordering principle, \( R \) has a least element \( r_k \) for some \( k \in \mathbb{N} \).

Define \( q = k \) and \( r = r_k \). By construction we have \( a = qb + r \) and \( r \geq 0 \), so it remains to show that \( r < b \). Well, if \( r \geq b \) then \( r - b \geq 0 \), but \( r - b = r_{k+1} \), so this would imply \( r_{k+1} \in R \), contradicting minimality of \( r \). Hence \( r < b \), so \( q, r \) are as required.

- **Uniqueness.** Suppose \( q', r' \) also satisfy \( a = q'b + r' \) and \( 0 \leq r' < b \). If we can show that \( r' = r \) then this proves that \( q = q' \): indeed, if \( qb + r = q'b + r \) then we can subtract \( r \) and then divide by \( b \), since \( b > 0 \).

First note that \( q' \geq 0 \). If \( q' < 0 \) then \( q' \leq -1 \), so

\[
a = q'b + r' \leq -b + r'
\]

and hence \( r' \geq a + b \geq b \) since \( a \geq 0 \). This contradicts the assumption that \( r < b \). So \( q' \geq 0 \).

Since \( q' \geq 0 \), we also know that \( a = q'b + r' \), and hence \( r' = r_{q'} \in R \). By minimality of \( r \) we have \( r \leq r' \). It remains to show that \( r = r' \). If not then \( r < r' \). Thus

\[
qb + r = q'b + r' > q'b + r \quad \Rightarrow \quad qb > q'b \quad \Rightarrow \quad q > q'
\]

and hence \( q = q' + t \) for some \( t \geq 1 \). But then

\[
q'b + r' = a = qb + r = (q' + t)b + r = q'b + (tb + r)
\]

so \( r' = tb + r \geq b \), contradicting \( r' < b \). So \( r = r' \) as desired, and hence \( q = q' \).

At long last, we are done.
</markdown><markdown>
### Definition 7.1.2

Let \( a, b \in \mathbb{Z} \) with \( b \neq 0 \), and let \( q, r \) be the unique integers such that

\[
a = qb + r \quad \text{and} \quad 0 \leq r < |b|
\]

We say \( q \) is the **quotient** and \( r \) is the **remainder** of \( a \) divided by \( b \).

### Example 7.1.3

Some examples of division include:

\[
14 = 2 \times 5 + 4, \quad -14 = -3 \times 5 + 1, \quad 15 = 3 \times 5 + 0
\]

### Definition 7.1.4

Let \( a, b \in \mathbb{Z} \). We say **\( b \) divides \( a \)**, or that \( b \) is a **divisor** (or **factor**) of \( a \), if there exists \( q \in \mathbb{Z} \) such that \( a = qb \). To denote the fact that \( b \) divides \( a \) we write \( b \mid a \) (LaTeX code: `\mid`). For the negation \( \neg(b \mid a) \) write \( b \nmid a \) (LaTeX code: `\nmid`).

Thus, when \( b \neq 0 \), saying \( b \mid a \) is equivalent to saying that the remainder of \( a \) divided by \( b \) is 0.

### Example 7.1.5

5 divides 15 since \( 15 = 3 \times 5 \). However, 5 does not divide 14: we know that the remainder of 14 divided by 5 is 4, not 0‚Äîand it can‚Äôt be both since we proved in the division theorem that remainders are unique!

### Exercise 7.1.6

Show that if \( a \in \mathbb{Z} \) then \( 1 \mid a, -1 \mid a \) and \( a \mid 0 \). For which integers \( a \) does \( a \mid 1 \)? For which integers \( a \) does 0 \(\mid a\)?

We now introduce the very basic notion of a **unit**. This notion is introduced to rule out trivialities. Units become interesting when talking about general rings, but in \( \mathbb{Z} \), the units are very familiar.

### Definition 7.1.7

Let \( u \in \mathbb{Z} \). We say \( u \) is a **unit** if \( u \mid 1 \); that is, \( u \) is a unit if there exists \( v \in \mathbb{Z} \) such that \( uv = 1 \).

### Proposition 7.1.8

The only units in \( \mathbb{Z} \) are 1 and \(-1\).

**Proof**

First note that 1 and \(-1\) are units, since \( 1 \cdot 1 = 1 \) and \((-1) \cdot (-1) = 1\). Now suppose that \( u \in \mathbb{Z} \) is a unit, and let \( v \in \mathbb{Z} \) be such that \( uv = 1 \). Certainly \( u \neq 0 \), since \( 0v = 0 \neq 1 \). If \( u > 1 \) or \( u < -1 \) then \( v = \frac{1}{u} \not\in \mathbb{Z} \). So we must have \( u \in \{-1, 1\} \).
</markdown><markdown>
Exercise 7.1.6 shows that \(-1\), 0 and 1 are, from the point of view of divisibility, fairly trivial. For this reason, most of the results we discuss regarding divisibility will concern nonzero nonunits, i.e. all integers except \(-1\), 0 or 1.

## Greatest common divisors

‚ú¶ **Definition 7.1.9**  
Let \(a, b \in \mathbb{Z}\). An integer \(d\) is a **greatest common divisor** of \(a\) and \(b\) if:

(a) \(d \mid a\) and \(d \mid b\);

(b) If \(q\) is another integer such that \(q \mid a\) and \(q \mid b\), then \(q \mid d\).

‚úê **Example 7.1.10**  
2 is a greatest common divisor of 4 and 6; indeed:

(a) \(4 = 2 \times 2\), and \(6 = 3 \times 2\), so 2 \(\mid\) 4 and 2 \(\mid\) 6;

(b) Suppose \(q \mid 4\) and \(q \mid 6\). The divisors of 4 are \(\pm 1, \pm 2, \pm 4\) and the divisors of 6 are \(\pm 1, \pm 2, \pm 3, \pm 6\). Since \(q\) divides both, it must be the case that \(q \in \{-2, -1, 1, 2\}\); in any case, \(q \mid 2\).

Likewise, \(-2\) is a greatest common divisor of 4 and 6.

‚úê **Exercise 7.1.11**  
There are two greatest common divisors of 6 and 15; find both.

We will now prove that greatest common divisors **exist**‚Äîthat is, any two integers have a greatest common divisor‚Äîand that they are **unique up to sign**.

‚ûó **Theorem 7.1.12**  
Every pair of integers \(a, b\) has a greatest common divisor.

**Proof**  
First note that if \(a = b = 0\), then 0 is a greatest common divisor for \(a\) and \(b\). Moreover, we may take \(a, b\) to be non-negative, since divisibility is insensitive to sign. So suppose that \(a, b \geq 0\) and that \(a, b\) are not both zero.

Define a set \(X \subseteq \mathbb{Z}\) by

\[ 
X = \{au + bv \mid u, v \in \mathbb{Z}, au + bv > 0\} 
\]

That is, \(X\) is the set of positive integers of the form \(au + bv\).
</markdown><markdown>
\( X \) is inhabited. To see this, note that \( a^2 > 0 \) or \( b^2 > 0 \) since \( a \neq 0 \) or \( b \neq 0 \), so letting \( u = a \) and \( v = b \) in the expression \( au + bv \), we see that

\[
au + bv = a^2 + b^2 > 0 \quad \Rightarrow \quad a^2 + b^2 \in X
\]

By the well-ordering principle, \( X \) has a least element \( d \), and by definition of \( X \) there exist \( u, v \in \mathbb{Z} \) such that \( d = au + bv \).

We will prove that \( d \) is a greatest common divisor for \( a \) and \( b \).

- \( d \mid a \). If \( a = 0 \), then this is immediate, so suppose that \( a > 0 \). Let \( q, r \in \mathbb{Z} \) be such that

\[
a = qd + r \quad \text{and} \quad 0 \leq r < d
\]

Moreover

\[
r = a - qd = a - q(au + bv) = a(1 - qu) + b(-qv)
\]

If \( r > 0 \) then this implies that \( r \in X \); but this would contradict minimality of \( d \), since \( r < d \). So we must have \( r = 0 \) after all.

- \( d \mid b \). The proof of this is identical to the proof that \( d \mid a \).

- Suppose \( q \) is an integer dividing both \( a \) and \( b \). Then \( q \mid au + bv \) by Exercise 0.16. Since \( au + bv = d \), we have \( q \mid d \).

So \( d \) is a greatest common divisor of \( a \) and \( b \) after all.

‚ú¶ **Exercise 7.1.13**  
Let \( a, b \in \mathbb{Z} \). If \( d \) and \( d' \) are two greatest common divisors of \( a \) and \( b \), then either \( d = d' \) or \( d = -d' \).

‚ùñ **Aside**  
A consequence of Theorem 7.1.12 and Exercise 7.1.13 is that every pair of integers has a unique non-negative greatest common divisor! Written symbolically, we can say

\[
\forall (a, b) \in \mathbb{Z} \times \mathbb{Z}, \exists! d \in \mathbb{Z}, \left( d \geq 0 \text{ and } d \text{ is a greatest common divisor for } a \text{ and } b \right)
\]

As discussed in Section 3.1, since this is a formula of the form ‚Äòfor all ‚Ä¶ there exists a unique ‚Ä¶‚Äô, this defines a function \(\gcd : \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}\). We won‚Äôt explicitly refer to the fact that \(\gcd\) is a function; rather, we‚Äôll just concern ourselves with its values, as in Notation 7.1.14.

‚ú¶ **Notation 7.1.14**  
Let \( a, b \in \mathbb{Z} \). Denote by \(\gcd(a, b)\) (LaTeX code: \texttt{\textbackslash mathrm\{gcd\}}) the (unique!) non-negative greatest common divisor of \( a \) and \( b \).
</markdown><markdown>
### Example 7.1.15

In Example 7.1.10, we saw that both 2 and \(-2\) are greatest common divisors of 4 and 6. Using Notation 7.1.14, we can now write \(\gcd(4,6) = 2\).

### Exercise 7.1.16

For each \(n \in \mathbb{Z}\), let \(D_n \subseteq \mathbb{Z}\) be the set of divisors of \(n\). Prove that \(D_a \cap D_b = D_{\gcd(a,b)}\) for all \(a, b \in \mathbb{Z}\).

Our goal for the rest of this subsection is to investigate the behaviour of greatest common divisors, find out how to compute them, and look into the implications they have for solutions to certain kinds of equations.

### Theorem 7.1.17

Let \(a, b, q, r \in \mathbb{Z}\), and suppose that \(a = qb + r\). Then

\[
\gcd(a, b) = \gcd(b, r)
\]

**Proof**

Let \(d = \gcd(a, b)\). We check that \(d\) satisfies the conditions required to be a greatest common divisor of \(b\) and \(r\).

Note that \(d \mid a\) and \(d \mid b\), so let \(s, t \in \mathbb{Z}\) be such that \(a = sd\) and \(b = td\).

- \(d \mid b\) by definition, and \(d \mid r\) since

  \[
  r = a - qb = sd - qtd = (s - qt)d
  \]

- Suppose \(d' \mid b\) and \(d' \mid r\); say \(b = ud'\) and \(r = vd'\) with \(u, v \in \mathbb{Z}\). Then \(d' \mid a\), since

  \[
  a = qb + r = qud' + vd' = (qu + v)d'
  \]

  so \(d' \mid d\) since \(d = \gcd(a, b)\).

So \(d\) is a greatest common divisor of \(b\) and \(r\). Since \(d > 0\), the result is shown. ‚ñ°

Combined with the division theorem (Theorem 7.1.1), Theorem 7.1.17 gives a relatively fast algorithm for computing the greatest common divisor of two integers, known as the **Euclidean algorithm**.
</markdown><markdown>
### Strategy 7.1.18 (Euclidean algorithm)

Let \( a, b \in \mathbb{Z} \). To compute \(\gcd(a, b)\), proceed as follows.

- Set \( r_0 = |a| \) and \( r_1 = |b| \).
- Given \( r_{n-2} \) and \( r_{n-1} \), define \( r_n \) to be the remainder of \( r_{n-2} \) divided by \( r_{n-1} \).
- Stop when \( r_n = 0 \); then \( r_{n-1} = \gcd(a, b) \).

### Example 7.1.19

We will find the greatest common divisor of 148 and 28.

\[
\begin{align*}
148 &= 5 \times 28 + 8 \\
28 &= 3 \times 8 + 4 \\
8 &= 2 \times 4 + 0 \quad \leftarrow \text{Stop!}
\end{align*}
\]

Hence \(\gcd(148, 28) = 4\). Here the sequence of remainders is given by:

\[
r_0 = 148, \quad r_1 = 28, \quad r_2 = 8, \quad r_3 = 4, \quad r_4 = 0
\]

### Example 7.1.20

The Euclidean algorithm works surprisingly quickly, even for relatively large numbers. Consider the problem of computing \(\gcd(1311, 5757)\) for example:

\[
\begin{align*}
5757 &= 4 \times 1311 + 513 \\
1311 &= 2 \times 513 + 285 \\
513 &= 1 \times 285 + 228 \\
285 &= 1 \times 228 + 57 \\
228 &= 4 \times 57 + 0 \quad \leftarrow \text{Stop!}
\end{align*}
\]

Hence \(\gcd(1311, 5757) = 57\). Here the sequence of remainders is given by:

\[
r_0 = 5757, \quad r_1 = 1311, \quad r_2 = 513, \quad r_3 = 285, \quad r_4 = 228, \quad r_5 = 57, \quad r_6 = 0
\]

### Example 7.1.21

Here's an example where one of the numbers is negative: we compute the value of \(\gcd(-420, 76)\):

\[
\begin{align*}
-420 &= (-6) \times 76 + 36 \\
76 &= 2 \times 36 + 4 \\
36 &= 9 \times 4 + 0 \quad \leftarrow \text{Stop!}
\end{align*}
\]

Hence \(\gcd(-420, 76) = 4\).
</markdown><markdown>
### Example 7.1.22

Use the Euclidean algorithm to compute the greatest common divisors of the following pairs of integers

\[
(12, 9), \quad (100, 35), \quad (7125, 1300), \quad (1010, 101010), \quad (-4, 14)
\]

The following theorem will be useful when we study modular arithmetic in [Section 7.3](#); it is called a ‚Äòlemma‚Äô for historical reasons, and is really an important result in its own right.

### Theorem 7.1.23 (B√©zout‚Äôs lemma)

Let \( a, b, c \in \mathbb{Z} \), and let \( d = \gcd(a, b) \). The equation

\[
ax + by = c
\]

has a solution \( (x, y) \in \mathbb{Z} \times \mathbb{Z} \) if and only if \( d \mid c \).

**Proof**

(‚áí) Write \( a = a'd \) and \( b = b'd \), for \( a', b' \in \mathbb{Z} \). If there exist \( x, y \in \mathbb{Z} \) such that \( ax + by = c \), then

\[
c = ax + by = a'dx + b'dy = (a'x + b'y)d
\]

and so \( d \mid c \).

(‚áê) Suppose \( d \mid c \), and let \( c = kd \) for some \( k \in \mathbb{Z} \).

If \( c = 0 \), then a solution is \( x = y = 0 \). If \( c < 0 \), then \( ax + by = c \) if and only if \( a(-x) + b(-y) = -c \); so we may assume that \( c > 0 \).

We proved in [Theorem 7.1.12](#) that a greatest common divisor of \( a \) and \( b \) is a least element of the set

\[
X = \{au + bv \mid u, v \in \mathbb{Z}, \, au + bv > 0\}
\]

So let \( u, v \in \mathbb{Z} \) be such that \( au + bv = d \). Then

\[
a(ku) + b(kv) = k(au + bv) = kd = c
\]

and so letting \( x = ku \) and \( y = kv \), we see that the equation \( ax + by = c \) has a solution \( (x, y) \in \mathbb{Z} \times \mathbb{Z} \).

B√©zout‚Äôs lemma completely characterises when the equation \( ax + by = c \) has a solution. An easy generalisation of B√©zout‚Äôs lemma provides a complete characterisation of when solutions to **linear Diophantine equations** exist, that is equations of the form

\[
ax + by = c
\]
</markdown><markdown>
where \( a, b, c \in \mathbb{Z} \). We will soon develop an algorithm for computing all solutions to these equations.

### Example 7.1.24

Here are some examples of applications of B√©zout‚Äôs lemma.

- Consider the equation \( 1311x + 5757y = 12963 \). We computed in Example 7.1.20 that \(\gcd(1311, 5757) = 57\). But \( 57 \nmid 12963 \) since \( 12963 = 227 \times 57 + 24 \). By B√©zout‚Äôs lemma, the equation \( 1311x + 5757y = 12963 \) has no integer solutions.

- For fixed \( z \), the equation \( 4u + 6v = z \) has solutions exactly when \( z \) is even, since \(\gcd(4, 6) = 2\).

- For fixed \( a, b \), the equation \( au + bv = 0 \) always has a solution. Indeed, setting \( u = b \) and \( v = -a \) gives a solution; but we knew one had to exist since by Exercise 7.1.6 we know that \( d \mid 0 \) for all \( d \in \mathbb{Z} \).

### Exercise 7.1.25

Which of the following equations have solutions?

(a) \( 12u + 9v = -18 \)

(b) \( 12u + 9v = 1 \)

(c) \( 100u + 35v = 125 \)

(d) \( 7125u + 1300v = 0 \)

(e) \( 1010u + 101010v = 1010101010101010 \)

(f) \( 14u - 4v = 12 \)

## Coprimality

### Definition 7.1.26

Let \( a, b \in \mathbb{Z} \). We say \( a \) and \( b \) are coprime (or relatively prime), and write \( a \perp b \) (read ‚Äò\( a \) is coprime to \( b \)‚Äô), if \(\gcd(a, b) = 1\).

### Example 7.1.27

\( 4 \perp 9 \). To see this, note that if \( d \mid 4 \) then \( d \in \{-4, -2, -1, 1, 2, 4\} \), and if \( d \mid 9 \) then \( d \in \{-9, -3, -1, 1, 3, 9\} \). Hence if \( d \mid 4 \) and \( d \mid 9 \), then \( d = 1 \) or \( d = -1 \). It follows that \(\gcd(4, 9) = 1\).
</markdown><markdown>
### Exercise 7.1.28
Which integers in the set \([15]\) are coprime to 15?

### Proposition 7.1.29
Let \(a, b \in \mathbb{Z}\). The following are equivalent:

1. \(a\) and \(b\) are coprime;
2. If \(d \in \mathbb{Z}\) with \(d \mid a\) and \(d \mid b\), then \(d\) is a unit.

**Proof**  
We prove that condition (1) implies condition (2), and vice versa.

- (1) \(\Rightarrow\) (2). Suppose \(a\) and \(b\) are coprime, and fix \(d \in \mathbb{Z}\) with \(d \mid a\) and \(d \mid b\). Then \(d \mid \gcd(a, b) = 1\), so \(d\) is a unit.

- (2) \(\Rightarrow\) (1). Suppose condition (2) above holds. We prove that 1 satisfies the conditions required to be a greatest common divisor of \(a\) and \(b\). The fact that \(1 \mid a\) and \(1 \mid b\) is automatic; and the fact that if \(d \mid a\) and \(d \mid b\) implies \(d \mid 1\) is precisely the condition (2) that we are assuming.

Hence the two conditions are equivalent.

### Exercise 7.1.30
Let \(a\) and \(b\) be integers, not both zero, and let \(d = \gcd(a, b)\). The integers \(\frac{a}{d}\) and \(\frac{b}{d}\) are coprime.

The following corollary is a specialisation of B√©zout‚Äôs lemma to the case when \(a\) and \(b\) are coprime.

### Corollary 7.1.31
Let \(a, b \in \mathbb{Z}\). The equation \(au + bv = 1\) has a solution if and only if \(a\) and \(b\) are coprime. Moreover, if \(a\) and \(b\) are coprime, then the equation \(au + bv = z\) has a solution for all \(z \in \mathbb{Z}\).

**Proof**  
By B√©zout‚Äôs lemma (Theorem 7.1.23), the equation \(au + bv = 1\) has a solution if and only if \(\gcd(a, b) \mid 1\). But the only positive divisor of 1 is 1, so a solution exists if and only if \(\gcd(a, b) = 1\), which is precisely the assertion that \(a\) and \(b\) are coprime.

If \(a\) and \(b\) are coprime, then \(1 = \gcd(a, b) \mid z\) for all \(z \in \mathbb{Z}\). So by B√©zout‚Äôs lemma again, the equation \(au + bv = z\) has a solution for all \(z \in \mathbb{Z}\).

A useful consequence of B√©zout‚Äôs lemma is the following result:

### Proposition 7.1.32
Let \(a, b, c \in \mathbb{Z}\). If \(a\) and \(b\) are coprime and \(a \mid bc\), then \(a \mid c\).
</markdown><markdown>
Proof

By B√©zout‚Äôs lemma (Theorem 7.1.23) there exist integers \( u \) and \( v \) such that \( au + bv = 1 \). Multiplying by \( c \) gives \( acu + bcv = c \). Since \( a \mid bc \), we can write \( bc = ka \) for some \( k \in \mathbb{Z} \), and so \( acu + kav = c \). But then

\[
(cu + kv)a = c
\]

which proves that \( a \mid c \).

## Linear Diophantine equations

We have now seen two important results:

- **The Euclidean algorithm**, which was a procedure for computing the greatest common divisor of two integers.
- **B√©zout‚Äôs lemma**, which provides a necessary and sufficient condition for equations of the form \( ax + by = c \) to have an integer solution.

We will now develop the reverse Euclidean algorithm, which provides a method for computing a solutions to (bivariate) linear Diophantine equations, when such a solution exists. Then we will prove a theorem that characterises all integer solutions in terms of a given solution.

‚úê **Example 7.1.33**

Suppose we want to find integers \( x \) and \( y \) such that \( 327x + 114y = 18 \). Running the Euclidean algorithm yields that \( \gcd(327, 114) = 3 \)‚Äîsee below. For reasons soon to become apparent, we rearrange each equation to express the remainder on its own.

\[
\begin{align*}
327 &= 2 \times 114 + 99 &\Rightarrow& \quad 99 = 327 - 2 \times 114 \quad (1) \\
114 &= 1 \times 99 + 15 &\Rightarrow& \quad 15 = 114 - 1 \times 99 \quad (2) \\
99 &= 6 \times 15 + 9 &\Rightarrow& \quad 9 = 99 - 6 \times 15 \quad (3) \\
15 &= 1 \times 9 + 6 &\Rightarrow& \quad 6 = 15 - 1 \times 9 \quad (4) \\
9 &= 1 \times 6 + 3 &\Rightarrow& \quad 3 = 9 - 1 \times 6 \quad (5) \\
6 &= 2 \times 3 + 0 \\
\end{align*}
\]

We can then express 3 in the form \( 327u + 114v \) by successively substituting the equations into each other:

- Equation (5) expresses 3 as a linear combination of 6 and 9. Substituting equation (4) yields:

\[
3 = 9 - 1 \times (15 - 1 \times 9) \quad \Rightarrow \quad 3 = 2 \times 9 - 1 \times 15
\]
</markdown><markdown>
- This now expresses 3 as a linear combination of 9 and 15. Substituting equation (3) yields:

  \[
  3 = 2 \times (99 - 6 \times 15) - 1 \times 15 \implies 3 = (-13) \times 15 + 2 \times 99
  \]

- This now expresses 3 as a linear combination of 15 and 99. Substituting equation (2) yields:

  \[
  3 = (-13) \times (114 - 1 \times 99) + 2 \times 99 \implies 3 = 15 \times 99 - 13 \times 114
  \]

- This now expresses 3 as a linear combination of 99 and 114. Substituting equation (1) yields:

  \[
  3 = 15 \times (327 - 2 \times 114) - 13 \times 114 \implies 3 = (-43) \times 114 + 15 \times 327
  \]

Now that we‚Äôve expressed 3 as a linear combination of 114 and 327, we‚Äôre nearly done: we know that 18 = 6 √ó 3, so multiplying through by 6 gives

\[
18 = (-258) \times 114 + 90 \times 327
\]

Hence \((x, y) = (90, -258)\) is a solution to the equation \(327x + 114y = 18\).

### Proof tip
Let \(a, b \in \mathbb{Z}\) and let \(d = \gcd(a, b)\). To find integers \(x, y\) such that \(ax + by = d\):

(i) Run the Euclidean algorithm on the pair \((a, b)\), keeping track of all quotients and remainders.

(ii) Rearrange each equation of the form \(r_{n-2} = q_n r_{n-1} + r_n\) to isolate \(r_n\).

(iii) Substitute for the remainders \(r_k\) in reverse order until \(\gcd(a, b)\) is expressed in the form \(ax + by\) for some \(x, y \in \mathbb{Z}\).

This process is called the reverse Euclidean algorithm.

### Exercise 7.1.34
Find a solution \((x, y) \in \mathbb{Z} \times \mathbb{Z}\) to the equation \(630x + 385y = 4340\).

Now that we have a procedure for computing one solution to the equation \(ax + by = c\), we need to come up with a procedure for computing all solutions. This can be done by proving the following theorem.
</markdown><markdown>
### Theorem 7.1.35

Let \( a, b, c \in \mathbb{Z} \), where \( a \) and \( b \) are not both zero. Suppose that \( x_0 \) and \( y_0 \) are integers such that \( ax_0 + by_0 = c \). Then, \( (x, y) \in \mathbb{Z} \times \mathbb{Z} \) is another solution to the equation \( ax + by = c \) if and only if

\[
x = x_0 + k \cdot \frac{b}{\gcd(a, b)} \quad \text{and} \quad y = y_0 - k \cdot \frac{a}{\gcd(a, b)}
\]

for some \( k \in \mathbb{Z} \).

Thus, as soon as we‚Äôve found one solution \( (x, y) = (x_0, y_0) \) to the equation \( ax + by = c \), this theorem tells us what all other solutions must look like.

**Proof of Theorem 7.1.35**

We prove the two directions separately.

(‚áí). First suppose that \( (x_0, y_0) \) is an integer solution to the equation \( ax + by = c \). Let \( k \in \mathbb{Z} \) and let

\[
x = x_0 + k \cdot \frac{b}{\gcd(a, b)} \quad \text{and} \quad y = y_0 - k \cdot \frac{a}{\gcd(a, b)}
\]

Then

\[
ax + by = a \left( x_0 + k \cdot \frac{b}{\gcd(a, b)} \right) + b \left( y_0 - k \cdot \frac{a}{\gcd(a, b)} \right) \quad \text{by definition of } x \text{ and } y
\]

\[
= (ax_0 + by_0) + ak \cdot \frac{b}{\gcd(a, b)} - kb \cdot \frac{a}{\gcd(a, b)} \quad \text{rearranging}
\]

\[
= (ax_0 + by_0) + \frac{kab - kab}{\gcd(a, b)} \quad \text{combining the fractions}
\]

\[
= ax_0 + by_0 \quad \text{since } kab - kab = 0
\]

\[
= c \quad \text{since } (x_0, y_0) \text{ is a solution}
\]

so \( (x, y) \) is indeed a solution to the equation.

(‚áê). First suppose that \( a \mid b \). Fix a solution \( (x_0, y_0) \) to the equation \( ax + by = c \), and let \( (x, y) \) be another solution. Then

\[
a(x - x_0) + b(y - y_0) = (ax_0 + by_0) - (ax + by) = c - c = 0
\]

so that

\[
a(x - x_0) = b(y_0 - y)
\]

Now \( a \) and \( b \) are coprime, so by Proposition 7.1.32, we have \( a \mid y_0 - y \) and \( b \mid x - x_0 \). Let \( k, \ell \in \mathbb{Z} \) be such that \( x - x_0 = kb \) and \( y_0 - y = \ell a \). Then substituting into the above
</markdown><markdown>
equation yields

\[ a \cdot kb = b \cdot \ell a \]

and hence \((k - \ell)ab = 0\). Since \(ab \neq 0\), we have \(k = \ell\), so that

\[ x = x_0 + kb \quad \text{and} \quad y = y_0 - ka \]

Now we drop the assumption that \(a \perp b\). Let \(\gcd(a, b) = d \geq 1\). We know that \(d \mid c\), by B√©zout's lemma (Theorem 7.1.23), and so

\[
\frac{a}{d} x + \frac{b}{d} y = \frac{c}{d}
\]

is another linear Diophantine equation, and moreover \(\frac{a}{d} \perp \frac{b}{d}\) by Exercise 7.1.30. By what we proved above, we have

\[ x = x_0 + k \cdot \frac{b}{d} \quad \text{and} \quad y = y_0 - k \cdot \frac{a}{d} \]

for some \(k \in \mathbb{Z}\). But this is exactly what we sought to prove!

### Example 7.1.36

We know that \((x, y) = (90, -258)\) is a solution to the equation \(327x + 114y = 18\), and

\[
\frac{327}{\gcd(327, 114)} = \frac{327}{3} = 109 \quad \text{and} \quad \frac{114}{\gcd(327, 114)} = \frac{114}{3} = 38
\]

so this theorem tells us that \((x, y) \in \mathbb{Z} \times \mathbb{Z}\) is a solution to the equation \(327x + 114y = 18\) if and only if

\[ x = 90 + 38k \quad \text{and} \quad y = -258 - 109k \]

for some \(k \in \mathbb{Z}\).

### Exercise 7.1.37

Find all integers \(x, y\) such that

\[ 630x + 385y = 4340 \]

### Least common multiples

You would be forgiven for wondering why so much of the foregoing section was devoted to greatest common divisors, with no mention of least common multiples. We will now give the latter some attention.
</markdown><markdown>
### Definition 7.1.38
Let \( a, b \in \mathbb{Z} \). An integer \( m \) is a **least common multiple** of \( a \) and \( b \) if:

(a) \( a \mid m \) and \( b \mid m \);

(b) If \( c \) is another integer such that \( a \mid c \) and \( b \mid c \), then \( m \mid c \).

The definition of least common multiple is **dual** to that of greatest common divisor ([Definition 7.1.9](#)). This means that many properties of greatest common divisors have corresponding ‚Äòdual‚Äô properties, which hold of least common multiples. As such, we will not say much here about least common multiples, and that which we *do* say is in the form of exercises.

### Exercise 7.1.39
Let \( a, b \in \mathbb{Z} \). Prove that \( a \) and \( b \) have a least common multiple. Furthermore, prove that least common multiples are unique up to sign, in the sense that if \( m, m' \) are two least common multiples of \( a \) and \( b \), then \( m = m' \) or \( m = -m' \).

As with greatest common divisors, [Exercise 7.1.39](#) justifies the following definition.

### Definition 7.1.40
Given \( a, b \in \mathbb{Z} \), denote by \(\mathrm{lcm}(a, b)\) (LaTeX code: `\mathrm{lcm}`) the non-negative least common multiple of \( a \) and \( b \).

### Exercise 7.1.41
Let \( a, b \in \mathbb{Z} \). Prove that \(\gcd(a, b) \cdot \mathrm{lcm}(a, b) = |ab|\).
</markdown><markdown>
## Section 7.2

# Prime numbers

Thinking of divisibility as a way of *breaking down* an integer, for example \(12 = 2 \times 2 \times 3\), our goal now is to show that there are particular integers that are *atomic*‚Äîthey are the building blocks of the integers, in the sense that:

- Every integer can be broken into a product of these atomic integers‚Ä¶
- ‚Ä¶and these atomic integers cannot themselves be broken down any further‚Ä¶
- ‚Ä¶and there is an essentially unique way to write an integer as a product of these atomic integers.

There are a couple of fairly vague terms used here: ‚Äòatomic‚Äô and ‚Äòessentially unique‚Äô. But as always, we will make these terms precise when we need to.

### Primes and irreducibles

There are two ways that we might want to characterise the so-called atomic integer that we just mentioned.

- One way that an integer might be atomic is if it allows us to break down products of integers‚Äîthis leads to the notion of *prime* (Definition 7.2.1).
- Another way is that an integer might be atomic is if it cannot be split up as a product of more than one integer (in a nontrivial way)‚Äîthis leads to the notion of *irreducible* (Definition 7.2.6).

Conveniently, as we will show in Theorem 7.2.11, these two notions coincide. But the fact that they coincide is not obvious, and uses essential properties of the integers that do not hold in more general structures.

The definition of *prime* that we are about to give comes from abstract algebra (specifically, from ring theory). It might seem strange, but we will soon be able to show that the more familiar definition‚Äîthat is, having exactly two positive divisors‚Äîis equivalent to this one.

‚ú¶ **Definition 7.2.1**  
An integer \( p \) is (ring theoretically) *prime* if \( p \) is a nonzero nonunit and, for all \( a, b \in \mathbb{Z} \), if \( p \mid ab \) then \( p \mid a \) or \( p \mid b \).
</markdown><markdown>
### Example 7.2.2

2 is prime. To see this, suppose it isn‚Äôt. Then there exist \(a, b \in \mathbb{Z}\) such that \(2 \mid ab\) but 2 divides neither \(a\) nor \(b\). Thus \(a\) and \(b\) are both odd, meaning that \(ab\) is odd... but this contradicts the assumption that \(2 \mid ab\).

However, 18 is not prime. Indeed, \(18 \mid 12 \times 15\) but 18 divides neither 12 nor 15.

### Exercise 7.2.3

Using Definition 7.2.1, prove that 3 and 5 are prime, and that 4 is not prime.

### Example 7.2.4

Let \(k \in \mathbb{Z}\) with \(0 < k < 5\). We‚Äôll show that \(5 \mid \binom{5}{k}\).

Well, by Theorem 4.2.14 we know that

\[
5! = \binom{5}{k} k! (5-k)!
\]

By Definition 4.1.14, we have

\[
5 \times 4! = \binom{5}{k} \times \underbrace{1 \times \cdots \times k}_{=k!} \times \underbrace{1 \times \cdots \times (5-k)}_{=(5-k)!}
\]

Since 5 is prime, it must divide one of the factors on the right-hand side of this equation. Thus, either 5 divides \(\binom{5}{k}\), or it divides \(\ell\) for some \(1 \leq \ell \leq k\) or \(1 \leq \ell \leq 5-k\). But \(k < 5\) and \(5-k < 5\), so it cannot divide any of these values of \(\ell\)‚Äîif it did, it would imply \(5 \leq \ell \leq k\) or \(5 \leq \ell \leq 5-k\), which is nonsense. Hence 5 must divide \(\binom{5}{k}\).

### Exercise 7.2.5

Let \(p \in \mathbb{Z}\) be a positive prime and let \(0 < k < p\). Show that \(p \mid \binom{p}{k}\).

We now arrive at our second notion of atomic, capturing the idea that it should not be possible to break an atomic integer into smaller parts.

### Definition 7.2.6

An integer \(a\) is **irreducible** if \(a\) is a nonzero nonunit and, for all \(m, n \in \mathbb{Z}\), if \(a = mn\), then either \(m\) or \(n\) is a unit. Otherwise, \(a\) is **reducible**.

The notion of irreducible captures more closely the more familiar notion of ‚Äòprime‚Äô, as the next result shows.

### Proposition 7.2.7

Let \(p \in \mathbb{Z}\) be a nonzero nonunit. Then \(p\) is irreducible if and only if the only divisors of \(p\) are \(p\), \(-p\), 1 and \(-1\).
</markdown><markdown>
### Proof

Suppose \( p \) is irreducible and that \( a \mid p \). Then \( p = ab \) for some \( b \in \mathbb{Z} \). Since \( p \) is irreducible, either \( a \) or \( b \) is a unit. If \( a \) is a unit then \( b = \pm p \), and if \( b \) is a unit then \( a = \pm p \). So the only divisors of \( p \) are \( \pm 1 \) and \( \pm p \).

Conversely, suppose that the only divisors of \( p \) are \( \pm 1 \) and \( \pm p \), and let \( a, b \in \mathbb{Z} \) with \( p = ab \). We want to prove that \( a \) or \( b \) is a unit. Since \( a \mid p \), we have \( a \in \{1, -1, p, -p\} \). If \( a = \pm 1 \), then \( a \) is a unit; if \( a = \pm p \), then \( b = \pm 1 \), so that \( b \) is a unit. In any case, either \( a \) or \( b \) is a unit, and hence \( p \) is irreducible.

### Example 7.2.8

A couple of examples of reducible and irreducible numbers are:

- \( 2 \) is irreducible: if \( 2 = mn \) then either \( m \) or \( n \) is even, otherwise we‚Äôd be expressing an even number as the product of two odd numbers. We may assume \( m \) is even, say \( m = 2k \); then \( 2 = 2kn \), so \( kn = 1 \) and hence \( n \) is a unit.

- \( 20 \) is reducible since \( 20 = 4 \times 5 \) and neither 4 nor 5 is a unit.

### Exercise 7.2.9

Let \( p \in \mathbb{Z} \). Prove that if \( p \) is ring theoretically prime, then \( p \) is irreducible.

### Lemma 7.2.10

Let \( a \in \mathbb{Z} \) be a nonzero nonunit. Then there are irreducibles \( p_1, \ldots, p_n \) such that \( a = p_1 \times \cdots \times p_n \).

#### Proof

We may assume \( a > 0 \), since if \( a < 0 \) we can just multiply by \(-1\).

We proceed by strong induction on \( a \geq 2 \). The base case has \( a = 2 \) since we consider only nonunits.

- **(Base case)** We have shown that 2 is irreducible, so setting \( p_1 = 2 \) yields a product of primes.

- **(Induction step)** Let \( a \geq 2 \) and suppose that each integer \( k \) with \( 2 \leq k \leq a \) has an expression as a product of irreducibles. If \( a + 1 \) is irreducible then we‚Äôre done; otherwise we can write \( a + 1 = st \), where \( s, t \in \mathbb{Z} \) are nonzero nonunits. We may assume further that \( s \) and \( t \) are positive. Moreover, \( s < a + 1 \) and \( t < a + 1 \) since \( s, t \geq 2 \).

By the induction hypothesis, \( s \) and \( t \) have expressions as products of irreducibles. Write

\[ s = p_1 \times \cdots \times p_m \quad \text{and} \quad t = q_1 \times \cdots \times q_n \]
</markdown><markdown>
This gives rise to an expression of \( a \) as a product of irreducibles:

\[
a = st = \underbrace{p_1 \times \cdots \times p_m}_{=s} \times \underbrace{q_1 \times \cdots \times q_n}_{=t}
\]

The result follows by induction.

---

### Theorem 7.2.11
Let \( p \in \mathbb{Z} \). Then \( p \) is ring theoretically prime if and only if \( p \) is irreducible.

**Proof**

We prove the two directions separately.

- **Prime \(\Rightarrow\) irreducible.** This was Exercise 7.2.9.

- **Irreducible \(\Rightarrow\) prime.** Suppose \( p \) is irreducible. Let \( a, b \in \mathbb{Z} \) and suppose \( p \mid ab \). We need to show that \( p \mid a \) or \( p \mid b \). It suffices to show that if \( p \nmid a \) then \( p \mid b \).

  So suppose \( p \nmid a \). Let \( d = \gcd(p, a) \). Since \( d \mid p \) and \( p \) is irreducible, we must have \( d = 1 \) or \( d = p \) by Proposition 7.2.7. Since \( p \nmid a \) and \( d \mid a \), we must therefore have \( d = 1 \).

  By B√©zout‚Äôs lemma (Theorem 7.1.23), there exist \( u, v \in \mathbb{Z} \) such that \( au + pv = 1 \). Multiplying by \( b \) gives \( abu + pbv = b \). Since \( p \mid ab \), there exists \( k \in \mathbb{Z} \) such that \( pk = ab \). Define \( q = ku + bv \); then

  \[
  b = abu + pbv = pku + pbv = p(ku + bv) = qp
  \]

  so \( p \mid b \), as required.

So we‚Äôre done.

Since primes and irreducibles are the same thing in \(\mathbb{Z}\), we will refer to them as ‚Äòprimes‚Äô, unless we need to emphasise a particular aspect of them.

### Prime factorisation

Having described prime numbers in two ways, each of which emphasises their nature of being ‚Äòunbreakable‚Äô by multiplication, we will extend Lemma 7.2.10 to prove that every integer can be expressed as a product of primes in an essentially unique way.
</markdown><markdown>
### Theorem 7.2.12 (Fundamental theorem of arithmetic)

Let \( a \in \mathbb{Z} \) be a nonzero nonunit. There exist primes \( p_1, \ldots, p_k \in \mathbb{Z} \) such that

\[ 
a = p_1 \times \cdots \times p_k 
\]

Moreover, this expression is essentially unique: if \( a = q_1 \times \cdots \times q_\ell \) is another expression of \( a \) as a product of primes, then \( k = \ell \) and, re-ordering the \( q_i \) if necessary, for each \( i \) there is a unit \( u_i \) such that \( q_i = u_i p_i \).

#### Proof

We showed that such a factorisation exists in Lemma 7.2.10, with the word ‚Äòprime‚Äô replaced by the word ‚Äòirreducible‚Äô. It remains to prove (essential) uniqueness.

Let \( k \) be least such that there is an expression of \( a \) as a product of \( k \) primes, namely \( a = p_1 \times \cdots \times p_k \). Let \( a = q_1 \times \cdots \times q_\ell \) be any other such expression. We prove by induction on \( k \) that \( \ell = k \) and, after re-ordering if necessary, for each \( i \) there is a unit \( u_i \) such that \( q_i = u_i p_i \).

- **(Base case)** If \( k = 1 \) then \( a = p_1 \) is itself prime. Then we have \( p_1 = q_1 \times \cdots \times q_\ell \). Since \( p_1 \) is prime, \( p_1 \mid q_j \) for some \( j \); by relabelling \( q_1 \) and \( q_j \) we may assume that \( j = 1 \), so that \( p_1 \mid q_1 \). By irreducibility of \( q_1 \) we have \( q_1 = u_1 p_1 \) for some unit \( u_1 \).

- **(Induction step)** Let \( k \geq 1 \) and suppose that any integer which can be expressed as a product of \( k \) primes is (essentially) uniquely expressible in such a way. Suppose \( a \) has an expression as a product of \( k+1 \) primes, and that \( k+1 \) is the least such number. Suppose also that

  \[
  a = p_1 \times \cdots \times p_k \times p_{k+1} = q_1 \times \cdots \times q_\ell
  \]

  Note that \( \ell \geq k+1 \). Since \( p_{k+1} \) is prime we must have \( p_{k+1} \mid q_j \) for some \( j \); by relabelling \( q_j \) and \( q_\ell \) if necessary, we may assume that \( j = \ell \), so that \( p_{k+1} \mid q_\ell \). As before, \( q_\ell = u_{k+1} p_{k+1} \) for some unit \( u_{k+1} \). Dividing through by \( p_{k+1} \) gives

  \[
  p_1 \times \cdots \times p_k = q_1 \times \cdots \times q_{\ell-1} \times u_{k+1}
  \]

  Replacing \( q_{\ell-1} \) by \( q_{\ell-1} u_{k+1} \), which is still prime, we can apply the induction hypothesis to obtain \( k = \ell - 1 \), so \( k+1 = \ell \), and, after reordering if necessary \( q_i = u_i p_i \) for all \( i \leq k \). Since this also holds for \( i = k+1 \), the induction step is complete.

The result follows by induction.

### Example 7.2.13

Here are some examples of numbers written as products of primes:

- \( 12 = 2 \times 2 \times 3 \). We could also write this as \( 2 \times 3 \times 2 \) or \((-2) \times (-3) \times 2\), and so on.

- \( 53 = 53 \) is an expression of 53 as a product of primes.
</markdown><markdown>
- \(-1000 = 2 \times 5 \times (-2) \times 5 \times 2 \times 5\).

- We can view any unit as a product of no primes. (Don‚Äôt dwell on this point for too long as it will not arise very often!)

### Exercise 7.2.14
Express the following numbers as products of primes:

\[ 16 \quad -240 \quad 5050 \quad 111111 \quad -123456789 \]

To make things slightly more concise, we introduce a standard way of expressing a number as a product of primes:

### Definition 7.2.15
The **canonical prime factorisation** of a nonzero integer \( a \) is the expression in the form

\[ a = u p_1^{j_1} \cdots p_r^{j_r} \]

where \( r \geq 0 \) and:

- \( u = 1 \) if \( a > 0 \), and \( u = -1 \) if \( a < 0 \);
- The numbers \( p_i \) are all positive primes;
- \( p_1 < p_2 < \cdots < p_r \);
- \( j_i \geq 1 \) for all \( i \).

We call \( j_i \) the **multiplicity** of \( p_i \) in \( a \), and we call \( u \) the **sign** of \( a \).

Typically we omit \( u \) if \( u = 1 \) (unless \( a = 1 \)), and just write a minus sign \((-)\) if \( u = -1 \).

### Example 7.2.16
The canonical prime factorisations of the integers given in Example 7.2.13 are:

- \( 12 = 2^2 \cdot 3 \).
- \( 53 = 53 \).
- \(-1000 = -2^3 \cdot 5^3 \).
- \( 1 = 1 \).
</markdown><markdown>
### Exercise 7.2.17
Write out the canonical prime factorisations of the numbers from Exercise 7.2.14, which were:

\[ 16 \quad -240 \quad 5050 \quad 111111 \quad -123456789 \]

The following exercise provides another tool for computing greatest common divisors of pairs of integers by looking at their prime factorisations.

### Exercise 7.2.18
Let \( p_1, p_2, \ldots, p_r \) be distinct primes, and let \( k_i, \ell_i \in \mathbb{N} \) for all \( 1 \leq i \leq r \). Define

\[ m = p_1^{k_1} \times p_2^{k_2} \times \cdots \times p_r^{k_r} \quad \text{and} \quad n = p_1^{\ell_1} \times p_2^{\ell_2} \times \cdots \times p_r^{\ell_r} \]

Prove that

\[ \gcd(m, n) = p_1^{u_1} \times p_2^{u_2} \times \cdots \times p_r^{u_r} \]

where \( u_i = \min\{k_i, \ell_i\} \) for all \( 1 \leq i \leq r \).

### Example 7.2.19
We use Exercise 7.2.18 to compute the greatest common divisor of 17640 and 6468.

First we compute the prime factorisations of 17640 and 6468:

\[ 17640 = 2^3 \cdot 3^2 \cdot 5 \cdot 7^2 \quad \text{and} \quad 6468 = 2^2 \cdot 3 \cdot 7^2 \cdot 11 \]

It now follows from Exercise 7.2.18 that

\[ \gcd(17640, 6468) = 2^2 \cdot 3^1 \cdot 5^0 \cdot 7^2 \cdot 11^0 \]

\[ = 4 \cdot 3 \cdot 1 \cdot 49 \cdot 1 \]

\[ = 588 \]

Exercise 7.2.18 allows us to provide a concise proof of the following result.

### Corollary 7.2.20
Let \( p \in \mathbb{Z} \) be prime, let \( a \in \mathbb{Z} \) be nonzero, and let \( k \geq 1 \). Then \( a \perp p^k \) if and only if \( p \nmid a \).

**Proof**

By the fundamental theorem of arithmetic, we can write

\[ a = p^j \times p_1^{j_1} \times \cdots \times p_r^{j_r} \]

where \( p_1, \ldots, p_r \) are the primes other than \( p \) appearing in the prime factorisation of \( a \), and \( j, j_i \in \mathbb{N} \) for all \( 1 \leq i \leq r \). Note that \( p \mid a \) if and only if \( j \geq 1 \).
</markdown><markdown>
Furthermore we have

\[ p^k = p^k \times p_1^0 \times \cdots \times p_r^0 \]

By Exercise 7.2.18 it follows that

\[ \gcd(a, p^k) = p^{\min\{j,k\}} \times p_1^0 \times \cdots \times p_r^0 = p^{\min\{j,k\}} \]

Now:

- If \(\min\{j,k\} = 0\) then \(j = 0\), in which case \(p \nmid a\), and \(\gcd(a, p^k) = p^0 = 1\);
- If \(\min\{j,k\} > 0\) then \(j \geq 1\), in which case \(p \mid a\), and \(p \mid \gcd(a, p^k)\), so \(\gcd(a, p^k) \neq 1\).

In particular, \(p \nmid a\) if and only if \(a \perp p^k\).

## Distribution of primes

So far we have seen several examples of prime numbers; to name a few, we‚Äôve seen 2, 3, 5 and 53. It might seem like the prime numbers go on forever, but proving this is less than obvious.

### Exercise 7.2.21

Let \(P\) be an inhabited finite set of positive prime numbers and let \(m\) be the product of all the elements of \(P\). That is, for some \(n \geq 1\) let

\[ P = \{p_1, \ldots, p_n\} \quad \text{and} \quad m = p_1 \times \cdots \times p_n \]

where each \(p_k \in P\) is a positive prime. Using the fundamental theorem of arithmetic, show that \(m + 1\) has a positive prime divisor which is not an element of \(P\).

### Theorem 7.2.22

There are infinitely many primes.

**Proof**

We prove that there are infinitely many positive prime numbers‚Äîthe result then follows immediately. Let \(P\) be the set of all positive prime numbers. Then \(P\) is inhabited, since \(2 \in P\), for example. If \(P\) were finite, then by Exercise 7.2.21, there would be a positive prime which is not an element of \(P\)‚Äîbut \(P\) contains all positive primes, so that is impossible. Hence there are infinitely many positive primes.

This is one proof of many and is attributed to Euclid, who lived around 2300 years ago. We might hope that a proof of the infinitude of primes gives some insight into how the primes are distributed. That is, we might ask questions like: how frequently do primes
</markdown><markdown>
As a starting point, Euclid‚Äôs proof gives an algorithm for writing an infinite list of primes:

- Let \( p_1 = 2 \); we know that 2 is prime;
- Given \( p_1, \ldots, p_n \), let \( p_{n+1} \) be the smallest positive prime factor of \( p_1 \times \cdots \times p_n + 1 \).

The first few terms produced would be:

- \( p_1 = 2 \) by definition;
- \( 2 + 1 = 3 \), which is prime, so \( p_2 = 3 \);
- \( 2 \times 3 + 1 = 7 \), which is prime, so \( p_3 = 7 \);
- \( 2 \times 3 \times 7 + 1 = 43 \), which is prime, so \( p_4 = 43 \);
- \( 2 \times 3 \times 7 \times 43 + 1 = 1807 = 13 \times 139 \), so \( p_5 = 13 \);
- \( 2 \times 3 \times 7 \times 43 \times 13 + 1 = 23479 = 53 \times 443 \), so \( p_6 = 53 \);
- \(\ldots\) and so on.

The sequence obtained, called the Euclid‚ÄìMullin sequence, is a bit bizarre:

\[ 2, 3, 7, 43, 13, 53, 5, 6221671, 38709183810571, 139, 2801, 11, 17, 5471, \ldots \]

Big primes like 38709183810571 often appear before small primes like 11. It remains unknown whether or not every positive prime number appears in this list!

The chaotic nature of this sequence makes it difficult to extract information about how the primes are distributed: the numbers \( p_1 \times \cdots \times p_n + 1 \) grow very quickly‚Äîindeed, it must be the case that \( p_1 \times \cdots \times p_n + 1 > 2^n \) for all \( n \)‚Äîso the upper bounds for the sequence grow at least exponentially.

Another proof of the infinitude of primes that gives a (slightly) tighter bound can be obtained using the following exercise.

**Exercise 7.2.23**  
Let \( n \in \mathbb{Z} \) with \( n > 2 \). Prove that the set \(\{ k \in \mathbb{Z} \mid n < k < n! \}\) contains a prime number.
</markdown><markdown>
## Section 7.3
# Modular arithmetic

Recall the definition of congruence modulo an integer from [Section 5.2](#).

### Definition 5.2.6
Fix \( n \in \mathbb{Z} \). Given integers \( a, b \in \mathbb{Z} \), we say \( a \) is **congruent** to \( b \) modulo \( n \), and write

\[ 
a \equiv b \mod n \quad (\text{LaTeX code: } a \equiv b \ \bmod{n}) 
\]

if \( n \) divides \( a - b \). If \( a \) is not congruent to \( b \) modulo \( n \), write

\[ 
a \not\equiv b \mod n \quad (\text{LaTeX code: } \not\equiv) 
\]

The number \( n \) is called the **modulus** of the congruence.

In [Section 5.2](#), we proved that congruence is an equivalence relation:

### Theorem 5.2.11
Let \( n \in \mathbb{Z} \). Then congruence modulo \( n \) is an equivalence relation on \( \mathbb{Z} \). That is:

(a) \( a \equiv a \mod n \) for all \( a \in \mathbb{Z} \);

(b) For all \( a, b \in \mathbb{Z} \), if \( a \equiv b \mod n \), then \( b \equiv a \mod n \);

(c) For all \( a, b, c \in \mathbb{Z} \), if \( a \equiv b \mod n \) and \( b \equiv c \mod n \), then \( a \equiv c \mod n \).

In this section, we turn our attention to addition, subtraction, multiplication and division: our goal is to find out how much arithmetic can be done with equality replaced by congruence. For example:

(i) Can we add a number to both sides of a congruence? That is, given \( a, b, c, n \in \mathbb{Z} \), is it the case that \( a \equiv b \mod n \) implies \( a + c \equiv b + c \mod n \)?

(ii) Can we multiply both sides of a congruence by a number? That is, given \( a, b, c, n \in \mathbb{Z} \), is it the case that \( a \equiv b \mod n \) implies \( ac \equiv bc \mod n \)?

(iii) Can we divide both sides of a congruence by a nonzero common factor? That is, given \( a, b, c, n \in \mathbb{Z} \) with \( c \not\equiv 0 \mod n \), is it the case that if \( ac \equiv bc \mod n \) implies \( a \equiv b \mod n \)?
</markdown><markdown>
The answers to (i) and (ii) are ‚Äòyes‚Äô, as we will prove; but surprisingly, the answer to (iii) is ‚Äòno‚Äô (except under certain circumstances). For example, \(2 \times 3 \equiv 4 \times 3 \mod 6\), but \(2 \not\equiv 4 \mod 6\), even though \(3 \not\equiv 0 \mod 6\).

In light of this, it is important from the outset to point out that, although congruence is written with a symbol that looks like that of equality (‚Äò\(\equiv\)‚Äô vs. ‚Äò=‚Äô), and although it is an equivalence relation, we can only treat congruence like equality inasmuch as we prove that we can. Specifically:

- In [Theorem 5.2.11](#) we proved that congruence is an equivalence relation. This allows us to make some basic inferences about congruences‚Äîfor example, transitivity means that the following implication is valid:

  \[
  -5 \equiv 18 \equiv 41 \equiv 64 \mod 23 \quad \Rightarrow \quad -5 \equiv 64 \mod 23
  \]

- [Theorem 7.3.3](#), which we will prove soon, tells us that we can treat congruence like equality for the purposes of addition, multiplication and subtraction. Thus it will be valid to write things like

  \[
  x \equiv 7 \mod 12 \quad \Rightarrow \quad 2x + 5 \equiv 19 \mod 12
  \]

  and we‚Äôll be able to replace values by congruent values in congruences, provided they‚Äôre only being added, subtracted or multiplied. For example, from the knowledge that \(2^{60} \equiv 1 \mod 61\) and \(60! \equiv -1 \mod 61\), we will be able to deduce

  \[
  2^{60} \cdot 3 \equiv 60! \cdot x \mod 61 \quad \Rightarrow \quad 3 \equiv -x \mod 61
  \]

After we have worked out what arithmetic properties carry over to congruence, we will be able to prove some interesting theorems involving congruences and discuss their applications.

The first result we prove gives us a few equivalent ways of talking about congruence.

**Proposition 7.3.1**  
Fix a modulus \(n\) and let \(a, b \in \mathbb{Z}\). The following are equivalent:

(i) \(a\) and \(b\) leave the same remainder when divided by \(n\);

(ii) \(a = b + kn\) for some \(k \in \mathbb{Z}\);

(iii) \(a \equiv b \mod n\).

**Proof**  
We prove (i) \(\iff\) (iii) and (ii) \(\iff\) (iii).

- (i) \(\Rightarrow\) (iii). Suppose \(a\) and \(b\) leave the same remainder when divided by \(n\), and let \(q_1, q_2, r \in \mathbb{Z}\) be such that

  \[
  a = q_1n + r, \quad b = q_2n + r \quad \text{and} \quad 0 \leq r < n
  \]
</markdown><markdown>
Then \( a - b = (q_1 - q_2)n \), which proves that \( n \mid a - b \), and so \( a \equiv b \mod n \).

- (iii) \(\Rightarrow\) (i). Suppose that \( a \equiv b \mod n \), so that \( b - a = qn \) for some \( q \in \mathbb{Z} \). Write

  \[
  a = q_1n + r_1, \quad b = q_2n + r_2 \quad \text{and} \quad 0 \leq r_1, r_2 < n
  \]

  We may further assume that \( r_1 \leq r_2 \). (If not, swap the roles of \( a \) and \( b \)‚Äîthis is fine, since \( n \mid b - a \) if and only if \( n \mid a - b \).) Now we have

  \[
  b - a = qn \Rightarrow (q_2n + r_2) - (q_1n + r_1) = qn
  \]

  \[
  \Rightarrow (q_2 - q_1 - q)n + (r_2 - r_1) = 0 \quad \text{rearranging}
  \]

  since \( 0 \leq r_1 \leq r_2 < n \) we have \( 0 \leq r_2 - r_1 < n \), so that \( r_2 - r_1 \) is the remainder of 0 when divided by \( n \). That is, \( r_2 - r_1 = 0 \), so \( r_1 = r_2 \). Hence \( a \) and \( b \) have the same remainder when divided by \( n \).

- (ii) \(\Leftrightarrow\) (iii). We unpack the definitions of (ii) and (iii) to see that they are equivalent. Indeed

  \[
  \text{(ii)} \Leftrightarrow a = b + kn \text{ for some } k \in \mathbb{Z}
  \]

  \[
  \Leftrightarrow a - b = kn \text{ for some } k \in \mathbb{Z} \quad \text{rearranging}
  \]

  \[
  \Leftrightarrow n \mid a - b \quad \text{by definition of divisibility}
  \]

  \[
  \Leftrightarrow a \equiv b \mod n \quad \text{by definition of congruence}
  \]

  \[
  \Leftrightarrow \text{(iii)}
  \]

**Discussion 7.3.2**

Where in the proof of Proposition 7.3.1 did we rely on the convention that the modulus \( n \) is positive? Is the result still true if \( n \) is negative?

We now prove that we can treat congruence like equality for the purposes of adding, subtracting and multiplying (but not dividing!) integers.

**Theorem 7.3.3 (Modular arithmetic)**

Fix a modulus \( n \), and let \( a_1, a_2, b_1, b_2 \in \mathbb{Z} \) be such that

\[
a_1 \equiv a_2 \mod n \quad \text{and} \quad b_1 \equiv b_2 \mod n
\]

Then the following congruences hold:

(a) \( a_1 + b_1 \equiv a_2 + b_2 \mod n \);

(b) \( a_1b_1 \equiv a_2b_2 \mod n \);

(c) \( a_1 - b_1 \equiv a_2 - b_2 \mod n \).
</markdown><markdown>
**Proof**

By [Definition 5.2.6](#) that \( n \mid a_1 - a_2 \) and \( n \mid b_1 - b_2 \), so there exist \( q_1, q_2 \in \mathbb{Z} \) such that

\[
a_1 - a_2 = qn \quad \text{and} \quad b_1 - b_2 = rn
\]

This implies that

\[
(a_1 + b_1) - (a_2 + b_2) = (a_1 - a_2) + (b_1 - b_2) = qn + rn = (q + r)n
\]

so \( n \mid (a_1 + b_1) - (a_2 + b_2) \). This proves (a).

The algebra for (b) is slightly more involved:

\[
a_1b_1 - a_2b_2 = (qn + a_2)(rn + b_2) - a_2b_2
\]
\[
= qrn^2 + a_2rn + b_2qn + a_2b_2 - a_2b_2
\]
\[
= qrn^2 + a_2rn + b_2qn
\]
\[
= (qrn + a_2r + b_2q)n
\]

This shows that \( n \mid a_1b_1 - a_2b_2 \), thus proving (b).

Now (a) and (b) together imply (c). Indeed, we know that \(-1 \equiv -1 \mod n\) and \(b_1 \equiv b_2 \mod n\), so by (b) we have \(-b_1 \equiv -b_2 \mod n\). We also know that \(a_1 \equiv a_2 \mod n\), and hence \(a_1 - b_1 \equiv a_2 - b_2 \mod n\) by (a).

**Theorem 7.3.3** allows us to perform algebraic manipulations with congruences as if they were equations, provided all we‚Äôre doing is adding, multiplying and subtracting.

‚úé **Example 7.3.4**

We will solve the congruence \(3x - 5 \equiv 2x + 3 \mod 7\) for \(x\):

\[
3x - 5 \equiv 2x + 3 \mod 7
\]
\[
\Leftrightarrow \quad x - 5 \equiv 3 \mod 7 \quad (\Rightarrow) \text{subtract } 2x \quad (\Leftarrow) \text{add } 2x
\]
\[
\Leftrightarrow \quad x \equiv 8 \mod 7 \quad (\Rightarrow) \text{add } 5 \quad (\Leftarrow) \text{subtract } 5
\]
\[
\Leftrightarrow \quad x \equiv 1 \mod 7 \quad \text{since } 8 \equiv 1 \mod 7
\]

So the integers \(x\) for which \(3x - 5\) and \(2x + 3\) leave the same remainder when divided by 7, are precisely the integers \(x\) which leave a remainder of 1 when divided by 7:

\[
3x - 5 \equiv 2x + 3 \mod 7 \quad \Leftrightarrow \quad x = 7q + 1 \text{ for some } q \in \mathbb{Z}
\]

‚úé **Exercise 7.3.5**

For which integers \(x\) does the congruence \(5x + 1 \equiv x + 8 \mod 3\) hold? Characterise such integers \(x\) in terms of their remainder when divided by 3.
</markdown><markdown>
So far this all feels like we haven‚Äôt done very much: we‚Äôve just introduced a new symbol 
\(\equiv\) which behaves just like equality... but does it really? The following exercises should 
expose some more ways in which congruence does behave like equality, and some in 
which it doesn‚Äôt.

### Exercise 7.3.6
Fix a modulus \( n \). Is it true that

\[ a \equiv b \mod n \quad \Rightarrow \quad a^k \equiv b^k \mod n \]

for all \( a, b \in \mathbb{Z} \) and \( k \in \mathbb{N} \)? If so, prove it; if not, provide a counterexample.

### Exercise 7.3.7
Fix a modulus \( n \). Is it true that

\[ k \equiv \ell \mod n \quad \Rightarrow \quad a^k \equiv a^\ell \mod n \]

for all \( k, \ell \in \mathbb{N} \) and \( a \in \mathbb{Z} \)? If so, prove it; if not, provide a counterexample.

### Exercise 7.3.8
Fix a modulus \( n \). Is it true that

\[ qa \equiv qb \mod n \quad \Rightarrow \quad a \equiv b \mod n \]

for all \( a, b, q \in \mathbb{Z} \) with \( q \not\equiv 0 \mod n \)? If so, prove it; if not, provide a counterexample.

### Example 7.3.9
Now that we have seen several things that we can do with modular arithmetic, let‚Äôs look 
at some things that we cannot do:

- We cannot talk about fractions in modular arithmetic; for instance, it is invalid to say 
  \( 2x \equiv 1 \mod 5 \) implies \( x \equiv \frac{1}{2} \mod 5 \).

- We cannot take square roots in modular arithmetic; for instance, it is invalid to say 
  \( x^2 \equiv 3 \mod 4 \) implies \( x \equiv \pm \sqrt{3} \mod 4 \). In fact, it is invalid to say \( x^2 \equiv 1 \mod 8 \) 
  implies \( x \equiv \pm 1 \mod 8 \), since for example \( 3^2 \equiv 1 \mod 8 \) but \( 3 \not\equiv \pm 1 \mod 8 \).

- We cannot replace numbers in exponents by other numbers they are congruent to; for 
  instance, it is invalid to say \( x^3 \equiv 2^3 \mod 4 \) implies \( x \equiv 2 \mod 4 \).

## Multiplicative inverses

We made a big deal about the fact that fractions don‚Äôt make sense in modular arithmetic. 
That is, it is invalid to say

\[ 2x \equiv 1 \mod 5 \quad \Rightarrow \quad x \equiv \frac{1}{2} \mod 5 \]
</markdown><markdown>
Despite this, we can still make sense of ‚Äòdivision‚Äô, provided we change what we mean when we say ‚Äòdivision‚Äô. Indeed, the congruence \(2x \equiv 1 \mod 5\) has a solution:

\[
\begin{align*}
2x &\equiv 1 \mod 5 \\
\Leftrightarrow 6x &\equiv 3 \mod 5 \quad (\Rightarrow) \text{ multiply by 3} \quad (\Leftarrow) \text{ subtract 3} \\
\Leftrightarrow x &\equiv 3 \mod 5 \quad \text{since } 6 \equiv 1 \mod 5
\end{align*}
\]

Here we didn‚Äôt divide by 2, but we still managed to cancel the 2 by instead multiplying through by 3. For the purposes of solving the equation this had the same effect as division by 2 would have had if we were allowed to divide. The key here was that \(2 \times 3 \equiv 1 \mod 5\).

‚ú¶ **Definition 7.3.10**  
Fix a modulus \(n\). Given \(a \in \mathbb{Z}\), a **multiplicative inverse** for \(a\) modulo \(n\) is an integer \(u\) such that \(au \equiv 1 \mod n\).

‚úê **Example 7.3.11**  
Some examples of multiplicative inverses are as follows:

- 2 is a multiplicative inverse of itself modulo 3, since \(2 \times 2 \equiv 4 \equiv 1 \mod 3\).
- 2 is a multiplicative inverse of 3 modulo 5, since \(2 \times 3 \equiv 6 \equiv 1 \mod 5\).
- 7 is also a multiplicative inverse of 3 modulo 5, since \(3 \times 7 \equiv 21 \equiv 1 \mod 5\).
- 3 has no multiplicative inverse modulo 6. Indeed, suppose \(u \in \mathbb{Z}\) with \(3u \equiv 1 \mod 6\). Then \(6 \mid 3u - 1\), so \(3u - 1 = 6q\) for some \(q \in \mathbb{Z}\). But then

\[
1 = 3u - 6q = 3(u - 2q)
\]

which implies that \(3 \mid 1\), which is nonsense.

Knowing when multiplicative inverses exist is very important for solving congruences: if \(u\) is a multiplicative inverse for \(a\) modulo \(n\), then we can solve equations of the form \(ax \equiv b \mod n\) extremely easily:

\[
ax \equiv b \mod n \quad \Rightarrow \quad x \equiv ub \mod n
\]

‚úê **Exercise 7.3.12**  
For \(n = 7, 8, 9, 10, 11, 12\), either find a multiplicative inverse for 6 modulo \(n\), or show that no multiplicative inverse exists. Can you spot a pattern?

Some authors write \(a^{-1}\) to denote multiplicative inverses. We refrain from this, since it suggests that multiplicative inverses are unique‚Äîbut they‚Äôre not, as you‚Äôll see in the following exercise.
</markdown><markdown>
### Exercise 7.3.13
Let \( n \) be a modulus and let \( a \in \mathbb{Z} \). Suppose that \( u \) is a multiplicative inverse for \( a \) modulo \( n \). Prove that, for all \( k \in \mathbb{Z} \), \( u + kn \) is a multiplicative inverse for \( a \) modulo \( n \).

### Proposition 7.3.14
Let \( a \in \mathbb{Z} \) and let \( n \) be a modulus. Then \( a \) has a multiplicative inverse modulo \( n \) if and only if \( a \perp n \).

**Proof**  
Note that \( a \) has a multiplicative inverse \( u \) modulo \( n \) if and only if there is a solution \( (u, v) \) to the equation \( au + nv = 1 \). Indeed, \( au \equiv 1 \mod n \) if and only if \( n \mid au - 1 \), which occurs if and only if there is some \( q \in \mathbb{Z} \) such that \( au - 1 = nq \). Setting \( q = -v \) and rearranging yields the desired equivalence.

By B√©zout‚Äôs lemma (Theorem 7.1.23), such a solution \( (u, v) \) exists if and only if \( \gcd(a, n) \mid 1 \). This occurs if and only if \( \gcd(a, n) = 1 \), i.e. if and only if \( a \perp n \).

**Proof tip**  
To solve a congruence of the form \( ax \equiv b \mod n \) when \( a \perp n \), first find a multiplicative inverse \( u \) for \( a \) modulo \( n \), and then simply multiply through by \( u \) to obtain \( x \equiv ub \mod n \).

### Corollary 7.3.15
Let \( a, p \in \mathbb{Z} \), where \( p \) is a positive prime. If \( p \nmid a \) then \( a \) has a multiplicative inverse modulo \( p \).

**Proof**  
Suppose \( p \nmid a \), and let \( d = \gcd(a, p) \). Since \( d \mid p \) and \( p \) is prime we have \( d = 1 \) or \( d = p \). Since \( d \mid a \) and \( p \nmid a \) we can‚Äôt have \( d = p \); therefore \( d = 1 \). By Proposition 7.3.14, \( a \) has a multiplicative inverse modulo \( p \).

### Example 7.3.16
11 is prime, so each of the integers \( a \) with \( 1 \leq a \leq 10 \) should have a multiplicative inverse modulo 11. And indeed, the following are all congruent to 1 modulo 11:

\[
\begin{align*}
1 \times 1 &= 1 \\
2 \times 6 &= 12 \\
3 \times 4 &= 12 \\
4 \times 3 &= 12 \\
5 \times 9 &= 45 \\
6 \times 2 &= 12 \\
7 \times 8 &= 56 \\
8 \times 7 &= 56 \\
9 \times 5 &= 45 \\
10 \times 10 &= 100 \\
\end{align*}
\]

### Exercise 7.3.17
Find all integers \( x \) such that \( 25x - 4 \equiv 4x + 3 \mod 13 \).
</markdown><markdown>
## Orders and totients

For any modulus \( n \), there are only finitely many possible remainders modulo \( n \). A nice consequence of this finiteness is that, when \( a \perp n \), we can choose some power of \( a \) to be its multiplicative inverse, as proved in the following exercise.

### Exercise 7.3.18
Let \( n \) be a modulus and let \( a \in \mathbb{Z} \) with \( a \perp n \). Prove that there exists \( k \geq 1 \) such that \( a^k \equiv 1 \mod n \).

Exercise 7.3.18, together with the well-ordering principle, justify the following definition.

### Definition 7.3.19
Let \( n \) be a modulus and let \( a \in \mathbb{Z} \) with \( a \perp n \). The **order** of \( a \) modulo \( n \) is the least \( k \geq 1 \) such that \( a^k \equiv 1 \mod n \).

Note that this definition makes sense by Exercise 7.3.18 and the well-ordering principle.

### Example 7.3.20
The powers of 7 modulo 100 are:

- \( 7^1 = 7 \), so \( 7^1 \equiv 7 \mod 100 \);
- \( 7^2 = 49 \), so \( 7^2 \equiv 49 \mod 100 \);
- \( 7^3 = 343 \), so \( 7^3 \equiv 43 \mod 100 \);
- \( 7^4 = 2401 \), so \( 7^4 \equiv 1 \mod 100 \).

Hence the order of 7 modulo 100 is 4, and \( 7^3 \) and 43 are multiplicative inverses of 7 modulo 100.

Our focus turns to computing specific values of \( k \) such that \( a^k \equiv 1 \mod n \), whenever \( a \in \mathbb{Z} \) and \( a \perp n \). We first focus on the case when \( n \) is prime; then we develop the machinery of **totients** to study the case when \( n \) is not prime.

### Lemma 7.3.21
Let \( a, b \in \mathbb{Z} \) and let \( p \in \mathbb{Z} \) be a positive prime. Then \( (a+b)^p \equiv a^p + b^p \mod p \).

**Proof**

By the binomial theorem (Theorem 4.2.17), we have

\[
(a+b)^p = \sum_{k=0}^{p} \binom{p}{k} a^k b^{p-k}
\]
</markdown><markdown>
By Exercise 7.2.5, \( p \mid \binom{p}{k} \) for all \( 0 < k < p \), and hence \( \binom{p}{k} a^k b^{p-k} \equiv 0 \mod p \) for all \( 0 < k < p \). Thus

\[
(a+b)^p \equiv \binom{p}{0} a^p b^{p-0} + \binom{p}{p} a^p b^{p-p} \equiv a^p + b^p \mod p
\]

as desired.

### Theorem 7.3.22 (Fermat‚Äôs little theorem)
Let \( a, p \in \mathbb{Z} \) with \( p \) a positive prime. Then \( a^p \equiv a \mod p \).

**Proof**

We may assume that \( a \geq 0 \), otherwise replace \( a \) by its remainder modulo \( p \).

We will prove that \( a^p \equiv a \mod p \) by induction on \( a \).

- **(BC)** Since \( p > 0 \) we have \( 0^p = 0 \), hence \( 0^p \equiv 0 \mod p \).

- **(IS)** Fix \( a \geq 0 \) and suppose \( a^p \equiv a \mod p \). Then \( (a + 1)^p \equiv a^p + 1^p \mod p \) by Lemma 7.3.21. Now \( a^p \equiv a \mod p \) by the induction hypothesis, and \( 1^p = 1 \), so we have \( (a+1)^p \equiv a+1 \mod p \).

By induction, we‚Äôre done.

The following consequence of Theorem 7.3.22 is often also referred to as ‚ÄòFermat‚Äôs little theorem‚Äô, but is slightly less general since it requires an additional hypothesis. In keeping with the wider mathematical community, we will refer to both Theorem 7.3.22 and Corollary 7.3.23 as ‚ÄòFermat‚Äôs little theorem‚Äô.

### Corollary 7.3.23 (Fermat‚Äôs little theorem ‚Äî alternative version)
Let \( a, p \in \mathbb{Z} \) with \( p \) a positive prime and \( p \nmid a \). Then \( a^{p-1} \equiv 1 \mod p \).

**Proof**

Since \( p \nmid a \), it follows that \( a \perp p \). Theorem 7.3.22 tells us that \( a^p \equiv a \mod p \). By Proposition 7.3.14, \( a \) has a multiplicative inverse \( b \) modulo \( p \). Hence

\[
a^p b \equiv ab \mod p
\]

But \( a^p b \equiv a^{p-1} ab \mod p \), and \( ab \equiv 1 \mod p \), so we get

\[
a^{p-1} \equiv 1 \mod p
\]

as required.

Corollary 7.3.23 can be useful for computing remainders of humongous numbers when divided by smaller primes.
</markdown><markdown>
### Section 7.3. Modular arithmetic

#### Example 7.3.24
We compute the remainder of \(2^{1000}\) when divided by 7. Since \(7 \nmid 2\), it follows from Fermat‚Äôs little theorem ([Corollary 7.3.23](#)) that \(2^6 \equiv 1 \mod 7\). Now \(1000 = 166 \times 6 + 4\), so

\[
2^{1000} \equiv 2^{166 \times 6 + 4} \equiv (2^6)^{166} \cdot 2^4 \equiv 16 \equiv 2 \mod 7
\]

so the remainder of \(2^{1000}\) when divided by 7 is 2.

#### Exercise 7.3.25
Find the remainder of \(3^{244886}\) when divided by 13.

Unfortunately, the hypothesis that \(p\) is prime in Fermat‚Äôs little theorem cannot be disposed of. For example, 6 is not prime, and \(5^{6-1} = 5^5 = 3125 \equiv 520 \times 6 + 5\), so \(5^5 \equiv 5 \mod 6\). Our next order of business is to generalise [Corollary 7.3.23](#) by removing the requirement that the modulus \(p\) be prime, and replacing \(p - 1\) by the totient of the modulus.

#### Definition 7.3.26
Let \(n \in \mathbb{Z}\). The **totient** of \(n\) is the natural number \(\varphi(n)\) (LaTeX code: `\varphi(n)`) defined by

\[
\varphi(n) = |\{k \in [|n|] \mid k \perp n\}|
\]

That is, \(\varphi(n)\) is the number of integers from 1 up to \(|n|\) which are coprime to \(n\). The function \(\varphi : \mathbb{Z} \to \mathbb{N}\) is called **Euler‚Äôs totient function**.

#### Example 7.3.27
Here are some examples of totients:

- The elements of \([6]\) which are coprime to 6 are 1 and 5, so \(\varphi(6) = 2\).

- If \(p\) is a positive prime, then by [Corollary 7.2.20](#), every element of \([p]\) is coprime to \(p\) except for \(p\) itself. Hence if \(p\) is a positive prime then \(\varphi(p) = p - 1\). More generally, if \(p\) is prime then \(\varphi(p) = |p| - 1\).

#### Exercise 7.3.28
Let \(n \in \mathbb{Z}\) and let \(p > 0\) be prime. Prove that if \(p \mid n\), then \(\varphi(pn) = p \cdot \varphi(n)\). Deduce that \(\varphi(p^k) = p^k - p^{k-1}\) for all prime \(p > 0\) and all \(k \geq 1\).

#### Exercise 7.3.29
Let \(n \in \mathbb{Z}\) and let \(p > 0\) be prime. Prove that if \(p \nmid n\), then \(\varphi(pn) = (p - 1) \varphi(n)\).

Together, [Exercises 7.3.28](#) and [7.3.29](#) allow us to compute the totient of any integer with up to two primes in its prime factorisation.
</markdown><markdown>
### Example 7.3.30

We compute \(\varphi(100)\). The prime factorisation of 100 is \(2^2 \times 5^2\). Applying [Exercise 7.3.28](#) twice

\[
\varphi(2^2 \times 5^2) = 2 \times 5 \times \varphi(2 \times 5) = 10 \varphi(10)
\]

Finally, [Exercise 7.3.29](#) tells us that

\[
\varphi(10) = \varphi(2 \times 5) = 1 \times \varphi(5) = 1 \times 4 = 4
\]

Hence \(\varphi(100) = 40\).

### Exercise 7.3.31

Prove that \(\varphi(100) = 40\), this time using the inclusion‚Äìexclusion principle.

Euler‚Äôs theorem uses totients to generalise Fermat‚Äôs little theorem ([Theorem 7.3.22](#)) to arbitrary moduli, not just prime ones.

### Theorem 7.3.32 (Euler‚Äôs theorem)

Let \(n\) be a modulus and let \(a \in \mathbb{Z}\) with \(a \perp n\). Then

\[
a^{\varphi(n)} \equiv 1 \mod n
\]

**Proof**

By definition of totient, the set \(X\) defined by

\[
X = \{k \in [n] \mid k \perp n\}
\]

has \(\varphi(n)\) elements. List the elements as

\[
X = \{x_1, x_2, \ldots, x_{\varphi(n)}\}
\]

Note that \(ax_i \perp n\) for all \(i\), so let \(y_i\) be the (unique) element of \(X\) such that \(ax_i \equiv y_i \mod n\).

Note that if \(i \neq j\) then \(y_i \neq y_j\). We prove this by contraposition; indeed, since \(a \perp n\), by [Proposition 7.3.14](#), \(a\) has a multiplicative inverse, say \(b\). Then

\[
y_i \equiv y_j \mod n \Rightarrow ax_i \equiv ax_j \mod n \Rightarrow bx_i \equiv bx_j \mod n \Rightarrow x_i \equiv x_j \mod n
\]

and \(x_i \equiv x_j \mod n\) if and only if \(i = j\). Thus

\[
X = \{x_1, x_2, \ldots, x_{\varphi(n)}\} = \{y_1, y_2, \ldots, y_{\varphi(n)}\}
\]

This means that the product of the ‚Äò\(x_i\)‚Äôs is equal to the product of the ‚Äò\(y_i\)‚Äôs, and hence

\[
x_1 \cdots x_{\varphi(n)} \equiv y_1 \cdots y_{\varphi(n)} \mod n \quad \text{since } \{x_1, \ldots\} = \{y_1, \ldots\}
\]

\[
\equiv (ax_1) \cdots (ax_{\varphi(n)}) \mod n \quad \text{since } y_i \equiv ax_i \mod n
\]

\[
\equiv a^{\varphi(n)} \cdot x_1 \cdots x_{\varphi(n)} \mod n \quad \text{rearranging}
\]
</markdown><markdown>
Since each \( x_i \) is coprime to \( n \), we can cancel the \( x_i \) terms (by multiplying by their multiplicative inverses) to obtain

\[ a^{\varphi(n)} \equiv 1 \mod n \]

as required.

### Example 7.3.33

Some examples of Euler‚Äôs theorem in action are as follows:

- We have seen that \( \varphi(6) = 2 \), and we know that \( 5 \perp 6 \). And, indeed,

  \[
  5^{\varphi(6)} = 5^2 = 25 = 4 \times 6 + 1
  \]

  so \( 5^{\varphi(6)} \equiv 1 \mod 6 \).

- By [Exercise 7.3.28](#), we have

  \[
  \varphi(121) = \varphi(11^2) = 11^2 - 11^1 = 121 - 11 = 110
  \]

  Moreover, given \( a \in \mathbb{Z} \), \( a \perp 121 \) if and only if \( 11 \nmid a \) by [Corollary 7.2.20](#). Hence \( a^{110} \equiv 1 \mod 121 \) whenever \( 11 \nmid a \).

### Exercise 7.3.34

Use Euler‚Äôs theorem to prove that the last two digits of \( 3^{79} \) are ‚Äò67‚Äô.

### Example 7.3.35

Let \( n \) be a modulus and let \( a \in \mathbb{Z} \) with \( a \perp n \). Prove that the order of \( a \) modulo \( n \) divides \( \varphi(n) \).

A formula for the totient of an arbitrary nonzero integer is proved in [Theorem 7.3.59](#)‚Äîits proof is an application of the Chinese remainder theorem [Theorem 7.3.46](#), and uses the techniques for counting finite sets discussed in [Section 8.1](#).

## Wilson‚Äôs theorem

We conclude this chapter on number theory with **Wilson‚Äôs theorem**, which is a nice result that completely characterises prime numbers in the sense that we can tell when a number is prime by computing the remainder of \( (n - 1)! \) when divided by \( n \).

Let‚Äôs test a few numbers first:
</markdown><markdown>
| \( n \) | \( (n-1)! \) | remainder |
|---|---|---|
| 2 | 1 | 1 |
| 3 | 2 | 2 |
| 4 | 6 | 2 |
| 5 | 24 | 4 |
| 6 | 120 | 0 |
| 7 | 720 | 6 |
| 8 | 5040 | 0 |

| \( n \) | \( (n-1)! \) | remainder |
|---|---|---|
| 9 | 40320 | 0 |
| 10 | 362880 | 0 |
| 11 | 3628800 | 10 |
| 12 | 39916800 | 0 |
| 13 | 479001600 | 12 |
| 14 | 6227020800 | 0 |
| 15 | 87178291200 | 0 |

It's tempting to say that an integer \( n > 1 \) is prime if and only if \( n \mid (n-1)! \), but this isn‚Äôt true since it fails when \( n = 4 \). But it‚Äôs extremely close to being true.

### Theorem 7.3.36 (Wilson‚Äôs theorem)
Let \( n > 1 \) be a modulus. Then \( n \) is prime if and only if \( (n-1)! \equiv -1 \mod n \).

The following sequence of exercises will piece together into a proof of Wilson‚Äôs theorem.

**Exercise 7.3.37**  
Let \( n \in \mathbb{Z} \) be composite. Prove that if \( n > 4 \), then \( n \mid (n-1)! \).

**Exercise 7.3.38**  
Let \( p \) be a positive prime and let \( a \in \mathbb{Z} \). Prove that, if \( a^2 \equiv 1 \mod p \), then \( a \equiv 1 \mod p \) or \( a \equiv -1 \mod p \).

Exercise 7.3.38 implies that the only elements of \([p-1]\) that are their own multiplicative inverses are 1 and \( p-1 \); this morsel of information allows us to deduce result in the following exercise.

**Exercise 7.3.39**  
Let \( p \) be a positive prime. Prove that \( (p-1)! \equiv -1 \mod p \).

**Proof of Wilson‚Äôs theorem (Theorem 7.3.36)**  
Let \( n > 1 \) be a modulus.
</markdown><markdown>
- If \( n \) is prime, then \( (n-1)! \equiv -1 \mod n \) by Exercise 7.3.39.

- If \( n \) is composite, then either \( n = 4 \) or \( n > 4 \). If \( n = 4 \) then

  \[
  (n-1)! = 3! = 6 \equiv 2 \mod 4
  \]

  and so \( (n-1)! \not\equiv -1 \mod n \). If \( n > 4 \), then

  \[
  (n-1)! \equiv 0 \mod n
  \]

  by Exercise 7.3.37.

Hence \( (n-1)! \equiv -1 \mod n \) if and only if \( n \) is prime, as desired.

Since Wilson‚Äôs theorem completely characterises the positive prime numbers, we could have defined ‚Äò\( n \) is prime‚Äô, for \( n > 1 \), to mean that \( (n-1)! \equiv -1 \mod n \). We don‚Äôt do this because, although this is an interesting result, it is not particularly useful in applications. We might even hope that Wilson‚Äôs theorem gives us an easy way to test whether a number is prime, but unfortunately even this is a bust: computing the remainder \( (n-1)! \) on division by \( n \) is not particularly efficient.

However, there are some nice applications of Wilson‚Äôs theorem, which we will explore now.

‚ú¶ **Example 7.3.40**

We‚Äôll compute the remainder of \( 3^{45} \cdot 44! \) when divided by 47. Note that \( 3^{45} \cdot 44! \) is equal to a monstrous number with 76 digits; I don‚Äôt recommend doing the long division! Anyway‚Ä¶

- 47 is prime, so we can apply both Fermat‚Äôs little theorem (Theorem 7.3.22) and Wilson‚Äôs theorem (Theorem 7.3.36).

- By Fermat‚Äôs little theorem, we know that \( 3^{46} \equiv 1 \mod 47 \). Since \( 3 \cdot 16 = 48 \equiv 1 \mod 47 \), we have

  \[
  3^{45} \equiv 3^{45} \cdot (3 \cdot 16) \equiv 3^{46} \cdot 16 \equiv 16 \mod 47
  \]

- By Wilson‚Äôs theorem, we have \( 46! \equiv -1 \mod 47 \). Now
  - \( 46 \equiv -1 \mod 47 \), so 46 is its own multiplicative inverse modulo 47.
  - The extended Euclidean algorithm yields \( 45 \cdot 23 \equiv 1 \mod 47 \).

  So we have

  \[
  44! = 44! \cdot (45 \cdot 23) \cdot (46 \cdot 46) \equiv 46! \cdot 23 \cdot 46 \equiv (-1) \cdot 23 \cdot (-1) \equiv 23 \mod 47
  \]

Putting this information together yields

\[
3^{45} \cdot 44! \equiv 16 \cdot 23 = 368 \equiv 39 \mod 47
\]

So the remainder left when \( 3^{45} \cdot 44! \) is divided by 47 is 39.
</markdown><markdown>
### Exercise 7.3.41

Let \( p \) be an odd positive prime. Prove that

\[
\left[ \left( \frac{p-1}{2} \right)! \right]^2 \equiv (-1)^{\frac{p+1}{2}} \pmod{p}
\]

### Chinese remainder theorem

We introduce the Chinese remainder theorem with an example.

### Example 7.3.42

We find all integer solutions \( x \) to the system of congruences

\[
x \equiv 2 \pmod{5} \quad \text{and} \quad x \equiv 4 \pmod{8}
\]

Note that \( x \equiv 4 \pmod{8} \) if and only if \( x = 4 + 8k \) for some \( k \in \mathbb{Z} \). Now, for all \( k \in \mathbb{Z} \) we have

\[
\begin{align*}
x &\equiv 2 \pmod{5} \\
\Leftrightarrow 4 + 8k &\equiv 2 \pmod{5} \quad \text{since } x = 4 + 8k \\
\Leftrightarrow 8k &\equiv -2 \pmod{5} \quad \text{subtracting 4} \\
\Leftrightarrow 3k &\equiv 3 \pmod{5} \quad \text{since } 8 \equiv -2 \equiv 3 \pmod{5} \\
\Leftrightarrow k &\equiv 1 \pmod{5} \quad \text{multiplying by a multiplicative inverse for 3 modulo 5}
\end{align*}
\]

So \( 4 + 8k \equiv 2 \pmod{5} \) if and only if \( k = 1 + 5\ell \) for some \( \ell \in \mathbb{Z} \).

Combining this, we see that \( x \) satisfies both congruences if and only if

\[
x = 4 + 8(1 + 5\ell) = 12 + 40\ell
\]

for some \( \ell \in \mathbb{Z} \).

Hence the integers \( x \) for which both congruences are satisfied are precisely those integers \( x \) such that \( x \equiv 12 \pmod{40} \).

### Exercise 7.3.43

Find all integer solutions \( x \) to the system of congruences:

\[
\begin{cases}
x \equiv -1 \pmod{4} \\
x \equiv 1 \pmod{9} \\
x \equiv 5 \pmod{11}
\end{cases}
\]

Express your solution in the form \( x \equiv a \pmod{n} \) for suitable \( n > 0 \) and \( 0 \leq a < n \).
</markdown><markdown>
### Exercise 7.3.44

Let \( m, n \) be coprime moduli and let \( a, b \in \mathbb{Z} \). Let \( u, v \in \mathbb{Z} \) be such that

\[
mu \equiv 1 \mod n \quad \text{and} \quad nv \equiv 1 \mod m
\]

In terms of \( a, b, m, n, u, v \), find an integer \( x \) such that

\[
x \equiv a \mod m \quad \text{and} \quad x \equiv b \mod n
\]

### Exercise 7.3.45

Let \( m, n \) be coprime moduli and let \( x, y \in \mathbb{Z} \). Prove that if \( x \equiv y \mod m \) and \( x \equiv y \mod n \), then \( x \equiv y \mod mn \).

### Theorem 7.3.46 (Chinese remainder theorem)

Let \( m, n \) be moduli and let \( a, b \in \mathbb{Z} \). If \( m \) and \( n \) are coprime, then there exists an integer solution \( x \) to the simultaneous congruences

\[
x \equiv a \mod m \quad \text{and} \quad x \equiv b \mod n
\]

Moreover, if \( x, y \in \mathbb{Z} \) are two such solutions, then \( x \equiv y \mod mn \).

**Proof**

Existence of a solution \( x \) is precisely the content of Exercise 7.3.44.

Now let \( x, y \in \mathbb{Z} \) be two solutions to the two congruences. Then

\[
\begin{cases}
x \equiv a \mod m \\
y \equiv a \mod m \implies x \equiv y \mod m
\end{cases}
\]

\[
\begin{cases}
x \equiv b \mod n \\
y \equiv b \mod n \implies x \equiv y \mod n
\end{cases}
\]

so by Exercise 7.3.45, we have \( x \equiv y \mod mn \), as required.

We now generalise the Chinese remainder theorem to the case when the moduli \( m, n \) are not assumed to be coprime. There are two ways we could make this generalisation: either we could reduce the more general version of the theorem to the version we proved in Theorem 7.3.46, or we could prove the more general version from scratch. We opt for the latter approach, but you might want to consider what a ‚Äòreductive‚Äô proof would look like.
</markdown><markdown>
### Theorem 7.3.47

Let \( m, n \) be moduli and let \( a, b \in \mathbb{Z} \). There exists an integer solution \( x \) to the system of congruences

\[
x \equiv a \pmod{m} \quad \text{and} \quad x \equiv b \pmod{n}
\]

if and only if \( a \equiv b \pmod{\gcd(m, n)} \).

Moreover, if \( x, y \in \mathbb{Z} \) are two such solutions, then \( x \equiv y \pmod{\mathrm{lcm}(m, n)} \).

**Proof**

Let \( d = \gcd(m, n) \), and write \( m = m'd \) and \( n = n'd \) for some \( m', n' \in \mathbb{Z} \).

We prove that an integer solution \( x \) to the system of congruences exists if and only if \( a \equiv b \pmod{d} \).

- (\(\Rightarrow\)) Suppose an integer solution \( x \) to the system of congruences exists. Then there exist integers \( k, \ell \) such that

  \[
  x = a + mk = b + n\ell
  \]

  But \( m = m'd \) and \( n = n'd \), so we have \( a + m'dk = b + n'd\ell \), and so

  \[
  a - b = (n'\ell - m'k)d
  \]

  so that \( a \equiv b \pmod{d} \), as required.

- (\(\Leftarrow\)) Suppose \( a \equiv b \pmod{d} \), and let \( t \in \mathbb{Z} \) be such that \( a - b = td \). Let \( u, v \in \mathbb{Z} \) be such that \( mu + nv = d \)‚Äîthese exist by B√©zout‚Äôs lemma (Theorem 7.1.23). Note also that, since \( m = m'd \) and \( n = n'd \), dividing through by \( d \) yields \( m'u + n'v = 1 \).

  Define

  \[
  x = an'v + bm'u
  \]

  Now we have

  \[
  \begin{align*}
  x &= an'v + bm'u & \text{by definition of } x \\
  &= an'v + (a - td)m'u & \text{since } a - b = td \\
  &= a(m'u + n'v) - tdm'u & \text{rearranging} \\
  &= a - tdm'u & \text{since } m'u + n'v = 1 \\
  &= a - tum & \text{since } m = m'd
  \end{align*}
  \]

  so \( x \equiv a \pmod{m} \). Likewise

  \[
  \begin{align*}
  x &= an'v + bm'u & \text{by definition of } x \\
  &= (b + td)n'v + bm'u & \text{since } a - b = td \\
  &= b(m'u + n'v) + tdn'v & \text{rearranging} \\
  &= b + tdn'v & \text{since } m'u + n'v = 1 \\
  &= b + tvn & \text{since } n = n'd
  \end{align*}
  \]
</markdown><markdown>
so \( x \equiv b \mod n \).

Hence \( x = an'v + bm'u \) is a solution to the system of congruences.

We now prove that if \( x, y \) are two integer solutions to the system of congruences, then they are congruent modulo \(\text{lcm}(a, b)\). First note that we must have

\[ 
x \equiv y \mod m \quad \text{and} \quad x \equiv y \mod n 
\]

so that \( x = y + km \) and \( x = y + \ell n \) for some \( k, \ell \in \mathbb{Z} \). But then

\[ 
x - y = km = \ell n 
\]

Writing \( m = m'd \) and \( n = n'd \), we see that \( km'd = \ell n'd \), so that \( km' = \ell n' \). But \( m', n' \) are coprime by Exercise 7.1.30, and hence \( m' \mid \ell \) by Proposition 7.1.32. Write \( \ell = \ell' m' \) for some \( \ell' \in \mathbb{Z} \). Then we have

\[ 
x - y = \ell n = \ell' m'n 
\]

and hence \( x \equiv y \mod m'n \). But \( m'n = \text{lcm}(m, n) \) by Exercise 7.1.41.

This theorem is in fact constructive, in that it provides an algorithm for finding all integer solutions \( x \) to a system of congruences

\[ 
x \equiv a \mod m \quad \text{and} \quad x \equiv b \mod n 
\]

as follows:

- Use the Euclidean algorithm to compute \( d = \gcd(m, n) \).

- If \( d \nmid a - b \) then there are no solutions, so stop. If \( d \mid a - b \), then proceed to the next step.

- Use the extended Euclidean algorithm to compute \( u, v \in \mathbb{Z} \) such that \( mu + nv = d \).

- The integer solutions \( x \) to the system of congruences are precisely those of the form

\[
x = \frac{anv + bmu + kmn}{d} \quad \text{for some } k \in \mathbb{Z}
\]

**Exercise 7.3.48**  
Verify that the algorithm outlined above is correct. Use it to compute the solutions to the system of congruences

\[ 
x \equiv 3 \mod 12 \quad \text{and} \quad x \equiv 15 \mod 20 
\]
</markdown><markdown>
### ‚òÖ Exercise 7.3.49

Generalise the Chinese remainder theorem to systems of arbitrarily (finitely) many congruences. That is, given \( r \in \mathbb{N} \), find precisely the conditions on moduli \( n_1, n_2, \ldots, n_r \) and integers \( a_1, a_2, \ldots, a_r \) such that an integer solution exists to the congruences

\[
x \equiv a_1 \mod n_1, \quad x \equiv a_2 \mod n_2, \quad \ldots \quad x \equiv a_r \mod n_r
\]

Find an explicit formula for such a value of \( x \), and find a suitable modulus \( n \) in terms of \( n_1, n_2, \ldots, n_r \) such that any two solutions to the system of congruences are congruent modulo \( n \).

### ‚òÖ Exercise 7.3.50

Prove that gaps between consecutive primes can be made arbitrarily large. That is, prove that for all \( n \in \mathbb{N} \), there exists an integer \( a \) such that the numbers

\[
a, \quad a+1, \quad a+2, \quad \ldots, \quad a+n
\]

are all composite.

### Application: tests for divisibility

The language of modular arithmetic provides a practical setting for proving tests for divisibility using number bases. Number bases were introduced in Chapter 0, and we gave a preliminary definition in Definition 0.6 of what a number base is. Our first job will be to justify why this definition makes sense at all‚Äîthat is, we need to prove that every natural number has a base-\( b \) expansion, and moreover, that it only has one of them. Theorem 7.3.51 says exactly this.

### Theorem 7.3.51

Let \( n \in \mathbb{N} \) and let \( b \in \mathbb{N} \) with \( b \geq 2 \). Then there exist unique \( r \in \mathbb{N} \) and \( d_0, d_1, \ldots, d_r \in \{0, 1, \ldots, b-1\} \) such that

\[
n = \sum_{i=0}^{r} d_i b^i
\]

and such that \( d_r \neq 0 \), except \( n = 0 \), in which case \( r = 0 \) and \( d_0 = 0 \).

**Proof**

We proceed by strong induction on \( n \).

- (BC) We imposed the requirement that if \( n = 0 \) then \( r = 0 \) and \( d_0 = 0 \); and this evidently satisfies the requirement that \( n = \sum_{i=0}^{r} d_i b^i \).
</markdown><markdown>
- **(IS)** Fix \( n \geq 0 \) and suppose that the requirements of the theorem are satisfied for all the natural numbers up to and including \( n \).

By the division theorem ([Theorem 7.1.1](#)), there exist unique \( u, v \in \mathbb{N} \) such that

\[
n + 1 = ub + v \quad \text{and} \quad v \in \{0, 1, \ldots, b - 1\}
\]

Since \( b \geq 2 \), we have \( u < n + 1 \), and so \( u \leq n \). It follows from the induction hypothesis that there exist unique \( r \in \mathbb{N} \) and \( d_1, \ldots, d_r \in \{0, 1, \ldots, b - 1\} \) such that

\[
u = \sum_{i=0}^{r} d_{i+1} b^i
\]

and \( d_r \neq 0 \). Writing \( d_0 = v \) yields

\[
n = ub + v = \sum_{i=0}^{r} d_{i+1} b^{i+1} + d_0 = \sum_{i=0}^{r} d_i b^i
\]

Since \( d_r \neq 0 \), this proves existence.

For uniqueness, suppose that there exists \( s \in \mathbb{N} \) and \( e_0, \ldots, e_s \in \{0, 1, \ldots, b - 1\} \) such that

\[
n + 1 = \sum_{j=0}^{s} e_j b^j
\]

and \( e_s \neq 0 \). Then

\[
n + 1 = \left( \sum_{j=1}^{s} e_j b^{j-1} \right) b + e_0
\]

so by the division theorem we have \( e_0 = d_0 = v \). Hence

\[
u = \frac{n + 1 - v}{b} = \sum_{j=1}^{s} e_j b^{j-1} = \sum_{i=1}^{r} d_i b^{i-1}
\]

so by the induction hypothesis, it follows that \( r = s \) and \( d_i = e_i \) for all \( 1 \leq i \leq r \). This proves uniqueness.

By induction, we‚Äôre done.

We now re-state the definition of base-\( b \) expansion, confident in the knowledge that this definition makes sense.

‚ú¶ **Definition 7.3.52**

Let \( n \in \mathbb{N} \). The **base-\( b \) expansion** of \( n \) is the unique string \( d_r d_{r-1} \ldots d_0 \) such that the conditions in [Theorem 7.3.51](#) are satisfied. The base-2 expansion is also known as the **binary expansion**, and the base-10 expansion is called the **decimal expansion**.
</markdown><markdown>
### Example 7.3.53

Let \( n \in \mathbb{N} \). Then \( n \) is divisible by 3 if and only if the sum of the digits in the decimal expansion of \( n \) is divisible by 3. Likewise, \( n \) is divisible by 9 if and only if the sum of the digits in the decimal expansion \( n \) is divisible by 9.

We prove this for divisibility by 3. Let

\[
n = d_r d_{r-1} \cdots d_1 d_0
\]

be the decimal expansion of \( n \), and let \( s = \sum_{i=0}^{r} d_i \) be the sum of the digits of \( n \).

Then we have

\[
n \equiv \sum_{i=0}^{r} d_i 10^i \pmod{3} \quad \text{since } n = \sum_i d_i 10^i
\]

\[
\equiv \sum_{i=0}^{r} d_i 1^i \pmod{3} \quad \text{since } 10 \equiv 1 \pmod{3}
\]

\[
\equiv \sum_{i=0}^{r} d_i \quad \text{since } 1^i = 1 \text{ for all } i
\]

\[
\equiv s \quad \text{by definition of } s
\]

Since \( n \equiv s \pmod{3} \), it follows that \( n \) is divisible by 3 if and only if \( s \) is divisible by 3.

### Exercise 7.3.54

Let \( n \in \mathbb{N} \). Prove that \( n \) is divisible by 5 if and only if the final digit in the decimal expansion of \( n \) is 5 or 0.

More generally, fix \( k \geq 1 \) and let \( m \) be the number whose decimal expansion is given by the last \( k \) digits of that of \( n \). Prove that \( n \) is divisible by \( 5^k \) if and only if \( m \) is divisible by \( 5^k \). For example, we have

\[
125 \mid 9 \, 550 \, 828 \, 230 \, 495 \, 875 \quad \Leftrightarrow \quad 125 \mid 875
\]

### Exercise 7.3.55

Let \( n \in \mathbb{N} \). Prove that \( n \) is divisible by 11 if and only if the alternating sum of the digits of \( n \) is divisible by 11. That is, prove that if the decimal expansion of \( n \) is \( d_r d_{r-2} \cdots d_0 \), then

\[
11 \mid n \quad \Leftrightarrow \quad 11 \mid d_0 - d_1 + d_2 - \cdots + (-1)^r d_r
\]

### Exercise 7.3.56

Let \( n \in \mathbb{N} \). Find a method for testing if \( n \) is divisible by 7 based on the decimal expansion of \( n \).
</markdown><markdown>
## Application: public-key cryptography

Public-key cryptography is a method of encryption and decryption that works according to the following principles:

- Encryption is done using a **public key** that is available to anyone.
- Decryption is done using a **private key** that is only known to the recipient.
- Knowledge of the private key should be extremely difficult to derive from knowledge of the public key.

Specifically, suppose that Alice wants to securely send Bob a message. As the recipient of the message, Bob has a public key and a private key. So:

- Bob sends the **public key** to Alice.
- Alice uses the public key to encrypt the message.
- Alice sends the encrypted message, which is visible (but encrypted) to anyone who intercepts it.
- Bob keeps the private key secret, and uses it upon receipt of the message to decrypt the message.

Notice that, since the public key can only be used to **encrypt** messages, a hacker has no useful information upon intercepting the message or the public key.

**RSA encryption** is an algorithm which provides one means of doing public-key cryptography using the theory of modular arithmetic. It works as follows.

**Step 1.** Let \( p \) and \( q \) be distinct positive prime numbers, and let \( n = pq \). Then \( \varphi(n) = (p-1)(q-1) \).

**Step 2.** Choose \( e \in \mathbb{Z} \) such that \( 1 < e < \varphi(n) \) and \( e \perp \varphi(n) \). The pair \( (n, e) \) is called the **public key**.

**Step 3.** Choose \( d \in \mathbb{Z} \) such that \( de \equiv 1 \mod \varphi(n) \). The pair \( (n, d) \) is called the **private key**.

**Step 4.** To encrypt a message \( M \) (which is encoded as an integer), compute \( K \in [n] \) such that \( K \equiv M^e \mod n \). Then \( K \) is the encrypted message.

**Step 5.** The original message \( M \) can be recovered since \( M \equiv K^d \mod n \).

Computing the private key \( (n, d) \) from the knowledge of \( (n, e) \) would allow a hacker to decrypt an encrypted message. However, doing so is typically very difficult when the
</markdown><markdown>
prime factors of \( n \) are large. So if we choose \( p \) and \( q \) to be very large primes‚Äîwhich we can do without much hassle at all‚Äîthen it becomes computationally infeasible for a hacker to compute the private key.

**Example.** Suppose I want to encrypt the message \( M \), which I have encoded as the integer 32. Let \( p = 13 \) and \( q = 17 \). Then \( n = 221 \) and \( \varphi(n) = 192 \). Let \( e = 7 \), and note that \( 7 \perp 192 \). Now \( 7 \times 55 \equiv 1 \mod 192 \), so we can define \( d = 55 \).

- The public key is \( (221, 7) \), which Bob sends to Alice. Now Alice can encrypt the message:
  \[
  32^7 \equiv 59 \mod 221
  \]
  Alice then sends Bob the encrypted message 59.

- The private key is \( (221, 55) \), so Bob can decrypt the message:
  \[
  59^{55} \equiv 32 \mod 221
  \]
  so Bob has received Alice‚Äôs message 32.

**Exercise 7.3.57**  
Prove that the RSA algorithm is correct. Specifically, prove:

(a) If \( n = pq \), for distinct positive primes \( p \) and \( q \), then \( \varphi(n) = (p-1)(q-1) \);

(b) Given \( 1 < e < \varphi(n) \) with \( e \perp \varphi(n) \), there exists \( d \in \mathbb{Z} \) with \( de \equiv 1 \mod \varphi(n) \).

(c) Given \( M, K \in \mathbb{Z} \) with \( K \equiv M^e \mod n \), it is indeed the case that \( K^d \equiv M \mod n \).

**Application: Euler‚Äôs totient function**

We now derive a formula for computing the totient of an arbitrary integer using the tools from Section 8.1‚Äîin particular, if you chose to read this section before learning about the multiplication principle, you should skip over this material.

**Theorem 7.3.58 (Multiplicativity of Euler‚Äôs totient function)**  
Let \( m, n \in \mathbb{Z} \) and let \( \varphi : \mathbb{Z} \to \mathbb{N} \) be Euler‚Äôs totient function (see Definition 7.3.26). If \( m \) and \( n \) are coprime, then \( \varphi(mn) = \varphi(m)\varphi(n) \).

**Proof**  
Since \( \varphi(-k) = \varphi(k) \) for all \( k \in \mathbb{Z} \), we may assume that \( m \geq 0 \) and \( n \geq 0 \). Moreover, if \( m = 0 \) or \( n = 0 \), then \( \varphi(m)\varphi(n) = 0 \) and \( \varphi(mn) = 0 \), so the result is immediate. Hence we may assume that \( m > 0 \) and \( n > 0 \).
</markdown><markdown>
Given \( k \in \mathbb{Z} \), define

\[
C_k = \{ a \in [k] \mid a \perp k \}
\]

By definition of Euler‚Äôs totient function, we thus have \(|C_k| = \varphi(k)\) for all \( k \in \mathbb{Z} \). We will define a bijection

\[
f : C_m \times C_n \to C_{mn}
\]

using the Chinese remainder theorem (Theorem 7.3.46).

Given \( a \in C_m \) and \( b \in C_n \), let \( f(a,b) \) be the element \( x \in [mn] \) such that

\[
\begin{cases} 
x \equiv a \pmod{m} \\
x \equiv b \pmod{n}
\end{cases}
\]

- **\( f \) is well-defined.** We check the properties of totality, existence and uniqueness.
  - **Totality.** We have accounted for all the elements of \( C_m \times C_n \) in our specification of \( f \).
  - **Existence.** By the Chinese remainder theorem, there exists \( x \in \mathbb{Z} \) such that \( x \equiv a \pmod{m} \) and \( x \equiv b \pmod{n} \). By adding an appropriate integer multiple of \( mn \) to \( x \), we may additionally require \( x \in [mn] \). It remains to check that \( x \perp mn \).

    So let \( d = \gcd(x,mn) \). If \( d > 1 \), then there is a positive prime \( p \) such that \( p \mid x \) and \( p \mid mn \). But then \( p \mid m \) or \( p \mid n \), meaning that either \( p \mid \gcd(x,m) \) or \( p \mid \gcd(x,n) \). But \( x \equiv a \pmod{m} \), so \( \gcd(x,m) = \gcd(a,m) \); and likewise \( \gcd(x,n) = \gcd(b,n) \). So this contradicts the assumption that \( a \perp m \) and \( b \perp n \). Hence \( x \perp mn \) after all.
  - **Uniqueness.** Suppose \( x, y \in C_{mn} \) both satisfy the two congruences in question. By the Chinese remainder theorem, we have \( x \equiv y \pmod{mn} \), and hence \( x = y + kmn \) for some \( k \in \mathbb{Z} \). Since \( x, y \in [mn] \), we have

    \[
    |k|mn = |kmn| = |x - y| \leq mn - 1 < mn
    \]

    This implies \(|k| < 1\), so that \( k = 0 \) and \( x = y \).

  so \( f \) is well-defined.

- **\( f \) is injective.** Let \( a, a' \in C_m \) and \( b, b' \in C_n \), and suppose that \( f(a,b) = f(a',b') \). Then there is an element \( x \in C_{mn} \) such that

  \[
  \begin{cases} 
  x \equiv a \pmod{m} \\
  x \equiv a' \pmod{m} \\
  x \equiv b \pmod{n} \\
  x \equiv b' \pmod{n}
  \end{cases}
  \]

  Hence \( a \equiv a' \pmod{m} \) and \( b \equiv b' \pmod{n} \). Since \( a, a' \in [m] \) and \( b, b' \in [n] \), we must have \( a = a' \) and \( b = b' \).
</markdown><markdown>
- *f* is surjective. Let \( x \in C_{mn} \). Let \( a \in [m] \) and \( b \in [n] \) be the (unique) elements such that \( x \equiv a \mod m \) and \( x \equiv b \mod n \), respectively. If \( a \in C_m \) and \( b \in C_n \), then we‚Äôll have \( f(a, b) = x \) by construction, so it remains to check that \( a \perp m \) and \( b \perp n \).

Suppose \( d \in \mathbb{Z} \) with \( d \mid a \) and \( d \mid m \). We prove that \( d = 1 \). Since \( x \equiv a \mod m \), we have \( d \mid x \) by Theorem 7.1.17. Since \( m \mid mn \), we have \( d \mid mn \). By definition of greatest common divisors, it follows that \( d \mid \gcd(x, mn) \). But \( \gcd(x, mn) = 1 \), so that \( d \) is a unit, and so \( a \perp m \) as required.

The proof that \( b \perp n \) is similar.

It was a lot of work to check that it worked, but we have defined a bijection \( f : C_m \times C_n \to C_{mn} \). By the multiplication principle, we have

\[
\varphi(m) \varphi(n) = |C_m| \cdot |C_n| = |C_m \times C_n| = |C_{mn}| = \varphi(mn)
\]

as required.

It turns out that [Theorem 7.3.58](#) and [Exercise 7.3.28](#) are precisely the ingredients we need to find a general formula for the totient of a nonzero integer.

### Theorem 7.3.59 (Formula for Euler‚Äôs totient function)

Let \( n \) be a nonzero integer. Then

\[
\varphi(n) = |n| \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right)
\]

where the product is indexed over positive primes \( p \) dividing \( n \).

**Proof**

Since \( \varphi(n) = \varphi(-n) \) for all \( n \in \mathbb{Z} \), we may assume that \( n > 0 \). Moreover

\[
\varphi(1) = 1 = 1 \cdot \prod_{p \mid 1} \left( 1 - \frac{1}{p} \right)
\]

Note that the product here is empty, and hence equal to 1, since there are no positive primes \( p \) which divide 1. So now suppose \( n > 1 \).

Using the fundamental theorem of arithmetic ([Theorem 7.2.12](#)), we can write

\[
n = p_1^{k_1} p_2^{k_2} \cdots p_r^{k_r}
\]

for primes \( 0 < p_1 < p_2 < \cdots < p_r \) and natural numbers \( k_1, k_2, \ldots, k_r \geq 1 \).

By repeated application of [Theorem 7.3.58](#), we have

\[
\varphi(n) = \prod_{i=1}^{r} \varphi(p_i^{k_i})
\]
</markdown><markdown>
By Exercise 7.3.28, we have

\[
\varphi(p_i^{k_i}) = p_i^{k_i} - p_i^{k_i-1} = p_i^{k_i} \left(1 - \frac{1}{p_i}\right)
\]

Combining these two results, it follows that

\[
\varphi(n) = \prod_{i=1}^{r} p_i^{k_i} \left(1 - \frac{1}{p_i}\right) = \left(\prod_{i=1}^{r} p_i^{k_i}\right) \left(\prod_{i=1}^{r} \left(1 - \frac{1}{p_i}\right)\right) = n \cdot \prod_{i=1}^{r} \left(1 - \frac{1}{p_i}\right)
\]

which is as required.
</markdown><markdown>
## Section 7.E
# Chapter 7 exercises

### Greatest common divisors

In Questions 7.1 to 7.3, use the Euclidean algorithm (Strategy 7.1.18) to find the greatest common divisor of the given pair of integers.

7.1. 382 and 218

7.2. 368475 and 26010

7.3. 24004512 and 10668672

7.4. Let \( a, b, c, d \in \mathbb{Z} \) and suppose that \( ad - bc = 1 \). Prove that \( a + b \) and \( c + d \) are coprime.

‚ú¶ **Definition 7.E.1**  
Let \( f(x) \) and \( g(x) \) be polynomials over \( \mathbb{Z} \). We say \( f(x) \) *divides* \( g(x) \) if there exists a polynomial \( q(x) \) over \( \mathbb{Z} \) such that \( g(x) = q(x)f(x) \).

7.5. Prove that \( 1 - x \) divides \( 1 - x^2 \) and that \( 1 - x \) does not divide \( 1 + x^2 \).

7.6. Prove that \( 1 + x + x^2 \) divides \( 1 + x^4 + x^5 \).

‚ú¶ **Definition 7.E.2**  
Let \( f(x) \) be a polynomial over \( \mathbb{Z} \). The *degree* of \( f \), written \( \deg(f(x)) \), is the greatest power of \( x \) in \( f \) whose coefficient is nonzero, unless \( f \) is the zero polynomial, whose degree is defined to be \(-\infty\). We adopt the convention that \((-\infty) + r = r + (-\infty)\) for all \( r \in \mathbb{N} \cup \{-\infty\} \).

7.7. Let \( f(x) \) and \( g(x) \) be polynomials over \( \mathbb{Z} \). Prove that \( \deg(f(x)g(x)) = \deg(f(x)) + \deg(g(x)) \).

7.8. Prove the following modified form of the division theorem (Theorem 7.1.1) for polynomials: given polynomials \( f(x) \) and \( g(x) \) over \( \mathbb{Z} \), with \( f(x) \neq 0 \), prove that there exist unique polynomials \( q(x) \) and \( r(x) \) over \( \mathbb{Z} \) such that \( g(x) = q(x)f(x) + r(x) \) and \( \deg(r(x)) < \deg(f(x)) \).
</markdown><markdown>
## Prime numbers

7.9. Prove that, for all \( k \in \mathbb{N} \), the function \( i : \mathbb{N}^k \to \mathbb{N} \) defined by

\[
i(n_1, n_2, \ldots, n_k) = p_1^{n_1} p_2^{n_2} \cdots p_k^{n_k}
\]

for all \( (n_1, n_2, \ldots, n_k) \in \mathbb{N}^k \) is an injection, where \( p_1, p_2, p_3, \ldots \) is an enumeration of the set of positive primes in increasing order. (Thus \( p_1 = 2, p_2 = 3, p_3 = 5 \), and so on.)

7.10. Use the result of Question 7.9 to construct a bijection

\[
\bigcup_{k \in \mathbb{N}} \left( \mathbb{N}^k \setminus \{(0, 0, \ldots, 0)\} \right) \to \{n \in \mathbb{N} \mid n \geq 2\}
\]

7.11. Use a method akin to that of Question 7.9 to define an injection \( \mathbb{Z}^k \to \mathbb{N} \).

7.12. Define a subset \( A \subseteq \mathbb{Z} \) by

\[
A = \{n \in \mathbb{N} \mid \exists k > 0, n \mid 12^k - 1\}
\]

Find all prime numbers in \( \mathbb{Z} \setminus A \).

## Base-\( b \) expansions

7.13. Let \( n \in \mathbb{N} \). Prove that the number of trailing 0s in the decimal expansion of \( n! \) is equal to

\[
\sum_{k=1}^{d} \left\lfloor \frac{n}{5^k} \right\rfloor
\]

where \( d \in \mathbb{N} \) is least such that \( 5^{d+1} > n \), and where \( \lfloor x \rfloor \) (LaTeX code: \texttt{\textbackslash lfloor, \textbackslash rfloor}) denotes the greatest integer less than or equal to \( x \in \mathbb{R} \) (called the floor of \( x \)).

7.14. Let \( b \in \mathbb{N} \) with \( b \geq 2 \). Find an expression in terms of \( n \in \mathbb{N} \) for the number of trailing 0s in the base-\( b \) expansion of \( n! \).

## True‚ÄìFalse questions

In Questions 7.15 to 7.20, determine (with proof) whether the statement is true or false.

7.15. There is an integer that is not coprime to any integer.

7.16. Every linear Diophantine equation has a solution.
</markdown><markdown>
7.17. Every integer \( n \) is coprime to its successor \( n + 1 \).

7.18. If the greatest common divisors of two integers is 3, and their least common multiple of 7, then their product is 21.

7.19. Every integer is congruent modulo 7 to its remainder when divided by 21.

7.20. Let \( k \geq 1 \). Then \( 15^k \equiv 1 \mod 6 \).

## Always‚ÄìSometimes‚ÄìNever questions

In Questions 7.21 to 7.29, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

7.21. Let \( a, b, c \in \mathbb{Z} \) and suppose that \( \gcd(a, b) = 2 \) and \( \gcd(b, c) = 2 \). Then \( \gcd(a, c) = 2 \).

7.22. Let \( a, b \in \mathbb{Z} \). Then \( \gcd(a, b) = \gcd(a, \gcd(a, b)) \).

7.23. Let \( a, b \geq 2 \). Then \( \gcd(a, b) = \text{lcm}(a, b) \).

7.24. Let \( n \in \mathbb{Z} \). Then \( n^2 + 1 \) is coprime to \( n^4 + n^2 + 1 \).

7.25. Let \( p \in \mathbb{Z} \) be prime. Then \( p \) is coprime to all integers \( a \neq p \).

7.26. Let \( p, q, r, s \in \mathbb{Z} \) be positive primes and suppose that \( pq = rs \). Then \( p = r \) and \( q = s \).

7.27. Let \( p_1, p_2, \ldots, p_s, q_1, q_2, \ldots, q_t \in \mathbb{Z} \) be prime and suppose that \( p_1 p_2 \ldots p_s = q_1 q_2 \ldots q_t \). Then \( s = t \).

7.28. Let \( a, b \in \mathbb{Z} \) and let \( p \) be a positive prime. Then \( p \mid (a + b)^p - a - b \).

7.29. Let \( n \geq 0 \) and let \( b > 0 \). Then \( n \) is congruent modulo \( b - 1 \) to the sum of the digits in its base-\( b \) expansion.
</markdown><markdown>
# Chapter 8

## Enumerative combinatorics
</markdown><markdown>
## Section 8.1

# Counting principles

In [Section 6.1](#) we were interested in establishing conditions under which a set is finite, and proving that we may perform certain operations on finite sets‚Äîsuch as unions and cartesian products‚Äîwithout losing the property of finiteness.

In this section, our attention turns to the task of finding the size of a set that is known to be finite. This process is called *counting* and is at the core of the mathematical field of combinatorics.

### Binomials and factorials revisited

We defined binomial coefficients \(\binom{n}{k}\) and factorials \(n!\) recursively in [Chapter 4](#), and proved elementary facts about them by induction. We will now re-define them combinatorially‚Äîthat is, we give them meaning in terms of sizes of particular finite sets. We will prove that the combinatorial and recursive definitions are equivalent, and prove facts about them using combinatorial arguments.

The reasons for doing so are manifold. The combinatorial definitions allow us to reason about binomials and factorials with direct reference to descriptions of finite sets, which will be particularly useful when we prove identities about them using *double counting*. Moreover, the combinatorial definitions remove the seemingly arbitrary nature of the recursive definitions‚Äîfor example, they provide a reason why it makes sense to define \(0! = 1\) and \(\binom{0}{0} = 1\).

‚ú¶ **Definition 8.1.1**  
Let \(X\) be a set and let \(k \in \mathbb{N}\). A *k-element subset* of \(X\) is a subset \(U \subseteq X\) such that \(|U| = k\). The set of all k-element subsets of \(X\) is denoted \(\binom{X}{k}\) (read: ‚ÄòX choose k‚Äô) ([LaTeX code: \texttt{\textbackslash binom\{X\}\{k\}}]).

Intuitively, \(\binom{X}{k}\) is the set of ways of picking \(k\) elements from \(X\), without repetitions, in such a way that order doesn‚Äôt matter. (If order mattered, the elements would be *sequences* instead of subsets.)

‚úê **Example 8.1.2**  
We find \(\binom{[4]}{k}\) for all \(k \in \mathbb{N}\).

- \(\binom{[4]}{0} = \{\emptyset\}\) since the only set with 0 elements is \(\emptyset\);
</markdown><markdown>
## Section 8.1. Counting principles

- \( \binom{[4]}{1} = \{ \{1\}, \{2\}, \{3\}, \{4\} \} \);
- \( \binom{[4]}{2} = \{ \{1,2\}, \{1,3\}, \{1,4\}, \{2,3\}, \{2,4\}, \{3,4\} \} \);
- \( \binom{[4]}{3} = \{ \{1,2,3\}, \{1,2,4\}, \{1,3,4\}, \{2,3,4\} \} \);
- \( \binom{[4]}{4} = \{ \{1,2,3,4\} \} \);

- If \( k > 5 \) then \( \binom{[4]}{k} = \emptyset \), since by Exercise 6.1.17, no subset of \([4]\) can have more than 4 elements.

### Proposition 8.1.3

If \( X \) is a finite set, then \( \mathcal{P}(X) = \bigcup_{k \leq |X|} \binom{X}{k} \).

**Proof**

Let \( U \subseteq X \). By Exercise 6.1.17, \( U \) is finite and \(|U| \leq |X|\). Thus \( U \in \binom{X}{|U|} \), and hence

\[
U \in \bigcup_{k \leq |X|} \binom{X}{k}.
\]

This proves that \( \mathcal{P}(X) \subseteq \bigcup_{k \leq |X|} \binom{X}{k} \).

The fact that \( \bigcup_{k \leq |X|} \binom{X}{k} \subseteq \mathcal{P}(X) \) is immediate, since elements of \( \binom{X}{k} \) are defined to be subsets of \( X \), and hence elements of \( \mathcal{P}(X) \).

### Definition 8.1.4

Let \( n, k \in \mathbb{N} \). Denote by \( \binom{n}{k} \) (read: ‚Äòn choose k‚Äô) the number of \( k \)-element subsets of \([n]\). That is, we define \( \binom{n}{k} = \left| \binom{[n]}{k} \right| \). The numbers \( \binom{n}{k} \) are called binomial coefficients.

Some authors use the notation \( nC_k \) or "C_k" instead of \( \binom{n}{k} \). We avoid this, as it is unnecessarily clunky.

Intuitively, \( \binom{n}{k} \) is the number of ways of selecting \( k \) things from \( n \), without repetitions, in such a way that order doesn‚Äôt matter.

The value behind this notation is that it allows us to express huge numbers in a concise and meaningful way. For example,

\[
\binom{4000}{11} = 103\,640\,000\,280\,154\,258\,645\,590\,429\,564\,000
\]
</markdown><markdown>
Although these two numbers are equal, their *expressions* are very different; the expression on the left is meaningful, but the expression on the right is completely meaningless out of context.

### ‚ú§ Writing tip
When expressing the sizes of finite sets described combinatorially, it is best to *avoid* evaluating the expression; that is, leave in the powers, products, sums, binomial coefficients and factorials! The reason for this is that performing the calculations takes the meaning away from the expressions.

### üñπ Example 8.1.5
In Example 8.1.2 we proved that:

\[
\binom{4}{0} = 1, \quad \binom{4}{1} = 4, \quad \binom{4}{2} = 6, \quad \binom{4}{3} = 4, \quad \binom{4}{4} = 1
\]

and that \(\binom{4}{k} = 0\) for all \(k > 5\).

### üñπ Exercise 8.1.6
Fix \(n \in \mathbb{N}\). Prove that \(\binom{n}{0} = 1\), \(\binom{n}{1} = n\) and \(\binom{n}{n} = 1\).

### ‚ú¶ Definition 8.1.7
Let \(X\) be a set. A *permutation* of \(X\) is a bijection \(X \to X\). Denote the set of all permutations of \(X\) by \(\mathrm{Sym}(X)\) and write \(S_n = \mathrm{Sym}([n])\) for \(n \in \mathbb{N}\).

### üñπ Example 8.1.8
There are six permutations of the set \([3]\). Representing each \(f \in S_3\) by the ordered triple \((f(1), f(2), f(3))\), these permutations are thus given by

\[
(1, 2, 3), \quad (1, 3, 2), \quad (2, 1, 3), \quad (2, 3, 1), \quad (3, 1, 2), \quad (3, 2, 1)
\]

For example, \((2, 3, 1)\) represents the permutation \(f : [3] \to [3]\) defined by \(f(1) = 2\), \(f(2) = 3\) and \(f(3) = 1\).

### üñπ Exercise 8.1.9
List all the permutations of the set \([4]\).

### ‚ú¶ Definition 8.1.10
Let \(n \in \mathbb{N}\). Denote by \(n!\) (read: ‚Äòn factorial‚Äô) the number of permutations of a set of size \(n\). That is, \(n! = |S_n|\). The numbers \(n!\) are called *factorials*.

### üñπ Example 8.1.11
Example 8.1.8 shows that \(3! = 6\).
</markdown><markdown>
# Products and procedures

We saw in Proposition 6.1.23 that, given two finite sets \( X \) and \( Y \), the product \( X \times Y \) is finite. We also found a formula for its size, namely \( |X \times Y| = |X| \cdot |Y| \). The *multiplication principle* (Strategy 8.1.20) generalises this formula to products that may contain any finite number of sets, not just two.

### Lemma 8.1.12

Let \(\{X_1, \ldots, X_n\}\) be a family of finite sets, with \( n \geq 1 \). Then \(\prod_{i=1}^{n} X_i\) is finite, and

\[
\left| \prod_{i=1}^{n} X_i \right| = |X_1| \cdot |X_2| \cdot \ldots \cdot |X_n|
\]

**Proof**

We proceed by induction on \( n \).

- **(Base case)** When \( n = 1 \), an element of \(\prod_{i=1}^{1} X_i\) is ‚Äòofficially‚Äô a sequence \((x_1)\) with \( x_1 \in X_1 \). This is the same as an element of \( X_1 \), in the sense that the assignments \((x_1) \mapsto x_1\) and \( x_1 \mapsto (x_1)\) define mutually inverse (hence bijective) functions between \(\prod_{i=1}^{1} X_i\) and \( X_1 \), and so

  \[
  \left| \prod_{i=1}^{1} X_i \right| = |X_1|
  \]

- **(Induction step)** Fix \( n \in \mathbb{N} \), and suppose that

  \[
  \left| \prod_{i=1}^{n} X_i \right| = |X_1| \cdot |X_2| \cdot \ldots \cdot |X_n|
  \]

  for all sets \( X_i \) for \( i \in [n] \). This is our induction hypothesis.

Now let \( X_1, \ldots, X_n, X_{n+1} \) be sets. We define a function

\[
F : \prod_{i=1}^{n+1} X_i \to \left( \prod_{i=1}^{n} X_i \right) \times X_{n+1}
\]

by letting \( F((x_1, \ldots, x_n, x_{n+1})) = ((x_1, \ldots, x_n), x_{n+1}) \). It is again easy to check that \( F \) is a bijection, and hence

\[
\left| \prod_{i=1}^{n+1} X_i \right| = \left| \prod_{i=1}^{n} X_i \right| \cdot |X_{n+1}|
\]
</markdown><markdown>
by Proposition 6.1.23. Applying the induction hypothesis, we obtain the desired result, namely

\[
\left| \prod_{i=1}^{n+1} X_i \right| = |X_1| \cdot |X_2| \cdot \ldots \cdot |X_n| \cdot |X_{n+1}|
\]

By induction, we‚Äôre done. ‚ñ°

Lemma 8.1.12 gives rise to a useful strategy for computing the size of a finite set \( X \)‚Äîsee Strategy 8.1.13. Intuitively, by devising a step-by-step procedure for specifying an element of \( X \), we are constructing a cartesian product \(\prod_{k=1}^{n} X_k\), where \( X_k \) is the set of choices to be made in the \( k \)th step. This establishes a bijection \(\prod_{k=1}^{n} X_k \to X\), which by bijective proof (Strategy 6.1.16(c)) lets us compute \( |X| \) as the product of the numbers of choices that can be made in each step.

### Strategy 8.1.13 (Multiplication principle (independent version))
Let \( X \) be a finite set. In order to compute \( |X| \), it suffices to find a step-by-step procedure for specifying elements of \( X \), such that:

- Each element is specified by a unique sequence of choices;
- Each step in the procedure is independent of the previous step;
- There are finitely many choices to be made at each step.

If there are \( n \in \mathbb{N} \) steps and \( m_k \in \mathbb{N} \) possible choices in the \( k \)th step, then \( |X| = \prod_{k=1}^{n} m_k \).

### Example 8.1.14
You go to an ice cream stand selling the following flavours:

vanilla, strawberry, chocolate, rum and raisin, mint choc chip, toffee crunch

You can have your ice cream in a tub, a regular cone or a choco-cone. You can have one, two or three scoops. We will compute how many options you have.

To select an ice cream, you follow the following procedure:

- **Step 1.** Choose a flavour. There are 6 ways to do this.
</markdown><markdown>
**Step 2.** Choose whether you‚Äôd like it in a tub, regular cone or choco-cone. There are 3 ways to do this.

**Step 3.** Choose how many scoops you‚Äôd like. There are 3 ways to do this.

Hence there are \(6 \times 3 \times 3 = 54\) options in total.

This may feel informal, but really what we are doing is establishing a bijection. Letting \(X\) be the set of options, the above procedure defines a bijection

\[ F \times C \times S \to X \]

where \(F\) is the set of flavours, \(C = \{\text{tub, regular cone, choco-cone}\}\) and \(S = [3]\) is the set of possible numbers of scoops.

‚úø **Example 8.1.15**

We will prove that \(|\mathcal{P}(X)| = 2^{|X|}\) for all finite sets \(X\).

Let \(X\) be a finite set and let \(n = |X|\). Write

\[ X = \{x_k \mid k \in [n]\} = \{x_1, x_2, \ldots, x_n\} \]

Intuitively, specifying an element of \(\mathcal{P}(X)\)‚Äîthat is, a subset \(U \subseteq X\)‚Äîis equivalent to deciding, for each \(k \in [n]\), whether \(x_k \in U\) or \(x_k \notin U\) (‚Äòin or out‚Äô), which in turn is equivalent to specifying an element of \(\{in, out\}^n\).

For example, taking \(X = [7]\), the subset \(U = \{1, 2, 6\}\) corresponds with the choices

1 in, 2 in, 3 out, 4 out, 5 out, 6 in, 7 out

and hence the element \((in, in, out, out, out, in, out) \in \{in, out\}^7\).

This defines a function \(i : \mathcal{P}(X) \to \{in, out\}^n\). This function is injective, since different subsets determine different sequences; and it is surjective, since each sequence determines a subset.

The above argument is sufficient for most purposes, but since this is the first bijective proof we have come across, we now give a more formal presentation of the details.

Define a function

\[ i : \mathcal{P}(X) \to \{in, out\}^n \]

by letting the \(k\)th component of \(i(U)\) be ‚Äòin‚Äô if \(x_k \in U\) or ‚Äòout‚Äô if \(x_k \notin U\), for each \(k \in [n]\).

We prove that \(i\) is a bijection.
</markdown><markdown>
- *i* is injective. To see this, take \( U, V \subseteq X \) and suppose \( i(U) = i(V) \). We prove that \( U = V \). So fix \( x \in X \) and let \( k \in [n] \) be such that \( x = x_k \). Then

  \[
  x \in U \iff \text{the } k\text{th component of } i(U) \text{ is ‚Äòin‚Äô} \quad \text{by definition of } i
  \]
  \[
  \iff \text{the } k\text{th component of } i(V) \text{ is ‚Äòin‚Äô} \quad \text{since } i(U) = i(V)
  \]
  \[
  \iff x \in V \quad \text{by definition of } i
  \]

  so indeed we have \( U = V \), as required.

- *i* is surjective. To see this, let \( v \in \{ \text{in, out} \}^n \), and let

  \[
  U = \{ x_k \mid \text{the } k\text{th component of } v \text{ is ‚Äòin‚Äô} \}
  \]

  Then \( i(U) = v \), since for each \( k \in [n] \) we have \( x_k \in U \) if and only if the \( k \)th component of \( v \) is ‚Äòin‚Äô, which is precisely the definition of \( i(U) \).

Hence

\[
|\mathcal{P}(X)| = |\{\text{in, out}\}^n| = 2^n
\]

as required.

Some authors actually write \( 2^X \) to refer to the power set of a set \( X \). This is justified by Example 8.1.15.

**Exercise 8.1.16**  
Let \( X \) and \( Y \) be finite sets, and recall that \( Y^X \) denotes the set of functions from \( X \) to \( Y \). Prove that \( |Y^X| = |Y|^{|X|} \).

**Exercise 8.1.17**  
Since September 2001, car number plates on the island of Great Britain have taken the form XX NN XXX, where each X can be any letter of the alphabet except for ‚ÄòI‚Äô, ‚ÄòQ‚Äô or ‚ÄòZ‚Äô, and NN is the last two digits of the year. [This is a slight simplification of what is really the case, but let‚Äôs not concern ourselves with too many details!] How many possible number plates are there? Number plates of vehicles registered in the region of Yorkshire begin with the letter ‚ÄòY‚Äô. How many Yorkshire number plates can be issued in a given year?

The multiplication principle in the form of Strategy 8.1.13 does not allow for steps later in a procedure to depend on those earlier in the procedure. To see why this is a problem, suppose we want to count the size of the set \( X = \{ (a, b) \in [n] \times [n] \mid a \neq b \} \). A step-by-step procedure for specifying such an element is as follows:

- **Step 1.** Select an element \( a \in [n] \). There are \( n \) choices.

- **Step 2.** Select an element \( b \in [n] \) with \( b \neq a \). There are \( n - 1 \) choices.
</markdown><markdown>
We would like to use Strategy 8.1.13 to deduce that \(|X| = n(n - 1)\). Unfortunately, this is not valid because the possible choices available to us in Step 2 depend on the choice made in Step 1. Elements of cartesian products do not depend on one another, and so the set of sequences of choices made cannot necessarily be expressed as a cartesian product of two sets. Thus we cannot apply Lemma 8.1.12. Oh no!

However, provided that the *number* of choices in each step remains constant, in spite of the choices themselves changing, it turns out that we can still compute the size of the set in question by multiplying together the numbers of choices.

This is what we prove next. We begin with a pairwise version (analogous to Exercise 6.1.22) and then prove the general version by induction (like in Lemma 8.1.12).

### Lemma 8.1.18
Fix \(m, n \in \mathbb{N}\). Let \(X\) be a finite set with \(|X| = m\), and for each \(a \in X\), let \(Y_a\) be a finite set with \(|Y_a| = n\). Then the set

\[ 
P = \{(a, b) \mid a \in X \text{ and } b \in Y_a \} 
\]

is finite and \(|P| = mn\).

**Proof**

Fix bijections \(f : [m] \to X\) and \(g_a : [n] \to Y_a\) for each \(a \in X\). Define \(h : [m] \times [n] \to P\) by letting \(h(i, j) = (f(i), g_{f(i)}(j))\) for each \((i, j) \in [m] \times [n]\). Then:

- \(h\) is well-defined, since for all \(i \in [m]\) and \(j \in [n]\) we have \(f(i) \in X\) and \(g_{f(i)}(j) \in Y_{f(i)}\).

- \(h\) is injective. To see this, fix \((i, j), (k, \ell) \in [m] \times [n]\) and assume that \(h(i, j) = h(k, \ell)\). Then \((f(i), g_{f(i)}(j)) = (f(k), g_{f(k)}(\ell))\), so that \(f(i) = f(k)\) and \(g_{f(i)}(j) = g_{f(k)}(\ell)\). Since \(f\) is injective, we have \(i = k\)‚Äîtherefore \(g_{f(i)}(j) = g_{f(i)}(\ell)\), and then since \(g_{f(i)}\) is injective, we have \(j = \ell\). Thus \((i, j) = (k, \ell)\), as required.

- \(h\) is surjective. To see this, let \((a, b) \in P\). Since \(f\) is surjective and \(a \in X\), we have \(a = f(i)\) for some \(i \in [m]\). Since \(g_a\) is surjective and \(b \in Y_a\), we have \(b = g_a(j)\) for some \(j \in [n]\). But then

  \[
  (a, b) = (f(i), g_a(j)) = (f(i), g_{f(i)}(j)) = h(i, j)
  \]

  so that \(h\) is surjective.

Since \(h\) is a bijection, we have \(|P| = |[m] \times [n]|\) by Theorem 6.1.14(iii), which is equal to \(mn\) by Proposition 6.1.23. ‚ñ°

We are now ready to state and prove the theorem giving rise to the multiplication principle in its full generality.
</markdown><markdown>
### Theorem 8.1.19 (Multiplication principle)

Let \( n \geq 1 \) and \( m_1, m_2, \ldots, m_n \in \mathbb{N} \). Suppose for each \( i \in [n] \) that we are given finite sets \( X_{a_1, \ldots, a_{i-1}}^{(i)} \) with \(|X_{a_1, \ldots, a_{i-1}}^{(i)}| = m_i\), where \( a_j \in X_{a_1, \ldots, a_{j-1}}^{(j)} \) for each \( j < i \). Define

\[
P = \{ (a_1, a_2, \ldots, a_n) \mid a_1 \in X^{(1)}, a_2 \in X_{a_1}^{(2)}, \ldots, a_n \in X_{a_1, \ldots, a_{n-1}}^{(n)} \}
\]

Then \( P \) is finite and \(|P| = m_1 \times m_2 \times \cdots \times m_n\).

#### Proof

We proceed by induction on \( n \geq 1 \).

- **(Base case)** When \( n = 1 \), the statement says that given \( m_1 \in \mathbb{N} \) and a finite set \( X^{(1)} \) with \(|X^{(1)}| = m_1\), then \( P = \{ (a_1) \mid a_1 \in X^{(1)} \} \) is finite and \(|P| = m_1\). This is true, since the function \( X^{(1)} \to P \) defined by \( a \mapsto (a) \) is evidently a bijection.

- **(Induction step)** Fix \( n \geq 1 \) and assume that the statement is true for this value of \( n \).

Let \( m_1, m_2, \ldots, m_n, m_{n+1} \in \mathbb{N} \) and suppose that we are given finite sets \( X_{a_1, \ldots, a_{i-1}}^{(i)} \) for each \( i \in [n+1] \) just as in the statement of the theorem, and let

\[
P = \{ (a_1, a_2, \ldots, a_{n+1}) \mid a_1 \in X^{(1)}, a_2 \in X_{a_1}^{(2)}, \ldots, a_{n+1} \in X_{a_1, \ldots, a_n}^{(n+1)} \}
\]

We need to prove that \(|P| = m_1 \times m_2 \times \cdots \times m_n \times m_{n+1}\).

So define

\[
Q = \{ (a_1, a_2, \ldots, a_n) \mid a_1 \in X^{(1)}, a_2 \in X_{a_1}^{(2)}, \ldots, a_n \in X_{a_1, \ldots, a_{n-1}}^{(n)} \}
\]

and, given \( q = (a_1, \ldots, a_n) \in Q \), define \( Y_q = X_{a_1, \ldots, a_n}^{(n+1)} \). Observe that there is an evident bijection

\[
\{ (q, a_{n+1}) \mid q \in Q, a_{n+1} \in Y_q \} \to P
\]

defined by \(((a_1, a_2, \ldots, a_n), a_{n+1}) \mapsto (a_1, a_2, \ldots, a_n, a_{n+1})\).

Now \(|Q| = m_1 \times m_2 \times \cdots \times m_n\), and \(|Y_q| = m_{n+1}\) for each \( q \in Q \), so it follows from Lemma 8.1.18 that

\[
|P| = (m_1 \times m_2 \times \cdots \times m_n) \times m_{n+1} = m_1 \times m_2 \times \cdots \times m_n \times m_{n+1}
\]

as required.

Strategy 8.1.20 summarises how Theorem 8.1.19 is useful in our proofs.
</markdown><markdown>
### Strategy 8.1.20 (Counting using the multiplication principle)

Let \( X \) be a finite set. In order to compute \(|X|\), it suffices to find a step-by-step procedure for specifying elements of \( X \), such that:

- Each element is specified by a unique sequence of choices;
- The choices available in each step depend only on choices made in previous steps;
- There are finitely many choices available in each step;
- The *number* of choices available in each step does not depend on choices made in previous steps;

If there are \( n \in \mathbb{N} \) steps and \( m_k \in \mathbb{N} \) possible choices in the \( k \)th step, then \(|X| = \prod_{k=1}^{n} m_k\).

### Example 8.1.21

We prove that there are six bijections \([3] \to [3]\). We can specify a bijection \( f : [3] \to [3] \) according to the following procedure.

- **Step 1.** Choose the value of \( f(1) \). There are 3 choices.

- **Step 2.** Choose the value of \( f(2) \). The values \( f(2) \) can take depend on the chosen value of \( f(1) \).
  - If \( f(1) = 1 \), then \( f(2) \) can be equal to 2 or 3.
  - If \( f(1) = 2 \), then \( f(2) \) can be equal to 1 or 3.
  - If \( f(1) = 3 \), then \( f(2) \) can be equal to 1 or 2.

  In each case, there are 2 choices for the value of \( f(2) \).

- **Step 3.** Choose the value of \( f(3) \). The values \( f(3) \) can take depend on the values of \( f(1) \) and \( f(2) \). We could split into the (six!) cases based on the values of \( f(1) \) and \( f(2) \) chosen in Steps 1 and 2; but we won‚Äôt. Instead, note that \(\{f(1), f(2)\}\) has two elements, and by injectivity \( f(3) \) must have a distinct value, so that \([3] \setminus \{f(1), f(2)\}\) has one element. This element must be the value of \( f(3) \). Hence there is only possible choice of \( f(3) \).

By the multiplication principle, there are \( 3 \times 2 \times 1 = 6 \) bijections \([3] \to [3]\).

### Exercise 8.1.22

Count the number of injections \([3] \to [4]\).

### Example 8.1.23

We count the number of ways we can shuffle a standard deck of cards in such a way that the colour of the cards alternate between red and black.
</markdown><markdown>
A procedure for choosing the order of the cards is as follows:

(i) Choose the colour of the first card. There are 2 such choices. This then determines the colours of the remaining cards, since they have to alternate.

(ii) Choose the order of the red cards. There are 26! such choices.

(iii) Choose the order of the black cards. There are 26! such choices.

By the multiplication principle, there are \(2 \cdot (26!)^2\) such rearrangements. This number is huge, and in general there is no reason to write it out. Just for fun, though:

325 288 005 235 264 929 014 077 766 819 257 214 042 112 000 000 000 000

## Sums and partitions

We saw in Proposition 6.1.19 that, given two finite sets \(X\) and \(Y\), the union \(X \cup Y\) is finite. We also found formulae for their size, namely \(|X \cup Y| = |X| + |Y| - |X \cap Y|\). The addition principle (Strategy 8.1.26) generalises this formula to any finite number of sets, provided the sets have no elements in common with one another‚Äîthat is they are pairwise disjoint. [The hypothesis of pairwise disjointness is removed in the inclusion‚Äìexclusion principle, which is studied in Section 8.2.]

If you have not covered Section 5.2 yet, you are encouraged to take a brief detour to read from Definition 5.2.21 to Exercise 5.2.26; the definition of a partition of a set is recalled below.

‚ú¶ **Definition 5.2.21**  
A **partition** of a set \(X\) is a collection \(\mathcal{U} = \{U_i \mid i \in I\}\) of subsets of \(X\) such that the following conditions hold:

(a) For each \(i \in I\), the subset \(U_i\) is inhabited;

(b) The sets \(U_i\) for \(i \in I\) are pairwise disjoint‚Äîthat is, \(U_i \cap U_j\) is empty for all \(i, j \in I\) with \(i \neq j\);

(c) \(\bigcup_{i \in I} U_i = X\).

In this section, we will simplify matters in two ways:
</markdown><markdown>
- When we say ‚Äòpartition‚Äô in this section (and [Section 8.2](#)), we will allow the sets in the partition to be empty‚Äîthat is, we will just need conditions (b) and (c) of [Definition 5.2.21](#) to hold.

- Since our sets are finite, so will the index set \( I \) be; so we will only ever partition our sets into finitely many pieces. That is, all of our partitions will take form \(\{U_1, U_2, \ldots, U_n\}\) for some \( n \in \mathbb{N} \).

With all of this said, let‚Äôs get right to it.

### Theorem 8.1.24 (Addition principle)
Let \( X \) be a set and let \(\{U_1, \ldots, U_n\}\) be a partition of \( X \) for some \( n \in \mathbb{N} \), such that each set \( U_i \) is finite. Then \( X \) is finite, and

\[
|X| = |U_1| + |U_2| + \cdots + |U_n|
\]

### Exercise 8.1.25
Prove [Theorem 8.1.24](#). The proof follows the same pattern as that of [Lemma 8.1.12](#). Be careful to make sure you identify where you use the hypothesis that the sets \( U_i \) are pairwise disjoint!

### Strategy 8.1.26 (Counting using the addition principle)
Let \( X \) be a finite set. In order to compute \( |X| \), it suffices to find a partition \( U_1, U_2, \ldots, U_n \) of \( X \); it then follows that

\[
|X| = \sum_{k=1}^{n} |U_i|.
\]

### Example 8.1.27
We will count the number of inhabited subsets of \([7]\) which either contain only even numbers, or contain only odd numbers.

Let \( O \) denote the set of inhabited subsets of \([7]\) containing only odd numbers, and let \( E \) denote the set of inhabited subsets of \([7]\) containing only even numbers. Note that \(\{O, E\}\) forms a partition of the set we are counting, and so our set has \(|O| + |E|\) elements.

- An element of \( O \) must be a subset of \(\{1, 3, 5, 7\}\). By [Example 8.1.15](#) there are \(2^4 = 16\) such subsets. Thus the number of inhabited subsets of \([7]\) containing only odd numbers is 15, since we must exclude the empty set. That is, \(|O| = 15\).

- A subset containing only even numbers must be a subset of \(\{2, 4, 6\}\). Again by [Example 8.1.15](#) there are \(2^3 = 8\) such subsets. Hence there are 7 inhabited subsets of \([7]\) containing only even numbers. That is, \(|E| = 7\).

Hence there are \(15 + 7 = 22\) inhabited subsets of \([7]\) containing only even or only odd numbers.
</markdown><markdown>
numbers. And here they are:

\[
\{1\} \quad \{3\} \quad \{5\} \quad \{7\} \quad \{1,3\} \quad \{2\} \quad \{4\} \quad \{6\} \\
\{1,5\} \quad \{1,7\} \quad \{3,5\} \quad \{3,7\} \quad \{5,7\} \quad \{2,4\} \quad \{2,6\} \quad \{4,6\} \\
\{1,3,5\} \quad \{1,3,7\} \quad \{1,5,7\} \quad \{3,5,7\} \quad \{1,3,5,7\} \quad \{2,4,6\}
\]

**Exercise 8.1.28**  
Pick your favourite integer \( n > 1000 \). For this value of \( n \), how many inhabited subsets of \([n]\) contain either only even or only odd numbers? (You need not evaluate exponents.)

We now consider some examples of finite sets which use both the multiplication principle and the addition principle.

**Example 8.1.29**  
A city has \( 6n \) inhabitants. The favourite colour of \( n \) of the inhabitants is orange, the favourite colour of \( 2n \) of the inhabitants is pink, and the favourite colour of \( 3n \) of the inhabitants is turquoise. The city government wishes to form a committee with equal representation from the three colour preference groups to decide how the new city hall should be painted. We count the number of ways this can be done.

Let \( X \) be the set of possible committees. First note that

\[
X = \bigcup_{k=0}^{n} X_k
\]

where \( X_k \) is the set of committees with exactly \( k \) people from each colour preference group. Indeed, we must have \( k \leq n \), since it is impossible to have a committee with more than \( n \) people from the orange preference group.

Moreover, if \( k \neq \ell \) then \( X_k \cap X_\ell = \emptyset \), since if \( k \neq \ell \) then a committee cannot simultaneously have exactly \( k \) people and exactly \( \ell \) people from each preference group.

By the addition principle, we have

\[
|X| = \sum_{k=0}^{n} |X_k|
\]

We count \( X_k \) for fixed \( k \) using the following procedure:

- **Step 1.** Choose \( k \) people from the orange preference group to be on the committee. There are \(\binom{n}{k}\) choices.
- **Step 2.** Choose \( k \) people from the pink preference group to be on the committee. There are \(\binom{2n}{k}\) choices.
</markdown><markdown>
### Section 8.1. Counting principles

- **Step 3.** Choose \( k \) people from the turquoise preference group to be on the committee. There are \(\binom{3n}{k}\) choices.

By the multiplication principle, it follows that \(|X_k| = \binom{n}{k} \binom{2n}{k} \binom{3n}{k}\). Hence

\[
|X| = \sum_{k=0}^{n} \binom{n}{k} \binom{2n}{k} \binom{3n}{k}
\]

### Exercise 8.1.30

In Example 8.1.29, how many ways could a committee be formed with a **representative** number of people from each colour preference group? That is, the proportion of people on the committee which prefer any of the three colours should be equal to the corresponding proportion of the population of the city.

### Pigeonhole principle

A nice application of the addition principle is to prove the **pigeonhole principle**, which is used heavily in combinatorics.

Informally, the pigeonhole principle says that if you assign pigeons to pigeonholes, and there are more pigeons than pigeonholes, then some pigeonhole must have more than one pigeon in it. We can (and do) generalise this slightly: it says that given \( q \in \mathbb{N} \), if you have more than \( q \) times as many pigeons than pigeonholes, then some pigeonhole must have more than \( q \) pigeons in it.

The proof is deceptively simple.

#### Theorem 8.1.31 (Pigeonhole principle)

Let \( q \in \mathbb{N} \), and let \( X \) and \( Y \) be finite sets with \(|X| = m \in \mathbb{N}\) and \(|Y| = n \in \mathbb{N}\). Then:

(a) If \( m > qn \), then for every function \( f : X \to Y \), there is some \( a \in Y \) such that \(|f^{-1}(\{a\})| > q\).

(b) If \( m \leq qn \), then there is a function \( f : X \to Y \) such that \(|f^{-1}(\{a\})| \leq q\) for all \( a \in Y \).

**Proof of (a)**

Suppose \( m > qn \). It follows from Exercise 5.2.25 that the sets \( f^{-1}(\{a\}) \) partition \( X \). Towards a contradiction, assume \(|f^{-1}(\{a\})| \leq q\) for all \( a \in Y \). Then by the addition principle

\[
m = |X| = \left| \bigcup_{a \in Y} f^{-1}(\{a\}) \right| = \sum_{a \in Y} |f^{-1}(\{a\})| \leq \sum_{a \in Y} q = |Y| \cdot q = qn
\]
</markdown><markdown>
This contradicts the assumption that \( m > qn \).

### Exercise 8.1.32
Prove part (b) of Theorem 8.1.31.

### Example 8.1.33
Let \( n, k \in \mathbb{N} \). Assume that you have \( n \) pairs of socks in a drawer, and each sock is one of \( k \) colours. We wish to know how many socks you must take out of the drawer before you can guarantee that you have a matching pair.

Let \( C \) be set of colours of the socks, so that \( |C| = k \), and let \( X \) be the set of socks that you have selected. We obtain a function \( f : X \to C \) that assigns to each sock \( x \) its colour \( f(x) \in C \). Given a colour \( c \in C \), the preimage \( f^{-1}[\{c\}] \) is the set of socks of colour \( c \) that we have selected.

Thus the question becomes: what size must \( X \) be in order to have \( |f^{-1}[\{c\}]| \geq 2 \) for some \( c \in C \)? [The English translation of this question is: how many socks must we have picked in order for two of the socks to have the same colour?]

Well, by the pigeonhole principle, we can guarantee \( |f^{-1}[\{c\}]| \geq 2 \) (or equivalently \( > 1 \)) if and only if \( |X| > |C| \). That is, we need to select at least \( k + 1 \) socks to guarantee a matching pair.

### Exercise 8.1.34
Six people are in a room. The atmosphere is tense, since each pair of people is either friends or enemies. There are no allegiances, so for example it is possible for a friend of a friend to be an enemy, or an enemy of a friend to be a friend, and so on. Prove that there is some set of three people that are either all each other‚Äôs friends or all each other‚Äôs enemies.

## Double counting

**Double counting** (also known as **counting in two ways**) is a proof technique that allows us to prove that two natural numbers are equal by establishing they are two expressions for the size of the same set. (More generally, by Theorem 6.1.14(iii), we can relate them to the sizes of two sets which are in bijection.)

The proof of Proposition 8.1.35 illustrates this proof very nicely. We proved it already by induction in Example 4.2.13; the combinatorial proof we now provide is much shorter and cleaner.

### Proposition 8.1.35
Let \( n \in \mathbb{N} \). Then \( 2^n = \sum_{k=0}^{n} \binom{n}{k} \).
</markdown><markdown>
**Proof**

We know that \( |\mathcal{P}([n])| = 2^n \) by Example 8.1.15 and that \(\mathcal{P}([n]) = \bigcup_{k=0}^{n} \binom{[n]}{k} \) by Proposition 8.1.3. Moreover, the sets \(\binom{[n]}{k}\) are pairwise disjoint, so by the addition principle it follows that

\[
2^n = |\mathcal{P}([n])| = \left| \bigcup_{k=0}^{n} \binom{[n]}{k} \right| = \sum_{k=0}^{n} \left| \binom{[n]}{k} \right| = \sum_{k=0}^{n} \binom{n}{k}
\]

‚ùñ **Strategy 8.1.36 (Double counting)**

In order to prove that two expressions involving natural numbers are equal, it suffices to define a set \( X \) and devise two counting arguments to show that \( |X| \) is equal to both expressions.

The next example counts elements of *different* sets and puts them in bijection to establish an identity.

‚ú£ **Proposition 8.1.37**

Let \( n, k \in \mathbb{N} \) with \( n \geq k \). Then

\[
\binom{n}{k} = \binom{n}{n-k}
\]

**Proof**

First note that \(\binom{n}{k} = \left| \binom{[n]}{k} \right|\) and \(\binom{n}{n-k} = \left| \binom{[n]}{n-k} \right|\), so in order to prove \(\binom{n}{k} = \binom{n}{n-k}\), it suffices by Strategy 6.1.16 to find a bijection \( f : \binom{[n]}{k} \to \binom{[n]}{n-k} \). Intuitively, this bijection arises because choosing \( k \) elements from \([n]\) to put into a subset is equivalent to choosing \( n-k \) elements from \([n]\) to leave out of the subset. Specifically, we define

\[
f(U) = [n] \setminus U \text{ for all } U \in \binom{[n]}{k}
\]

Note first that \( f \) is well-defined, since if \( U \subseteq [n] \) with \(|U| = k\), then \([n] \setminus U \subseteq [n]\) and \(|[n] \setminus U| = |[n]| - |U| = n-k\) by Exercise 6.1.21. We now prove \( f \) is a bijection:

- \( f \) is injective. Let \( U, V \subseteq [n] \) and suppose \([n] \setminus U = [n] \setminus V\). Then for all \( k \in [n] \), we have

  \[
  k \in U \iff k \notin [n] \setminus U \quad \text{by definition of set difference}
  \]
  \[
  \iff k \notin [n] \setminus V \quad \text{since } [n] \setminus U = [n] \setminus V
  \]
  \[
  \iff k \in V \quad \text{by definition of set difference}
  \]

  so \( U = V \), as required.
</markdown><markdown>
- **\( f \) is surjective.** Let \( V \in \binom{[n]}{n-k} \). Then \(|[n] \setminus V| = n - (n-k) = k\) by Exercise 6.1.21, so that \([n] \setminus V \in \binom{[n]}{k}\). But then

  \[
  f([n] \setminus V) = [n] \setminus ([n] \setminus V) = V
  \]

  by Exercise 2.2.30.

Since \( f \) is a bijection, we have

\[
\binom{n}{k} = \left| \binom{[n]}{k} \right| = \left| \binom{[n]}{n-k} \right| = \binom{n}{n-k}
\]

as required.

We put a lot of detail into this proof. A slightly less formal proof might simply say that \(\binom{n}{k} = \binom{n}{n-k}\) since choosing \( k \) elements from \([n]\) to put into a subset is equivalent to choosing \( n-k \) elements from \([n]\) to leave out of the subset. This would be fine as long as the members of the intended audience of your proof could reasonably be expected to construct the bijection by themselves.

The proof of Proposition 8.1.38 follows this more informal format.

### Proposition 8.1.38
Let \( n, k, \ell \in \mathbb{N} \) with \( n \geq k \geq \ell \). Then

\[
\binom{n}{k} \binom{k}{\ell} = \binom{n}{\ell} \binom{n-\ell}{k-\ell}
\]

**Proof**

Let‚Äôs home in on the left-hand side of the equation. By the multiplication principle, \(\binom{n}{k} \binom{k}{\ell}\) is the number of ways of selecting a \( k \)-element subset of \([n]\) and an \(\ell\)-element subset of \([k]\). Equivalently, it‚Äôs the number of ways of selecting a \( k \)-element subset of \([n]\) and then an \(\ell\)-element subset of the \( k \)-element subset that we just selected. To make this slightly more concrete, let‚Äôs put it this way:

\(\binom{n}{k} \binom{k}{\ell}\) is the number of ways of painting \( k \) balls red from a bag of \( n \) balls, and painting \(\ell\) of the red balls blue. This leaves us with \(\ell\) blue balls and \( k - \ell \) red balls.

Now we need to find an equivalent interpretation of \(\binom{n}{\ell} \binom{n-\ell}{k-\ell}\). Well, suppose we pick the \(\ell\) elements to be coloured blue first. To make up the rest of the \( k \)-element subset, we now have to select \( k - \ell \) elements, and there are now \( n - \ell \) to choose from. Thus

\(\binom{n}{\ell} \binom{n-\ell}{k-\ell}\) is the number of ways of painting \(\ell\) balls from a bag of \( n \) balls blue, and painting \( k - \ell \) of the remaining balls red.
</markdown><markdown>
Thus, both numbers represent the number of ways of painting $\ell$ balls blue and $k - \ell$ balls red from a bag of $n$ balls. Hence they are equal.

### Exercise 8.1.39
Make the proof of [Proposition 8.1.38](#) more formal by defining a bijection between sets of the appropriate sizes.

### Exercise 8.1.40
Provide a combinatorial proof that if $n, k \in \mathbb{N}$ with $n \geq k$, then

\[
\binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}
\]

Deduce that the combinatorial definition of binomial coefficients ([Definition 8.1.4](#)) is equivalent to the recursive definition ([Definition 4.1.15](#)).

The following proposition demonstrates that the combinatorial definition of factorials ([Definition 8.1.10](#)) is equivalent to the recursive definition ([Definition 4.1.14](#)).

### Theorem 8.1.41
$0! = 1$ and if $n \in \mathbb{N}$ then $(n+1)! = (n+1) \cdot n!$.

**Proof**  
The only permutation of $\emptyset$ is the empty function $e : \emptyset \to \emptyset$. Hence $S_0 = \{e\}$ and $0! = |S_0| = 1$.

Let $n \in \mathbb{N}$. A permutation of $[n+1]$ is a bijection $f : [n+1] \to [n+1]$. Specifying such a bijection is equivalent to carrying out the following procedure:

- Choose the (unique!) element $k \in [n+1]$ such that $f(k) = n+1$. There are $n+1$ choices for $k$.

- Choose the values of $f$ at each $\ell \in [n+1]$ with $\ell \neq k$. This is equivalent to finding a bijection $[n+1] \setminus \{k\} \to [n]$. Since $|[n+1] \setminus \{k\}| = |[n]| = n$, there are $n!$ such choices.

By the multiplication principle, we have

\[
(n+1)! = |S_{n+1}| = (n+1) \cdot n!
\]

so we‚Äôre done.

We now revisit [Theorem 4.2.14](#); this time, our proof will be combinatorial, rather than inductive.
</markdown><markdown>
### Theorem 8.1.42
Let \( n, k \in \mathbb{N} \). Then

\[
\binom{n}{k} = 
\begin{cases} 
\frac{n!}{k!(n-k)!} & \text{if } k \leq n \\ 
0 & \text{if } k > n 
\end{cases}
\]

**Proof**  
Suppose \( k > n \). By Exercise 6.1.17, if \( U \subseteq [n] \) then \(|U| \leq n\). Hence if \( k > n \), then \(\binom{n}{k} = \emptyset\), and so \(\binom{n}{k} = 0\), as required.

Now suppose \( k \leq n \). We will prove that \( n! = \binom{n}{k} \cdot k! \cdot (n-k)! \); the result then follows by dividing through by \( k!(n-k)! \). We prove this equation by counting the number of elements of \( S_n \).

A procedure for defining an element of \( S_n \) is as follows:

(i) Choose which elements will appear in the first \( k \) positions of the list. There are \(\binom{n}{k}\) such choices.

(ii) Choose the order of these \( k \) elements. There are \( k! \) such choices.

(iii) Choose the order of the remaining \( n-k \) elements. There are \( (n-k)! \) such choices.

By the multiplication principle, \( n! = \binom{n}{k} \cdot k! \cdot (n-k)! \).

Note that the proof of Theorem 8.1.42 relied only on the combinatorial definitions of binomial coefficients and factorials; we didn‚Äôt need to know how to compute them at all! The proof was *much* shorter, cleaner and, in some sense, more meaningful, than the inductive proof we gave in Theorem 4.2.14.

We conclude this section with some more examples and exercises in which double counting can be used.

### Exercise 8.1.43
Let \( n, k \in \mathbb{N} \) with \( k \leq n + 1 \). Prove that

\[
k \binom{n}{k} = (n-k+1) \binom{n}{k-1}
\]

### Example 8.1.44
Let \( m, n, k \in \mathbb{N} \). We prove that

\[
\sum_{\ell=0}^{k} \binom{m}{\ell} \binom{n}{k-\ell} = \binom{m+n}{k}
\]
</markdown><markdown>
by finding a procedure for counting the number of \( k \)-element subsets of an appropriate \((m+n)\)-element set. Specifically, let \( X \) be a set containing \( m \) cats and \( n \) dogs. Then \(\binom{m+n}{k}\) is the number of \( k \)-element subsets \( U \subseteq X \). We can specify such a subset according to the following procedure.

- **Step 1.** Split into cases based on the number \( \ell \) of cats in \( U \). Note that we must have \( 0 \leq \ell \leq k \), since the number of cats must be a natural number and cannot exceed \( k \) as \(|U| = k\). Moreover, these cases are mutually exclusive. Hence by the addition principle we have

  \[
  \binom{m+n}{k} = \sum_{\ell=0}^{k} a_\ell
  \]

  where \( a_\ell \) is the number of subsets of \( X \) containing \( \ell \) cats and \( k-\ell \) dogs.

- **Step 2.** Choose \( \ell \) cats from the \( m \) cats in \( X \) to be elements of \( U \). There are \(\binom{m}{\ell}\) such choices.

- **Step 3.** Choose \( k-\ell \) dogs from the \( n \) dogs in \( X \) to be elements of \( U \). There are \(\binom{n}{k-\ell}\) such choices.

The multiplication principle shows that \( a_\ell = \binom{m}{\ell} \binom{n}{k-\ell} \). Hence

\[
\binom{m+n}{k} = \sum_{\ell=0}^{k} \binom{m}{\ell} \binom{n}{k-\ell}
\]

as required.

‚úë **Exercise 8.1.45**

Given natural numbers \( n, a, b, c \) with \( a + b + c = n \), define the trinomial coefficient

\[
\binom{n}{a, b, c}
\]

to be the number of ways of partitioning \([n]\) into three sets of sizes \( a, b \) and \( c \), respectively. That is, 

\[
\binom{n}{a, b, c}
\]

is the size of the set

\[
\left\{ (A, B, C) \mid A \subseteq [n], \, B \subseteq [n], \, C \subseteq [n], \, |A| = a, \, |B| = b, \, |C| = c, \, \text{and} \, A \cup B \cup C = [n] \right\}
\]

By considering trinomial coefficients, prove that if \( a, b, c \in \mathbb{N} \), then \((a+b+c)!\) is divisible by \( a! \cdot b! \cdot c! \).
</markdown><markdown>
## Section 8.2

# Alternating sums

Using the addition principle, together with double counting, turned out to be very useful for proving combinatorial identities involving sums in [Section 8.1](#). In this section, we turn our attention to **alternating sums**, which are sums whose terms alternate between positive and negative. As we will see later, sums of this kind can be used to computing sizes of unions of not-necessarily-disjoint sets‚Äîthis has all manner of uses and applications.

An example of such a sum is the following.

\[
\binom{6}{0} - \binom{6}{1} + \binom{6}{2} - \binom{6}{3} + \binom{6}{4} - \binom{6}{5} + \binom{6}{6}
\]

We can express such sums more succinctly by observing that, given \( k \in \mathbb{N} \), we have

\[
(-1)^k = 
\begin{cases} 
1 & \text{if } k \text{ is even} \\
-1 & \text{if } k \text{ is odd}
\end{cases}
\]

For example, the sum above could be expressed as

\[
\sum_{k=0}^{6} (-1)^k \binom{6}{k}
\]

It so happens that this sum evaluates to zero:

\[
1 - 6 + 15 - 20 + 15 - 6 + 1 = 0
\]

The goal of the following exercise is to demonstrate how... annoying... it is to prove identities involving alternating sums using induction.

### Exercise 8.2.1

Prove by induction that

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = 0
\]

for all \( n \in \mathbb{N} \).

Evidently we need a better approach.

If you stare at the equation in Exercise 8.2.1 for long enough, you should be able to convince yourself that

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = \sum_{\text{even } k} \binom{n}{k} - \sum_{\text{odd } k} \binom{n}{k}
\]

and it suffices to prove that

\[
\sum_{\text{even } k} \binom{n}{k} = \sum_{\text{odd } k} \binom{n}{k}
\]

This will be our strategy in the proof of Proposition 8.2.2, which serves as our prototype for the abstract material to come.
</markdown><markdown>
For the sake of readability, we left implicit that \( k \) is varying over (the even or odd elements of) the set \(\{0, 1, \ldots, n\}\) in each sum‚Äîwe shall adopt this practice throughout this section.

### Proposition 8.2.2

Let \( n \in \mathbb{N} \). Then

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = 0.
\]

**Proof**

As we observed, it suffices to prove

\[
\sum_{\text{even } k} \binom{n}{k} = \sum_{\text{odd } k} \binom{n}{k}
\]

So define

\[
\mathcal{E} = \{ U \subseteq [n] \mid |U| \text{ is even} \} \quad \text{and} \quad \mathcal{O} = \{ U \subseteq [n] \mid |U| \text{ is odd} \}
\]

That is, \(\mathcal{E}\) is the set of all even-sized subsets of \([n]\), and \(\mathcal{O}\) is the set of all odd-sized subsets of \([n]\).

Note that the sets \(\binom{[n]}{k}\) for even \(k \leq n\) partition \(\mathcal{E}\), and the sets \(\binom{[n]}{k}\) for odd \(k \leq n\) partition \(\mathcal{O}\). So by the addition principle, we have

\[
|\mathcal{E}| = \bigcup_{\text{even } k} \binom{[n]}{k} = \sum_{\text{even } k} \binom{n}{k} \quad \text{and} \quad |\mathcal{O}| = \bigcup_{\text{odd } k} \binom{[n]}{k} = \sum_{\text{odd } k} \binom{n}{k}
\]

It suffices to show that \(|\mathcal{E}| = |\mathcal{O}|\). To do this, define a function \( f : \mathcal{E} \to \mathcal{O} \) for \( U \in \mathcal{E} \) by

\[
f(U) = 
\begin{cases} 
U \cup \{n\} & \text{if } n \notin U \\
U \setminus \{n\} & \text{if } n \in U 
\end{cases}
\]

That is, \( f \) puts \( n \) into a subset if it wasn‚Äôt already there, and removes it if it was. Then:

- **\( f \) is well-defined.** Given \( U \in \mathcal{E} \), note that \(|f(U)| = |U| \pm 1\); since \(|U|\) is even, we have that \(|f(U)|\) is odd, so that \( f(U) \in \mathcal{O} \).

- **\( f \) is bijective.** Define \( g : \mathcal{O} \to \mathcal{E} \) by letting

\[
g(V) = 
\begin{cases} 
V \cup \{n\} & \text{if } n \notin V \\
V \setminus \{n\} & \text{if } n \in V 
\end{cases}
\]

for all \( V \in \mathcal{O} \). The proof that \( g \) is well-defined is identical to that of \( f \). Moreover, given \( U \in \mathcal{E} \), we have:
</markdown><markdown>
If \( n \in U \), then \( f(U) = U \setminus \{n\} \), so that \( g(f(U)) = (U \setminus \{n\}) \cup \{n\} = U \).

If \( n \notin U \), then \( f(U) = U \cup \{n\} \), so that \( g(f(U)) = (U \cup \{n\}) \setminus \{n\} = U \).

Hence \( g(f(U)) = U \) for all \( U \in \mathcal{E} \). An identical computation reveals that \( f(g(V)) = V \) for all \( V \in \mathcal{O} \), and so \( g \) is an inverse for \( f \).

Putting all of this together, it follows from the fact that \( f : \mathcal{E} \to \mathcal{O} \) is a bijection that \( |\mathcal{E}| = |\mathcal{O}| \), and so

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = \sum_{\text{even } k} \binom{n}{k} - \sum_{\text{odd } k} \binom{n}{k} = |\mathcal{E}| - |\mathcal{O}| = 0
\]

as required.

Wait a minute‚Äîdidn‚Äôt I say this would be a *better* approach than induction? That proof felt like a lot of work. The reason for working through this proof is that it highlights the ideas that we will use throughout this section. These ideas will allow us to derive a general proof strategy, called the *involution principle* (Strategy 8.2.24), which greatly simplifies proofs of results of this nature‚Äîindeed, we will prove Proposition 8.2.2 again using the involution principle in Example 8.2.25.

With that said, Proposition 8.2.2 highlights the following general strategy for proving that an alternating sum evaluates to zero.

**Strategy 8.2.3** (Proving that an alternating sum evaluates to zero)

Let \( a_0, a_1, \ldots, a_n \in \mathbb{N} \). In order to prove that

\[
\sum_{k=0}^{n} (-1)^k a_k = 0,
\]

it suffices to find:

(i) A partition \( U_0, U_2, \ldots \) of a set \( \mathcal{E} \), with \( |U_k| = a_k \) for all even \( k \);

(ii) A partition \( U_1, U_3, \ldots \) of a set \( \mathcal{O} \), with \( |U_k| = a_k \) for all odd \( k \); and

(iii) A bijection \( \mathcal{E} \to \mathcal{O} \).

**Exercise 8.2.4**

Use Strategy 8.2.3 to prove that

\[
\sum_{k=0}^{n} (-1)^k \cdot k \cdot \binom{n}{k} = 0
\]

for all \( n \geq 2 \).

Unfortunately Strategy 8.2.3 is still somewhat limited. For a start, it tells us nothing about how to evaluate an alternating sum that *doesn‚Äôt* end up being equal to zero. Also,
</markdown><markdown>
it ignores a key clue from the proof of Proposition 8.2.2: namely, the function \( f : \mathcal{E} \to \mathcal{O} \) and its inverse \( g : \mathcal{O} \to \mathcal{E} \) were defined identically. They are both restrictions of a function \( h : \mathcal{P}([n]) \to \mathcal{P}([n]) \) defined in the same way:

\[
h(U) = 
\begin{cases} 
U \cup \{n\} & \text{if } n \notin U \\
U \setminus \{n\} & \text{if } n \in U 
\end{cases}
\]

This function has the property that \( h(h(U)) = U \) for all \( U \subseteq [n] \) (that is, \( h \) is an involution), and \( h \) restricts to a bijection between the set of even-sized subsets of \([n]\) and the set of odd-sized subsets of \([n]\) (that is, \( h \) swaps parity).

This property of being a parity-swapping involution will be the key to deriving the involution principle.

## Involutions

An involution is a function that is its own inverse.

### Definition 8.2.5
Let \( X \) be a set. An **involution** of \( X \) is a function \( h : X \to X \) such that \( h \circ h = \text{id}_X \).

### Example 8.2.6
Consider the function \( h : \mathbb{R} \to \mathbb{R} \) defined by \( h(x) = 1 - x \) for each \( x \in \mathbb{R} \). Then \( h \) is an involution, since for all \( x \in \mathbb{R} \) we have \( h(h(x)) = 1 - (1 - x) = x \).

### Exercise 8.2.7
Given a set \( X \), prove that the relative complement function \( r : \mathcal{P}(X) \to \mathcal{P}(X) \), defined by \( r(U) = X \setminus U \) for all \( U \subseteq X \), is an involution.

### Exercise 8.2.8
Prove that every involution is a bijection.

### Exercise 8.2.9
Let \( h : X \to X \) be an involution and let \( a \in X \). Prove that \( h \) either fixes \( a \)‚Äîthat is, \( h(a) = a \)‚Äîor swaps it with another element \( b \in X \)‚Äîthat is, \( h(a) = b \) and \( h(b) = a \).

The involution that we used in the proof of Proposition 8.2.2 was an instance of toggling an element in a subset‚Äîthat is, removing it if it is there, and putting it in if it is not.

Toggling is so useful that we assign special notation.
</markdown><markdown>
### Definition 8.2.10

Let \( X \) be a set. The toggle operation \(\oplus\) (LaTeX code: `\oplus`) is defined by letting

\[
U \oplus a = 
\begin{cases} 
U \cup \{a\} & \text{if } a \notin U \\
U \setminus \{a\} & \text{if } a \in U 
\end{cases}
\]

for each \( U \subseteq X \) and each \( a \in X \).

### Example 8.2.11

Taking \( X = [3] \) and \( a = 3 \), we have:

\[
\begin{align*}
\emptyset \oplus 3 &= \{3\} \\
\{1\} \oplus 3 &= \{1, 3\} \\
\{2\} \oplus 3 &= \{2, 3\} \\
\{1, 2\} \oplus 3 &= \{1, 2, 3\} \\
\{3\} \oplus 3 &= \emptyset \\
\{1, 3\} \oplus 3 &= \{1\} \\
\{2, 3\} \oplus 3 &= \{2\} \\
\{1, 2, 3\} \oplus 3 &= \{1, 2\}
\end{align*}
\]

The next two exercises are generalisations of facts that we showed in the proof of Proposition 8.2.2.

### Exercise 8.2.12

Let \( X \) be a set and let \( a \in X \). Prove that the function \( T_a : \mathcal{P}(X) \to \mathcal{P}(X) \) defined by \( T_a(U) = U \oplus a \) for all \( U \subseteq X \) is an involution.

### Exercise 8.2.13

Let \( X \) be a finite set and let \( a \in X \). Prove that, for all \( U \subseteq X \), if \( |U| \) is even then \( |U \oplus a| \) is odd, and if \( |U| \) is odd then \( |U \oplus a| \) is even.

The property of the toggle operation that you proved in Exercise 8.2.13 is an instance of **parity-swapping**. While toggling swaps the parity of the size of a subset, we can generalise the notion of parity more generally, provided we have a notion of what it means for an element of a set to be ‚Äòeven‚Äô or ‚Äòodd‚Äô.

A first attempt to define ‚Äòeven‚Äô and ‚Äòodd‚Äô might be to simply partition a set \( X \) as \( X = E \cup O \), for disjoint subsets \( E, O \subseteq X \)‚Äîthe elements of \( E \) will be deemed to be ‚Äòeven‚Äô and the elements of \( O \) will be deemed to be ‚Äòodd‚Äô. But it will be helpful later on to go one step further than this: we will partition \( X \) into finitely many pieces, indexed by natural numbers, and the natural number will determine the parities of the elements of \( X \).
</markdown><markdown>
### Definition 8.2.14

Let \( X \) be a set and let \( \mathcal{U} = \{U_0, U_1, \ldots, U_n\} \) be a partition of \( X \) for some \( n \in \mathbb{N} \). The **parity** of an element \( a \in X \) (relative to \( \mathcal{U} \)) is the parity‚Äî**even** or **odd**‚Äîof the unique \( k \in \{0, 1, \ldots, n\} \) such that \( a \in U_k \).

Write \( X^+ = \{a \in X \mid a \text{ has even parity}\} \) (\(\LaTeX\) code: \( X^+ \)) and \( X^- = \{a \in X \mid a \text{ has odd parity}\} \) (\(\LaTeX\) code: \( X^- \)).

Note that, with notation as in Definition 8.2.14, we have partitions of \( X^+ \) and \( X^- \) as

\[
X^+ = \bigcup_{\text{even } k} U_k \quad \text{and} \quad X^- = \bigcup_{\text{odd } k} U_k
\]

### Example 8.2.15

Let \( X \) be a finite set, and consider the partition of \( \mathcal{P}(X) \) given by \( U_k = \binom{X}{k} \) for all \( 0 \leq k \leq n \). With respect to this partition, an element \( U \in \mathcal{P}(X) \) has even parity if and only if \(|U|\) is even, and odd parity if and only if \(|U|\) is odd.

For example, we have

\[
\mathcal{P}([2])^+ = \{\emptyset, \{1, 2\}\} \quad \text{and} \quad \mathcal{P}([2])^- = \{\{1\}, \{2\}\}
\]

### Example 8.2.16

Let \( m, n \in \mathbb{N} \) and let \( X \) be the set of all functions \([n] \to [n]\). For each \( k \leq n \), define

\[
X_k = \{f : [n] \to [n] \mid |\{a \in [n] \mid f(a) = a\}| = k\}
\]

That is, for each \( k \leq n \), the set \( X_k \) is the set of all functions \( f : [n] \to [n] \) that fix exactly \( k \) elements of \([n]\).

A function \( f : [n] \to [n] \) has even parity with respect to this partition if it fixes an even number of elements, and odd parity if it fixes an odd number of elements.

### Definition 8.2.17

Let \( X \) be a set and let \(\{U_0, U_1, \ldots, U_n\}\) be a partition of \( X \) for some \( n \in \mathbb{N} \). A function \( f : X \to X \) **swaps parity** (or is **parity-swapping**) if, for all \( a \in X \), if \( a \) has even parity then \( f(a) \) has odd parity, and if \( a \) has odd parity then \( f(a) \) has even parity.

### Example 8.2.18

With parity defined as in Example 8.2.15, the result of Exercise 8.2.12 says precisely that, for every set \( X \) and element \( a \in X \), the toggle function \( T_a : \mathcal{P}(X) \to \mathcal{P}(X) \) swaps parity, where \( T_a \) is defined by \( T_a(U) = U \oplus a \) for all \( U \subseteq X \).
</markdown><markdown>
### Exercise 8.2.19
Let \( X \) be a finite set. Under what conditions does the involution \( r : \mathcal{P}(X) \to \mathcal{P}(X) \) given by \( r(U) = X \setminus U \) for all \( U \subseteq X \) swap parity?

### Exercise 8.2.20
Let \( n \in \mathbb{N} \) and let \( X \) be the set of all functions \([n] \to [n]\), partitioned as in Example 8.2.16, so that a function \( f : [n] \to [n] \) has even parity if it fixes an even number of elements, and odd parity if it fixes an odd number of elements. Find a parity-swapping function \( X \to X \).

The next two following technical results will be used fundamentally in the proof of Theorem 8.2.23.

### Lemma 8.2.21
Let \( X \) be a finite set, let \(\{U_0, U_1, \ldots, U_n\}\) be a partition of \( X \) for some \( n \in \mathbb{N} \), and let \( h : X \to X \) be a parity-swapping involution. Then \( h \) induces a bijection \( f : X^+ \to X^- \) defined by \( f(x) = h(x) \) for all \( x \in X^+ \).

**Proof**

First note that the definition of \( f : X^+ \to X^- \) by letting \( f(x) = h(x) \) for all \( x \in X^+ \) is well-defined since \( h \) swaps parity. Indeed, if \( x \in X^+ \), then \( x \) has even parity, so that \( f(x) = h(x) \) has odd parity, meaning that \( f(x) \in X^- \).

To see that \( f \) is a bijection, define a function \( g : X^- \to X^+ \) by \( g(x) = h(x) \) for all \( x \in X^- \). Again, \( g \) is well-defined since \( h \) swaps parity.

Finally note that \( g \) is an inverse for \( f \)‚Äîgiven \( x \in X^+ \), we have
\[ 
g(f(x)) = h(h(x)) = x 
\]
and likewise \( f(g(x)) = x \) for all \( x \in X^- \).

Since \( f \) has an inverse, it is a bijection.

### Lemma 8.2.22
Let \( X \) be a finite set, let \(\{U_1, U_2, \ldots, U_n\}\) be a partition of \( X \) for some \( n \in \mathbb{N} \), and let \( h : X \to X \) be a parity-swapping involution. Then
\[
\sum_{k=1}^{n} (-1)^k |U_k| = 0
\]

**Proof**

By Lemma 8.2.21 we know that \( h : X \to X \) restricts to a bijection \( X^+ \to X^- \), and so we have \(|X^+| = |X^-|\). By the addition principle, we have
\[
\sum_{k=0}^{n} (-1)^k |U_k| = \sum_{\text{even } k} |U_k| - \sum_{\text{odd } k} |U_k| = |X^+| - |X^-| = 0
\]
</markdown><markdown>
Lemma 8.2.22 gets us well on our way to deriving the involution principle. In fact, it already makes Strategy 8.2.3 obsolete: we can now prove that an alternating sum is equal to zero simply by finding a parity-swapping involution from a suitably partitioned set to itself!

But in practice, it might not be easy (or even possible) to define a parity-swapping involution \( h : X \to X \) on the whole set \( X \). In such cases, we do the best that we can: define \( h \) on some subset \( D \subseteq X \), and worry about what is left over afterwards.

### Theorem 8.2.23
Let \( X \) be a finite set, let \( \{U_1, U_2, \ldots, U_n\} \) be a partition of \( X \) for some \( n \in \mathbb{N} \), let \( D \subseteq X \), let \( h : D \to D \) be a parity-swapping involution, and let \( F_k = U_k \setminus D \) for each \( k \in [n] \). Then

\[
\sum_{k=1}^{n} (-1)^k |U_k| = \sum_{k=1}^{n} (-1)^k |F_k|
\]

**Proof**

Note first that the sets \( U_k \cap D \) for \( k \in [n] \) partition \( D \), with the elements of \( D \) having the same parities as they did when they were considered as elements of \( X \).

It follows from Lemma 8.2.22 that

\[
\sum_{k=1}^{n} |U_k \cap D| = 0
\]

Moreover \( |U_k| = |U_k \cap D| + |U_k \setminus D| \) for each \( k \in [n] \) by the addition principle. Since \( F_k = U_k \setminus D \) for each \( k \in [n] \), we have

\[
\sum_{k=1}^{n} (-1)^k |U_k| = \sum_{k=1}^{n} (-1)^k |U_k \cap D| + \sum_{k=1}^{n} (-1)^k |U_k \setminus D| = \sum_{k=1}^{n} (-1)^k |F_k|
\]

as required.

We have suggestively used the letter \( D \) to refer to where the involution is defined, and the letter \( F \) to refer to the elements where the involution fails.
</markdown><markdown>
### Strategy 8.2.24 (Involution principle)

Let \( a_1, a_2, \ldots, a_n \in \mathbb{N} \). In order to evaluate an alternating sum \(\sum_{k=1}^{n} (-1)^k a_k\), it suffices to follow the following steps:

(i) Find a set \( X \) with a partition \(\{U_1, U_2, \ldots, U_n\}\), such that \(|U_k| = a_k\) for all \( k \in [n] \).

(ii) Find a parity-swapping involution \( h : D \to D \) for some subset \( D \subseteq X \)‚Äîoften it is easiest to specify the values of \( h \) first, and take \( D \) to be the set of elements of \( X \) for which the specification makes sense.

(iii) Evaluate \(\sum_{k=1}^{n} (-1)^k |F_k|\), where \( F_k = U_k \setminus D \) for all \( k \in [n] \)‚Äîthat is, count the elements of each \( U_k \) where the involution failed to be well-defined, and add them positively or negatively according to their parity.

It will often be the case that many of the sets \( F_k \) are empty, simplifying matters greatly.

---

This is rather abstract, so let‚Äôs see some examples of the involution principle in action.

### Example 8.2.25

Here is a succinct proof that \(\sum_{k=0}^{n} (-1)^k \binom{n}{k} = 0\) for all \( n \in \mathbb{N} \) using the involution principle.

Let \( n \in \mathbb{N} \) and define \( U_k = \binom{[n]}{k} \) for all \( 0 \leq k \leq n \)‚Äîthese sets form a partition of \(\mathcal{P}([n])\), and \(|U_k| = \binom{n}{k}\) for each \( 0 \leq k \leq n \).

By Exercise 8.2.12, the function \( h : \mathcal{P}([n]) \to \mathcal{P}([n]) \) defined by \( h(U) = U \oplus n \) is a parity-swapping involution. By the involution principle (Strategy 8.2.24) with \( D = \mathcal{P}([n]) \), we have \( U_k \setminus D = \emptyset \) for each \( 0 \leq k \leq n \), and hence

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} = 0
\]

as required.

### Exercise 8.2.26

Repeat Exercise 8.2.4 using the involution principle‚Äîthat is, use the involution principle to prove that

\[
\sum_{k=0}^{n} (-1)^k \cdot k \cdot \binom{n}{k} = 0
\]

for all \( n \geq 2 \).
</markdown><markdown>
### Exercise 8.2.27

Use the involution principle to prove that

\[
\sum_{k=0}^{n} (-1)^k \binom{n}{k} \binom{k}{\ell} = 0
\]

for all \( n, \ell \in \mathbb{N} \) with \( \ell < n \).

The next example is slightly more involved, because we find an involution that is not defined on the whole set being counted. This generalises the result of Example 8.2.25.

### Proposition 8.2.28

Let \( n, r \in \mathbb{N} \) with \( r \leq n \). Then

\[
\sum_{k=0}^{r} (-1)^k \binom{n}{k} = (-1)^r \binom{n-1}{r}.
\]

**Proof**

Let \( X \) be the set of subsets of \([n]\) of size \(\leq r\), and for each \( 0 \leq k \leq r \), let \( U_k = \binom{[n]}{k} \).

Note that the sets \( U_k \) partition \( X \) for \( 0 \leq k \leq r \).

Define \( h(U) = U \oplus n \) for all \( U \in X \). Since \( h \) is defined by toggling \( n \), it is a parity-swapping involution.

The only way that \( h \) can fail to be well-defined is if \(|h(U)| > r\). Since \(|U \oplus n| = |U| \pm 1\) for all \( U \in X \), the only way we can have \(|h(U)| > r\) is if \(|U| = r\) and \( n \notin U \), in which case \( h(U) = U \cup \{n\} \) has size \( r + 1 \).

Hence \( F_k = \emptyset \) for all \( k < r \), and \( F_r = \{ U \subseteq [n] \mid |U| = r \text{ and } n \notin U \} \). Specifying an element of \( F_r \) is therefore equivalent to specifying a subset of \([n-1]\) of size \( r \), so that

\[
|F_r| = \binom{n-1}{r}.
\]

Putting this all together, we obtain

\[
\sum_{k=0}^{r} (-1)^k \binom{n}{k} = \sum_{k=0}^{r} (-1)^k |F_k| = (-1)^r \binom{n-1}{r}
\]

as required.

The next example is slightly more colourful.

### Example 8.2.29

Let \( a, b, r \in \mathbb{N} \) with \( a \leq r \leq b \). We prove that

\[
\sum_{k=0}^{r} (-1)^k \binom{a}{k} \binom{b}{r-k} = \binom{b-a}{r}
\]
</markdown><markdown>
Consider a population of \( b \) animals, of which exactly \( a \) are cats. A government of exactly \( r \) animals must be formed, and a Feline Affairs Committee‚Äîwhich is a branch of the government‚Äîmust be chosen from amongst the cats. The Feline Affairs Committee may have any size, but its size is bounded by the size of the government.

Let \( X \) be the set of all pairs \( (G, C) \), where \( G \) is a government and \( C \subseteq G \) is the Feline Affairs Committee.

For \( k \leq r \), let \( U_k \) be the set of all government‚Äìcommittee pairs \( (G, C) \) such that \( |C| = k \)‚Äîthat is, such that exactly \( k \) cats sit on the Feline Affairs Committee. Note that parity is determined by the number of cats on the Feline Affairs Committee: indeed, \( (G, C) \) has even parity if \( |C| \) is even, and odd parity if \( |C| \) is odd.

Given a government‚Äìcommittee pair \( (G, C) \), let \( h(G, C) = (G, C \oplus x) \), where \( x \in G \) is the youngest cat on the government. That is, if the youngest cat on the government is on the Feline Affairs Committee, then that cat is removed from the committee; and if the youngest cat on the government is not on the Feline Affairs Committee, then that cat is added to the committee.

Evidently \( h \) is an involution, and it swaps parity since it adds or removes one cat to or from the Feline Affairs Committee.

The only way that \( h \) can fail to be well-defined is if there are no cats on the government, in which case \( k = 0 \). Thus by the involution principle

\[
\sum_{k=0}^{r} (-1)^k \binom{a}{k} \binom{b}{r-k} = (-1)^0 \cdot \left| \left\{ (G, \emptyset) \in X \mid G \text{ contains no cats} \right\} \right|
\]

But there are exactly \( b-a \) non-cats in the animal population, so that

\[
\left| \left\{ (G, \emptyset) \in X \mid G \text{ contains no cats} \right\} \right| = \binom{b-a}{r}
\]

and hence we have

\[
\sum_{k=0}^{r} (-1)^k \binom{a}{k} \binom{b}{r-k} = \binom{b-a}{r}, \text{ as required.}
\]

If you dislike reasoning about animals, Example 8.2.29 could be reformulated by taking:

- \( X = \{ (A, B) \mid A \subseteq B \cap [a], B \subseteq [b], |B| = r \} \);
- \( U_k = \{ (A, B) \in X \mid |A| = k \} \) for all \( k \leq r \); and
- \( h(A, B) = h(A \oplus x, B) \), where \( x \) is the least element of \( B \cap [a] \).

You are encouraged to verify the details!
</markdown><markdown>
### Exercise 8.2.30

Let \( n \in \mathbb{N} \) and consider the set

\[ 
X = \{(k, i) \mid k \leq n, \, i \in [k]\} 
\]

For example, if \( n = 3 \) then \( X = \{(1, 1), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3)\} \).

(a) Prove that \(|X| = \sum_{k=0}^{n} k\).

(b) Use the involution principle to prove that

\[
\sum_{k=0}^{n} (-1)^k k = 
\begin{cases} 
\frac{n}{2} & \text{if } n \text{ is even} \\ 
\frac{n+1}{2} & \text{if } n \text{ is odd} 
\end{cases}
\]

### Inclusion‚Äìexclusion principle

Our final application of the involution principle will be to prove the inclusion‚Äìexclusion principle, which is used for computing the sizes of unions of sets that are not necessarily pairwise disjoint.

We saw in Proposition 6.1.19 how to compute the size of a union of two not-necessarily-disjoint sets:

\[
|X \cup Y| = |X| + |Y| - |X \cap Y|
\]

So far so good. But what if we have three or four sets instead of just two?

### Exercise 8.2.31

Let \( X, Y, Z \) be sets. Show that

\[
|X \cup Y \cup Z| = |X| + |Y| + |Z| - |X \cap Y| - |X \cap Z| - |Y \cap Z| + |X \cap Y \cap Z|
\]

Let \( W \) be another set. Derive a similar formula for \(|W \cup X \cup Y \cup Z|\).

The inclusion‚Äìexclusion principle is a generalisation of Exercise 8.2.31 to arbitrary finite collections of finite sets, but it is stated in a slightly different way in order to make the proof more convenient.
</markdown><markdown>
### Theorem 8.2.32 (Inclusion‚Äìexclusion principle)

Let \( n \in \mathbb{N} \), let \( X_i \) be a finite set for each \( i \in [n] \), and let \( X = X_1 \cup X_2 \cup \cdots \cup X_n \). Then

\[
\sum_{I \subseteq [n]} (-1)^{|I|} |X_I| = 0
\]

where \( X_I = \{ a \in X \mid a \in X_i \text{ for all } i \in I \} \).

The statement of Theorem 8.2.32 looks fairly abstract, so before we prove it, let‚Äôs examine its content. The sum is over all subsets \( I \subseteq [n] \), and then the power \((-1)^{|I|}\) is equal to 1 if \( I \) has an even number of elements, and \(-1\) if \( I \) has an odd number of elements. Moreover, if \( I \) is inhabited then \( X_I \) is the intersection of the sets \( X_i \) for \( i \in I \)‚Äîfor example \( X_{\{2,3,5\}} = X_2 \cap X_3 \cap X_5 \); on the other hand, a careful examination of the definition of \( X_I \) reveals that \( X_\emptyset = X \).

Thus when \( n = 3 \), the sum \(\sum_{I \subseteq [3]} (-1)^{|I|} |X_I|\) can be evaluated as

\[
|X| - |X_1| - |X_2| - |X_3| + |X_1 \cap X_2| + |X_1 \cap X_3| + |X_2 \cap X_3| - |X_1 \cap X_2 \cap X_3|
\]

The theorem says that this sum is equal to zero, and solving for \(|X| = |X_1 \cup X_2 \cup X_3|\) yields an equivalent equation to that in Exercise 8.2.31.

#### Proof of Theorem 8.2.32

We will prove the inclusion‚Äìexclusion principle using the involution principle.

First we introduce some notation:

- Define \( S = \{ (I, a) \mid I \subseteq [n], a \in X_I \} \). We can think of an element \((I, a) \in S\) as being an element \( a \in X \) together with a label \( I \) indicating that \( a \in X_i \) for all \( i \in I \).

- For each \( 0 \leq k \leq n \), define \( S_k = \{ (I, a) \in S \mid |I| = k \} \).

- For each \( a \in X \), let \( i_a = \min \{ k \in [n] \mid a \in X_k \} \).

Note that the sets \( S_0, S_1, S_2, \ldots, S_n \) form a partition of \( S \), so we can consider the parity of an element \((I, a) \in S\)‚Äînamely, the parity of \((I, a)\) is even if \(|I|\) is even, and odd if \(|I|\) is odd.

Define a function \( f : S \to S \) by letting

\[
f(I, a) = (I \oplus i_a, a)
\]

for each \( I \subseteq [n] \) and each \( a \in X_I \). Then:

- \( f \) is an involution since by Exercise 8.2.12 we have

\[
f(f(I, a)) = f(I \oplus i_a, a) = ((I \oplus i_a) \oplus i_a, a) = (I, a)
\]
</markdown><markdown>
- \( f \) is parity-swapping, since \(|I \oplus i_a|\) and \(|I|\) have opposite parity for each \(a \in X\).

By the involution principle, we have

\[
\sum_{k=0}^{n} (-1)^k |S_k| = 0
\]

Now for fixed \(I \subseteq [n]\), let \(T_I = \{(I, a) \mid a \in X\}\). Then for each \(0 \leq k \leq n\), the sets \(T_I\) for \(|I| = k\) partition \(S_k\), and moreover \((-1)^k = (-1)^{|I|}\), so that by the addition principle we have

\[
\sum_{k=0}^{n} (-1)^k |S_k| = \sum_{k=0}^{n} \sum_{I \in \binom{[n]}{k}} (-1)^{|I|} |T_I| = \sum_{I \subseteq [n]} (-1)^{|I|} |T_I| = 0
\]

Finally note that, for each \(I \subseteq [n]\), the function \(g_I : X_I \to T_I\) defined by \(g_I(a) = (I, a)\) for all \(a \in X_I\) is a bijection, with inverse given by \(g_I^{-1}(I, a) = a\) for all \((I, a) \in T_I\).

Hence \(|X_I| = |T_I|\), and the result is proved.

It is more common to see the inclusion‚Äìexclusion principle stated in one two equivalent forms, stated here as Corollaries 8.2.33 and 8.2.34.

### Corollary 8.2.33
Let \(X_1, X_2, \ldots, X_n\) be sets. Then

\[
\left| \bigcup_{i=1}^{n} X_i \right| = \sum_{k=1}^{n} \left( \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} (-1)^{k-1} |X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}| \right)
\]

**Proof**

Moving all terms to the left-hand side of the equation and observing that \(-(-1)^{k-1} = (-1)^k\), the statement is equivalent to

\[
\left| \bigcup_{i=1}^{n} X_i \right| - \sum_{k=1}^{n} \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} (-1)^k |X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}| = 0
\]

But using the notation of Theorem 8.2.32, we have

\[
\left| \bigcup_{i=1}^{n} X_i \right| = |X| = (-1)^{|\emptyset|} |X_{\emptyset}|
\]

and for all \(1 \leq i_1 < i_2 < \cdots < i_k \leq n\), we have

\[
(-1)^k |X_{i_1} \cap X_{i_2} \cap \cdots \cap X_{i_k}| = (-1)^{| \{i_1, i_2, \ldots, i_k\} |} |X_{\{i_1, i_2, \ldots, i_k\}}|
\]

and so we see that this is just a restatement of Theorem 8.2.32.
</markdown><markdown>
### Corollary 8.2.34

Let \( X \) be a set and let \( U_1, U_2, \ldots, U_n \subseteq X \). Then

\[
\left| X \setminus \bigcup_{i=1}^{n} U_i \right| = |X| + \sum_{k=1}^{n} \left( \sum_{1 \leq i_1 < i_2 < \cdots < i_k \leq n} (-1)^k |U_{i_1} \cap U_{i_2} \cap \cdots \cap U_{i_k}| \right)
\]

**Proof**

Since \(\bigcup_{i=1}^{n} U_i \subseteq X\), we have

\[
\left| X \setminus \bigcup_{i=1}^{n} U_i \right| = |X| - \left| \bigcup_{i=1}^{n} U_i \right|
\]

The result then follows immediately from [Corollary 8.2.33](#).

### Strategy 8.2.35 (Finding the size of a union by inclusion‚Äìexclusion)

In order to find the size of a union of \(\bigcup_{i=1}^{n} X_i\), it suffices to:

- Add the sizes of the individual sets \(X_i\);
- Subtract the sizes of the double-intersections \(X_i \cap X_j\);
- Add the sizes of the triple-intersections \(X_i \cap X_j \cap X_k\);
- Subtract the sizes of the quadruple-intersections \(X_i \cap X_j \cap X_k \cap X_\ell\);
- \(\ldots\) and so on \(\ldots\)

Continue alternating until the intersection of all the sets is covered.

### Example 8.2.36

We count how many subsets of \([12]\) contain a multiple of 3. Precisely, we count the number of elements of the set

\[
X_3 \cup X_6 \cup X_9 \cup X_{12}
\]

where \(X_k = \{ S \subseteq [12] \mid k \in S \}\). We will apply the inclusion‚Äìexclusion principle:

(i) An element \(S \in X_3\) is precisely a set of the form \(\{3\} \cup S'\), where \(S' \subseteq [12] \setminus \{3\}\). Since \([12] \setminus \{3\}\) has 11 elements, there are \(2^{11}\) such subsets. So \(|X_3| = 2^{11}\), and likewise \(|X_6| = |X_9| = |X_{12}| = 2^{11}\).

(ii) An element \(S \in X_3 \cap X_6\) is a set of the form \(\{3, 6\} \cup S'\), where \(S' \subseteq [12] \setminus \{3, 6\}\). Thus there are \(2^{10}\) such subsets, so \(|X_3 \cap X_6| = 2^{10}\). And likewise

\[
|X_3 \cap X_9| = |X_3 \cap X_{12}| = |X_6 \cap X_9| = |X_6 \cap X_{12}| = |X_9 \cap X_{12}| = 2^{10}
\]
</markdown><markdown>
(iii) Reasoning as in the last two cases, we see that

\[
|X_3 \cap X_6 \cap X_9| = |X_3 \cap X_6 \cap X_{12}| = |X_3 \cap X_9 \cap X_{12}| = |X_6 \cap X_9 \cap X_{12}| = 2^9
\]

(iv) \ldots and \(|X_3 \cap X_6 \cap X_9 \cap X_{12}| = 2^8\).

Thus the number of subsets of \([12]\) which contain a multiple of 3 is

\[
\underbrace{4 \times 2^{11}}_{\text{by (i)}} - \underbrace{6 \times 2^{10}}_{\text{by (ii)}} + \underbrace{4 \times 2^9}_{\text{by (iii)}} - \underbrace{2^8}_{\text{by (iv)}}
\]

which is equal to 3840.

**Exercise 8.2.37**  
How many natural numbers less than 1000 are multiples of 2, 3, 5 or 7?
</markdown><markdown>
## Chapter 8 exercises

### Finite sets

8.1. Let \( n \in \mathbb{N} \) and let \( f : [n] \to [n] \) be a function. Prove that \( f \) is injective if and only if \( f \) is surjective.

8.2. Prove that \( |\mathbb{Z}/n\mathbb{Z}| = n \) for all \( n \geq 1 \).

8.3. Let \( A, B \) and \( C \) be sets. Prove that if \( A \triangle B \) and \( B \triangle C \) are finite, then \( A \triangle C \) is finite, where the notation \( \triangle \) refers to the symmetric difference operation (Definition 2.E.1)‚Äîthat is, \( A \triangle B = (A \setminus B) \cup (B \setminus A) \), and so on.

### Counting

8.4. Let \( X \) and \( Y \) be finite sets with \( |X| = m \in \mathbb{N} \) and \( |Y| = n \in \mathbb{N} \). Prove that there are \( 2^{mn} \) relations from \( X \) to \( Y \).

8.5. Let \( X \) be a set and let \( R \) be a relation on \( X \). Prove that \( R \) is reflexive if and only if \( \Delta_X \subseteq \text{Gr}(R) \), where \( \Delta_X \) is the diagonal subset of \( X \times X \) (see Definition 5.1.17). Deduce that if \( X \) is finite and \( |X| = n \in \mathbb{N} \), then there are \( 2^{n(n-1)} \) reflexive relations on \( X \).

8.6. Let \( X \) be a finite set with \( |X| = n \in \mathbb{N} \). Prove that there are \( 2^{\binom{n}{2}} \cdot 2^n \) symmetric relations on \( X \).

8.7. Let \( X \) be a finite set with \( |X| = n \in \mathbb{N} \). Prove that there are \( 3^{\binom{n}{2}} \cdot 2^n \) antisymmetric relations on \( X \).

8.8. Let \( X \) be a finite set with \( |X| = n \in \mathbb{N} \), let \( \sim \) be an equivalence relation on \( X \), and suppose that there is some natural number \( k \) such that \( |[a]_\sim| = k \) for all \( a \in X \). Prove that \( k \) divides \( n \), and that \( |X/\sim| = \frac{n}{k} \).

8.9. Let \( n, k \in \mathbb{N} \) with \( k \leq n \). Prove that the number of functions \( f : [n] \to [n] \) that fix exactly \( k \) elements of \( [n] \) is equal to \( \binom{n}{k} (n-1)^{n-k} \).

### Double counting

8.10. Let \( a, b, m, n \in \mathbb{N} \). Prove each of the following by double counting.
</markdown><markdown>
### Section 8.E. Chapter 8 exercises

(a) \( a(m+n) = am + an \)  
(b) \( a^{m+n} = a^m \cdot a^n \)  
(c) \( (a^m)^n = a^{mn} \)  
(d) \( (ab)^n = a^n \cdot b^n \)

8.11. Prove that 

\[
\sum_{k=0}^{n} \binom{n}{k}^2 = \binom{2n}{n}
\]

for all \( n \in \mathbb{N} \).

8.12. Prove that 

\[
\sum_{k=m}^{n} \binom{n}{k} \binom{k}{m} = 2^{n-m} \binom{n}{m}
\]

for all \( m, n \in \mathbb{N} \) with \( m \leq n \).

8.13. Prove that 

\[
\sum_{j=0}^{k} \binom{n-j}{k-j} = \binom{n+1}{k}
\]

for all \( n, k \in \mathbb{N} \) with \( k \leq n \).

8.14. Prove that 

\[
\sum_{k=1}^{n} \sum_{\ell=0}^{k} \binom{n}{k} \binom{n-k}{\ell} = n \cdot 3^{n-1}
\]

for all \( n \in \mathbb{N} \).

8.15. Prove that 

\[
\binom{n}{r+s+1} = \sum_{k=r+1}^{n-s} \binom{k-1}{r} \binom{n-k}{s}
\]

for all \( n, r, s \in \mathbb{N} \).

8.16. Let \( a_1, a_2, \ldots, a_r \in \mathbb{N} \) and let \( n = a_1 + a_2 + \cdots + a_r \). Prove that 

\[
\binom{n}{(a_1, a_2, \ldots, a_r)} = \prod_{k=0}^{r-1} \left( n - \sum_{i=1}^{k} a_i \right) \binom{n}{a_{k+1}}
\]

where 

\[
\binom{n}{(a_1, a_2, \ldots, a_r)}
\]

is the number of ordered \( r \)-tuples \( (U_1, U_2, \ldots, U_r) \) such that \( U_1, U_2, \ldots, U_r \) is a partition of \([n]\) and \(|U_k| = a_k\) for all \( k \in [r] \).

### Alternating sums

8.17. Let \( X \) be a finite set. Prove that if \(|X|\) is odd then there is no parity-swapping involution \( X \to X \).

8.18. Find the number of subsets of \([100]\) that do not contain a multiple of 8.

### Miscellaneous exercises

8.19. Let \( n \in \mathbb{N} \). For each \( k \in \mathbb{N} \), find the coefficient of \( x^k \) in the polynomial \( p(x) \) defined by \( p(x) = (1 + x + x^2)^n \).
</markdown><markdown>
8.20. Fix \( n \in \mathbb{N} \). For each \( k \in \mathbb{N} \), find the coefficient of \( x^k \) in the polynomial \( p(x) \) defined by 

\[
p(x) = \sum_{i=0}^{n} x^{n-i}(1+x)^i.
\]

### True‚ÄìFalse questions

In Questions 8.21 to 8.25, determine (with proof) whether the statement is true or false.

8.21. Every finite set has a unique enumeration.

8.22. The union of a finite number of finite sets is finite.

8.23. Every finite set is a proper subset of another finite set.

8.24. There is a finite set with infinitely many subsets.

8.25. For every partition \(\mathcal{U} = \{U_0, U_1, \ldots, U_n\}\) of a finite set \( X \), there is a function \( f : X \to X \) that is a parity-swapping involution with respect to \(\mathcal{U}\).

### Always‚ÄìSometimes‚ÄìNever questions

In Questions 8.26 to 8.32, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

8.26. Let \( X \) and \( Y \) be finite sets and let \( f : X \to Y \) be a function. If \( |X| \leq |Y| \), then \( f \) is injective.

8.27. Let \( X \) be a finite set. Then there is some proper subset \( U \subsetneq X \) such that \( |U| = |X| \).

8.28. Let \( X \) be an infinite set. Then \( \mathbb{N} \subseteq X \).

8.29. Let \( X \) be a finite set. Then \( \mathcal{P}(X) \) is finite.

8.30. Let \( a, b \in \mathbb{R} \) with \( a < b \). Then the set \([a, b] \cap \mathbb{Q}\) is finite.

8.31. Let \( X \) and \( Y \) be sets with \( X \) finite, and let \( f : X \to Y \) be an injection. Then \( Y \) is finite.

8.32. Let \( X \) and \( Y \) be sets with \( X \) finite, and let \( f : X \to Y \) be a surjection. Then \( Y \) is finite.
</markdown><markdown>
# Chapter 9

## Real numbers
</markdown><markdown>
## Section 9.1  
Inequalities and means

We first encountered the real numbers in [Chapter 0](#), when the real numbers were introduced using a vague (but intuitive) notion of an *infinite number line* ([Definition 0.25](#)):

\[
\begin{array}{cccccccccc}
-5 & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & 5 \\
\end{array}
\]

This section will scrutinise the set of real numbers in its capacity as a *complete ordered field*. Decomposing what this means:

- A *field* is a set with a notion of ‚Äòzero‚Äô and ‚Äòone‚Äô, in which it makes sense to talk about addition, subtraction, multiplication, and division by everything except zero. Examples are \(\mathbb{Q}\), \(\mathbb{R}\), and \(\mathbb{Z}/p\mathbb{Z}\) when \(p\) is a prime number (but not when \(p\) is composite). However, \(\mathbb{Z}\) is not a field, since we can‚Äôt freely divide by nonzero elements‚Äîfor example, \(1 \in \mathbb{Z}\) and \(2 \in \mathbb{Z}\), but no integer \(n\) satisfies \(2n = 1\).

- An *ordered field* is a field which is equipped with a well-behaved notion of order. Both \(\mathbb{Q}\) and \(\mathbb{R}\) are ordered fields, but \(\mathbb{Z}/p\mathbb{Z}\) is not. We‚Äôll see why soon.

- A *complete ordered field* is an ordered field in which every set with an upper bound has a *least upper bound*. As we will see, \(\mathbb{Q}\) is not a complete ordered field, but \(\mathbb{R}\) is.

This is made (extremely) precise in [Section B.2](#).

## Magnitude and scalar product

In this part of the section, we home in on sets of the form \(\mathbb{R}^n\), for \(n \in \mathbb{N}\). Elements of \(\mathbb{R}^n\) are sequences of the form \((x_1, x_2, \ldots, x_n)\), where each \(x_i \in \mathbb{R}\). With our interpretation of the reals \(\mathbb{R}\) as a *line*, we can interpret a sequence \((x_1, x_2, \ldots, x_n)\) as a point in *n-dimensional space*:

- 0-dimensional space is a single point. The set \(\mathbb{R}^0\) has one element, namely the empty sequence \(()\), so this makes sense.

- 1-dimensional space is a line. This matches our intuition that \(\mathbb{R} = \mathbb{R}^1\) forms a line.

- 2-dimensional space is a *plane*. The elements of \(\mathbb{R}^2\) are pairs \((x, y)\), where \(x\) and \(y\) are both real numbers. We can interpret the pair \((x, y)\) as *coordinates* for a point which is situated \(x\) units to the right of \((0, 0)\) and \(y\) units above \((0, 0)\) (where negative values of \(x\) or \(y\) reverse this direction)‚Äîsee [Figure 9.1](#).
</markdown><markdown>
With this intuition in mind, we set up the following notation.

### Notation 9.1.1
Let \( n \in \mathbb{N} \). Elements of \( \mathbb{R}^n \) will be denoted \( \vec{x}, \vec{y}, \vec{z}, \ldots \) (\LaTeX{} code: `\vec`) and called (n-dimensional) vectors. Given a vector \( \vec{x} \in \mathbb{R}^n \), we write \( x_i \) for the \( i \)th component of \( \vec{x} \), so that

\[
\vec{x} = (x_1, x_2, \ldots, x_n)
\]

The element \( (0, 0, \ldots, 0) \in \mathbb{R}^n \) is called the **origin** or **zero vector** of \( \mathbb{R}^n \), and is denoted by \( \vec{0} \).

Moreover, if \( \vec{x}, \vec{y} \in \mathbb{R}^n \) and \( a \in \mathbb{R} \) we write

\[
\vec{x} + \vec{y} = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \quad \text{and} \quad a\vec{x} = (ax_1, ax_2, \ldots, ax_n)
\]

### Example 9.1.2
For all \( \vec{x} \in \mathbb{R}^n \), we have

\[
\vec{x} + \vec{0} = \vec{x} \quad \text{and} \quad 1\vec{x} = \vec{x}
\]
</markdown><markdown>
### Definition 9.1.3

Let \(\vec{x} \in \mathbb{R}^n\). The **magnitude** of \(\vec{x}\) is the real number \(\|\vec{x}\|\) (LaTeX code: `\lVert \vec{x} \rVert`) defined by

\[
\|\vec{x}\| = \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}
\]

Given vectors \(\vec{x}, \vec{y} \in \mathbb{R}^n\), the **distance** from \(\vec{x}\) to \(\vec{y}\) is defined to be \(\|\vec{y} - \vec{x}\|\). Thus the magnitude of a vector can be thought of as the distance from that vector to the origin.

### Example 9.1.4

In \(\mathbb{R}^2\), Definition 9.1.3 says that

\[
\| (x, y) \| = \sqrt{x^2 + y^2}
\]

This matches the intuition obtained from the Pythagorean theorem on the sides of right-hand triangles. For example, consider the triangle with vertices \((0,0)\), \((4,0)\) and \((4,3)\):

The hypotenuse of the triangle has magnitude

\[
\| (4, 3) \| = \sqrt{4^2 + 3^2} = \sqrt{25} = 5
\]

### Exercise 9.1.5

Let \(\vec{x}, \vec{y} \in \mathbb{R}^n\). Prove that \(\|\vec{x} - \vec{y}\| = \|\vec{y} - \vec{x}\|\). That is, the distance from \(\vec{x}\) to \(\vec{y}\) is equal to the distance from \(\vec{y}\) to \(\vec{x}\).

### Exercise 9.1.6

Prove that if \(x \in \mathbb{R}\) then the magnitude \(\| (x) \|\) is equal to the absolute value \(|x|\).

### Exercise 9.1.7

Let \(\vec{x} \in \mathbb{R}^n\). Prove that \(\|\vec{x}\| = 0\) if and only if \(\vec{x} = \vec{0}\).
</markdown><markdown>
## The triangle inequality and the Cauchy‚ÄìSchwarz inequality

The first, and simplest, inequality that we investigate is the (one-dimensional version of the) **triangle inequality** (Theorem 9.1.9). It is so named because of a generalisation to higher dimensions (Theorem 9.1.19), which can be interpreted geometrically as saying that the sum of two side lengths of a triangle is greater than or equal to the third side length.

The triangle inequality is used very frequently in mathematical proofs‚Äîyou will encounter it repeatedly in this chapter‚Äîyet its proof is surprisingly simple.

Before we can prove the triangle inequality, we need the following fact about square roots of real numbers.

### Lemma 9.1.8
Let \( x, y \in \mathbb{R} \). If \( 0 \leq x \leq y \), then \( \sqrt{x} \leq \sqrt{y} \).

**Proof**  
Suppose \( 0 \leq x \leq y \). Note that, by definition of the square root symbol, we have \( \sqrt{x} \geq 0 \) and \( \sqrt{y} \geq 0 \).

Suppose \( \sqrt{x} > \sqrt{y} \). By two applications of Theorem B.2.36(d), we have

\[
y = \sqrt{y} \cdot \sqrt{y} < \sqrt{x} \cdot \sqrt{x} = x
\]

so that \( y < x \). But this contradicts the assumption that \( x \leq y \). Hence \( \sqrt{x} \leq \sqrt{y} \), as required.

### Theorem 9.1.9 (Triangle inequality in one dimension)
Let \( x, y \in \mathbb{R} \). Then \( |x+y| \leq |x| + |y| \). Moreover, \( |x+y| = |x| + |y| \) if and only if \( x \) and \( y \) have the same sign.

**Proof**  
Note first that \( xy \leq |xy| \); indeed, \( xy \) and \( |xy| \) are equal if \( xy \) is non-negative, and otherwise we have \( xy < 0 < |xy| \). Also \( x^2 = |x|^2 \) and \( y^2 = |y|^2 \). Hence

\[
(x+y)^2 = x^2 + 2xy + y^2 \leq |x|^2 + 2|xy| + |y|^2 = (|x| + |y|)^2
\]

Taking (nonnegative) square roots yields

\[
|x+y| \leq ||x| + |y||
\]

by Lemma 9.1.8. But \( |x| + |y| \geq 0 \), so \( ||x| + |y|| = |x| + |y| \). This completes the first part of the proof.

Equality holds in the above if and only if \( xy = |xy| \), which occurs if and only if \( xy \geq 0 \). But this is true if and only if \( x \) and \( y \) are both non-negative or both non-positive‚Äîthat is, they have the same sign.
</markdown><markdown>
### Example 9.1.10

Let \( x, y \in \mathbb{R} \). We prove that

\[
\frac{|x+y|}{1+|x+y|} \leq \frac{|x|}{1+|x|} + \frac{|y|}{1+|y|}
\]

First note that, if \( 0 \leq s \leq t \), then

\[
\frac{s}{1+s} \leq \frac{t}{1+t}
\]

To see this, note that

\[
s \leq t \Rightarrow 1+s \leq 1+t \quad \text{rearranging}
\]

\[
\Rightarrow \frac{1}{1+t} \leq \frac{1}{1+s} \quad \text{since } 1+s, 1+t > 0
\]

\[
\Rightarrow 1 - \frac{1}{1+s} \leq 1 - \frac{1}{1+t} \quad \text{rearranging}
\]

\[
\Rightarrow \frac{s}{1+s} \leq \frac{t}{1+t} \quad \text{rearranging}
\]

Now letting \( s = |x+y| \) and \( t = |x| + |y| \), we have \( s \leq t \) by the triangle inequality, and hence

\[
\frac{|x+y|}{1+|x+y|} \leq \frac{|x|}{1+|x|+|y|} + \frac{|y|}{1+|x|+|y|} \leq \frac{|x|}{1+|x|} + \frac{|y|}{1+|y|}
\]

as required.

### Exercise 9.1.11

Let \( n \in \mathbb{N} \) and let \( x_i \in \mathbb{R} \) for each \( i \in [n] \). Prove that

\[
\left| \sum_{i=1}^{n} x_i \right| \leq \sum_{i=1}^{n} |x_i|
\]

with equality if and only if the numbers \( x_i \) are either all non-positive or all non-negative.

### Exercise 9.1.12

Let \( x, y \in \mathbb{R} \). Prove that

\[
||x| - |y|| \leq |x-y|
\]

We will generalise the triangle inequality to arbitrary dimensions in Theorem 9.1.19. Our proof will go via the **Cauchy‚ÄìSchwarz inequality** (Theorem 9.1.16). To motivate the Cauchy‚ÄìSchwarz inequality, we introduce another geometric notion called the scalar product of two vectors.
</markdown><markdown>
### Definition 9.1.13

Let \(\vec{x}, \vec{y} \in \mathbb{R}^n\). The **scalar product** (or **dot product**) of \(\vec{x}\) with \(\vec{y}\) is the real number \(\vec{x} \cdot \vec{y}\) (LaTeX code: `\cdot`) defined by

\[
\vec{x} \cdot \vec{y} = \sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \cdots + x_n y_n
\]

### Example 9.1.14

Let \(\vec{x} \in \mathbb{R}^n\). Then \(\vec{x} \cdot \vec{x} = \|\vec{x}\|^2\). Indeed

\[
\vec{x} \cdot \vec{x} = \sum_{i=1}^{n} x_i^2 = \|\vec{x}\|^2
\]

### Exercise 9.1.15

Let \(\vec{x}, \vec{y}, \vec{z}, \vec{w} \in \mathbb{R}^n\) and let \(a, b, c, d \in \mathbb{R}\). Prove that

\[
(a\vec{x} + b\vec{y}) \cdot (c\vec{z} + d\vec{w}) = ac(\vec{x} \cdot \vec{z}) + ad(\vec{x} \cdot \vec{w}) + bc(\vec{y} \cdot \vec{z}) + bd(\vec{y} \cdot \vec{w})
\]

Intuitively, the scalar product of two vectors \(\vec{x}\) and \(\vec{y}\) measures the extent to which \(\vec{x}\) and \(\vec{y}\) fail to be **orthogonal**. In fact, if \(\theta\) is the acute angle formed between the lines \(\ell_1\) and \(\ell_2\), where \(\ell_1\) passes through \(\vec{0}\) and \(\vec{x}\) and \(\ell_2\) passes through \(\vec{0}\) and \(\vec{y}\), then a formula for the scalar product of \(\vec{x}\) and \(\vec{y}\) is given by

\[
\vec{x} \cdot \vec{y} = \|\vec{x}\| \|\vec{y}\| \cos \theta
\]

Evidently, \(\vec{x}\) and \(\vec{y}\) are orthogonal if and only if \(\cos \theta = 0\), in which case \(\vec{x} \cdot \vec{y} = 0\) as well. We cannot prove this yet, though, as we have not yet defined trigonometric functions or explored their properties, but hopefully this provides some useful intuition.
</markdown><markdown>
The Cauchy‚ÄìSchwarz inequality provides a useful comparison of the size of a scalar product of two vectors with the magnitudes of the vectors.

### Theorem 9.1.16 (Cauchy‚ÄìSchwarz inequality)
Let \( n \in \mathbb{N} \) and let \( x_i, y_i \in \mathbb{R} \) for each \( i \in [n] \). Then

\[
|\vec{x} \cdot \vec{y}| \leq \|\vec{x}\| \|\vec{y}\|
\]

with equality if and only if \( a\vec{x} = b\vec{y} \) for some \( a, b \in \mathbb{R} \) which are not both zero.

**Proof**

If \( \vec{y} = \vec{0} \), then this is trivial: both sides of the equation are equal to zero! So assume that \( \vec{y} \neq \vec{0} \). In particular, by Exercise 9.1.7, we have \( \|\vec{y}\| > 0 \).

Define \( k = \frac{\vec{x} \cdot \vec{y}}{\|\vec{y}\|^2} \). Then

\[
\begin{align*}
0 &\leq \|\vec{x} - k\vec{y}\|^2 \quad \text{since squares are nonnegative} \\
&= (\vec{x} - k\vec{y}) \cdot (\vec{x} - k\vec{y}) \quad \text{by Example 9.1.14} \\
&= (\vec{x} \cdot \vec{x}) - 2k(\vec{x} \cdot \vec{y}) + k^2(\vec{y} \cdot \vec{y}) \quad \text{by Exercise 9.1.15} \\
&= \|\vec{x}\|^2 - \frac{(\vec{x} \cdot \vec{y})^2}{\|\vec{y}\|^2} \quad \text{by definition of } k
\end{align*}
\]

Multiplying through by \( \|\vec{y}\|^2 \), which is non-negative and therefore doesn‚Äôt change the sign of the inequality, yields

\[
0 \leq \|\vec{x}\|^2 \|\vec{y}\|^2 - (\vec{x} \cdot \vec{y})^2
\]

which is equivalent to what was to be proved.

Evidently, equality holds if and only if \( \|\vec{x} - k\vec{y}\| = 0 \), which by Exercise 9.1.7 occurs if and only if \( \vec{x} - k\vec{y} = 0 \). Now:

- If \( \vec{x} - k\vec{y} = 0 \), then we have

\[
\vec{x} - k\vec{y} = 0
\]

\[
\Leftrightarrow \vec{x} - \frac{\vec{x} \cdot \vec{y}}{\|\vec{y}\|^2} \vec{y} = 0 \quad \text{by definition of } k
\]

\[
\Leftrightarrow \|\vec{y}\|^2 \vec{x} = (\vec{x} \cdot \vec{y}) \vec{y} \quad \text{rearranging}
\]

If \( \vec{y} \neq \vec{0} \) then let \( a = \|\vec{y}\|^2 \) and \( b = \vec{x} \cdot \vec{y} \); otherwise, let \( a = 0 \) and \( b = 1 \). In both cases, we have \( a\vec{x} = b\vec{y} \) and \( a, b \) are not both zero.

If \( a\vec{x} = b\vec{y} \) for some \( a, b \in \mathbb{R} \) not both zero, then either:
</markdown><markdown>
### Section 9.1. Inequalities and means

- \( a = 0 \) and \( b \neq 0 \), in which case \(\vec{y} = 0\) and we have equality in the Cauchy‚ÄìSchwarz inequality; or
- \( a \neq 0 \), in which case \(\vec{y} = \frac{b}{a} \vec{x}\). Write \( c = \frac{b}{a} \). Then

\[
|\vec{x} \cdot \vec{y}| = |\vec{x} \cdot (c\vec{x})| \\
= |c(\vec{x} \cdot \vec{x})| \quad \text{by Exercise 9.1.15} \\
= |c| \|\vec{x}\|^2 \quad \text{by Example 9.1.14} \\
= \|\vec{x}\| \|c\vec{x}\| \quad \text{rearranging} \\
= \|\vec{x}\| \|\vec{y}\|
\]

In either case, we have equality in the Cauchy‚ÄìSchwarz inequality.

So equality holds if and only if \( a\vec{x} = b\vec{y} \) for some \( a, b \in \mathbb{R} \) not both zero.

#### Example 9.1.17
Let \( a, b, c \in \mathbb{R} \). We‚Äôll prove that

\[
ab + bc + ca \leq a^2 + b^2 + c^2
\]

and examine when equality holds.

Letting \(\vec{x} = (a, b, c)\) and \(\vec{y} = (b, c, a)\) yields

\[
\vec{x} \cdot \vec{y} = ab + bc + ca
\]

and

\[
\|\vec{x}\| = \sqrt{a^2 + b^2 + c^2} = \sqrt{b^2 + c^2 + a^2} = \|\vec{y}\|
\]

Hence \(\|\vec{x}\| \|\vec{y}\| = a^2 + b^2 + c^2\). By the Cauchy‚ÄìSchwarz inequality, it follows that

\[
\vec{x} \cdot \vec{y} = ab + bc + ca \leq a^2 + b^2 + c^2 = \|\vec{x}\| \|\vec{y}\|
\]

as required. Equality holds if and only if \( k(a, b, c) = \ell(b, c, a) \) for some \( k, \ell \in \mathbb{R} \) not both zero. We may assume \( k \neq 0 \)‚Äîotherwise, swap the vectors \(\vec{x}\) and \(\vec{y}\) in what follows. Then, letting \( t = \frac{\ell}{k} \), we have

\[
k(a, b, c) = \ell(b, c, a) \\
\Leftrightarrow (a, b, c) = (tb, tc, ta) \quad \text{by definition of } t \\
\Leftrightarrow (a, b, c) = (t^2c, t^2a, t^2b) \quad \text{substituting } a = tb \text{ etc.} \\
\Leftrightarrow (a, b, c) = (t^3a, t^3b, t^3c) \quad \text{substituting } a = tb \text{ etc. again} \\
\Leftrightarrow \vec{x} = t^3 \vec{x}
\]

This occurs if and only if either \( (a, b, c) = (0, 0, 0) \), or \( t = 1 \), in which case

\[
(a, b, c) = (tb, tc, ta) = (b, c, a)
\]

So equality holds if and only if \( a = b = c \).
</markdown><markdown>
### Exercise 9.1.18

Let \( r \in \mathbb{N} \) and let \( a_1, a_2, \ldots, a_r \in \mathbb{R} \) be such that \( a_1^2 + a_2^2 + \cdots + a_n^2 = 6 \). Prove that

\[
(a_1 + 2a_2 + \cdots + na_n)^2 \leq n(n+1)(2n+1)
\]

and determine when equality holds.

We now use the Cauchy‚ÄìSchwarz inequality to generalise the one-dimensional version of the triangle inequality (Theorem 9.1.9) to arbitrary (finite) dimensions.

### Theorem 9.1.19 (Triangle inequality)

Let \(\vec{x}, \vec{y} \in \mathbb{R}^n\). Then

\[
\|\vec{x} + \vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|
\]

with equality if and only if \( a\vec{x} = b\vec{y} \) for some real numbers \( a, b \geq 0 \).

**Proof**

We proceed by calculation:

\[
\|\vec{x} + \vec{y}\|^2 = (\vec{x} + \vec{y}) \cdot (\vec{x} + \vec{y}) \quad \text{by Example 9.1.14}
\]

\[
= (\vec{x} \cdot \vec{x}) + 2(\vec{x} \cdot \vec{y}) + (\vec{y} \cdot \vec{y}) \quad \text{by Exercise 9.1.15}
\]

\[
\leq (\vec{x} \cdot \vec{x}) + 2|\vec{x} \cdot \vec{y}| + (\vec{y} \cdot \vec{y}) \quad \text{since } a \leq |a| \text{ for all } a \in \mathbb{R}
\]

\[
\leq \|\vec{x}\|^2 + 2\|\vec{x}\|\|\vec{y}\| + \|\vec{y}\|^2 \quad \text{by Example 9.1.14 and Cauchy‚ÄìSchwarz}
\]

\[
= (\|\vec{x}\| + \|\vec{y}\|)^2 \quad \text{rearranging}
\]

Taking (nonnegative) square roots of both sides yields

\[
\|\vec{x} + \vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|
\]

by Lemma 9.1.8, as required.

Equality holds if and only if the two ‚Äò\(\leq\)‚Äô symbols in the above derivation are in fact ‚Äò\(=\)‚Äô symbols.

- The first inequality is equality if and only if \(\vec{x} \cdot \vec{y} = |\vec{x} \cdot \vec{y}|\), which holds if and only if \(\vec{x} \cdot \vec{y} \geq 0\).

- The second inequality is equality if and only if equality holds in the Cauchy‚ÄìSchwarz inequality. In turn, this occurs if and only if \( a\vec{x} = b\vec{y} \) for some \( a, b \in \mathbb{R} \). We may, moreover, assume that \( a \geq 0 \)‚Äîif not, replace \( a \) and \( b \) by their negatives.

If \( a = 0 \) then we can take \( b = 0 \). If \( a > 0 \), then by Example 9.1.14 and Exercise 9.1.15, we have

\[
\vec{x} \cdot \left( \frac{b}{a} \vec{x} \right) = \frac{b}{a} \|\vec{x}\|^2
\]
</markdown><markdown>
which is non-negative if and only if \( b \geq 0 \), since we are assuming that \( a \geq 0 \).

Thus, equality holds in the triangle inequality if and only if \( a\vec{x} = b\vec{y} \) for some \( a, b \geq 0 \).

This general version of the triangle inequality has a geometric interpretation in terms of‚Äîyou guessed it‚Äîtriangles. Any three points \( \vec{a}, \vec{b}, \vec{c} \in \mathbb{R}^n \) form a (potentially flat) triangle:

The side lengths \( u, v, w \) are given by the following equations:

\[
u = \|\vec{b} - \vec{a}\|, \quad v = \|\vec{c} - \vec{b}\|, \quad w = \|\vec{a} - \vec{c}\|
\]

The triangle inequality says tells us that \( u \leq v + w \). Indeed:

\[
\begin{align*}
u &= \|\vec{b} - \vec{a}\| & \text{by definition of } u \\
  &= \|(\vec{b} - \vec{c}) + (\vec{c} - \vec{a})\| & \text{rearranging} \\
  &\leq \|\vec{b} - \vec{c}\| + \|\vec{c} - \vec{a}\| & \text{by the triangle inequality} \\
  &= \|\vec{c} - \vec{b}\| + \|\vec{a} - \vec{c}\| & \text{by Exercise 9.1.5} \\
  &= v + w & \text{by definition of } v \text{ and } w
\end{align*}
\]

That is, the triangle inequality says that the sum of two side lengths of a triangle is greater than or equal to the third side length. Moreover, it tells us \( u = v + w \) precisely when \( k(\vec{a} - \vec{c}) = \ell(\vec{c} - \vec{b}) \) for some \( k, \ell \geq 0 \). If \( k = 0 \) then

\[
\vec{c} = \vec{b} = 0\vec{a} + (1 - 0)\vec{b}
\]

if \( k > 0 \), then \( k + \ell > 0 \), so we have

\[
\vec{c} = \frac{k}{k + \ell} \vec{a} + \frac{\ell}{k + \ell} \vec{b} = \frac{k}{k + \ell} \vec{a} + \left(1 - \frac{k}{k + \ell}\right) \vec{b}
\]
</markdown><markdown>
Examining this a bit more closely yields that \( u = v + w \) if and only if

\[
\vec{c} = t \vec{a} + (1 - t) \vec{b}
\]

for some \( 0 \leq t \leq 1 \), which is to say precisely that \(\vec{c}\) lies on the line segment between \(\vec{a}\) and \(\vec{b}\). In other words, equality holds in the triangle inequality only if the three vertices of the triangle are collinear, which is to say that the triangle whose vertices are the points \(\vec{a}, \vec{b}\) and \(\vec{c}\), is flat.

## Inequalities of means

Our goal now is to explore different kinds of average‚Äîspecifically, means‚Äîof finite sets of non-negative real numbers. We will compare the relative sizes of these means with respect to one another, with emphasis on three particular kinds of mean: the **arithmetic mean** (Definition 9.1.20), the **geometric mean** (Definition 9.1.21) and the **harmonic mean** (Definition 9.1.29). These means in fact assemble into a continuum of means, called **generalised means** (Definition 9.1.37), all of which can be compared with one another.

### Definition 9.1.20
Let \( n \geq 1 \). The (arithmetic) mean of real numbers \( x_1, \ldots, x_n \) is

\[
\frac{1}{n} \sum_{i=1}^{n} x_i = \frac{x_1 + x_2 + \cdots + x_n}{n}
\]

### Definition 9.1.21
Let \( n \geq 1 \). The geometric mean of non-negative real numbers \( x_1, \ldots, x_n \) is

\[
\left( \prod_{i=1}^{n} x_i \right)^{1/n} = \sqrt[n]{x_1 \cdot x_2 \cdot \cdots \cdot x_n}
\]

The following theorem is commonly known as the **AM‚ÄìGM inequality**.

### Theorem 9.1.22 (Inequality of arithmetic and geometric means)
Let \( n \in \mathbb{N} \) and \( x_1, x_2, \ldots, x_n \geq 0 \). Then

\[
\sqrt[n]{x_1 \cdots x_n} \quad \text{(geometric mean)} \leq \frac{x_1 + \cdots + x_n}{n} \quad \text{(arithmetic mean)}
\]

with equality if and only if \( x_1 = \cdots = x_n \).
</markdown><markdown>
## Proof when \( n = 2 \)

We need to show that, if \( x, y \in \mathbb{R} \) with \( x, y \geq 0 \), then

\[
\sqrt{xy} \leq \frac{x+y}{2}
\]

with equality if and only if \( x = y \).

First note that the square roots of \( x \) and \( y \) exist since they are non-negative. Now

\[
0 \leq (\sqrt{x} - \sqrt{y})^2 \quad \text{since squares are nonnegative}
\]

\[
= (\sqrt{x})^2 - 2\sqrt{xy} + (\sqrt{y})^2 \quad \text{expanding}
\]

\[
= x - 2\sqrt{xy} + y \quad \text{rearranging}
\]

Rearranging the inequality \( 0 \leq x - 2\sqrt{xy} + y \) yields the desired result.

If \( \sqrt{xy} = \frac{x+y}{2} \), then we can rearrange the equation as follows:

\[
\sqrt{xy} = \frac{x+y}{2} \implies 2\sqrt{xy} = x+y \quad \text{multiplying by 2}
\]

\[
\implies 4xy = x^2 + 2xy + y^2 \quad \text{squaring both sides}
\]

\[
\implies x^2 - 2xy + y^2 = 0 \quad \text{rearranging}
\]

\[
\implies (x-y)^2 = 0 \quad \text{factorising}
\]

\[
\implies x-y = 0 \quad \text{since } a^2 = 0 \implies a = 0 \text{ for } a \in \mathbb{R}
\]

\[
\implies x = y \quad \text{rearranging}
\]

So we have proved both parts of the theorem.

### Example 9.1.23

We use the AM‚ÄìGM inequality to prove that the area of a rectangle with fixed perimeter is maximised when the rectangle is a square.

Indeed, fix a perimeter \( p > 0 \), and let \( x, y > 0 \) be side lengths of a rectangle with perimeter \( p \)‚Äîthat is, \( x \) and \( y \) satisfy the equation \( 2x + 2y = p \). The area \( a \) of the rectangle satisfies \( a = xy \). By the AM‚ÄìGM inequality, we have

\[
a = xy \leq \left( \frac{x+y}{2} \right)^2 = \frac{p^2}{16}
\]

Equality holds if and only if \( x = y \), in other words, if and only if the rectangle is a square.

### Exercise 9.1.24

Let \( a, b > 0 \) be real numbers. Prove that

\[
\frac{a^2 + b^2}{2} \geq ab.
\]
</markdown><markdown>
### Example 9.1.25

Let \( x > 0 \). We find the minimum possible value of \( x + \frac{9}{x} \). By the AM‚ÄìGM inequality, we have

\[
x + \frac{9}{x} \geq 2 \sqrt{x \cdot \frac{9}{x}} = 2 \sqrt{9} = 6
\]

with equality if and only if \( x = \frac{9}{x} \), which occurs if and only if \( x = 3 \). Hence the minimum value of \( x + \frac{9}{x} \) when \( x > 0 \) is 6.

### Exercise 9.1.26

Let \( x > 0 \) and let \( n \in \mathbb{N} \). Find the minimum possible value of \( \sum_{k=-n}^{n} x^k \).

Exercises 9.1.27 and 9.1.28 complete the proof of the AM‚ÄìGM inequality (Theorem 9.1.22). Before proceeding with the exercises, let‚Äôs fix some notation: for each \( n \in \mathbb{N} \), let \( p_{\text{AM‚ÄìGM}}(n) \) be the assertion that the AM‚ÄìGM inequality holds for collections of \( n \) numbers; that is, \( p_{\text{AM‚ÄìGM}}(n) \) is the assertion:

For all \( x_1, x_2, \ldots, x_n \geq 0 \), we have

\[
\sqrt[n]{\prod_{i=1}^{n} x_i} \leq \frac{1}{n} \sum_{i=1}^{n} x_i
\]

with equality if and only if \( x_1 = x_2 = \cdots = x_n \).

Note that we already proved \( p_{\text{AM‚ÄìGM}}(2) \).

### Exercise 9.1.27

Let \( r \in \mathbb{N} \) and let \( x_1, x_2, \ldots, x_{2r} \in \mathbb{R} \). Write

\[
a = \frac{1}{r} \sum_{i=1}^{r} x_i \quad \text{and} \quad g = \sqrt[r]{\prod_{i=1}^{r} x_i}
\]

for the arithmetic and geometric means, respectively, of the numbers \( x_1, \ldots, x_r \); write

\[
a' = \frac{1}{r} \sum_{i=r+1}^{2r} x_i \quad \text{and} \quad g' = \sqrt[r]{\prod_{i=r+1}^{2r} x_i}
\]

for the arithmetic and geometric means, respectively, of the numbers \( x_{r+1}, \ldots, x_{2r} \); and write

\[
A = \frac{1}{2r} \sum_{i=1}^{2r} x_i \quad \text{and} \quad G = \sqrt[2r]{\prod_{i=1}^{2r} x_i}
\]

for the arithmetic and geometric means, respectively, of all the numbers \( x_1, \ldots, x_{2r} \).
</markdown><markdown>
Prove that

\[
A = \frac{a + a'}{2} \quad \text{and} \quad G = \sqrt{gg'}
\]

Deduce that, for each \( r \in \mathbb{N} \), if \( p_{\text{AM-GM}}(r) \) is true then \( p_{\text{AM-GM}}(2r) \) is true. Deduce further than \( p_{\text{AM-GM}}(2^m) \) is true for all \( m \in \mathbb{N} \).

### Exercise 9.1.28
Let \( r \geq 2 \) and let \( x_1, \ldots, x_{r-1} \in \mathbb{N} \). Define

\[
x_r = \frac{1}{r-1} \sum_{i=1}^{r-1} x_i
\]

Prove that

\[
\frac{1}{r} \sum_{i=1}^{r} x_i = x_r
\]

Assuming \( p_{\text{AM-GM}}(r) \), deduce that

\[
x_r^r \geq \prod_{i=1}^{r} x_i \equiv \left( \prod_{i=1}^{r-1} x_i \right) \cdot x_r
\]

with equality if and only if \( x_1 = x_2 = \cdots = x_r \). Deduce that \( p_{\text{AM-GM}}(r) \) implies \( p_{\text{AM-GM}}(r-1) \). Use Exercise 9.1.27 to deduce further that \( p_{\text{AM-GM}}(n) \) is true for all \( n \geq 1 \).

We now introduce another kind of mean, called the **harmonic mean**.

### Definition 9.1.29
Let \( n \in \mathbb{N} \). The **harmonic mean** of nonzero real numbers \( x_1, x_2, \ldots, x_n \) is

\[
\left( \frac{1}{n} \sum_{i=1}^{n} x_i^{-1} \right)^{-1} = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}}
\]

The harmonic mean of two nonzero real numbers \( x \) and \( y \) has a simpler expression:

\[
\left( \frac{x^{-1} + y^{-1}}{2} \right)^{-1} = \frac{2xy}{x+y}
\]

The harmonic mean arises naturally when considering rates of change of quantities over fixed amounts of time.
</markdown><markdown>
### Example 9.1.30

The cities of York and Leeds are located \( d > 0 \) miles apart. Two cars drive from York to Leeds, then immediately turn around and drive back. The two cars depart from York at the same time and arrive back in York at the same time.

- The first car drives from York to Leeds at a constant speed of \( u \) miles per hour, and drives back to York at a constant speed of \( v \) miles per hour.
- The second car drives from York to Leeds and back again at the same constant speed of \( w \) miles per hour.

According to the following formula from physics:

\[
\text{speed} \times \text{time} = \text{distance}
\]

the time spent driving by the first car is \( \frac{d}{u} + \frac{d}{v} \), and the time spent driving by the second car is \( \frac{2d}{w} \).

Since the cars spend the same amount of time driving, it follows that

\[
\frac{2d}{w} = \frac{d}{u} + \frac{d}{v} \implies w = \frac{2uv}{u+v}
\]

That is, the second car‚Äôs speed is the harmonic mean of the two speeds driven by the first car.

As might be expected, we now prove a theorem relating the harmonic means with the other means we have established so far‚Äîthis theorem is known as the GM‚ÄìHM inequality.

### Theorem 9.1.31 (Inequality of geometric and harmonic means)

Let \( n \in \mathbb{N} \) and \( x_1, x_2, \ldots, x_n > 0 \). Then

\[
\frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + \cdots + \frac{1}{x_n}} \leq \sqrt[n]{x_1 x_2 \cdots x_n}
\]

with equality if and only if \( x_1 = \cdots = x_n \).

**Proof when \( n = 2 \)**

We need to prove that if \( x, y > 0 \), then

\[
\frac{2}{\frac{1}{x} + \frac{1}{y}} \leq \sqrt{xy}
\]
</markdown><markdown>
This is almost immediate from the AM‚ÄìGM inequality (Theorem 9.1.22). Indeed, since all numbers in sight are positive, we can take reciprocals to see that this inequality is equivalent to the assertion that

\[
\frac{1}{\sqrt{xy}} \leq \frac{x^{-1} + y^{-1}}{2}
\]

But \(\frac{1}{\sqrt{xy}} = \sqrt{x^{-1}y^{-1}}\), so this is immediate from the AM‚ÄìGM inequality.

‚úé **Exercise 9.1.32**  
Prove the GM‚ÄìHM inequality for general values of \(n \in \mathbb{N}\).

Another example of a mean that has applications in probability theory and statistics is that of the quadratic mean.

‚ú¶ **Definition 9.1.33**  
Let \(n \in \mathbb{N}\). The quadratic mean (or root-mean-square) of real numbers \(x_1, x_2, \ldots, x_n\) is

\[
\left( \frac{1}{n} \sum_{i=1}^{n} x_i^2 \right)^{\frac{1}{2}} = \sqrt{\frac{x_1^2 + x_2^2 + \cdots + x_n^2}{n}}
\]

The following theorem is, predictably, known as the QM‚ÄìAM inequality (or RMS‚ÄìAM inequality); it is a nice application of the Cauchy‚ÄìSchwarz inequality.

‚ú§ **Theorem 9.1.34 (Inequality of quadratic and arithmetic means)**  
Let \(n > 0\) and \(x_1, x_2, \ldots, x_n \geq 0\). Then

\[
\frac{x_1 + \cdots + x_n}{n} \leq \sqrt{\frac{x_1^2 + x_2^2 + \cdots + x_n^2}{n}}
\]

with equality if and only if \(x_1 = \cdots = x_n\).

**Proof**  
Define \(\vec{x} = (x_1, x_2, \ldots, x_n)\) and \(\vec{y} = (1, 1, \ldots, 1)\).

Then

\[
x_1 + x_2 + \cdots + x_n = \vec{x} \cdot \vec{y} \quad \text{by definition of scalar product}
\]

\[
\leq \|\vec{x}\| \|\vec{y}\| \quad \text{by Cauchy‚ÄìSchwarz}
\]

\[
= \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} \cdot \sqrt{n} \quad \text{evaluating the magnitudes}
\]
</markdown><markdown>
Dividing through by \( n \) yields

\[
\frac{x_1 + x_2 + \cdots + x_n}{n} \leq \sqrt{\frac{x_1^2 + x_2^2 + \cdots + x_n^2}{n}}
\]

as required. Equality holds if and only if equality holds in the Cauchy‚ÄìSchwarz inequality, which occurs if and only if

\[
(ax_1, ax_2, \ldots, ax_n) = (b, b, \ldots, b)
\]

for some \( a, b \in \mathbb{R} \) not both zero. If \( a = 0 \) then \( b = 0 \), so we must have \( a \neq 0 \). Hence equality holds if and only if \( x_i = \frac{b}{a} \) for all \( i \in [n] \)‚Äîin particular, if and only if \( x_1 = x_2 = \cdots = x_n \).

To summarise, what we have proved so far is

\[
\text{harmonic mean} \leq \text{geometric mean} \leq \text{arithmetic mean} \leq \text{quadratic mean}
\]

with equality in each case if and only if the real numbers whose means we are taking are all equal.

The following exercise allows us to bookend our chain of inequalities with the minimum and maximum of the collections of numbers.

**Exercise 9.1.35**  
Let \( n > 0 \) and let \( x_1, x_2, \ldots, x_n \) be positive real numbers. Prove that

\[
\min\{x_1, x_2, \ldots, x_n\} \leq \left( \frac{1}{n} \sum_{i=1}^{n} x_i^{-1} \right)^{-1} \quad \text{and} \quad \max\{x_1, x_2, \ldots, x_n\} \geq \left( \frac{1}{n} \sum_{i=1}^{n} x_i^2 \right)^{\frac{1}{2}}
\]

with equality in each case if and only if \( x_1 = x_2 = \cdots = x_n \).

‚òÖ **Generalised means**

We conclude this section by mentioning a generalisation of the results we have proved about means. We are not yet ready to prove the results that we mention; they are only here for the sake of interest.

‚ú¶ **Definition 9.1.36**  
The extended real number line is the (ordered) set \([-‚àû, ‚àû]\), defined by

\[
[-‚àû, ‚àû] = \mathbb{R} \cup \{-‚àû, ‚àû\}
\]

where \(\mathbb{R}\) is the set of real numbers with its usual ordering, and \(-‚àû, ‚àû\) are new elements ordered in such a way that \(-‚àû < x < ‚àû\) for all \( x \in \mathbb{R} \).
</markdown><markdown>
### Section 9.1. Inequalities and means

Note that the extended real line does not form a field‚Äîthe arithmetic operations are not defined on \(-\infty\) or \(\infty\), and we will at no point treat \(-\infty\) and \(\infty\) as real numbers; they are merely elements which extend the reals to add a least element and a greatest element.

‚ú¶ **Definition 9.1.37**  
Let \( p \in [-\infty, \infty] \), let \( n \in \mathbb{N} \), and let \( x_1, x_2, \ldots, x_n \) be positive real numbers. The **generalised mean with exponent** \( p \) (or simply **p-mean**) \( x_1, x_2, \ldots, x_n \) is the real number \( M_p(x_1, x_2, \ldots, x_n) \) defined by

\[
M_p(x_1, x_2, \ldots, x_n) = \left( \frac{1}{n} \sum_{i=1}^{n} x_i^p \right)^{\frac{1}{p}}
\]

if \( p \not\in \{-\infty, 0, \infty\} \), and by

\[
M_p(x_1, x_2, \ldots, x_n) = \lim_{q \to p} M_q(x_1, x_2, \ldots, x_n)
\]

if \( p \in \{-\infty, 0, \infty\} \), where the notation \(\lim\) refers to the limit as \( q \) tends to \( p \), as discussed in Section B.3.

We can see immediately that the harmonic, arithmetic and quadratic means of a finite set of positive real numbers are the p-means for a suitable value of \( p \): the harmonic mean is the \((-1)\)-mean, the arithmetic mean is the 1-mean, and the quadratic mean is the 2-mean. Furthermore, Proposition 9.1.38 exhibits the **minimum** as the \((-‚àû)\)-mean, the **geometric mean** as the 0-mean, and the **maximum** as the \(\infty\)-mean.

‚ú§ **Proposition 9.1.38**  
Let \( n > 0 \) and let \( x_1, x_2, \ldots, x_n > 0 \). Then

- \( M_{-\infty}(x_1, x_2, \ldots, x_n) = \min\{x_1, x_2, \ldots, x_n\} \);
- \( M_0(x_1, x_2, \ldots, x_n) = \sqrt[n]{x_1 x_2 \cdots x_n} \); and
- \( M_{\infty}(x_1, x_2, \ldots, x_n) = \max\{x_1, x_2, \ldots, x_n\} \).

All of the inequalities of means we have seen so far will be subsumed by Theorem 9.1.39, which compares the p-mean and q-mean of a set of numbers for all values of \( p, q \in [-\infty, \infty] \).
</markdown><markdown>
### Theorem 9.1.39

Let \( n > 0 \), let \( x_1, x_2, \ldots, x_n \geq 0 \) and let \( p, q \in [-\infty, \infty] \) with \( p < q \). Then

\[
M_p(x_1, x_2, \ldots, x_n) \leq M_q(x_1, x_2, \ldots, x_n)
\]

with equality if and only if \( x_1 = x_2 = \cdots = x_n \).

Theorem 9.1.39 implies each of the following:

- **HM‚Äìmin inequality** ([Exercise 9.1.35](#)): take \( p = -\infty \) and \( q = -1 \);
- **GM‚ÄìHM inequality** ([Theorem 9.1.31](#)): take \( p = -1 \) and \( q = 0 \);
- **AM‚ÄìGM inequality** ([Theorem 9.1.22](#)): take \( p = 0 \) and \( q = 1 \);
- **QM‚ÄìAM inequality** ([Theorem 9.1.34](#)): take \( p = 1 \) and \( q = 2 \);
- **max‚ÄìQM inequality** ([Exercise 9.1.35](#)): take \( p = 2 \) and \( q = \infty \).
</markdown><markdown>
## Section 9.2
**Completeness and convergence**

For most of the results that we proved in [Section 9.1](#), it did not matter that we were talking about real numbers. We could just as well have been working with any other ordered field, such as the rational numbers‚Äîthat is, most of the results in [Section 9.1](#) remain true by replacing \(\mathbb{R}\) by \(\mathbb{Q}\) (or any other ordered field) throughout.

From here onwards, we isolate the property of \(\mathbb{R}\) that separates it from \(\mathbb{Q}\)‚Äînamely, **completeness**. It is completeness that will allow us to define and explore the fundamental concepts of mathematical analysis: sequences, functions, convergence, limits, continuity, differentiability, and so on.

The property of completeness concerns least upper bounds for certain sets of real numbers.

### Definition 9.2.1
Let \(A \subseteq \mathbb{R}\). A real number \(m\) is an **upper bound** for \(A\) if \(a \leq m\) for all \(a \in A\). A **supremum** of \(A\) is a least upper bound of \(A\); that is, a real number \(m\) such that:

(i) \(m\) is an upper bound of \(A\)‚Äîthat is, \(a \leq m\) for all \(a \in A\); and

(ii) \(m\) is least amongst all upper bounds for \(A\)‚Äîthat is, for all \(x \in \mathbb{R}\), if \(a \leq x\) for all \(a \in A\), then \(x \leq m\).

### Example 9.2.2
We prove that 1 is a supremum of the open interval \((0, 1)\).

(i) Let \(a \in (0, 1)\). Then \(a < 1\), so that 1 is an upper bound of \((0, 1)\).

(ii) Let \(x \in \mathbb{R}\) be another upper bound of \((0, 1)\). If \(x < 1\), then we have

\[
x = \frac{x + x}{2} < \frac{x + 1}{2} < \frac{1 + 1}{2} = 1
\]

and so \(x < \frac{x + 1}{2} \in (0, 1)\). This contradicts the assumption that \(x\) is an upper bound of \((0, 1)\). It follows that \(x \geq 1\), as required.

Hence 1 is indeed a supremum of \((0, 1)\).

### Exercise 9.2.3
Define the notions of **lower bound** and **infimum**, and find the infimum of the open interval \((0, 1)\).
</markdown><markdown>
The following proposition provides a convenient way of testing whether a real number is a supremum of a subset.

### Proposition 9.2.4
Let \( A \subseteq \mathbb{R} \) and suppose \( m \in \mathbb{R} \) is an upper bound of \( A \). Then \( m \) is a supremum of \( A \) if and only if, for all \( \varepsilon > 0 \), there exists \( a \in A \) such that \( a > m - \varepsilon \).

**Proof**

- (\(\Rightarrow\)). Suppose \( m \) is a supremum of \( A \), and let \( \varepsilon > 0 \). If there is no \( a \in A \) such that \( a > m - \varepsilon \), then \( a \leq m - \varepsilon \) for all \( a \in A \). But this contradicts the assumption that \( m \) is a supremum of \( a \), since \( m - \varepsilon \) is an upper bound of \( A \) that is less than \( m \). So there exists \( a \in A \) with \( a > m - \varepsilon \), as required.

- (\(\Leftarrow\)). Suppose that, for all \( \varepsilon > 0 \), there exists \( a \in A \) with \( a > m - \varepsilon \), and let \( x \in \mathbb{R} \) be an upper bound of \( A \). In order to prove that \( m \) is a supremum of \( A \), we must prove that \( m \leq x \).

  Suppose \( x < m \), and define \( \varepsilon = m - x \). Then \( \varepsilon > 0 \), so there exists \( a \in A \) such that

  \[
  a > m - \varepsilon = m - (m - x) = x
  \]

  But this contradicts the assumption that \( x \) is an upper bound of \( A \). So we must have \( m \leq x \), as required.

### Theorem 9.2.5 (Uniqueness of suprema)
Let \( A \) be a subset of \( \mathbb{R} \). If \( m_1 \) and \( m_2 \) are suprema of \( A \), then \( m_1 = m_2 \).

**Proof**

Since \( m_1 \) is an upper bound of \( A \) and \( m_2 \) is a supremum of \( A \), we have \( m_2 \leq m_1 \) by Definition 9.2.1(ii). Likewise, since \( m_2 \) is an upper bound of \( A \) and \( m_1 \) is a supremum of \( A \), we have \( m_1 \leq m_2 \) by Definition 9.2.1(ii) again. But this implies that \( m_1 = m_2 \).

An analogous result proves that a subset of \( \mathbb{R} \) may have at most one infimum. This allows us to introduce the following notation.

### Definition 9.2.6
Let \( A \subseteq \mathbb{R} \). The supremum of \( A \), if it exists, is denoted by \( \sup(A) \); the infimum of \( A \), if it exists, is denoted by \( \inf(A) \).

Now that we are more familiar with suprema, here is the completeness axiom in its full glory.
</markdown><markdown>
### Axiom 9.2.7 (Completeness axiom)
Let \( A \subseteq \mathbb{R} \) be inhabited. If \( A \) has an upper bound, then \( A \) has a supremum.

The true power of the completeness axiom will become apparent later in the section when we discuss the existence of limits of sequences of real numbers.

Before we embark on that adventure, we first prove that the rational numbers are not complete, by exhibiting a subset of \( \mathbb{Q} \) that has no rational supremum.

### Proposition 9.2.8
Let \( A = \{ x \in \mathbb{Q} \mid x^2 < 2 \} \). Then \( A \) does not have a rational supremum.

A quick proof of Proposition 9.2.8 would be to verify that \( \sqrt{2} \), which is irrational, is a supremum of \( A \), and use uniqueness of suprema to deduce that there can be no rational supremum. However, this is cheating. Failure of completeness is an intrinsic property‚Äîwe should be able to prove Proposition 9.2.8 without venturing outside of the realm of rational numbers at all. That is, we cannot use irrational numbers in our proof. This makes the proof significantly longer, but significantly more satisfying.

**Proof of Proposition 9.2.8**

Towards a contradiction, suppose that \( A \) has a supremum \( q \).

First note that \( q > 0 \). Indeed, \( 1^2 < 2 \), so that \( 1 \in A \), and so \( q \geq 1 > 0 \).

Next, we prove that \( q^2 = 2 \). Indeed:

- Assume \( q^2 < 2 \), so that \( 2 - q^2 > 0 \). For each \( n \geq 1 \), we have

  \[
  \left( q + \frac{1}{n} \right)^2 = q^2 + \frac{2q}{n} + \frac{1}{n^2}
  \]

  Choose \( n \) sufficiently large that \( \frac{2q}{n} < \frac{2 - q^2}{2} \) and \( \frac{1}{n^2} < \frac{2 - q^2}{2} \). Then by the above, we observe that

  \[
  \left( q + \frac{1}{n} \right)^2 < q^2 + \frac{2 - q^2}{2} + \frac{2 - q^2}{2} = q^2 + (2 - q^2) = 2
  \]

  and so \( q + \frac{1}{n} \in A \). But \( q + \frac{1}{n} > q \), so this contradicts the assumption that \( q \) is an upper bound of \( A \).

- Assume \( q^2 > 2 \), so that \( q^2 - 2 > 0 \). For each \( n \geq 1 \), we have

  \[
  \left( q - \frac{1}{n} \right)^2 = q^2 - \frac{1}{n} \left( 2q - \frac{1}{n} \right)
  \]
</markdown><markdown>
Choose \( n \) sufficiently large that \( \frac{1}{n} < q (< 2q) \) and \( \frac{2q}{n} < q^2 - 2 \). Then by the above work, we observe that

\[
\left( q - \frac{1}{n} \right)^2 > q^2 - \frac{2q}{n} > q^2 - (q^2 - 2) = 2
\]

Moreover \( q - \frac{1}{n} > 0 \) since \( \frac{1}{n} < q \).

Suppose that \( q - \frac{1}{n} \) is not an upper bound for \( A \). Then there is some \( x \in A \) with \( x > q - \frac{1}{n} > 0 \). But then \( (q - \frac{1}{n})^2 < x^2 < 2 \), contradicting the fact that \( \left( q - \frac{1}{n} \right)^2 > 2 \).

So \( q - \frac{1}{n} \) is an upper bound for \( A \), contradicting the fact that \( q \) is a supremum of \( A \).

So we must have \( q^2 = 2 \). But this is impossible‚Äîthe proof is identical to that of Proposition 4.3.12, but with all instances of ‚Äò\(\sqrt{2}\)‚Äô replaced by ‚Äò\( q \)‚Äô in the proof.

So \(\{ x \in \mathbb{Q} \mid x^2 < 2 \}\) has no rational supremum.

## Sequences of real numbers

The rest of this chapter is dedicated to studying **convergence** of sequences of real numbers. We will use the completeness axiom to find sufficient conditions for a sequence to converge.

### Definition 9.2.9

A **sequence of real numbers** is a function \( x : \mathbb{N} \to \mathbb{R} \). Given a sequence \( x \), we write \( x_n \) instead of \( x(n) \) and write \( (x_n)_{n \geq 0} \), or even just \( (x_n) \), instead of \( x : \mathbb{N} \to \mathbb{R} \). The values \( x_n \) are called the **terms** of the sequence, and the variable \( n \) is called the **index** of the term \( x_n \).

### Example 9.2.10

Some very basic but very boring examples of sequences are **constant sequences**. For example, the constant sequence with value 0 is

\[
(0, 0, 0, 0, 0, 0, \ldots)
\]

More generally, for fixed \( a \in \mathbb{R} \), the constant sequence with value \( a \) is defined by \( x_n = a \) for all \( n \in \mathbb{N} \).

### Example 9.2.11

Sequences can be defined just like functions. For example, there is a sequence defined by \( x_n = 2^n \) for all \( n \in \mathbb{N} \). Writing out the first few terms, this sequence is

\[
(1, 2, 4, 8, 16, \ldots)
\]
</markdown><markdown>
Sometimes it will be convenient to start the indexing of our sequence from numbers other than 0, particularly when an expression involving a variable \( n \) isn‚Äôt defined when \( n = 0 \). We‚Äôll denote such sequences by \( (x_n)_{n \geq 1} \) or \( (x_n)_{n \geq 2} \), and so on.

### Example 9.2.12
Let \( (z_n)_{n \geq 2} \) be the sequence defined by \( z_n = \frac{(n+1)(n+2)}{(n-1)n} \) for all \( n \geq 2 \):

\[
\left( 6, \frac{10}{3}, \frac{5}{2}, \frac{21}{10}, \cdots \right)
\]

The indexing of this sequence begins at 2, rather than 0, since when \( n = 0 \) or \( n = 1 \), the expression \( \frac{(n+1)(n+2)}{(n-1)n} \) is undefined. We could reindex the sequence: by letting \( z'_n = z_{n+2} \) for all \( n \geq 0 \), we obtain a new sequence \( (z'_n)_{n \geq 0} \) defined by \( z'_n = \frac{(n+3)(n+4)}{(n+1)(n+2)} \) whose indexing starts from 0. Fortunately for us, such matters won‚Äôt cause any problems‚Äîit‚Äôs just important to make sure that whenever we define a sequence, we make sure the terms make sense for all of the indices.

## Convergence of sequences

Of particular interest to us will be sequences whose terms get closer and closer to a fixed real number. This phenomenon is called **convergence**.

### Example 9.2.13
Consider the sequence \( (y_n)_{n \geq 1} \) defined by \( y_n = \frac{1}{n} \) for all \( n \geq 1 \):

\[
\left( 1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \frac{1}{5}, \cdots \right)
\]

The terms \( y_n \) become closer and closer to 0 as \( n \) grows.

### Example 9.2.14
Define a sequence \( (r_n)_{n \geq 0} \) by \( r_n = \frac{2n}{n+1} \) for all \( n \in \mathbb{N} \). Some of the values of this sequence are illustrated in the following table:

\[
\begin{array}{c|c|c}
n & r_n & \text{decimal expansion} \\
\hline
0 & 0 & 0 \\
1 & 1 & 1 \\
2 & \frac{4}{3} & 1.333\ldots \\
3 & \frac{3}{2} & 1.5 \\
10 & \frac{20}{11} & 1.818\ldots \\
100 & \frac{200}{101} & 1.980\ldots \\
1000 & \frac{2000}{1001} & 1.998\ldots \\
\vdots & \vdots & \vdots \\
\end{array}
\]
</markdown><markdown>
As \( n \) increases, the values of \( r_n \) become closer and closer to 2.

The precise sense in which the terms of the sequences in Examples 9.2.13 and 9.2.14 ‚Äòget closer‚Äô to 0 and 2, respectively, is called convergence, which we will define momentarily in Definition 9.2.15.

First, let‚Äôs try to work out what the definition should be for a sequence \( (x_n) \) to converge to a real number \( a \).

A na√Øve answer might be to say that the sequence is ‚Äòeventually equal to \( a \)‚Äô‚Äîthat is, after some point in the sequence, all terms are equal to \( a \). Unfortunately, this isn‚Äôt quite good enough: if it were, then the values \( r_n = \frac{2n}{n+1} \) from Example 9.2.14 would be equal to 2 for sufficiently large \( n \). However, if for some \( n \in \mathbb{N} \) we have \( \frac{2n}{n+1} = 2 \), then it follows that \( 2n = 2(n+1) \); rearranging this gives \( 1 = 0 \), which is a contradiction.

However, this answer isn‚Äôt too far from giving us what we need. Instead of saying that the terms \( x_n \) are eventually equal to \( a \), we might want to say that they become infinitely close to \( a \), whatever that means.

We can‚Äôt really make sense of an ‚Äòinfinitely small positive distance‚Äô (e.g. Exercise 1.1.41), so we might instead make sense of ‚Äòinfinitely close‚Äô by saying that the terms \( x_n \) eventually become as close to \( a \) as we could possibly want them to be. Spelling this out, this means that for any positive distance \( \varepsilon \) (read: ‚Äòepsilon‚Äô)[^1] no matter how small, the terms \( x_n \) are eventually within distance \( \varepsilon \) of \( a \). In summary:

> **Definition 9.2.15**  
> Let \( (x_n) \) be a sequence and let \( a \in \mathbb{R} \). We say that \( (x_n) \) converges to \( a \), and write \( (x_n) \to a \), if the following condition holds:
> 
> \[
> \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n \geq N, \, |x_n - a| < \varepsilon
> \]
> 
> The value \( a \) is called a limit of \( (x_n) \). Moreover, we say that a sequence \( (x_n) \) converges if it has a limit, and diverges otherwise.

Sometimes, we may write ‚Äò\( x_n \to a \) as \( n \to \infty \)‚Äô to mean \( (x_n) \to a \); this indicates that the terms \( x_n \) approach \( a \) as \( n \) increases without bound. Take heed of the fact that the symbol ‚Äò\(\infty\)‚Äô here does not have meaning on its own‚Äîit is simply a means of suggesting that as the index \( n \) gets greater, the values \( x_n \) of the terms in the sequence get closer to the limit.

[^1]: The lower case Greek letter epsilon (\(\varepsilon\)) is traditionally used in analysis to denote a positive quantity whose value can be made arbitrarily small. We will encounter this letter frequently in this section and the next when discussing convergence.
</markdown><markdown>
Before we move onto some examples, let‚Äôs quickly digest the definition of the expression \((x_n) \to a\). The following table presents a suggestion of how you might read the expression ‚Äò\(\forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n \geq N, |x_n - a| < \varepsilon\)‚Äô in English.

| Symbols | English |
|---------|---------|
| \(\forall \varepsilon > 0\) | For any positive distance \(\varepsilon\) (no matter how small)... |
| \(\exists N \in \mathbb{N}\) | ...there is a stage in the sequence... |
| \(\forall n \geq N\) | ...after which all terms in the sequence... |
| \(|x_n - a| < \varepsilon\) | ...are within distance \(\varepsilon\) of \(a\). |

Thus, a sequence \((x_n)\) converges to \(a\) if ‚Äòfor any positive distance \(\varepsilon\) (no matter how small), there is a stage in the sequence after which all terms in the sequence are within \(\varepsilon\) of \(a\)‚Äô. After reading this a few times, you should hopefully be content that this definition captures what is meant by saying that the terms in the sequence are eventually as close to \(a\) as we could possibly want them to be.

We are now ready to see some examples of convergent (and divergent) sequences. When reading the following proofs, keep in mind the logical structure‚Äîthat is, the alternating quantifiers \(\forall \varepsilon \ldots \exists N \ldots \forall n \ldots\)‚Äîin the definition of \((x_n) \to a\).

### Proposition 9.2.16

The sequence \((y_n)\) defined by \(y_n = \frac{1}{n}\) for all \(n \geq 1\) converges to 0.

**Proof**

By Definition 9.2.15, we need to prove

\[
\forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n \geq N, \left| \frac{1}{n} - 0 \right| < \varepsilon
\]

So fix \(\varepsilon > 0\). Our goal is to find \(N \in \mathbb{N}\) such that \(\left| \frac{1}{n} \right| < \varepsilon\) for all \(n \geq N\).

Let \(N\) be any natural number which is greater than \(\frac{1}{\varepsilon}\). Then for all \(n \geq N\), we have

\[
\left| \frac{1}{n} \right| = \frac{1}{n} \quad \text{since } \frac{1}{n} > 0 \text{ for all } n \geq 1
\]

\[
\leq \frac{1}{N} \quad \text{since } n \geq N
\]

\[
< \frac{1}{1/\varepsilon} \quad \text{since } N > \frac{1}{\varepsilon}
\]

\[
= \varepsilon
\]

Hence \(|y_n| < \varepsilon\) for all \(n \geq N\). Thus we have proved that \((y_n) \to 0\).

### Remark 9.2.17

The value of \(N\) you need to find in the proof of convergence will usually depend on
</markdown><markdown>
the parameter \(\varepsilon\). (For instance, in Proposition 9.2.16, we defined \(N\) to be some natural number greater than \(\frac{1}{\varepsilon}\).) This is to be expected‚Äîremember that \(\varepsilon\) is the distance away from the limit that the terms are allowed to vary after the \(N\)th term. If you make this distance smaller, you‚Äôll probably have to go further into the sequence before your terms are all close enough to \(a\). In particular, the value of \(N\) will usually grow as the value of \(\varepsilon\) gets smaller. This was the case in Proposition 9.2.16: note that \(\frac{1}{\varepsilon}\) increases as \(\varepsilon\) decreases.

‚úê **Example 9.2.18**

Let \((r_n)\) be the sequence from Example 9.2.14 defined by \(r_n = \frac{2n}{n+1}\) for all \(n \in \mathbb{N}\).

We‚Äôll prove that \((r_n) \to 2\). So fix \(\varepsilon > 0\). We need to find \(N \in \mathbb{N}\) such that

\[
\left| \frac{2n}{n+1} - 2 \right| < \varepsilon \text{ for all } n \geq N
\]

To find such a value of \(n\), we‚Äôll first do some algebra. Note first that for all \(n \in \mathbb{N}\) we have

\[
\left| \frac{2n}{n+1} - 2 \right| = \left| \frac{2n - 2(n+1)}{n+1} \right| = \left| \frac{-2}{n+1} \right| = \frac{2}{n+1}
\]

Rearranging the inequality \(\frac{2}{n+1} < \varepsilon\) gives \(\frac{n+1}{2} > \frac{1}{\varepsilon}\), and hence \(n > \frac{2}{\varepsilon} - 1\).

To be clear, what we‚Äôve shown so far is that a **necessary** condition for \(|r_n - 2| < \varepsilon\) to hold is that \(n > \frac{2}{\varepsilon} - 1\). This informs us what the desired value of \(N\) might look like‚Äîwe will then verify that the desired inequality holds.

So define \(N = \frac{2}{\varepsilon} - 1\). For all \(n \geq N\), we have

\[
\left| \frac{2n}{n+1} - 2 \right| = \frac{2}{n+1} \quad \text{by the above work}
\]

\[
\leq \frac{2}{N+1} \quad \text{since } n \geq N
\]

\[
< \frac{2}{\left(\frac{2}{\varepsilon} - 1\right) + 1} \quad \text{since } N > \frac{2}{\varepsilon} - 1
\]

\[
= \frac{2}{2/\varepsilon} \quad \text{rearranging}
\]

\[
= \varepsilon \quad \text{rearranging}
\]

Thus, as claimed, we have \(|r_n - 2| < \varepsilon\) for all \(n \geq N\). It follows that \((r_n) \to 2\), as required.

‚úê **Exercise 9.2.19**

Let \((x_n)\) be the constant sequence with value \(a \in \mathbb{R}\). Prove that \((x_n) \to a\).

‚úê **Exercise 9.2.20**

Prove that the sequence \((z_n)\) defined by \(z_n = \frac{n+1}{n+2}\) converges to 1.
</markdown><markdown>
Here's a slightly more involved example.

### Proposition 9.2.21
Let \( r \in (-1, 1) \). Then \( (r^n) \to 0 \).

**Proof**

If \( r = 0 \), then \( r^n = 0 \) for all \( n \geq 1 \), and so for any \( \varepsilon > 0 \) and \( n \geq 1 \) we have

\[
|r^n - 0| = |0| = 0 < \varepsilon
\]

so that \( (r^n) \to 0 \) as required.

So assume \( r \neq 0 \) and let \( a = \frac{1}{|r|} > 1 \). Then \( a = 1 + \delta \) for some \( \delta > 0 \), so that by the binomial theorem we have

\[
a^n = (1 + \delta)^n = 1 + n\delta + \sum_{k=2}^{n} \binom{n}{k} \delta^{n-k} \geq 1 + n\delta
\]

for all \( n \geq 1 \).

Now let \( \varepsilon > 0 \), and let \( N \geq 2 \) be such that \( 1 + N\delta > \frac{1}{\varepsilon} \); any \( N > \frac{1 - \varepsilon}{\delta \varepsilon} \) will do.

Then for all \( n \geq N \), we have

\[
|r^n| = \frac{1}{a^n} \leq \frac{1}{a^N} \leq \frac{1}{1 + N\delta} < \frac{1}{1/\varepsilon} = \varepsilon
\]

and so \( (r^n) \to 0 \), as required.

### Divergence

Before we go too much further, let‚Äôs see some examples of sequences which diverge. Recall (Definition 9.2.15) that a sequence \( (x_n) \) converges if \( (x_n) \to a \) for some \( a \in \mathbb{R} \). Spelling this out symbolically, to say ‚Äò\( (x_n) \) converges‚Äô is to say the following:

\[
\exists a \in \mathbb{R}, \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n \geq N, |x_n - a| < \varepsilon
\]

We can negate this using the tools of Section 1.3: to say that a sequence \( (x_n) \) diverges is to say the following:

\[
\forall a \in \mathbb{R}, \exists \varepsilon > 0, \forall N \in \mathbb{N}, \exists n \geq N, |x_n - a| \geq \varepsilon
\]

In more intuitive terms: for all possible candidates for a limit \( a \in \mathbb{R} \), there is a positive distance \( \varepsilon \) such that, no matter how far down the sequence you go (say \( x_N \)), you can find a term \( x_n \) beyond that point which is at distance \( \geq \varepsilon \) away from \( a \).
</markdown><markdown>
### Example 9.2.22

Let \((x_n)\) be the sequence defined by \(x_n = (-1)^n\) for all \(n \in \mathbb{N}\):

\[
(1, -1, 1, -1, 1, -1, \ldots)
\]

We'll prove that \((x_n)\) diverges. Fix \(a \in \mathbb{R}\). Intuitively, if \(a\) is non-negative, then it must be at distance \(\geq 1\) away from \(-1\), and if \(a\) is negative, then it must be at distance \(\geq 1\) away from 1. We'll now make this precise.

So let \(\varepsilon = 1\), and fix \(N \in \mathbb{N}\). We need to find \(n \geq N\) such that \(|(-1)^n - a| \geq 1\). We'll split into cases based on whether \(a\) is non-negative or negative.

- Suppose \(a \geq 0\). Then \(-1 - a \leq -1 < 0\), so that we have

  \[
  |-1 - a| = a - (-1) = a + 1 \geq 1
  \]

  So let \(n = 2N + 1\). Then \(n \geq N\) and \(n\) is odd, so that

  \[
  |x_n - a| = |(-1)^n - a| = |-1 - a| \geq 1
  \]

- Suppose \(a < 0\). Then \(1 - a > 1 > 0\), so that we have

  \[
  |1 - a| = 1 - a > 1
  \]

  So let \(n = 2N\). Then \(n \geq N\) and \(n\) is even, so that

  \[
  |x_n - a| = |(-1)^n - a| = |1 - a| \geq 1
  \]

In both cases, we've found \(n \geq N\) such that \(|x_n - a| \geq 1\). It follows that \((x_n)\) diverges.

Example 9.2.22 is an example of a **periodic sequence**‚Äîthat is, it's a sequence that repeats itself. It is difficult for such sequences to converge since, intuitively speaking, they jump up and down a lot. (In fact, the only way that a periodic sequence can converge is if it is a constant sequence!)

### Exercise 9.2.23

Let \((y_n)\) be the sequence defined by \(y_n = n\) for all \(n \in \mathbb{N}\):

\[
(0, 1, 2, 3, \ldots)
\]

Prove that \((y_n)\) diverges.

### Exercise 9.2.24

Let \(r \in \mathbb{R}\). Recall that \((r^n) \to 0\) if \(|r| < 1\) (this was Proposition 9.2.21) and that \((r^n)\) diverges if \(r = -1\) (this was Example 9.2.22). Prove that \((r^n)\) diverges if \(|r| > 1\).
</markdown><markdown>
# 'Eventually'

Consider the following sequence:

\[
\left( 1, 2, -10, 7, \frac{1}{\sqrt{2}}, 0, 0, 0, 0, 0, \ldots \right)
\]

It takes some nonzero values initially, but after the 5th term in the sequence, it remains constant with the value 0. For most intents and purposes, we can treat it as a constant sequence: after a certain point, it is constant, and so any properties involving limits of constant sequences will also be true of this sequence.

Situations like this arise frequently. For example, we might not need a sequence to be increasing ([Definition 9.2.44](#))‚Äîwe might just need it to be increasing after some finite stage.

We use the word ‚Äòeventually‚Äô to refer to this phenomenon. (In fact, the word ‚Äòeventually‚Äô is a new kind of quantifier!)

## Definition 9.2.25

Let \( p(x) \) be a logical formula with free variable \( x \) ranging over sequences of real numbers. We say \( p((x_n)_{n>0}) \) is **eventually true** if \( p((x_n)_{n \geq k}) \) is true for some \( k \in \mathbb{N} \).

## Example 9.2.26

Some examples of the word ‚Äòeventually‚Äô include:

- A sequence \( (x_n) \) is **eventually constant** if \( (x_n)_{n \geq k} \) is constant for some \( k \in \mathbb{N} \)‚Äîthat is, if there is some \( k \in \mathbb{N} \) such that \( x_m = x_n \) for all \( m, n \geq k \).

- A sequence \( (x_n) \) is **eventually nonzero** if there is some \( k \in \mathbb{N} \) such that \( x_n \neq 0 \) for all \( n \geq k \).

- Two sequences \( (x_n) \) and \( (y_n) \) are **eventually equal** if there is some \( k \in \mathbb{N} \) such that \( x_n = y_n \) for all \( n \geq k \).

## Example 9.2.27

The definition of \( (x_n) \to a \) can be equivalently phrased as:

For all \( \varepsilon > 0 \), the sequence \( (x_n) \) eventually satisfies \( |x_n - a| < \varepsilon \).

This is because ‚Äò\(\exists N \in \mathbb{N}, \forall n \geq N, |x_n - a| < \varepsilon\)‚Äô means precisely that \( |x_n - a| \) is eventually less than \( \varepsilon \).
</markdown><markdown>
### Exercise 9.2.28
Prove that if a sequence \((x_n)\) converges to a nonzero limit, then \((x_n)\) is eventually nonzero. Find a sequence \((x_n)\) that converges to zero, but is not eventually nonzero.

### Exercise 9.2.29
Let \((x_n)\) be a sequence and let \(p(x)\) be a logical formula. What does it mean to say that \(p(x_n)\) is not eventually true? Find a sentence involving the phrase ‚Äònot eventually‚Äô that is equivalent to the assertion that \((x_n)\) diverges.

The next theorem will allow us to use the word ‚Äòeventually‚Äô in our proofs, without worrying about whether we‚Äôre being precise.

### Theorem 9.2.30 (‚ÄòEventually‚Äô preserves conjunction and disjunction)
Let \((x_n)\) be a sequence, and let \(p(x)\) and \(q(x)\) be logical formula with free variable \(x\) ranging over sequences of real numbers.

(a) If \(p(x_n)\) is eventually true and \(q(x_n)\) is eventually true, then \(p(x_n) \land q(x_n)\) is eventually true.

(b) If \(p(x_n)\) is eventually true or \(q(x_n)\) is eventually true, then \(p(x_n) \lor q(x_n)\) is eventually true.

**Proof**

(a) Let \(k, \ell \in \mathbb{N}\) be such that \(p(x_n)\) is true for all \(n \geq k\) and \(q(x_n)\) is true for all \(n \geq \ell\). Define \(N = \max\{k, \ell\}\). Then for all \(n \geq N\), we have \(p(x_n)\) is true since \(n \geq N \geq k\), and \(q(x_n)\) is true since \(n \geq N \geq \ell\), so that \(p(x_n) \land q(x_n)\) is true for all \(n \geq N\). Hence \(p(x_n) \land q(x_n)\) is eventually true.

(b) Assume that \(p(x_n)\) is eventually true. Then there is some \(k \in \mathbb{N}\) such that \(p(x_n)\) is true for all \(n \geq k\). But then \(p(x_n) \lor q(x_n)\) is true for all \(n \geq k\), so that \(p(x_n) \lor q(x_n)\) is eventually true. Likewise, if \(q(x_n)\) is eventually true, then \(p(x_n) \lor q(x_n)\) is eventually true.

The next exercise urges you not to become too complacent with your use of the word ‚Äòeventually‚Äô.

### Exercise 9.2.31 (‚ÄòEventually‚Äô does not preserve negation)
Find a sequence \((x_n)\) and a logical formula \(p(x)\) such that \(p(x_n)\) is neither eventually true nor eventually false. (Thus ‚Äò\(p(x_n)\) is eventually false‚Äô does not imply ‚Äò\(\neg p(x_n)\) is eventually true‚Äô.)
</markdown><markdown>
The following proposition justifies our use of ‚Äòeventually‚Äô in proofs regarding limits‚Äîit implies that limiting behaviour of a sequence is not affected by changing (or completely disregarding) the finitely many terms at the beginning of the sequence.

### Theorem 9.2.32
Let \((x_n)\) and \((y_n)\) be sequences. If \((x_n)\) and \((y_n)\) are eventually equal, then \((x_n)\) converges if and only if \((y_n)\) converges and, if \((x_n) \to a \in \mathbb{R}\), then \((y_n) \to a\) as well.

**Proof**

- First assume that \((x_n)\) converges to \(a \in \mathbb{R}\). We prove that \((y_n) \to a\).

  So fix \(\varepsilon > 0\). Since \((x_n) \to a\), eventually we have \(|x_n - a| < \varepsilon\) by Example 9.2.27. But eventually \(x_n = y_n\), and so we eventually have

  \[
  |y_n - a| = |x_n - a| < \varepsilon
  \]

  as required.

- Now assume that \((x_n)\) diverges. We prove that \((y_n)\) diverges. So let \(a \in \mathbb{R}\), and fix \(\varepsilon > 0\) such that, for all \(N \in \mathbb{N}\), we have \(|x_n - a| \geq \varepsilon\) for some \(n \geq N\).

  Let \(M \in \mathbb{N}\) and define \(N = \max\{k, N\}\), where \(k \in \mathbb{N}\) is such that \(x_n = y_n\) for all \(n \geq k\).

  Since \((x_n)\) diverges, there is some \(n \geq N\) such that \(|x_n - a| \geq \varepsilon\). But then \(n \geq N \geq M\) and

  \[
  |y_n - a| = |x_n - a| \geq \varepsilon
  \]

  so that \((y_n)\) diverges.

### Computing limits

Finding limits of sequences can be tricky. Theorem 9.2.34 makes it slightly easier by saying that if a sequence is built up using arithmetic operations‚Äîaddition, subtraction, multiplication and division‚Äîfrom sequences whose limits you know, then you can simply apply those arithmetic operations to the limits.

In order to prove part of Theorem 9.2.34, however, the following lemma will be useful.

### Lemma 9.2.33
Let \((x_n)\) be a sequence of real numbers. If \((x_n)\) converges, then \((x_n)\) is bounded‚Äîthat is, there is some real number \(k\) such that \(|x_n| \leq k\) for all \(n \in \mathbb{N}\).

**Proof**

Let \(a \in \mathbb{R}\) be such that \((x_n) \to a\). Letting \(\varepsilon = 1\) in the definition of convergence, it
</markdown><markdown>
follows that there exists some \( N \in \mathbb{N} \) such that \( |x_n - a| < 1 \) for all \( n \geq N \). It follows that \(-1 < x_n - a < 1\) for all \( n \geq N \), and hence \(-(1 - a) < x_n < 1 + a\) for all \( n \geq N \).

Now define

\[
k = \max\{ |x_0|, |x_1|, \ldots, |x_{N-1}|, |1-a|, |1+a| \} + 1
\]

For all \( n < N \), we have

\[
-k < -|x_n| \leq x_n \leq |x_n| < k
\]

so that \( |x_n| < k \). For all \( n \geq N \), we have

\[
-k < -|1-a| \leq -(1-a) < x_n < 1+a \leq |1+a| < k
\]

so that \( |x_n| < k \).

Hence \( |x_n| < k \) for all \( n \in \mathbb{N} \), as required.

\[
\Box
\]

### Theorem 9.2.34

Let \( (x_n) \) and \( (y_n) \) be sequences of real numbers, let \( a, b \in \mathbb{R} \), and suppose that \( (x_n) \to a \) and \( (y_n) \to b \). Then

(a) \( (x_n + y_n) \to a + b \);

(b) \( (x_n - y_n) \to a - b \);

(c) \( (x_n y_n) \to ab \); and

(d) \( \left( \frac{x_n}{y_n} \right) \to \frac{a}{b} \), so long as \( b \neq 0 \).

**Proof of (a) and (c)**

(a). Fix \( \varepsilon > 0 \). We need to prove that eventually \( |(x_n + y_n) - (a + b)| < \varepsilon \).

- Since \( (x_n) \to a \), we eventually have \( |x_n - a| < \frac{\varepsilon}{2} \);
- Since \( (y_n) \to b \), we eventually have \( |y_n - b| < \frac{\varepsilon}{2} \).

It follows from the triangle inequality (Theorem 9.1.9) that we eventually have

\[
|(x_n + y_n) - (a + b)| = |(x_n - a) + (y_n - b)| \leq |x_n - a| + |y_n - b| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2}
\]

as required.

(c). This one is a little harder. Fix \( \varepsilon > 0 \). Since \( (x_n) \) converges, it follows from Lemma 9.2.33 that there is some real number \( k \) with \( |x_n| < k \) for all \( n \in \mathbb{N} \).

- Since \( (x_n) \to a \), we eventually have \( |x_n - a| < \frac{\varepsilon}{2|b|} \);
</markdown><markdown>
Since \((y_n) \to b\), we eventually have \(|x_n - b| < \frac{\varepsilon}{2k}\).

Then using the triangle inequality again, eventually we have:

\[
\begin{align*}
|x_n y_n - ab| &= |x_n(y_n - b) + b(x_n - a)| & \text{rearranging} \\
&\leq |x_n(y_n - b)| + |b(x_n - a)| & \text{by the triangle inequality} \\
&= |x_n||y_n - b| + |b||x_n - a| & \text{rearranging} \\
&< k|y_n - b| + |b||x_n - a| & \text{since } |x_n| < k \text{ for all } n \\
&< k \frac{\varepsilon}{2k} + |b| \frac{\varepsilon}{2|b|} & \text{(eventually)} \\
&= \varepsilon & \text{rearranging}
\end{align*}
\]

Hence \((x_n y_n) \to ab\), as required.

‚úê **Exercise 9.2.35**

Prove parts (b) and (d) of [Theorem 9.2.34](#).

Theorem 9.2.34 *appears* obvious, but as you can see in the proof, it is more complicated than perhaps expected. It was worth the hard work, though, because we can now compute more complicated limits formed in terms of arithmetic operations by taking the limits of the individual components.

The following example uses Theorem 9.2.34 to prove that \(\left(\frac{2n}{n+1}\right) \to 2\) in a much simpler way than we saw in Example 9.2.18.

‚úê **Example 9.2.36**

We provide another proof that the sequence \((r_n)\) of Example 9.2.14, defined by \(r_n = \frac{2n}{n+1}\) for all \(n \in \mathbb{N}\), converges to 2.

For all \(n \geq 1\), dividing by the top and bottom gives

\[
r_n = \frac{2}{1 + \frac{1}{n}}
\]

The constant sequences \((2)\) and \((1)\) converge to 2 and 1, respectively; and by Proposition 9.2.16, we know that \(\left(\frac{1}{n}\right) \to 0\). It follows that

\[
(r_n) \to \frac{2}{1+0} = 2
\]

as required.

‚úê **Exercise 9.2.37**

Let \((x_n)\) be a sequence of real numbers converging to a real number \(a\), and let \(p(x) = a_0 + a_1 x + \cdots + a_d x^d\) be a polynomial function. Prove that \((p(x_n)) \to p(a)\), and that \(\left(\frac{1}{p(x_n)}\right) \to \frac{1}{p(a)}\) if \(p(a) \neq 0\).
</markdown><markdown>
The so-called *squeeze theorem* provides another means of computing limits. It says that if we can eventually ‚Äòsqueeze‚Äô the terms of a sequence \((y_n)\) between terms of two other sequences that converge to the same limit, then we can deduce that \((y_n)\) converges to the same limit.

### Theorem 9.2.38 (Squeeze theorem)
Let \((x_n)\), \((y_n)\) and \((z_n)\) be sequences of real numbers such that:

(i) \((x_n) \to a\) and \((z_n) \to a\); and

(ii) Eventually \(x_n \leq y_n \leq z_n\).

Then \((y_n) \to a\).

**Proof**

Fix \(\varepsilon > 0\). We need to prove that, eventually, \(|y_n - a| < \varepsilon\).

Since \((x_n) \to a\) and \((z_n) \to a\), we eventually have \(|x_n - a| < \varepsilon\) and \(|z_n - a| < \varepsilon\).

Fix \(N \in \mathbb{N}\) such that for all \(n \geq N\) we have \(|x_n - a| < \varepsilon\), \(|z_n - a| < \varepsilon\) and \(x_n < y_n < z_n\). Given \(n \geq N\):

- If \(y_n \geq a\), then we have \(a \leq y_n \leq z_n\), and so

  \[
  |y_n - a| = y_n - a \leq z_n - a = |z_n - a| < \varepsilon
  \]

- If \(y_n < a\), then we have \(x_n \leq y_n < a\), and so

  \[
  |y_n - a| = a - y_n \leq a - x_n = |x_n - a| < \varepsilon
  \]

In both cases we have proved \(|y_n - a| < \varepsilon\). It follows that \((y_n) \to a\).

### Example 9.2.39
Fix \(k \geq 1\). We prove that the sequence \(\left(\frac{1}{n^k}\right)_{n \geq 1}\) converges to zero.

Note that \(n^k > n\), so that we have \(0 < \frac{1}{n^k} \leq \frac{1}{n}\) for all \(n \in \mathbb{N}\). We know that \(\left(\frac{1}{n}\right) \to 0\) by Example 9.2.13, and \((0) \to 0\) since it is a constant sequence, so the squeeze theorem implies that \(\left(\frac{1}{n^k}\right) \to 0\).

### Exercise 9.2.40
Fix \(r \in \mathbb{N}\), and let \(p(x) = a_0 + a_1x + \cdots + a_rx^r\) and \(q(x) = b_0 + b_1x + \cdots + b_rx^r\) be polynomials with real coefficients. Prove that if \(b_r \neq 0\), then \(\left(\frac{p(n)}{q(n)}\right) \to \frac{a_r}{b_r}\).
</markdown><markdown>
## Uniqueness of limits

We now prove that a sequence can have at most one limit. This will allow us to talk about ‚Äòthe‚Äô limit of a sequence, and introduce notation for the limit of a sequence.

### Theorem 9.2.41 (Uniqueness of limits)
Let \( (x_n) \) be a sequence and let \( a, b \in \mathbb{R} \). If \( (x_n) \to a \) and \( (x_n) \to b \), then \( a = b \).

**Proof**  
We‚Äôll prove that \( |a - b| = 0 \), which will imply that \( a = b \). To do this, we‚Äôll prove that \( |a - b| \) is not positive: we already know it‚Äôs non-negative, so this will imply that it is equal to zero. To prove that \( |a - b| \) is not positive, we‚Äôll prove that it is less than every positive number.

So fix \( \varepsilon > 0 \). Then also \( \frac{\varepsilon}{2} > 0 \). The definition of convergence ([Definition 9.2.15](#)) tells us that eventually \( |x_n - a| < \frac{\varepsilon}{2} \) and \( |x_n - b| < \frac{\varepsilon}{2} \).

By the triangle inequality ([Theorem 9.1.9](#)), it follows that eventually

\[
|a - b| = |(a - x_n) + (x_n - b)| \quad \text{by cancelling the } x_n \text{ terms}
\]

\[
\leq |a - x_n| + |x_n - b| \quad \text{by the triangle inequality}
\]

\[
= |x_n - a| + |x_n - b| \quad \text{by Exercise 9.1.5}
\]

\[
< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon \quad \text{(eventually)}
\]

Since \( |a - b| < \varepsilon \) for all \( \varepsilon > 0 \), it follows that \( |a - b| \) is a non-negative real number that is less than every positive real number, so that it is equal to zero.

Since \( |a - b| = 0 \), we have \( a - b = 0 \), and so \( a = b \).

Theorem 9.2.41 justifies the following notation.

### Definition 9.2.42
Let \( (x_n) \) be a convergent sequence. The limit of \( (x_n) \) is denoted by \( \lim_{n \to \infty} (x_n) \) (LaTeX code: `\lim_{n \to \infty}`).

[The usual warnings about the symbol \( \infty \) apply.]

### Example 9.2.43
Proposition 9.2.16 and example 9.2.18 tell us that

\[
\lim_{n \to \infty} \left( \frac{1}{n} \right) = 0 \quad \text{and} \quad \lim_{n \to \infty} \left( \frac{2n}{n+1} \right) = 2
\]
</markdown><markdown>
## Existence of limits

It is often useful to know *that* a sequence converges, but not necessary to go to the arduous lengths of computing its limit. However, as it currently stands, we don‚Äôt really have any tools for proving that a sequence converges other than finding a limit for it! The remainder of this section is dedicated to deriving tools for finding out when a sequence does or does not converge, without needing to know exactly what the limit is.

Perhaps the most fundamental result is the **monotone convergence theorem** (Theorem 9.2.48), since it underlies the proofs of all the other results that we will prove. What it says is that if the terms in a sequence always increase, or always decrease, and the set of terms in the sequence is bounded, then the sequence converges to a limit.

The sequence \((r_n)\) from Example 9.2.14, defined by \(r_n = \frac{2n}{n+1}\) for all \(n \in \mathbb{N}\), is an example of such a sequence. We proved that it converged by computing its limit in Example 9.2.18 and again in Example 9.2.36. We will soon (Example 9.2.51) use the monotone convergence theorem to give *yet another proof* that it converges, but this time without going to the trouble of first finding its limit.

Before we can state the monotone convergence theorem, we must first define what we mean by a **monotonic sequence**.

### Definition 9.2.44
A sequence of real numbers \((x_n)\) is...

- ...**increasing** if \(m \leq n\) implies \(x_m \leq x_n\) for all \(m, n \in \mathbb{N}\);
- ...**decreasing** if \(m \leq n\) implies \(x_m \geq x_n\) for all \(m, n \in \mathbb{N}\).

If a sequence is either increasing or decreasing, we say it is **monotonic**.

### Example 9.2.45
The sequence \((x_n)\) defined by \(x_n = n^2\) for all \(n \in \mathbb{N}\) is increasing, since for all \(m, n \in \mathbb{N}\), if \(m \leq n\), then \(m^2 \leq n^2\). To see this, note that if \(m \leq n\), then \(n - m \geq 0\) and \(n + m \geq 0\), so that

\[
n^2 - m^2 = (n - m)(n + m) \geq 0 \cdot 0 = 0
\]

and hence \(n^2 \geq m^2\), as required.

### Example 9.2.46
The sequence \((r_n)\) from Examples 9.2.14 and 9.2.36, defined by \(r_n = \frac{2n}{n+1}\) for all \(n \in \mathbb{N}\),
</markdown><markdown>
is increasing. To see this, suppose \( m \leq n \). Then \( n = m + k \) for some \( k \geq 0 \). Now

\[
\begin{align*}
0 \leq k & \quad \text{by assumption} \\
\Rightarrow m^2 + km + m \leq m^2 + km + m + k & \quad \text{adding } m^2 + km + m \text{ to both sides} \\
\Rightarrow m(m + k + 1) \leq (m + 1)(m + k) & \quad \text{factorising} \\
\Rightarrow m(n + 1) \leq (m + 1)n & \quad \text{since } n = m + k \\
\Rightarrow \frac{m}{m + 1} \leq \frac{n}{n + 1} & \quad \text{dividing both sides by } (m + 1)(n + 1) \\
\Rightarrow r_m \leq r_n & \quad \text{by definition of } (r_n)
\end{align*}
\]

Note that the step where we divided through by \( (m + 1)(n + 1) \) is justified since this quantity is positive.

It is perhaps useful to add that to come up with this proof, it is more likely that you would start with the assumption \( r_m \leq r_n \) and derive that \( k \geq 0 \)‚Äînoting that all steps are reversible then allows us to write it in the ‚Äòcorrect‚Äô order.

### Exercise 9.2.47
Prove that the sequence \( (5^n - n^5)_{n \geq 0} \) is eventually increasing‚Äîthat is, there is some \( k \in \mathbb{N} \) such that \( (5^n - n^5)_{n \geq k} \) is an increasing sequence.

The monotone convergence theorem underlies all of the other tools for proving convergence of sequences that are to follow. It makes essential use of the completeness axiom.

### Theorem 9.2.48 (Monotone convergence theorem)
Let \( (x_n) \) be a sequence of real numbers.

(a) If \( (x_n) \) is increasing and has an upper bound, then it converges;

(b) If \( (x_n) \) is decreasing and has a lower bound, then it converges.

**Proof of (a)**  
We prove (a) here‚Äîpart (b) is Exercise 9.2.49.

So suppose \( (x_n) \) is increasing and has an upper bound. Then:

(i) \( x_m \leq x_n \) for all \( m \leq n \); and

(ii) There is some real number \( u \) such that \( u \geq x_n \) for all \( n \in \mathbb{N} \).

Condition (ii) tells us that the set \( \{x_n \mid n \in \mathbb{N}\} \subseteq \mathbb{R} \) has an upper bound. By the completeness axiom, it has a supremum \( a \). We prove that \( (x_n) \to a \).

So let \( \varepsilon > 0 \). We need to find \( N \in \mathbb{N} \) such that \( |x_n - a| < \varepsilon \) for all \( n \geq N \).
</markdown><markdown>
Since \( a \) is a supremum of \(\{ x_n \mid n \in \mathbb{N} \}\), there is some \( N \in \mathbb{N} \) such that \( x_N > a - \varepsilon \).

Since \((x_n)\) is increasing, by (i) we have \( x_N \leq x_n \) for all \( n \geq N \). Moreover, since \( a \) is an upper bound of the sequence, we actually have \( x_N \leq x_n \leq a \) for all \( n \geq N \).

Putting this together, for all \( n \geq N \), we have

\[
|x_n - a| = a - x_n \quad \text{since } x_n - a \leq 0
\]
\[
\leq a - x_N \quad \text{since } x_N \leq x_n \text{ for all } n \geq N
\]
\[
< \varepsilon \quad \text{since } x_N > a - \varepsilon
\]

It follows that \((x_n) \to a\), as required.

### Exercise 9.2.49
Prove part (b) of the monotone convergence theorem ([Theorem 9.2.48](#)). That is, prove that if a sequence \((x_n)\) is decreasing and has a lower bound, then it converges.

### Example 9.2.50
The monotone convergence theorem can be used to show that many of the sequences that we have already seen converge, although it doesn‚Äôt tell us what their limit is. For example, \(\left(\frac{1}{n}\right)\) converges since it is a decreasing sequence that is bounded below by 0.

### Example 9.2.51
Let \((r_n)\) be our recurring example sequence from [Examples 9.2.14, 9.2.36 and 9.2.46](#), defined by \( r_n = \frac{2n}{n+1} \) for all \( n \in \mathbb{N} \). We proved in [Example 9.2.46](#) that \((r_n)\) is increasing. Moreover, for all \( n \in \mathbb{N} \) we have

\[
r_n = \frac{2n}{n+1} < \frac{2(n+1)}{n+1} = 2
\]

and so \((r_n)\) is bounded above by 2. By the monotone convergence theorem, the sequence \((r_n)\) converges. Unfortunately, the monotone convergence theorem does not tell us what the limit of \((r_n)\) is, but we have already computed it twice!

### Exercise 9.2.52
Use the monotone convergence theorem to prove that the sequence \(\left(\frac{n!}{n^n}\right)\) converges.

### Exercise 9.2.53
A sequence \((x_n)\) is defined recursively by \( x_0 = 0 \) and \( x_{n+1} = \sqrt{2 + x_n} \) for all \( n \geq 0 \). That is,

\[
x_n = \sqrt{2 + \sqrt{2 + \sqrt{\cdots + \sqrt{2}}}}
\]

Prove that \((x_n)\) converges.
</markdown><markdown>
## Section 9.2. Completeness and convergence

We now define the notion of a **subsequence** of a sequence. A subsequence of a sequence is just like a subset of a set, except we can only pick out terms in a sequence in the order they appear.

### Definition 9.2.54
Let \((x_n)\) be a sequence of real numbers. A **subsequence** of \((x_n)\) is a sequence of the form \((x_{n_i})_{i \geq 0}\), where \(n_i < n_j\) for all \(0 \leq i < j\).

In Definition 9.2.54 we were careful to write \((x_{n_i})_{i \geq 0}\) rather than just \((x_{n_i})\), because we wanted to emphasise that the indexing variable is \(i\), rather than \(n\). This is good practice in any situation where confusion might arise over which variable is the indexing variable.

### Example 9.2.55
Define a sequence \((x_n)\) by \(x_n = (-1)^n\) for all \(n \geq 0\).

\[
(x_n)_{n \geq 0} = (1, -1, 1, -1, 1, -1, \ldots)
\]

The subsequence \((x_{2i})\) is the constant sequence with value 1, since for each \(i \geq 0\) we have \(x_{2i} = (-1)^{2i} = 1\), and the subsequence \((x_{2i+1})\) is the constant sequence with value \(-1\), since for each \(i \geq 0\) we have \(x_{2i+1} = (-1)^{2i+1} = -1\).

### Theorem 9.2.56
Let \((x_n)\) be a sequence, let \(a \in \mathbb{R}\), and suppose \((x_n) \to a\). Then every subsequence of \((x_n)\) converges to \(a\).

**Proof**

Let \((x_{n_i})_{i \geq 0}\) be a subsequence of \((x_n)\). We need to prove that \((x_{n_i}) \to a\) as \(i \to \infty\). To this end, fix \(\varepsilon > 0\). We need to find \(I \geq 0\) such that \(|x_{n_i} - a| < \varepsilon\) for all \(i \geq I\).

Since \((x_n) \to a\) as \(n \to \infty\), there exists some \(N \geq 0\) such that \(|x_n - a| < \varepsilon\) for all \(n \geq N\). Let \(I \geq 0\) be least such that \(n_I \geq N\). We know that \(I\) exists since we have \(0 \leq n_0 < n_1 < n_2 < \ldots\).

But then for all \(i \geq I\), we have \(n_i \geq n_I \geq N\), and hence \(|x_{n_i} - a| < \varepsilon\) by definition of \(N\).

Hence the subsequence \((x_{n_i})\) converges to \(a\), as required.

### Exercise 9.2.57
Prove that a subsequence of an increasing sequence is increasing, that a subsequence of a decreasing sequence is decreasing, and that a subsequence of a constant sequence is constant.
</markdown><markdown>
We can use the monotone convergence theorem and the squeeze theorem to prove the following very powerful result, which is related to a notion in the field of topology known as sequential compactness.

### Theorem 9.2.58 (Bolzano‚ÄìWeierstrass theorem)
Every bounded sequence of real numbers has a convergent subsequence.

*Proof*  
Let \((x_n)\) be a sequence of real numbers and let \(a, b \in \mathbb{R}\) be such that \(a < x_n < b\) for each \(n \geq 0\)‚Äîthe numbers \(a\) and \(b\) exist since the sequence \((x_n)\) is bounded.

Our strategy is as follows. The sequence \((x_n)\) is entirely contained inside the interval \([a, b]\), which has length \(\ell = b - a\). Letting \(c = \frac{a+b}{2}\) be the (arithmetic) mean of \(a\) and \(b\), we see that one of the intervals \([a, c]\) or \([c, b]\), or possibly both, must contain infinitely many terms of the sequence \((x_n)\)‚Äîbut then this defines a subsequence of \((x_n)\) which is entirely contained inside a sub-interval of \([a, b]\) whose length is \(\frac{\ell}{2}\). We iterate this process inductively, obtaining smaller and smaller intervals that contain infinitely many terms in the sequence \((x_n)\). The end-points of these intervals are then bounded monotone sequences‚Äîthe sequence of lower end-points is increasing, and the sequence of upper end-points is decreasing. The monotone convergence theorem implies that both sequences converge. We will prove that they converge to the same limit, thereby ‚Äòtrapping‚Äô a subsequence of \((x_n)\), which will converge by the squeeze theorem.

Now let‚Äôs put our strategy into action. We will define the terms \(n_i, a_i\) and \(b_i\) by induction on \(i\), and then verify that the resulting subsequence \((x_{n_i})_{i \geq 0}\) converges.

First, define \(n_0 = 0, a_0 = a\) and \(b_0 = b\).

Now fix \(i \geq 0\) and suppose that the numbers \(n_i, a_i\) and \(b_i\) have been defined in such a way that:

(i) \(x_{n_i} \in [a_i, b_i]\);

(ii) \(x_n \in [a_i, b_i]\) for infinitely many \(n > n_i\);

(iii) \(a_j \leq a_i < b_i \leq b_j\) for all \(j \leq i\); and

(iv) \(b_i - a_i = \frac{\ell}{2^i}\).

Write \(c_i = \frac{a_i + b_i}{2}\). By condition (ii), it must be case that infinitely many of the terms \(x_n\), for \(n > n_i\), are contained in either \([a_i, c_i]\) or in \([c_i, b_i]\). In the former case, define \(a_{i+1} = a_i\) and \(b_{i+1} = c_i\); and in the latter case define \(a_{i+1} = c_i\) and \(b_{i+1} = b_i\); and then define \(n_{i+1} > n_i\) such that \(x_{n_{i+1}} \in [a_{i+1}, b_{i+1}]\).

Note that conditions (i)‚Äì(iv) are satisfied, with \(i\) now replaced by \(i + 1\). Indeed, (i) and (ii) are satisfied by definition of \(a_{i+1}, b_{i+1}\) and \(n_{i+1}\). Condition (iii) is satisfied since
</markdown><markdown>
either \( a_{i+1} = a_i \) or \( a_{i+1} = \frac{a_i + c_i}{2} \geq a_i \), and likewise for \( b_{i+1} \). Condition (iv) is satisfied since

\[
c_i - a_i = \frac{a_i + b_i}{2} - a_i = \frac{b_i - a_i}{2} = \frac{\ell / 2^i}{2} = \frac{\ell}{2^{i+1}}
\]

and likewise \( b_i - c_i = \frac{\ell}{2^{i+1}} \).

Since by construction we have \( n_i < n_{i+1} \) for each \( i \geq 0 \), we have defined a subsequence \( (x_{n_i})_{i \geq 0} \) of \( (x_n) \).

Now the sequence \( (a_i) \) is increasing and is bounded above by \( b \), and the sequence \( (b_i) \) is decreasing and is bounded below by \( a \). By the monotone convergence theorem \( (a_i) \to a^\star \) and \( (b_i) \to b^\star \) for some \( a^\star, b^\star \in \mathbb{R} \). But moreover we have

\[
\frac{\ell}{2^i} = b_i - a_i \to b^\star - a^\star
\]

Since \(\frac{\ell}{2^i} \to 0\), we have \( b^\star - a^\star = 0 \) by uniqueness of limits, and so \( a^\star = b^\star \). Write \( x^\star \) for the common value of \( a^\star \) and \( b^\star \).

Finally, we have \( a_i \leq x_{n_i} \leq b_i \) for all \( i \geq 0 \), so that \( x_{n_i} \to x^\star \) by the squeeze theorem.

The Bolzano‚ÄìWeierstrass theorem can be used to prove that a sequence converges by verifying that its terms get arbitrarily close together. Such sequences are called Cauchy sequences, and the fact that all Cauchy sequences converge is proved in Theorem 9.2.62.

### Definition 9.2.59
A **Cauchy sequence** is a sequence \( (x_n) \) of real numbers such that, for all \( \varepsilon > 0 \), there exists \( N \in \mathbb{N} \) such that \( |x_m - x_n| < \varepsilon \) for all \( m, n \geq N \).

### Example 9.2.60
Let \( (r_n) \) be our favourite recurring example sequence from Examples 9.2.14, 9.2.36, 9.2.46 and 9.2.51, defined by \( r_n = \frac{2n}{n+1} \) for all \( n \in \mathbb{N} \). We prove that \( (r_n) \) is Cauchy.

First note that, given \( m, n \geq 1 \), we have

\[
|r_m - r_n| = \left| \frac{2m}{m+1} - \frac{2n}{n+1} \right| = \frac{2|m-n|}{(m+1)(n+1)} = \frac{2\left|\frac{1}{n} - \frac{1}{m}\right|}{(1 + \frac{1}{m})(1 + \frac{1}{n})}
\]

Now fix \( \varepsilon > 0 \), and let \( N \in \mathbb{N} \) be such that \( \frac{1}{m} < \frac{\varepsilon}{2} \) and \( \frac{1}{n} < \frac{\varepsilon}{2} \) for all \( m, n > N \). Note that such a value of \( N \) exists by Example 9.2.13.
</markdown><markdown>
Now let \( m, n \geq N \). Then \(\left| \frac{1}{n} - \frac{1}{m} \right| < \frac{\varepsilon}{2}\) since both \(\frac{1}{m}\) and \(\frac{1}{n}\) are elements of \((0, \frac{\varepsilon}{2})\). Moreover \(1 + \frac{1}{m} > 1\) and \(1 + \frac{1}{n} > 1\). It follows that, for all \(m, n \geq N\), we have

\[
|r_m - r_n| < \frac{2 \cdot \frac{\varepsilon}{2}}{1 \cdot 1} = \varepsilon
\]

Hence \((r_n)\) is Cauchy, as claimed.

The following exercise generalises the previous example.

**Exercise 9.2.61**  
Prove that every convergent sequence is a Cauchy sequence.

**Theorem 9.2.62 (Cauchy criterion)**  
Every Cauchy sequence of real numbers converges.

**Proof**  
Let \((x_n)\) be a Cauchy sequence of real numbers.

First note that \((x_n)\) is bounded. To see this, note that by definition of Cauchy sequences, there is some \(N \in \mathbb{N}\) such that \(|x_m - x_n| < 1\) for all \(m, n \geq N\). In particular, \(|x_m - x_N| < 1\) for all \(m \geq N\). This means that the sequence \((x_n)\) is bounded below by

\[
a = \min\{x_0, x_1, \ldots, x_{N-1}, x_N - 1\}
\]

and is bounded above by

\[
b = \max\{x_0, x_1, \ldots, x_{N-1}, x_N + 1\}
\]

By the Bolzano‚ÄìWeierstrass theorem (Theorem 9.2.58), the sequence \((x_n)\) has a convergent subsequence \((x_{n_i})\). Let \(x^* = \lim_{i \to \infty} (x_{n_i})\). We prove that \((x_n) \to x^*\).

So let \(\varepsilon > 0\). Fix \(M\) sufficiently large that:

- \(|x_{n_i} - x^*| < \frac{\varepsilon}{3}\) for all \(n_i \geq M\); and
- \(|x_n - x_m| < \frac{\varepsilon}{3}\) for all \(m, n \geq M\).

Such a value of \(M\) exists by convergence of \((x_{n_i})\) and the Cauchy property of \((x_n)\).
</markdown><markdown>
Fix \( n \geq M \), and let \( i \in \mathbb{N} \) be arbitrary such that \( n_i \geq M \). Then we have

\[
|x_n - x^*| 
= |(x_n - x_M) + (x_M - x_{n_i}) + (x_{n_i} - x^*)| \quad \text{rearranging}
\]
\[
\leq |x_n - x_M| + |x_M - x_{n_i}| + |x_{n_i} - x^*| \quad \text{by the triangle inequality}
\]
\[
< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \quad \text{by the above properties}
\]
\[
= \varepsilon
\]

Hence \( (x_n) \to x^* \), as required.
\end{markdown><markdown>
## Section 9.3

# Series and sums

A *series* can be thought of as the result of adding up all of the terms in a sequence. The uses of series inside and outside of mathematics is widespread, particularly in analysis and statistics. In fact, we will use series repeatedly when we study probability theory in Chapter 11!

Unfortunately the definition of a series is not quite as simple as ‚Äòthe result of adding up all of the terms in a sequence‚Äô. For a start, we haven‚Äôt defined what means to add up infinitely many numbers, and sometimes this might not even be possible‚Äîfor example, you might encounter problems if you try adding up all of the terms in the constant sequence \( (1, 1, 1, \ldots) \).

The definition of a series, then, is that of a *formal sum* (see Definition 9.3.1). The word ‚Äòformal‚Äô here means that it is an expression that *represents* a sum, but is not actually evaluated. So for example

\[ 1 + 1 + 1 + \cdots \]

is a series.

We will then separately define what it means for it to be possible to evaluate an infinite sum represented by a series (Definition 9.3.3); this definition implies that the series \( 1 + 1 + 1 + \cdots \) is not summable, for example.

### Definition 9.3.1

A (real) *series* is a formal sum of a sequence \( (a_n)_{n \geq 0} \), denoted by

\[
\sum_{n \geq 0} a_n \quad (\text{LaTeX code: } \sum_{n \geq 0})
\]

or by

\[
\sum_{n=0}^{\infty} a_n \quad (\text{LaTeX code: } \sum_{n=0}^{\infty}),
\]

or even by \( a_0 + a_1 + a_2 + \cdots \).

As with sequences, it is possible for a series to be indexed from a different starting number, like in the next example.

### Example 9.3.2

The sequence \( \left( \frac{1}{k} \right)_{k \geq 1} \) defines the series

\[
\sum_{k \geq 1} \frac{1}{k} = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \cdots
\]
</markdown><markdown>
We will soon see that this series **diverges** ([Theorem 9.3.27](#)), since adding these terms together one by one yields unboundedly larger and larger real numbers.

Series in isolation are not particularly useful or interesting. They become so by defining what it means to evaluate them‚Äîat least, when it is possible to do so.

### Definition 9.3.3

Let \( N \in \mathbb{N} \). The \( N \)th **partial sum** of a series \(\sum_{n \geq 0} a_n\) is the real number \( s_N = \sum_{n=0}^{N} a_n \).

We say that the series **converges** if the sequence of partial sums \((s_N)_{N \geq 0}\) converges; in this case, the **sum** of the series is the real number \(\lim_{N \to \infty} (s_N)\), also written \(\sum_{n \geq 0} a_n\).

If the sequence of partial sums \((s_N)_{N \geq 0}\) diverges, then we say the series **diverges**.

### Example 9.3.4

Consider the series

\[
S = \sum_{n \geq 2} \binom{n}{2}^{-1}
\]

We prove that \( S \) converges and its sum is 2.

To see this, note that for all \( n \geq 2 \), we have by [Theorem 4.2.14](#) that

\[
\binom{n}{2}^{-1} = \left( \frac{n(n-1)}{2} \right)^{-1} = \frac{2}{n(n-1)} = \frac{2}{n-1} - \frac{2}{n}
\]

Therefore, for all \( N \geq 2 \), the \( N \)th partial sum of \( S \) is given by

\[
s_N = \sum_{n=2}^{N} \binom{n}{2}^{-1} = \left( \frac{2}{1} - \frac{2}{2} \right) + \left( \frac{2}{2} - \frac{2}{3} \right) + \cdots + \left( \frac{2}{N-1} - \frac{2}{N} \right) = 2 - \frac{2}{N}
\]

It follows that \( S = \lim_{N \to \infty} (s_N) = 2 \), as required.

### Exercise 9.3.5

Prove that the series \(\sum_{n \geq 3} \binom{n}{3}^{-1}\) converges, and find its sum.

### Example 9.3.6

We prove that the series \(\sum_{n \geq 0} 1\) diverges. Indeed, for all \( N \in \mathbb{N} \), we have

\[
\sum_{n=0}^{N} 1 = \underbrace{1 + 1 + \cdots + 1}_{N+1 \text{ times}} = N + 1
\]
</markdown><markdown>
Thus the sequence of partial sums is unbounded, so does not converge to a real number.

### Exercise 9.3.7
Prove that the series \(\sum_{n \geq 0} (-1)^n\) diverges.

The underlying reason why the series in Example 9.3.6 and Exercise 9.3.7 diverge is that their terms do not get smaller and smaller. We will prove in Theorem 9.3.22 that in order for a series to converge, its terms must tend to zero.

### Theorem 9.3.8 (Sum of geometric series)
Let \(r \in (-1, 1)\). Then \(\sum_{n \geq 0} r^n = \frac{1}{1-r}\).

**Proof**  
Given \(N \in \mathbb{N}\), the \(N\)th partial sum \(s_N\) of the series is given by

\[
s_N = \sum_{n=0}^{N} r^n = 1 + r + r^2 + \cdots + r^N
\]

Note that

\[
r s_N = \sum_{n=0}^{N} r^{n+1} = r + r^2 + \cdots + r^{N+1} = s_{N+1} - 1
\]

and hence

\[
(1-r)s_N = s_N - r s_N = s_N - (s_{N+1} - 1) = 1 - (s_{N+1} - s_N) = 1 - r^{N+1}
\]

and hence dividing by \(1-r\), which is permissible since \(r \neq 1\), yields

\[
s_N = \frac{1 - r^{N+1}}{1-r}
\]

Since \(|r| < 1\), we have \(r^{N+1} \to 0\) by Proposition 9.2.21, and so

\[
\sum_{n \geq 0} r^n = \lim_{N \to \infty} \frac{1 - r^{N+1}}{1-r} = \frac{1 - 0}{1-r} = \frac{1}{1-r}
\]

as claimed.

### Exercise 9.3.9
Prove that the series \(\sum_{n \geq 0} r^n\) diverges for all \(r \in \mathbb{R} \setminus (-1, 1)\).

The next result allows us to add two series together by adding their terms, and to multiply a series by a constant by multiplying their terms by the constant.
</markdown><markdown>
### Theorem 9.3.10 (Linearity of summation)

Let \(\sum_{n \geq 0} a_n\) and \(\sum_{n \geq 0} b_n\) be convergent series. Then

(a) The series \(\sum_{n \geq 0} (a_n + b_n)\) is convergent, and its sum is \(\sum_{n \geq 0} a_n + \sum_{n \geq 0} b_n\);

(b) For all \(c \in \mathbb{R}\), the series \(\sum_{n \geq 0} ca_n\) is convergent, and its sum is \(c \sum_{n \geq 0} a_n\).

#### Proof of (a)

For (a), note that the partial sums of \(\sum_{n \geq 0} (a_n + b_n)\) are given by

\[
\sum_{n=0}^{N} (a_n + b_n) = \sum_{n=0}^{N} a_n + \sum_{n=0}^{N} b_n
\]

so we may apply Theorem 9.2.34(a) to obtain

\[
\sum_{n \geq 0} (a_n + b_n)
\]

\[
= \lim_{N \to \infty} \left( \sum_{n=0}^{N} (a_n + b_n) \right)
\]

\[
= \lim_{N \to \infty} \left( \sum_{n=0}^{N} a_n + \sum_{n=0}^{N} b_n \right)
\]

\[
= \lim_{N \to \infty} \left( \sum_{n=0}^{N} a_n \right) + \lim_{N \to \infty} \left( \sum_{n=0}^{N} b_n \right)
\]

\[
= \sum_{n \geq 0} a_n + \sum_{n \geq 0} b_n
\]

as required.

### Exercise 9.3.11

Prove part (b) of Theorem 9.3.10, and deduce that if \(\sum_{n \geq 0} a_n\) and \(\sum_{n \geq 0} b_n\) are convergent series, then \(\sum_{n \geq 0} (a_n - b_n)\) converges, and its sum is equal to \(\sum_{n \geq 0} a_n - \sum_{n \geq 0} b_n\).

### Expansions of real numbers in number bases

We now take a brief detour away from the general theory of series to discuss an application, namely expansions of real numbers in number bases.
</markdown><markdown>
You are likely familiar with decimal expansions of real numbers, for example

\[
\frac{1}{2} = 0.5, \quad \frac{1}{7} = 0.142857142857\ldots, \quad \sqrt{2} = 1.414213562373\ldots
\]

A decimal expansion is really a series in disguise. For example

\[
0.142857\ldots = 1 \cdot \frac{1}{10} + 4 \cdot \frac{1}{10^2} + 2 \cdot \frac{1}{10^3} + 8 \cdot \frac{1}{10^4} + 5 \cdot \frac{1}{10^5} + 7 \cdot \frac{1}{10^6} + \cdots
\]

Thus when we write out a decimal expansion of a (non-negative, say) real number \( x \) as \( x = x_0.x_1x_2x_3\ldots \), what we are really saying is that

\[
x = x_0 + \sum_{i \geq 1} x_i \cdot 10^{-i}
\]

where \( x_0 \in \mathbb{N} \) and \( x_i \in \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\} \) for all \( i \geq 1 \).

We can apply this to other number bases. For example, the binary (base-2) expansion of \( \frac{1}{2} \) is 0.1 since \( \frac{1}{2} = 1 \cdot 2^{-1} \), and the binary expansion of \( \frac{1}{3} \) is 0.010101\ldots since

\[
0.010101\ldots (2) = \sum_{n \geq 1} \frac{1}{2^{2n}} = \frac{1}{4} \sum_{k \geq 0} \frac{1}{4^k} = \frac{1}{4} \cdot \frac{1}{1 - \frac{1}{4}} = \frac{1}{3}
\]

Our goal is to give a precise definition of the base-\( b \) expansion of a real number for an arbitrary number base \( b > 1 \).

In order to do this, we must prove that they are well-defined‚Äîthat is, every real number has a unique base-\( b \) expansion. But wait! It‚Äôs not true! Expansions are not quite unique‚Äîfor example, we have

\[
0.999\ldots = \sum_{i \geq 1} 9 \cdot 10^{-i} = \frac{9}{10} \sum_{k \geq 0} \frac{1}{10^k} = \frac{9}{10} \cdot \frac{1}{1 - \frac{1}{10}} = 1 = 1.000\ldots
\]

Conveniently, the only problem with uniqueness occurs with recurring 0s and recurring 9s: every number that has a decimal expansion with recurring 9s also has one with recurring 0s. More generally, in a base \( b > 0 \), recurring ‚Äò\( b - 1 \)‚Äôs may be replaced by recurring 0s.

We are now ready to proceed.
</markdown><markdown>
### Theorem 9.3.12

Let \( b \in \mathbb{N} \) with \( b > 1 \). For all \( x \in [0, \infty) \), there is a unique series \( \sum_{i \geq 0} \frac{x_i}{b^i} \) such that:

(i) \( x_0 \in \mathbb{N} \), and \( x_i \in \{0, 1, \ldots, b - 1\} \) for all \( i \geq 1 \);

(ii) The series \( \sum_{i \geq 0} \frac{x_i}{b^i} \) converges, and its sum is equal to \( x \); and

(iii) The sequence \( (x_i) \) is not eventually equal to \( b - 1 \).

#### Proof of existence

Fix \( x \geq 0 \). Define integers \( x_i \) and real numbers \( y_i \in [0, 1) \) for \( i \in \mathbb{N} \) recursively as follows:

- Let \( x_0 \in \mathbb{N} \) be such that \( x_0 \leq x < x_0 + 1 \), and let \( y_0 = x - x_0 \)‚Äînote that \( 0 \leq y_0 < 1 \), as required.

- Given \( i \in \mathbb{N} \), let \( x_{i+1} \in \mathbb{Z} \) such that \( x_{i+1} \leq by_i < x_{i+1} + 1 \), and let \( y_{i+1} = by_i - x_{i+1} \)‚Äînote \( 0 \leq y_{i+1} < 1 \) as required.

For all \( n \in \mathbb{N} \) we have

\[
x - \sum_{i=0}^{n} \frac{x_i}{b^i} = \frac{y_n}{b^n}
\]

We can prove this by induction:

- **(Base case)** We have \( x - \frac{x_0}{b^0} = x - x_0 = y_0 \) by construction.

- **(Induction step)** Fix \( n \geq 0 \) and suppose that \( x - \sum_{i=0}^{n} \frac{x_i}{b^i} = \frac{y_n}{b^n} \). Then

\[
x - \sum_{i=0}^{n+1} \frac{x_i}{b^i} = \frac{y_n}{b^n} - \frac{x_{n+1}}{b^{n+1}} = \frac{by_n - x_{n+1}}{b^{n+1}} = \frac{y_{n+1}}{b^{n+1}}
\]

as required.

We now verify the conditions in the statement of the theorem.

- Condition (i) is satisfied by construction of the sequence \( (x_i) \); indeed, we defined \( x_i \) to be integers for all \( i \in \mathbb{N} \), and if \( i \geq 1 \) then the fact that \( x_i \in \{0, 1, \ldots, b - 1\} \) follows from the facts that \( x_i \leq by_{i-1} < x_i + 1 \) and \( y_i \in [0, 1) \).

- To see that (ii) holds, note that for all \( n \in \mathbb{N} \) we have

\[
0 \leq x - \sum_{i=0}^{n} \frac{x_i}{b^i} = \frac{y_n}{b^n} < \frac{1}{b^n}
\]
</markdown><markdown>
But \( \left( \frac{1}{b^n} \right) \to 0 \) by Proposition 9.2.21, and so \( \sum_{i=0}^{n} \frac{x_i}{b^i} \) converges to \( x \).

- To prove (iii), suppose there is some \( n \in \mathbb{N} \) such that \( x_i = b - 1 \) for all \( i > n \). Then

\[
y_n = b^n \left( x - \sum_{i=0}^{n} \frac{x_i}{b^i} \right)
\]

as we proved above

\[
= b^n \left( \sum_{i=0}^{\infty} \frac{x_i}{b^i} - \sum_{i=0}^{n} \frac{x_i}{b^i} \right)
\]

by condition (ii)

\[
= b^n \sum_{i=n+1}^{\infty} \frac{x_i}{b^i}
\]

simplifying

\[
= b^n \sum_{i=n+1}^{\infty} \frac{b-1}{b^i}
\]

since \( x_i = b - 1 \) for all \( i > n \)

\[
= b^n \sum_{j=0}^{\infty} \frac{b-1}{b^{j+n+1}}
\]

substituting \( j = i - n - 1 \)

\[
= b^n \cdot \frac{b-1}{b^{n+1}} \sum_{j=0}^{\infty} \frac{1}{b^j}
\]

rearranging

\[
= b^n \cdot \frac{b-1}{b^{n+1}} \cdot \frac{1}{1 - \frac{1}{b}}
\]

by Theorem 9.3.8

\[
= 1
\]

simplifying

But this contradicts the fact that \( y_n \in [0, 1) \). So we do indeed have that \( (x_i) \) is not eventually equal to \( b - 1 \).

This completes the proof of existence. ‚ñ°

**Exercise 9.3.13**  
Prove the ‚Äòuniqueness‚Äô part of Theorem 9.3.12‚Äîthat is, prove that for all \( x \in [0, \infty) \), if

\[
\sum_{i \geq 0} \frac{u_i}{b^i} \quad \text{and} \quad \sum_{i \geq 0} \frac{v_i}{b^i}
\]

are two series satisfying conditions (i)‚Äì(iii) of Theorem 9.3.12, then \( u_i = v_i \) for all \( i \in \mathbb{N} \).

Theorem 9.3.12 justifies the following definition.
</markdown><markdown>
### Definition 9.3.14

Let \( b > 1 \). The **base-\( b \) expansion** of a real number \( x \) is the unique signed series \( \pm \sum_{i \geq 0} \frac{x_i}{b^i} \) such that:

(i) \( x_0 \in \mathbb{N} \), and \( x_i \in \{0, 1, \ldots, b - 1\} \) for all \( i \geq 1 \);

(ii) The series \( \sum_{i \geq 0} \frac{x_i}{b^i} \) converges, and its sum is equal to \( |x| \); and

(iii) The sequence \( (x_i) \) is not eventually equal to \( b - 1 \).

To denote the fact that this is the base-\( b \) expansion of \( x \), we may also write

\[
x = \pm x_0 . x_1 x_2 x_3 \cdots \, (b) \quad \text{or} \quad x = \pm d_r d_{r-1} \ldots d_1 d_0 . x_1 x_2 \cdots \, (b)
\]

where \( d_r d_{r-1} \ldots d_1 d_0 \) is the base-\( b \) expansion of \( |x_0| \) (as in Definition 0.6), and \( \pm \) is the sign of \( x \) (positive or negative).

The recursive definition of the sequence \( (x_i) \) in the proof of Theorem 9.3.12 yields an algorithm for computing base-\( b \) expansions.

### Strategy 9.3.15 (Finding base-\( b \) expansions of real numbers)

In order to find the base-\( b \) expansion of a real number \( x \):

- Let \( x_0 \in \mathbb{N} \) be such that \( x_0 \leq |x| < x_0 + 1 \), and define \( y_0 = |x| - x_0 \).

- For \( n \in \mathbb{N} \), given \( x_n \) and \( y_n \), let \( x_{n+1} \in \mathbb{N} \) be such that \( x_{n+1} \leq by_n < x_{n+1} + 1 \), and let \( y_{n+1} = by_n - x_{n+1} \). [Note that the value of \( x_{n+1} \) depends only on the value of \( y_n \), not on \( x_n \).]

Then \( x = \pm x_0 . x_1 x_2 x_3 \cdots \, (b) \), where \( \pm \) is \( '+' \) if \( x \geq 0 \), and \( '-' \) if \( x < 0 \).

### Example 9.3.16

Let's find the decimal expansion of \( \frac{1}{3} \).

- \( 0 \leq \frac{1}{3} < 1 \), so \( x_0 = 0 \) and \( y_0 = \frac{1}{3} - 0 = \frac{1}{3} \).

- \( 3 \leq 10 \cdot \frac{1}{3} < 4 \), so \( x_1 = 3 \) and \( y_1 = \frac{10}{3} - 3 = \frac{1}{3} \).

- \( 3 \leq 10 \cdot \frac{1}{3} < 4 \), so \( x_2 = 3 \) and \( y_2 = \frac{10}{3} - 3 = \frac{1}{3} \).

- \(\ldots\) evidently, this pattern repeats. (In fact, this can be proved by induction!)

So \( \frac{1}{3} = 0.33333\ldots \).
</markdown><markdown>
### Example 9.3.17

Now let‚Äôs find the decimal expansion of \( \frac{1}{7} \).

- \( 0 \leq \frac{1}{7} < 1 \), so \( x_0 = 0 \) and \( y_0 = \frac{1}{7} - 0 = \frac{1}{7} \).

- \( 1 \leq 10 \cdot \frac{1}{7} < 2 \), so \( x_1 = 1 \) and \( y_1 = \frac{10}{7} - 1 = \frac{3}{7} \).

- \( 4 \leq 10 \cdot \frac{3}{7} < 5 \), so \( x_2 = 4 \) and \( y_2 = \frac{30}{7} - 4 = \frac{2}{7} \).

- \( 2 \leq 10 \cdot \frac{2}{7} < 3 \), so \( x_3 = 2 \) and \( y_3 = \frac{20}{7} - 2 = \frac{6}{7} \).

- \( 8 \leq 10 \cdot \frac{6}{7} < 9 \), so \( x_4 = 8 \) and \( y_4 = \frac{60}{7} - 8 = \frac{4}{7} \).

- \( 5 \leq 10 \cdot \frac{4}{7} < 6 \), so \( x_5 = 5 \) and \( y_5 = \frac{40}{7} - 5 = \frac{5}{7} \).

- \( 7 \leq 10 \cdot \frac{5}{7} < 8 \), so \( x_6 = 7 \) and \( y_6 = \frac{50}{7} - 7 = \frac{1}{7} \).

- ...and now it repeats with the same pattern, since \( y_6 = y_0 \).

So \( \frac{1}{7} = 0.142857142857 \ldots \).

### Exercise 9.3.18

Use Strategy 9.3.15 to find the decimal expansion of \( \frac{1}{6} \).

### Exercise 9.3.19

Use Strategy 9.3.15 to find the **binary** expansion of \( \frac{1}{7} \).

### Exercise 9.3.20

Prove that between any two distinct real numbers, there is a rational number.

We will use expansions of real numbers in Chapter 10 to prove that the set of real numbers is **uncountably infinite**‚Äîthat is, even though the sets \( \mathbb{N} \) of natural numbers and \( \mathbb{R} \) of real numbers are both infinite, the size of the infinitude of \( \mathbb{R} \) is greater than that of \( \mathbb{N} \).

Now let‚Äôs return to learning about series in the abstract.

### Tests for convergence and divergence

Sometimes all we need to know about a series is whether it converges or diverges. In such cases, it can be very tricky to find an exact value for the sum of the series. We now develop some techniques for determining whether or not a series converges.
</markdown><markdown>
### Theorem 9.3.21 (Cauchy‚Äôs convergence test)
A series \(\sum_{n \geq 0} a_n\) converges if and only if, for all \(\varepsilon > 0\), there is some \(K \in \mathbb{N}\) such that

\[
\left| \sum_{n=n_0}^{n_1} a_n \right| < \varepsilon \text{ for all } n_1 \geq n_0 \geq K
\]

**Proof**

Let \((s_N)_{N \geq 0}\) be the sequence of partial sums of the series. By Theorem 9.2.62, we know that \(\sum_{n \geq 0} a_n\) is convergent if and only if \((s_N)_{N \geq 0}\) is a Cauchy sequence.

But the assertion that \((s_N)_{N \geq 0}\) is Cauchy is equivalent to the condition in the statement of this theorem: note that we may replace \(K\) by any larger value (in particular, we may assume \(K \geq 1\), and so

\[
\sum_{n=n_0}^{n_1} a_n = s_{n_1} - s_{n_0-1}
\]

as required.

We will use Cauchy‚Äôs convergence test frequently in our proofs. One example of how Cauchy‚Äôs convergence test can be put to work is the term test, which is very useful for proving that a series diverges‚Äîin fact, it instantly implies that the series in Example 9.3.6 and Exercise 9.3.7 diverge.

### Theorem 9.3.22 (Term test)
Let \(S = \sum_{n \geq 0} a_n\) be a series. If \(S\) converges, then \((a_n) \to 0\).

**Proof**

Fix \(\varepsilon > 0\). Since \(S\) converges, by Cauchy‚Äôs convergence test (Theorem 9.3.21) there exists \(K \in \mathbb{N}\) such that

\[
\left| \sum_{n=n_0}^{n_1} a_n \right| < \varepsilon \text{ for all } n_1 \geq n_0 \geq K.
\]

But then for all \(k \geq N\) we have \(k + 1 \geq k \geq K\), and so

\[
|a_k| = \left| \sum_{n=k}^{k+1} a_n \right| < \varepsilon
\]

so that \((a_k) \to 0\), as required.

### Example 9.3.23
The series \(\sum_{n \geq 0} n\) diverges since the sequence \((n)\) does not tend to zero.
</markdown><markdown>
### Exercise 9.3.24

Let \( a \in \mathbb{R} \). Prove that the series \(\sum_{n \geq 0} a\) converges if and only if \( a = 0 \).

### Theorem 9.3.25 (Comparison test)

Let \(\sum_{n \geq 0} a_n\) and \(\sum_{n \geq 0} b_n\) be series.

(a) If \(\sum_{n \geq 0} a_n\) converges and eventually \(0 \leq b_n \leq a_n\), then \(\sum_{n \geq 0} b_n\) converges; and

(b) If \(\sum_{n \geq 0} a_n\) diverges and eventually \(0 < a_n \leq b_n\), then \(\sum_{n \geq 0} b_n\) diverges.

**Proof of (a)**

Suppose \(\sum_{n \geq 0} a_n\) converges and its sum is equal to \(A\). Let \(N \in \mathbb{N}\) be sufficiently large that \(0 \leq b_n \leq a_n\) for all \(n \geq N\).

For all \(L \geq K \geq N\) we have

\[
\sum_{n=0}^{L} b_n = \sum_{n=0}^{K} b_n + \underbrace{\sum_{n=K+1}^{L} b_n}_{\geq 0} \geq \sum_{n=0}^{K} b_n
\]

and so the sequence of partial sums of \(\sum_{n \geq 0} b_n\) is eventually increasing.

Also for all \(M \geq N\) we have

\[
\sum_{n=0}^{M} b_n = \sum_{n=0}^{N} b_n + \sum_{n=N+1}^{M} b_n \leq \sum_{n=0}^{N} b_n + \sum_{n=N+1}^{M} a_n = \sum_{n=0}^{N} (b_n - a_n) + \sum_{n \geq 0} a_n
\]

Moreover \(a_n \geq 0\) for all \(n > M\), and so we have

\[
\sum_{n=0}^{M} b_n \leq \sum_{n=0}^{N} (b_n - a_n) + \sum_{n \geq 0} a_n
\]

Thus the sequence of partial sums of \(\sum_{n \geq 0} b_n\) is eventually bounded above.

By the monotone convergence theorem (Theorem 9.2.48), the sequence of partial sums of \(\sum_{n \geq 0} b_n\) converges, hence so does the series.
</markdown><markdown>
### Exercise 9.3.26
Prove part (b) of [Theorem 9.3.25](#).

The next result is a nice example of an indirect use of the term test ([Theorem 9.3.22](#)): although the terms in the series \(\sum_{n \geq 1} \frac{1}{n}\) converge to zero, we can manipulate it to bound it below by a series whose terms are unbounded.

### Theorem 9.3.27 (Divergence of the harmonic series)
The series \(\sum_{n \geq 1} \frac{1}{n}\) diverges.

**Proof**  
By rounding up denominators to the next power of 2, we get

\[
\frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} + \cdots \geq \frac{1}{1} + \frac{1}{2} + \frac{1}{4} + \frac{1}{4} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \cdots
\]

This diverges since we‚Äôre adding \(\frac{1}{2}\) infinitely many times.

More precisely, define a sequence \((a_n)_{n \geq 1}\) by letting \(a_n = \frac{1}{2^k}\) for the least \(k \in \mathbb{N}\) with \(\frac{1}{2^k} \leq \frac{1}{n}\). Then \(0 \leq a_n \leq \frac{1}{n}\) for each \(n \in \mathbb{N}\).

Now note that

\[
\sum_{n \geq 1} a_n = 1 + \sum_{k \geq 0} \sum_{n = 2^k}^{2^{k+1} - 1} \frac{1}{2^{k+1}} = 1 + \sum_{k \geq 0} (2^{k+1} - 2^k) \cdot \frac{1}{2^{k+1}} = 1 + \sum_{k \geq 0} \frac{1}{2}
\]

which diverges by the term test since \(\left(\frac{1}{2}\right) \not\to 0\).

Thus \(\sum_{r \geq 0} \frac{1}{r}\) diverges by the comparison test.

### Exercise 9.3.28
Prove that \(\sum_{n \geq 1} n^{-r}\) diverges for all real \(r > 1\).

### Theorem 9.3.29 (Alternating series test)
Let \((a_n)\) be a sequence such that \((a_n) \to 0\) and \(a_n \geq 0\) for all \(n \in \mathbb{N}\). If \((a_n)\) is decreasing‚Äîthat is, if \(a_m \geq a_n\) for all \(m, n \in \mathbb{N}\) with \(m \leq n\)‚Äîthen the series \(\sum_{n \geq 0} (-1)^n a_n\) converges.
</markdown><markdown>
Proof

Define sequences \((e_N)\) and \((o_N)\) by \(e_N = s_{2N}\) and \(o_N = s_{2N+1}\) for all \(n \in \mathbb{N}\). That is, \((e_N)\) is the sequence of even partial sums, and \((o_N)\) is the sequence of odd partial sums.

Then for all \(N \in \mathbb{N}\), we have

\[
e_{N+1} = s_{2N+2} = s_{2N} - a_{2N+1} + a_{2N+2} = e_N - \underbrace{(a_{2N+1} - a_{2N+2})}_{\geq 0} \leq e_N
\]

so that \((e_N)\) is a decreasing sequence, and

\[
e_N = a_0 - a_1 + \sum_{k=1}^{N-1} \underbrace{(a_{2k} - a_{2k+1})}_{\geq 0} + \underbrace{a_{2N}}_{\geq 0} \geq a_0 - a_1
\]

so that \((e_N)\) is bounded below. By the monotone convergence theorem (Theorem 9.2.48), the sequence \((e_N)\) converges.

Likewise, for all \(N \in \mathbb{N}\), we have

\[
o_{N+1} = s_{2N+3} = s_{2N+1} + a_{2N+2} - a_{2N+1} = o_N + \underbrace{(a_{2N+2} - a_{2N+1})}_{\geq 0} \geq o_N
\]

so that \((o_N)\) is an increasing sequence, and

\[
o_N = a_0 + \sum_{k=0}^{N} \underbrace{(a_{2k+2} - a_{2k+1})}_{\leq 0} - \underbrace{a_{2N+2}}_{\geq 0} \leq a_0
\]

so that \((o_N)\) is bounded above. By the monotone convergence theorem again, the sequence \((o_N)\) converges.

Moreover for all \(N \in \mathbb{N}\) we have

\[
o_N - e_N = a_{2N+1} \geq 0 \quad \text{and} \quad e_{N+1} - o_N = a_{2N+2} \geq 0
\]

so that \(e_N \leq o_N \leq e_{N+1}\) for all \(N \in \mathbb{N}\).

But then by the squeeze theorem (Theorem 9.2.38), \((e_N)\) and \((o_N)\) converge to the same limit \(A \in \mathbb{R}\).

Finally, let \(\varepsilon > 0\) and let \(K \in \mathbb{N}\) be sufficiently large that \(|e_M - A| < \varepsilon\) and \(|o_M - A| < \varepsilon\) for all \(M \geq K\). Then given \(N \geq 2K\), if \(N\) is even then

\[
|s_N - A| = |e_{\frac{N}{2}} - A| < \varepsilon
\]

and if \(N\) is odd then

\[
|s_N - A| = |o_{\frac{N-1}{2}} - A| < \varepsilon
\]

as required. So \((s_N) \to A\), and so the series converges.
</markdown><markdown>
### Example 9.3.30

The sequence \( \left( \frac{1}{n} \right) \) is positive and decreasing, so the series

\[
\sum_{n \geq 1} \frac{(-1)^n}{n} = -1 + \frac{1}{2} - \frac{1}{3} + \frac{1}{4} - \frac{1}{5} + \cdots
\]

converges.

### Exercise 9.3.31

Prove that if \( (a_n) \) is a sequence such that \( (a_n) \to 0 \) and \( a_n \leq 0 \) for all \( n \in \mathbb{N} \). Prove that if \( (a_n) \) is an increasing sequence, then the series \( \sum_{n \geq 0} (-1)^n a_n \) converges.

### Exercise 9.3.32

Find a decreasing sequence \( (a_n) \) of non-negative real numbers such that \( \sum_{n \geq 0} (-1)^n a_n \) diverges.

## Absolute convergence

#### Definition 9.3.33

A series \( \sum_{n \geq 0} a_n \) converges absolutely if the series \( \sum_{n \geq 0} |a_n| \) converges.

### Example 9.3.34

For all \( r \in (-1, 1) \), the geometric series \( \sum_{n \geq 0} r^n \) converges absolutely. Indeed, for all \( r \in (-1, 1) \) we have \( |r| \in (-1, 1) \) as well, and so \( \sum_{n \geq 0} |r|^n \) converges by Theorem 9.3.8.

### Example 9.3.35

Let \( \sum_{n \geq 0} a_n \) be a convergent series.

If \( a_n \geq 0 \) for all \( n \in \mathbb{N} \), then the series absolutely, since \( |a_n| = a_n \) for all \( n \in \mathbb{N} \).

Likewise, if \( a_n \leq 0 \) for all \( n \in \mathbb{N} \), then

\[
\sum_{n \geq 0} |a_n| = \sum_{n \geq 0} (-a_n) = - \sum_{n \geq 0} a_n
\]

by linearity of summation, and so again the series converges absolutely.

### Exercise 9.3.36

Find a series that converges, but does not converge absolutely.

### Exercise 9.3.37

Prove Theorem 9.3.10 with ‚Äòconvergent‚Äô replaced by ‚Äòabsolutely convergent‚Äô throughout.
</markdown><markdown>
Absolutely convergent series enjoy some properties that are not enjoyed by series that converge but not absolutely‚Äîfor example, they do not depend on what order you choose to add up their terms. We will prove this in [Theorem 9.3.42](#).

The **ratio test** is useful for proving that a series converges absolutely.

### Theorem 9.3.38 (Ratio test)

Let \((a_n)\) be a sequence of real numbers, and suppose that

\[
\left| \frac{a_{n+1}}{a_n} \right| \to \ell \geq 0.
\]

(a) If \(\ell < 1\), then \(\sum_{n \geq 0} a_n\) converges absolutely.

(b) If \(\ell > 1\), then \(\sum_{n \geq 0} a_n\) diverges.

**Proof of (a)**

Assume \(\ell < 1\), and pick \(\varepsilon\) with \(0 < \varepsilon < 1 - \ell\). Define \(r = \ell + \varepsilon\) and note that \(0 < r < 1\).

Since

\[
\left| \frac{a_{n+1}}{a_n} \right| \to \ell,
\]

there exists \(N \in \mathbb{N}\) such that

\[
\left| \frac{a_{n+1}}{a_n} - \ell \right| < \varepsilon \quad \text{for all } n \geq N.
\]

But then

\[
0 \leq \left| \frac{a_{n+1}}{a_n} \right| < \ell + \varepsilon = r.
\]

Note that for all \(n \geq N\) we have

\[
|a_n| = |a_N| \times \left| \frac{a_{N+1}}{a_N} \right| \times \cdots \times \left| \frac{a_n}{a_{n-1}} \right| < |a_N| r^{n-N}.
\]

The series \(\sum_{n=N}^\infty r^{n-N}\) converges by [Theorem 9.3.8](#) since \(r \in (-1, 1)\), and so the series

\[
\sum_{n=N}^M |a_n|
\]

converges by the comparison test, as required.

### Exercise 9.3.39

Prove part (b) of [Theorem 9.3.38](#).

### Example 9.3.40

Let \(r \in \mathbb{R}\) and consider the series \(\sum_{n \geq 1} \frac{r^n}{n}\). Then

\[
\left| \frac{r^{n+1}}{n+1} \cdot \frac{n}{r^n} \right| = \frac{n}{n+1} |r| \to |r|
\]
</markdown><markdown>
By the ratio test, if \( |r| < 1 \) then the series converges absolutely, and if \( |r| > 1 \) then the series diverges.

The ratio test tells us nothing about what happens when \( |r| = 1 \). However, we already know: we proved in Theorem 9.3.27 that this series diverges when \( r = 1 \), and in Example 9.3.30 that it converges when \( r = -1 \).

### Exercise 9.3.41
Use the ratio test to prove that the series \(\sum_{n \geq 0} \frac{x^n}{n!}\) converges for all \( x \in \mathbb{R} \).

---

### Theorem 9.3.42 (Absolutely convergent series can be reordered)
Let \(\sum_{n \geq 0} a_n\) be an absolutely convergent series and let \(\sigma : \mathbb{N} \to \mathbb{N}\) be a bijection. Then the series \(\sum_{n \geq 0} a_{\sigma(n)}\) converges absolutely, and \(\sum_{n \geq 0} a_{\sigma(n)} = \sum_{n \geq 0} a_n\).

**Proof**

Write \( A = \sum_{n \geq 0} a_n \). In order to prove \(\sum_{n \geq 0} a_{\sigma(n)} = A\), we need to prove that for all \(\varepsilon > 0\), there is some \( N \in \mathbb{N} \) such that

\[
\left| \sum_{n=0}^{K} a_{\sigma(n)} - A \right| < \varepsilon \text{ for all } K \geq N.
\]

So let \(\varepsilon > 0\). Then:

1. Since \(\sum_{n \geq 0} a_n = A\), there is some \( M_1 \in \mathbb{N} \) such that

\[
\left| \sum_{n=0}^{L} a_n - A \right| < \frac{\varepsilon}{2}
\]

for all \( L \geq M_1 \).

2. Since \(\sum_{n \geq 0} a_n\) converges absolutely, it follows from Cauchy‚Äôs convergence test (Theorem 9.3.21) there is some \( M_2 \in \mathbb{N} \) such that

\[
\sum_{n=n_0}^{n_1} |a_n| < \frac{\varepsilon}{2}
\]

for all \( n_1 \geq n_0 \geq M_2 \).

Let \( M \) be the greater of \( M_1 \) and \( M_2 \).

Now let \( N \in \mathbb{N} \) be such that \(\{0, 1, \ldots, M\} \subseteq \{\sigma(0), \sigma(1), \ldots, \sigma(N)\}\). This ensures that the terms \( a_0, a_1, \ldots, a_M \) appear amongst the terms \( a_{\sigma(0)}, a_{\sigma(1)}, \ldots, a_{\sigma(N)} \). Such a value
</markdown><markdown>
exists since \( \sigma \) is a bijection; for example, we can take \( N \) to be the greatest value of \( \sigma^{-1}(i) \) for \( i \leq M \). Note that \( N \geq M \).

It remains to prove that

\[
\left| \sum_{n=0}^{K} a_{\sigma(n)} - A \right| < \varepsilon \text{ for all } K \geq N.
\]

So let \( K \geq N \), let \( L = \max \{ \sigma(0), \sigma(1), \ldots, \sigma(K) \} \), and let \( m_0, m_1, \ldots, m_r \in \mathbb{N} \) be such that the terms in the list \( a_0, a_1, \ldots, a_L \) that remain after the terms \( a_{\sigma(0)}, \ldots, a_{\sigma(K)} \) have been deleted are precisely the terms \( a_{m_0}, a_{m_1}, \ldots, a_{m_r} \). Thus

\[
\sum_{n=0}^{K} a_{\sigma(n)} = \sum_{n=0}^{L} a_n - \sum_{i=0}^{r} a_{m_i}
\]

Note in particular that \( L \geq K \). Additionally, \( m_i \geq N \) for all \( i \leq r \), since we ensured that the terms \( a_0, a_1, \ldots, a_M \) appear amongst the terms \( a_{\sigma(0)}, a_{\sigma(1)}, \ldots, a_{\sigma(N)} \).

By the triangle inequality (Theorem 9.1.9) we have

\[
\left| \sum_{n=0}^{K} a_{\sigma(n)} - A \right| = \left| \sum_{n=0}^{L} a_n - A - \sum_{i=0}^{r} a_{m_i} \right| \leq \left| \sum_{n=0}^{L} a_n - A \right| + \left| \sum_{i=0}^{r} a_{m_i} \right|
\]

Now conditions (1) and (2) above give the following:

1. \( L \geq K \geq N \geq M \geq M_1 \), and so

\[
\left| \sum_{n=0}^{L} a_n - A \right| < \frac{\varepsilon}{2}.
\]

2. By the triangle inequality again, we have

\[
\left| \sum_{i=0}^{r} a_{m_i} \right| \leq \sum_{i=0}^{r} |a_{m_i}| \leq \sum_{n=N}^{L} |a_n| < \frac{\varepsilon}{2}
\]

since \( L \geq N \geq M \geq M_2 \).

Putting all of this together, we have

\[
\left| \sum_{n=0}^{K} a_{\sigma(n)} - A \right| \leq \left| \sum_{n=0}^{L} a_n - A \right| + \left| \sum_{i=0}^{r} a_{m_i} \right| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\]

as required.

By replacing \( a_n \) with \( |a_n| \) in the above proof, it follows that \( \sum_{n \geq 0} |a_{\sigma(n)}| \) converges to the same limit as \( \sum_{n \geq 0} |a_n| \). In particular, it converges, and so the series \( \sum_{n \geq 0} a_{\sigma(n)} \) converges absolutely.
</markdown><markdown>
### Example 9.3.43

Consider the following geometric series

\[
\sum_{n \geq 0} \frac{(-1)^n}{2^n} = 1 - \frac{1}{2} + \frac{1}{4} - \frac{1}{8} + \frac{1}{16} - \frac{1}{32} + \cdots = \frac{1}{1 - \left(-\frac{1}{2}\right)} = \frac{2}{3}
\]

As noted in Example 9.3.34, this series converges absolutely. So if we were to add the terms in any other way, then we‚Äôd obtain the same result. For example

\[
1 + \frac{1}{4} - \frac{1}{2} + \frac{1}{8} + \frac{1}{16} + \frac{1}{64} - \frac{1}{32} - \frac{1}{128} + \cdots = \frac{2}{3}
\]

Absolute convergence is what allows us to do this.

Theorem 9.3.42 allows us to index absolutely convergent series over sets other than \(\mathbb{N}\). This will be useful in Chapter 11, when the numbers we add are probabilities that are more naturally indexed by the outcomes of a random process than by natural numbers.

### Definition 9.3.44

Let \( I = \{i_n \mid n \in \mathbb{N}\} \) be a set with \( i_m \neq i_n \) for all \( m, n \in \mathbb{N} \) with \( m \neq n \), and let \((a_i)_{i \in I}\) be an \( I \)-indexed sequence of real numbers‚Äîformally \((a_i)_{i \in I}\) is a function \( a : I \to \mathbb{R} \), like in Definition 9.2.9. The \( I \)-indexed series \(\sum_{i \in I} a_i\) is defined by

\[
\sum_{i \in I} a_i = \sum_{n \geq 0} a_{i_n}
\]

Note that, for general sums, the value of \(\sum_{i \in I} a_i\) might depend on how the set \( I \) is enumerated. However, in practice, we will only use the notation \(\sum_{i \in I} a_i\) when either (i) the series \(\sum_{n \geq 0} a_{i_n}\) converges absolutely, in which case the terms can be reordered however we like; or (ii) the terms \( a_{i_n} \) are all non-negative, in which case the series either converges absolutely, or diverges no matter how the elements of \( I \) are ordered.

### Example 9.3.45

Suppose \((a_k)_{k \in \mathbb{Z}}\) is a sequence of non-negative real numbers. Given \( n \in \mathbb{N} \), define

\[
i_n = 
\begin{cases} 
\frac{n}{2} & \text{if } n \text{ is even} \\
-\frac{n+1}{2} & \text{if } n \text{ is odd}
\end{cases}
\]

Then \(\mathbb{Z} = \{i_n \mid n \in \mathbb{N}\} = \{0, -1, 1, -2, 2, \ldots\}\), and so

\[
\sum_{k \in \mathbb{Z}} a_k = a_0 + a_{-1} + a_1 + a_{-2} + a_2 + a_3 + a_{-3} + \cdots \overset{*}{=} \sum_{i \geq 0} a_i + \sum_{i \geq 1} a_{-i}
\]
</markdown><markdown>
as expected. The step \((\star)\) implicitly used non-negativity of the terms in the sequence: either the sequence diverges, or it converges absolutely, in which case we can shuffle the order of the terms.

### Exercise 9.3.46
Let \( J = \{ j_n \mid n \in \mathbb{N} \} \) be a set such that, for all \( m, n \in \mathbb{N} \), we have \( j_m \neq j_n \). Let \((a_j)_{j \in J}\) be a \( J \)-indexed sequence such that either (i) \(\sum_{j \in J} a_j\) is absolutely convergent, or (ii) \( a_j \geq 0 \) for all \( j \in J \). Prove that if there is a bijection \(\sigma : I \to J\), then \(\sum_{i \in I} a_{\sigma(i)} = \sum_{j \in J} a_j\).

Absolutely convergent series can be multiplied using the so-called **Cauchy product**, which is a kind of convolution operation on series.

### Theorem 9.3.47 (The Cauchy product)
Let \(\sum_{n \geq 0} a_n\) and \(\sum_{n \geq 0} b_n\) be absolutely convergent series. Then the series

\[
\sum_{n \geq 0} \left( \sum_{k=0}^{n} a_k b_{n-k} \right) = \left( \sum_{i \geq 0} a_i \right) \left( \sum_{j \geq 0} b_j \right)
\]

**Proof**

By linearity of summation we have

\[
\left( \sum_{n \geq 0} a_n \right) \left( \sum_{n \geq 0} b_n \right) = \sum_{i \geq 0} \left( \sum_{j \geq 0} a_i b_j \right) = \sum_{(i,j) \in \mathbb{N} \times \mathbb{N}} a_i b_j
\]

Note that this series converges absolutely. Indeed, given \(\varepsilon > 0\), since \(\sum_{n \geq 0} a_n\) converges absolutely, there is some \(N \in \mathbb{N}\) such that \(\sum_{i > N} |a_n| < \frac{\varepsilon}{\sum_{j \in \mathbb{N}} |b_j|}\), and then

\[
\sum_{i > N} \sum_{j = 0}^{\infty} |a_i b_j| = \sum_{i > N} \left( |a_i| \sum_{j = 0}^{\infty} |b_j| \right) < \varepsilon
\]

as required.

Now define \(P = \{ (n, k) \in \mathbb{N} \times \mathbb{N} \mid k \leq n \}\), and note that

\[
\sum_{n \geq 0} \left( \sum_{k=0}^{n} a_k b_{n-k} \right) = \sum_{(n,k) \in P} a_k b_{n-k}
\]
</markdown><markdown>
The function \(\sigma : \mathbb{N} \times \mathbb{N} \to P\) defined by \(\sigma(i, j) = (i + j, i)\) is a bijection. Since the series converges absolutely, we may apply Exercise 9.3.46 to obtain

\[
\sum_{(n,k) \in P} a_k b_{n-k} = \sum_{(i,j) \in \mathbb{N} \times \mathbb{N}} a_i b_{(i+j)-i} = \sum_{(i,j) \in \mathbb{N} \times \mathbb{N}} a_i b_j
\]

as required.

An example of how the Cauchy product can come in handy is to prove the multiplicativity of the **exponential function**, defined in Definition 9.3.48.

‚ú¶ **Definition 9.3.48**

The **exponential function** is the function \(\exp : \mathbb{R} \to \mathbb{R}\) defined by

\[
\exp(x) = \sum_{n \geq 0} \frac{x^n}{n!}
\]

for all \(x \in \mathbb{R}\).

Note that the exponential function is well-defined by Exercise 9.3.41.

‚ú£ **Theorem 9.3.49 (Multiplicativity of the exponential function)**

Let \(x, y \in \mathbb{R}\). Then \(\exp(x + y) = \exp(x) \exp(y)\).

**Proof**

First note that the function

\[
R : \{(n,k) \in \mathbb{N} \times \mathbb{N} \mid k \leq n\} \to \mathbb{N} \times \mathbb{N}
\]

defined by \(R(n,k) = (k, n-k)\) is a bijection. Indeed:

- Let \((n,k), (m, \ell) \in \mathbb{N} \times \mathbb{N}\) with \(k \leq n\) and \(\ell \leq m\). If \((k, n-k) = (\ell, m-\ell)\) then \(k = \ell\), and so \(n-k = m-\ell\), so that \(m = n\). So \(R\) is injective.

- Let \((a, b) \in \mathbb{N} \times \mathbb{N}\). Then \(b \leq a + b\) and \(b = (a + b) - b\), and so \((a, b) = R(a + b, a)\). So \(R\) is surjective.
</markdown><markdown>
Now we proceed by computation:

\[
\exp(x+y) = \sum_{n \geq 0} \frac{(x+y)^n}{n!} \quad \text{by definition of exp}
\]

\[
= \sum_{n \geq 0} \sum_{k=0}^{n} \frac{1}{n!} \binom{n}{k} x^k y^{n-k} \quad \text{by the binomial theorem (Theorem 4.2.17)}
\]

\[
= \sum_{n \geq 0} \sum_{k=0}^{n} \frac{1}{k!(n-k)!} x^k y^{n-k} \quad \text{by Theorem 4.2.14}
\]

\[
= \sum_{a \geq 0} \sum_{b \geq 0} \frac{1}{a!b!} x^a y^b \quad \text{using the reindexing bijection } R
\]

\[
= \left( \sum_{a \geq 0} \frac{x^a}{a!} \right) \left( \sum_{b \geq 0} \frac{y^b}{b!} \right) \quad \text{by linearity of summation}
\]

\[
= \exp(x) \exp(y) \quad \text{by definition of exp}
\]

This is as required.

### Exercise 9.3.50
Let \( x \in (-1, 1) \). Prove that \(\sum_{n \geq 1} nx^{n-1} = \frac{1}{(1-x)^2}\).

### The constant \( e \)

We conclude this section by defining \( e \), a mathematical constant used heavily in calculus and analysis. We will prove that \( e \) is irrational.

#### Lemma 9.3.51
The series \(\sum_{n \geq 0} \frac{1}{n!}\) converges.

**Proof**

This follows from the ratio test. Indeed

\[
\frac{1/(n+1)!}{1/n!} = \frac{n!}{(n+1)!} = \frac{1}{n+1} \to 0
\]

so by the ratio test (Theorem 9.3.38), the series converges.

#### Definition 9.3.52
The real number \( e \), also known as Euler‚Äôs constant, is defined by \( e = \exp(1) = \sum_{n \geq 0} \frac{1}{n!} \).
</markdown><markdown>
The number \( e \), like its more famous cousin \( \pi \), is one of the fundamental constants of mathematics. It might seem arbitrary now, but it has remarkable analytic properties that are beyond the scope of this book.

### Example 9.3.53
We can prove that \( 2 < e < 3 \). Indeed \( e > 2 \) since

\[
e > \frac{1}{0!} + \frac{1}{1!} = 1 + 1 = 2
\]

To see that \( e < 3 \), note that \( n! \geq 2^{n-1} \) for all \( n \geq 1 \), with strict inequality for \( n \geq 2 \), and so

\[
e < \frac{1}{0!} + \sum_{n \geq 1} \frac{1}{2^{n-1}} = 1 + \frac{1}{1 - \frac{1}{2}} = 3
\]

### Exercise 9.3.54
Prove that \(\exp(x) = e^x\) for all \( x \in \mathbb{Q} \).

### Theorem 9.3.55
\( e \) is irrational.

**Proof**

Towards a contradiction, suppose that \( e \in \mathbb{Q} \).

Then \( k!e \in \mathbb{Z} \) for some natural number \( k \geq 2 \). Indeed, letting \( e = \frac{a}{b} \) for some \( a, b \in \mathbb{Z} \) with \( b \neq 0 \), we obtain \( |b|e = \pm a \in \mathbb{N} \), and so we can take \( k \) to be the greatest of 2 and \( |b| \).

Now observe that

\[
k!e = \sum_{n \geq 0} \frac{k!}{n!} = \sum_{n=0}^{k} \frac{k!}{n!} + \sum_{n \geq k+1} \frac{k!}{n!}
\]

Define \( c = \sum_{n \geq k+1} \frac{k!}{n!} \). We will prove that \( c \in \mathbb{Z} \) and \( 0 < c < 1 \), which is a contradiction.

Note that for all \( n \leq k \) we have \( n! \) divides \( k! \), and so \( \sum_{n=0}^{k} \frac{k!}{n!} \in \mathbb{Z} \). Therefore

\[
c = k!e - \sum_{n=0}^{k} \frac{k!}{n!} \in \mathbb{Z}
\]

since this is the difference of two integers.
</markdown><markdown>
Now all the terms in the sum \(\sum_{n \geq k+1} \frac{k!}{n!}\) are positive, and so

\[
c = \sum_{n = k+1} \frac{k!}{n!} > \frac{k!}{(k+1)!} = \frac{1}{k+1} > 0
\]

Furthermore, for all \(n \geq k+1\), we have

\[
\frac{k!}{n!} = \frac{1 \times 2 \times \cdots \times k}{1 \times 2 \times \cdots \times k \times (k+1) \times \cdots \times n} = \frac{1}{(k+1) \times \cdots \times n} \leq \frac{1}{(k+1)^{n-k}}
\]

It follows that

\[
c = \sum_{n \geq k+1} \frac{k!}{n!} \leq \sum_{n \geq k+1} \frac{1}{(k+1)^{n-k}} \quad \text{as we just observed}
\]

\[
= \sum_{r \geq 0} \frac{1}{(k+1)^{r+1}} \quad \text{substituting } r = n-k-1
\]

\[
= \frac{1}{k+1} \sum_{r \geq 0} \frac{1}{(k+1)^r} \quad \text{by linearity}
\]

\[
= \frac{1}{k+1} \cdot \frac{1}{1 - \frac{1}{k+1}} \quad \text{by Theorem 9.3.8}
\]

\[
= \frac{1}{k} \quad \text{rearranging}
\]

\[
< 1 \quad \text{since } k \geq 2
\]

But this implies that \(0 < c < 1\), which is nonsense since \(c \in \mathbb{Z}\).

We have arrived at a contradiction, so it follows that \(e\) is irrational.
\end{markdown><markdown>
## Section 9.E
# Chapter 9 exercises

### Inequalities and means

9.1. Compute \(\|2(-1, -5, 1, 5, 1) - (2, -1, 1, -2, 8)\|\).

9.2. Let \(a, b \in \mathbb{R}\) and let \(a_0, a_1, a_2, \ldots, a_n \in \mathbb{R}\) with \(a_0 = a\) and \(a_n = b\). Prove that
\[
|a - b| \leq \sum_{k=1}^{n} |a_{k-1} - a_k|.
\]

9.3. Let \(n \in \mathbb{N}\). Prove that \(\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}\) for all \(\vec{x}, \vec{y} \in \mathbb{R}^n\).

9.4. Let \(n \in \mathbb{N}\), let \(\vec{x}, \vec{y} \in \mathbb{R}^n\) and let \(a, b, c, d \in \mathbb{R}\). Prove that \((a\vec{x} + b\vec{y}) \cdot (c\vec{x} + d\vec{y}) = ac\|\vec{x}\|^2 + bd\|\vec{y}\|^2 + (ad + bc)(\vec{x} \cdot \vec{y})\).

9.5. Let \(a, b, c, d \in \mathbb{R}\). Assume that \(a^2 + b^2 = 9\), \(c^2 + d^2 = 25\) and \(ac + bd = 15\). Find the value of \(\frac{a + 2b}{c + 2d}\).

9.6. Let \(\vec{a} \in \mathbb{R}^3\) and \(r > 0\), and define
\[
B(\vec{a}; r) = \{\vec{x} \in \mathbb{R}^3 \mid \|\vec{x} - \vec{a}\| < r\}
\]
Prove that \(\|\vec{x} - \vec{y}\| < 2r\) for all \(\vec{x}, \vec{y} \in B(\vec{a}; r)\).

9.7. Prove that \(a^3 + b^3 + c^3 \geq abc\) for all positive real numbers \(a, b, c\). When does equality occur?

9.8. Let \(a, b, c > 0\). Prove that
\[
\frac{b + c}{a} + \frac{c + a}{b} + \frac{a + b}{c} \geq 6,
\]
with equality if and only if \(a = b = c\).

9.9. Let \(a, b, c \in \mathbb{R}\). Prove that
\[
a^4 + b^4 + c^4 \geq \frac{(ab + bc + ca)^2}{3}.
\]
When does equality occur?

9.10. **Unit cost averaging** is an investment strategy in which an investor purchases asset in equal instalments over a fixed period of time.

Let \(a, b \in \mathbb{R}\) with \(a < b\), let \(n \geq 1\), and let \(f : [a, b] \rightarrow (0, \infty)\) be such that at time \(t \in [a, b]\), a stock is trading at ¬£\(f(t)\) per share. Assume that you purchase a total of ¬£\(M\) of the stock in equal instalments of ¬£\(\frac{M}{n}\) at times \(t_0, t_1, \ldots, t_{n-1}\), where \(M \in [0, \infty)\), \(n \geq 1\), and
\[
t_i = a + i \cdot \frac{b - a}{n} \quad \text{for all } 0 \leq i < n.
\]
</markdown><markdown>
(a) Prove that the value of the shares that you hold at time \( b \) is equal to

\[
¬£ \frac{f(b)}{\text{HM}(f(t_0), f(t_1), \ldots, f(t_{n-1}))} \cdot M
\]

where \(\text{HM}(a_1, a_2, \ldots, a_n)\) denotes the harmonic mean of \( a_1, a_2, \ldots, a_n \in \mathbb{R} \).

(b) Under what condition have you made a profit?

**Sequences**

9.11. Does there exist a sequence \((x_n)\) such that \((x_{n+1} - x_n) \to 0\) but \((x_n)\) diverges?

**True‚ÄìFalse questions**

In Questions 9.12 to 9.18, determine (with proof) whether the statement is true or false.

9.12. For all \( x, y \in \mathbb{R} \) we have \(|x - y| \leq |x| - |y|\).

9.13. The function \( f : \mathbb{R}^n \to \mathbb{R} \) defined by \( f(\vec{x}) = \|\vec{x}\| \) for all \(\vec{x} \in \mathbb{R}^n\) is injective.

9.14. The function \( f : \mathbb{R}^n \to \mathbb{R} \) defined by \( f(\vec{x}) = \|\vec{x}\| \) for all \(\vec{x} \in \mathbb{R}^n\) is surjective.

9.15. For all \(\vec{x}, \vec{y} \in \mathbb{R}^n\) we have \(\|2\vec{x} - 3\vec{y}\| \leq 2\|\vec{x}\| + 3\|\vec{y}\|\).

9.16. Every convergent sequence is bounded.

9.17. Every convergent sequence is eventually monotone.

9.18. Every subsequence of a divergent sequence diverges.

**Always‚ÄìSometimes‚ÄìNever questions**

In Questions 9.19 to 9.26, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

9.19. Let \(\vec{x}, \vec{y} \in \mathbb{R}^n\) and suppose that \(\|\vec{x} - \vec{y}\| = 0\). Then \(\vec{x} = \vec{y}\).

9.20. Let \( x_1, x_2, \ldots, x_n \geq 0 \). Then the harmonic mean of the \( x_i \)s is greater than their quadratic mean.

9.21. Let \( y_1, y_2, \ldots, y_m \geq 0 \). Then the geometric mean of the \( y_j \)s is less than or equal to their quadratic mean.

9.22. Let \((x_n)\) be a sequence of real numbers and suppose that the subsequences \((x_{2n})\) and \((x_{2n+1})\) both converge. Then \((x_n)\) converges.
</markdown><markdown>
9.23. Let \((x_n)\) be a sequence of real numbers and suppose that the subsequences \((x_{2n})\), \((x_{2n+1})\) and \((x_{3n})\) all converge. Then \((x_n)\) converges.

9.24. Let \(f : \mathbb{R} \to \mathbb{R}\), let \((x_n)\) be a sequence of real numbers, and suppose that \((x_n) \to a \in \mathbb{R}\). Then \((x_n^2) \to a^2\).

9.25. Let \(f : \mathbb{R} \to \mathbb{R}\), let \((x_n)\) be a sequence of real numbers, and suppose that \((x_n) \to a \in \mathbb{R}\). Then \((f(x_n)) \to f(a)\).

9.26. Let \((a_n)\) be a sequence of real numbers and suppose that \((a_n) \to 0\). Then \(\sum_{n \geq 0} a_n\) converges.
</markdown>It seems the page is blank, so there is no text to extract.<markdown>
# Chapter 10

## Infinity
</markdown><markdown>
## Section 10.1

# Cardinality

[Section 6.1](#) was all about defining a notion of *size* for finite sets, and using this definition to compare and contrast the sizes of finite sets by constructing injections, surjections and bijections between them.

We made some progress in comparing the sizes of infinite sets in [Section 6.2](#), but only to a certain extent: at this point, we can only distinguish between two sizes of infinity, namely ‚Äòcountable‚Äô and ‚Äòuncountable‚Äô. While this is interesting, we can do much better‚Äîthat is where this section comes in.

The *cardinality* (defined in [Definition 10.1.1](#)) of a set can be understood as a measure of its size, generalising the notion of size for finite sets. In particular:

- Whereas the size of a *finite* set \( X \) is a natural number \(|X| \in \mathbb{N}\), the cardinality of an arbitrary set is a *cardinal number*‚Äîbut if the set happens to be finite, then this cardinal number is equal to the natural number given by its size.

- We proved in [Theorem 6.1.14](#) that two finite sets \( X \) and \( Y \) have equal size if and only if there is a bijection \( X \to Y \). We generalise this fact to arbitrary sets by building it into the definition of cardinality: that is, two sets \( X \) and \( Y \) will have equal cardinality if and only if there is a bijection \( X \to Y \).

Without further ado, behold the definition of cardinality.

### Definition 10.1.1

The *cardinality* of a set \( X \) is an element \(|X|\) of the collection \(\text{Card}\) (\(\LaTeX\) code: \(\mathtt{\{Card\}}\)) of all *cardinal numbers*, defined so that the following properties hold:

(i) For every set \( X \), there is a unique cardinal number \(\kappa \in \text{Card}\) such that \(|X| = \kappa\);

(ii) For all sets \( X \) and \( Y \), we have \(|X| = |Y|\) if and only if there exists a bijection \( X \to Y \);

(iii) \(\mathbb{N} \subseteq \text{Card}\), and if \( X \) is finite, then its cardinality \(|X|\) is equal to its size; and

(iv) For each cardinal number \(\kappa\), there exists a set \([\kappa]\) with \(|[\kappa]| = \kappa\), with \([n]\) defined as in [Definition 2.1.9](#) for all \( n \in \mathbb{N} \subseteq \text{Card}\).

A cardinal number \(\kappa \in \text{Card} \setminus \mathbb{N}\) is called an *infinite cardinal number*.
</markdown><markdown>
Condition (iii) ensures that cardinality generalises the notion of size: whereas size is defined only for finite sets, cardinality is defined for both finite and infinite sets.

[Definition 10.1.1](#) tells us only what properties cardinal numbers satisfy; it doesn‚Äôt tell us what they actually are, how they are defined, or even whether they exist. This is on purpose: the construction of the cardinal numbers is very intricate, and far beyond the scope of this introductory textbook.

One cardinal number of interest to us is the cardinality of the natural numbers. In a sense that we will make precise later ([Theorem 10.1.13](#)), we can think of this cardinal number as being the smallest infinite cardinal number.

‚ú¶ **Definition 10.1.2**  
The cardinal number \(\aleph_0\) (LaTeX code: `\aleph_0`), called **aleph naught** (or **aleph null**), is defined by \(\aleph_0 = |\mathbb{N}|\).

The symbol \(\aleph\) is the Hebrew letter *aleph*. The cardinal number \(\aleph_0\) is the first infinite cardinal number in a hierarchy of so-called *well-orderable cardinals*.

‚úê **Example 10.1.3**  
A set \(X\) is countably infinite if and only if \(|X| = \aleph_0\). Indeed, to say that \(X\) is countably infinite is to say that there is a bijection \(\mathbb{N} \to X\), which by [Definition 10.1.1(ii)](#) is equivalent to saying that \(\aleph_0 = |\mathbb{N}| = |X|\).

In light of [Example 10.1.3](#), we have

\[
|\mathbb{N}| = |\mathbb{Z}| = |\mathbb{Q}| = \aleph_0
\]

So what about \(\mathbb{R}\)? We know that \(|\mathbb{R}| \neq \aleph_0\) by [Theorem 6.2.20](#). The cardinality of the real numbers has its own notation, given next.

‚ú¶ **Definition 10.1.4**  
The **cardinality of the continuum** is the cardinal number \(\mathfrak{c}\) (LaTeX code: `\mathfrak{c}`) defined by \(\mathfrak{c} = |\mathbb{R}|\).

‚úê **Exercise 10.1.5**  
Consider the function \(f : \mathbb{R} \to (-1, 1)\) defined by

\[
f(x) = \frac{x}{1 + |x|}
\]

for all \(x \in \mathbb{R}\). Prove that \(f\) is a bijection, and deduce that \(|(-1, 1)| = \mathfrak{c}\).

‚úê **Exercise 10.1.6**  
Let \(a, b \in \mathbb{R}\) with \(a < b\). Prove that each of the sets \((a, b)\), \([a, b)\), \((a, b]\) and \([a, b]\) has cardinality \(\mathfrak{c}\).
</markdown><markdown>
## Ordering the cardinal numbers

In [Theorem 6.1.14](#) we proved that, given finite sets \( X \) and \( Y \), we can prove that \(|X| \leq |Y|\) by constructing an injection \( X \to Y \). This made intuitive sense: for an injection \( X \to Y \) to exist, there must be sufficiently many elements in \( Y \) to be able to spread out all of the elements of \( X \).

The intuition bestowed upon us by the finite case yields the following definition.

‚ú¶ **Definition 10.1.7**  
Given cardinal numbers \( \kappa \) and \( \lambda \), we say that \( \kappa \) is **less than or equal to** \( \lambda \), and write \( \kappa \leq \lambda \), if there exists an injection \([\kappa] \to [\lambda]\). We say \( \kappa \) is **less than** \( \lambda \), and write \( \kappa < \lambda \), if \( \kappa \leq \lambda \) and \( \kappa \neq \lambda \).

A word of warning is pertinent at this point. Given \( m, n \in \mathbb{N} \), but regarding \( m \) and \( n \) as cardinal numbers, [Definition 10.1.7](#) defines the expression ‚Äò\( m \leq n \)‚Äô to mean that there exists an injection \([m] \to [n]\). This is not how ‚Äò\( m \leq n \)‚Äô is typically defined for natural numbers \( m \) and \( n \). Fortunately for us, we proved in [Theorem 6.1.7](#) that \( m \leq n \) (in the usual sense) if and only if there is an injection \([m] \to [n]\). This ensures that these two notions of ‚Äò\(\leq\)‚Äô‚Äîfor cardinal numbers and for natural numbers‚Äîare consistent with one another.

This is typical for the definitions we will make involving cardinal numbers, particularly in [Section 10.2](#): results that we proved for natural numbers in [Chapter 8](#) will be generalised to form definitions for cardinal numbers, but then these generalised definitions are consistent with the usual definitions for natural numbers. While it is not worth losing sleep over these matters, it is good practice to check that the definitions we make are not mutually contradictory!

We may at times write \( \lambda \geq \kappa \) to mean \( \kappa \leq \lambda \), and \( \lambda > \kappa \) to mean \( \kappa < \lambda \). Note that \( \lambda \geq \kappa \) should not be interpreted to mean ‚Äòthere exists a surjection \([\lambda] \to [\kappa]\‚Äô‚Äîfor example, that would imply that \( 1 \geq 0 \) is false, which is nonsense.

In any case [Definition 10.1.7](#) provides the following proof strategy.

‚ùñ **Strategy 10.1.8**  
Let \( X \) and \( Y \) be sets.

- In order to prove \(|X| \leq |Y|\), it suffices to find an injection \( X \to Y \).
- In order to prove \(|X| < |Y|\), it suffices to find an injection \( X \to Y \), and prove that there does not exist a surjection \( X \to Y \).
</markdown><markdown>
### Example 10.1.9
\(\aleph_0 \leq \mathfrak{c}\). To see this, note that the function \(i : \mathbb{N} \to \mathbb{R}\) given by \(i(n) = n\) for all \(n \in \mathbb{N}\) is an injection.

### Exercise 10.1.10
Prove that \(n < \aleph_0\) for all \(n \in \mathbb{N}\).

### Exercise 10.1.11
Prove that \(\leq\) is a reflexive, transitive relation on Card. That is, prove that:

(a) \(\kappa \leq \kappa\) for all cardinal numbers \(\kappa\); and

(b) For all cardinal numbers \(\kappa, \lambda, \mu\), if \(\kappa \leq \lambda\) and \(\lambda \leq \mu\), then \(\kappa \leq \mu\).

### Theorem 10.1.12 (Cantor‚Äôs theorem)
Let \(X\) be a set. Then \(|X| < |\mathcal{P}(X)|\).

**Proof**  
The function \(x \mapsto \{x\}\) evidently defines an injection \(X \to \mathcal{P}(X)\), so \(|X| \leq |\mathcal{P}(X)|\). The fact that \(|X| \neq |\mathcal{P}(X)|\) is then immediate from Exercise 3.2.16, which proves that no function \(X \to \mathcal{P}(X)\) is surjective.

Cantor‚Äôs theorem implies that there is no bound on how large a cardinal number can be. Indeed, if \(\kappa\) is any cardinal number, then Cantor‚Äôs theorem implies that

\[
\kappa = |[\kappa]| < |\mathcal{P}([\kappa])|
\]

and so \(|\mathcal{P}([\kappa])|\) is a larger cardinal number yet.

Earlier in the section we claimed that \(\aleph_0\) is, in a suitable sense, the smallest infinite cardinal number. Theorem 10.1.13 makes it clear what we mean by this.

### Theorem 10.1.13
The cardinal number \(\aleph_0\) is the smallest infinite cardinal in the following sense:

(a) \(n \leq \aleph_0\) for all \(n \in \mathbb{N}\); and

(b) For all cardinal numbers \(\kappa\), if \(n \leq \kappa\) for all \(n \in \mathbb{N}\), then \(\aleph_0 \leq \kappa\).

Thus \(\aleph_0\) can be thought of as the cardinal supremum of \(\mathbb{N} \subseteq \text{Card}\).

**Proof**  
For part (a), note that for each \(n \in \mathbb{N}\), we have \([n] \subseteq \mathbb{N}\), and so the function \(i : [n] \to \mathbb{N}\)
</markdown><markdown>
given by \( i(k) = k \) for all \( k \in [n] \) is an injection. It follows that

\[
n = |[n]| \leq |\mathbb{N}| = \aleph_0
\]

as required.

For part (b), fix a cardinal number \(\kappa\) and assume that \( n \leq \kappa \) for all \( n \in \mathbb{N} \). Then there exist injections \( j_n : \{0, 1, \ldots, n - 1\} \to [\kappa] \) for each \( n \in \mathbb{N} \).

We will use these injections to construct an injection \( f : \mathbb{N} \to [\kappa] \) in the following way: each function \( j_n : \{0, 1, \ldots, n\} \to [\kappa] \) takes exactly \( n + 1 \) values. This means that for all \( n \in \mathbb{N} \), the function \( j_{n+1} \) takes at least one value in \([\kappa]\) that the function \( j_n \) does not take‚Äîwe will define \( f(n+1) \) to be one of these values.

Now let‚Äôs define \( f \) properly: define \( f(n) \in [\kappa] \) for \( n \in \mathbb{N} \) recursively as follows:

- \( f(0) = j_0(0) \in [\kappa] \).

- Fix \( n \in \mathbb{N} \) and suppose \( f(k) \) has been defined for all \( k \leq n \), such that \( f(k) \neq f(\ell) \) for \( \ell < k \). Let

\[
A = \{ a \leq n + 1 \mid j_{n+1}(a) \neq f(k) \text{ for all } k \leq n \}
\]

Note that \( A \) is inhabited: the set \(\{ j_{n+1}(a) \mid 0 \leq a \leq n + 1 \}\) has size \( n + 2 \) since \( j_{n+1} \) is injective, and the set \(\{ f(k) \mid 0 \leq k \leq n \}\) has size \( n + 1 \) by construction, so at least one \( a \leq n + 1 \) must satisfy the requirement that \( j_{n+1}(a) \neq f(k) \) for all \( k \leq n \).

Let \( a \) be the least element of \( A \), and define \( f(n+1) = j_{n+1}(a) \). Then by construction we have \( j_{n+1}(a) \neq f(k) \) for any \( k < n + 1 \), as required.

By construction, the function \( f \) is injective, since for all \( n \in \mathbb{N} \), the value \( f(n) \in [\kappa] \) was defined so that \( f(n) \neq f(k) \) for any \( k < n \).

Hence \(\aleph_0 = |\mathbb{N}| \leq |[\kappa]| = \kappa\), as required.

The Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem
-------------------------------------

Even if two sets \( X \) and \( Y \) have the same cardinality, it is not always easy to find a bijection \( X \to Y \). This is problematic if we want to prove that they *do* have the same cardinality! The Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem greatly simplifies this process, allowing us to deduce that \( |X| = |Y| \) from the existence of injections \( X \to Y \) and \( Y \to X \).

**Theorem 10.1.14 (Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem)**  
The relation \(\leq\) on Card is antisymmetric. That is, for all cardinal numbers \(\kappa\) and \(\lambda\), if \(\kappa \leq \lambda\) and \(\lambda \leq \kappa\), then \(\kappa = \lambda\).
</markdown><markdown>
## Section 10.1. Cardinality

### Proof

This is one of the most involved proofs that we will see in this book, and so we break it into steps. Some details are left as exercises, in part because the details cloud the bigger picture of the proof, and in part because they provide good practice with working with all the definitions.

Let \( \kappa \) and \( \lambda \) be cardinal numbers and assume that \( \kappa \leq \lambda \) and \( \lambda \leq \kappa \). Then there exist injections \( f : [\kappa] \to [\lambda] \) and \( g : [\lambda] \to [\kappa] \).

The steps we will follow are these:

- **Step 1.** We will use the injections \( f \) and \( g \) to partition \( [\kappa] \) and \( [\lambda] \) into equivalence classes. Intuitively, two elements of \( [\kappa] \) will be ‚Äòequivalent‚Äô if one can be obtained from the other by successively applying \( g \circ f \), and likewise for \( [\lambda] \) with \( f \circ g \).

- **Step 2.** We will prove that \( f \) and \( g \) induce a bijection \( [\kappa]/\sim \to [\lambda]/\approx \)‚Äîthat is, they pair up the \( \sim \)-equivalence classes with the \( \approx \)-equivalence classes.

- **Step 3.** We will prove that there is a bijection between each pair of the paired-up equivalence classes.

- **Step 4.** We will piece together the bijections between equivalence classes to obtain a bijection \( [\kappa] \to [\lambda] \).

So here we go.

**Step 1.** Define a relation \( \sim \) on \( [\kappa] \) by letting

\[
a \sim b \iff (g \circ f)^n(a) = b \text{ or } (g \circ f)^n(b) = a \text{ for some } n \in \mathbb{N}
\]

for all \( a, b \in [\kappa] \). Here \( (g \circ f)^n \) refers to the \( n \)-fold composite of \( g \circ f \), that is

\[
(g \circ f)^n(x) = ((g \circ f) \circ (g \circ f) \circ \cdots \circ (g \circ f))(x)
\]

In other words, \( a \sim b \) means that we can get from \( a \) to \( b \), or from \( b \) to \( a \), by applying the function \( g \circ f \) some number of times.

üìé **Exercise 10.1.15**  
Prove that \( \sim \) is an equivalence relation on \( [\kappa] \).

Likewise the relation \( \approx \) on \( [\lambda] \), defined by letting

\[
c \approx d \iff (f \circ g)^n(c) = d \text{ or } (f \circ g)^n(d) = c \text{ for some } n \in \mathbb{N}
\]

for all \( c, d \in [\lambda] \), is an equivalence relation.
</markdown><markdown>
Step 2. Define functions \( p : [\kappa]/\sim \to [\lambda]/\approx \) and \( q : [\lambda]/\approx \to [\kappa]/\sim \) by

\[
p([a]_\sim) = [f(a)]_\approx \quad \text{and} \quad q([b]_\approx) = [g(b)]_\sim
\]

for all \([a]_\sim \in [\kappa]/\sim\) and \([b]_\approx \in [\lambda]/\approx\).

‚úèÔ∏è **Exercise 10.1.16**  
Prove that \( p \) and \( q \) are well-defined, and that \( q \) is an inverse for \( p \).

In particular, \( p \) defines a bijection \([ \kappa ]/\sim \to [ \lambda ]/\approx \).

Step 3. Fix \( a \in [\kappa] \) and let \( b = f(a) \in [\lambda] \). We prove that there is a bijection \([a]_\sim \to [b]_\approx\). There are three possible cases:

- **Case 1.** Suppose \([a]_\sim\) contains an element \( a_0 \) such that \( a_0 \neq g(y) \) for any \( y \in [\lambda] \).  
  Define a function \( h_a : [a]_\sim \to [b]_\approx \) by letting \( h_a(x) = f(x) \) for all \( x \in [a]_\sim \).

- **Case 2.** Suppose \([b]_\approx\) contains an element \( b_0 \) such that \( b_0 \neq f(x) \) for any \( x \in [\kappa] \).  
  Define a function \( k_b : [b]_\approx \to [a]_\sim \) by letting \( k_b(y) = g(y) \) for all \( y \in [b]_\approx \).

- **Case 3.** Otherwise, define \( h_a : [a]_\sim \to [b]_\approx \) by \( h_a(x) = f(x) \) for all \( x \in [a]_\sim \).

‚úèÔ∏è **Exercise 10.1.17**  
Prove that each of the functions defined in the above three cases is a bijection.

Step 4. By taking \( h_a = k_b^{-1} : [a]_\sim \to [b]_\approx \) in Case 2 above, we have proved that:

- There is a bijection \( p : [\kappa]/\sim \to [\lambda]/\approx \); and

- For each \( E = [a]_\sim \in [\kappa]/\sim \), there is a bijection \( h_a : E \to p(E) \).

By Exercise 5.2.30, it follows that there is a bijection \([\kappa] \to [\lambda]\), as required. This completes the proof.

The Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem is not just an interesting fact: it is useful for proving that two sets have the same cardinality without having to explicitly construct a bijection between them.

üí° **Strategy 10.1.18 (Proving that two sets have equal cardinality)**  
Let \( X \) and \( Y \) be sets. In order to prove that \(|X| = |Y|\), it suffices to show that there exists an injection \( X \to Y \) and an injection \( Y \to X \).

‚úèÔ∏è **Example 10.1.19**  
Let \( a, b \in \mathbb{R} \) with \( a < b \), and consider the chain of functions

\[
(0, 1) \xrightarrow{f_1} (a, b) \xrightarrow{f_2} [a, b] \xrightarrow{f_3} [0, 1] \xrightarrow{f_4} (0, 1)
\]
</markdown><markdown>
defined by:

- \( f_1(t) = a + t(b-a) \) for all \( t \in (0, 1) \).
- \( f_2(t) = t \) for all \( t \in (a, b) \).
- \( f_3(t) = \frac{t-a}{b-a} \) for all \( t \in [a, b] \).
- \( f_4(t) = \frac{1}{4} + \frac{1}{2}t \) for all \( t \in [0, 1] \).

Each of these functions is injective by Exercise 3.2.8. Hence we can compose these functions to obtain injections from any of these sets to any other‚Äîfor example, \( f_1 \circ f_4 \circ f_3 : [0, 1] \to (a, b) \) is an injection.

By the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem, it follows that

\[
|(0,1)| = |(a,b)| = |[a,b]| = |[0,1]|
\]

### Example 10.1.20

Here is a proof that \( \mathbb{N} \times \mathbb{N} \) is countably infinite using the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem.

Define \( f : \mathbb{N} \to \mathbb{N} \times \mathbb{N} \) by \( f(n) = (n,0) \) for all \( n \in \mathbb{N} \). Given \( m,n \in \mathbb{N} \) we have

\[
f(m) = f(n) \implies (m,0) = (n,0) \implies m = n
\]

So \( f \) is injective.

Next, define \( g : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) by \( g(m,n) = 2^m \cdot 3^n \) for all \( m,n \in \mathbb{N} \). Then given \( (m,n), (p,q) \in \mathbb{N} \times \mathbb{N} \), if \( g(m,n) = g(p,q) \), then \( 2^m \cdot 3^n = 2^p \cdot 3^q \). It follows from the fundamental theorem of arithmetic (Theorem 7.2.12) that \( m = n \) and \( p = q \), so that \( g \) is injective.

Since \( f : \mathbb{N} \to \mathbb{N} \times \mathbb{N} \) and \( g : \mathbb{N} \times \mathbb{N} \to \mathbb{N} \) are injective, it follows from the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem that

\[
|\mathbb{N} \times \mathbb{N}| = |\mathbb{N}| = \aleph_0
\]

so that \( \mathbb{N} \times \mathbb{N} \) is countably infinite.

We can use the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem to prove that \( \mathcal{P}(\mathbb{N}) \) has the cardinality of the continuum.

### Theorem 10.1.21

\[
|\mathcal{P}(\mathbb{N})| = \mathfrak{c}
\]
</markdown><markdown>
## Proof

Define a function \( f : \mathcal{P}(\mathbb{N}) \to \mathbb{R} \) as follows. Given \( U \subseteq \mathbb{N} \), define

\[
U_n = 
\begin{cases} 
1 & \text{if } n \in U \\ 
0 & \text{if } n \notin U 
\end{cases}
\]

Let \( f(U) \) be the real number whose decimal expansion is \( U_0.U_1U_2 \ldots \). Then \( f \) is an injection: given \( U, V \subseteq \mathbb{N} \), if \( f(U) = f(V) \), then \( f(U) \) and \( f(V) \) have the same decimal expansion, so that \( U_n = V_n \) for all \( n \in \mathbb{N} \). But that says precisely that \( n \in U \) if and only if \( n \in V \) for all \( n \in \mathbb{N} \), so that \( U = V \).

Next, define a function \( g : [0, 1) \to \mathcal{P}(\mathbb{N}) \) as follows: given \( x \in \mathbb{R} \), let the binary expansion of \( x \) be

\[
x = 0.x_1x_2x_3x_4 \cdots \tag{2}
\]

Define \( g(x) = \{ i \in \mathbb{N} \mid x_i = 1 \} \). Then \( g \) is injective by uniqueness of binary expansions again.

Then:

- Since \( f \) is injective we have \( |\mathcal{P}(\mathbb{N})| \leq |\mathbb{R}| = \mathfrak{c} \).
- Since \( g \) is injective we have \( \mathfrak{c} = |[0, 1)| \leq |\mathcal{P}(\mathbb{N})| \).

By the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem ([Theorem 10.1.14](#)), it follows that \( |\mathcal{P}(\mathbb{N})| = \mathfrak{c} \).

### Exercise 10.1.22

Let \( \mathcal{F}(\mathbb{N}) \) be the set of all finite subsets of \( \mathbb{N} \), and define \( f : \mathcal{F}(\mathbb{N}) \to \mathbb{N} \) by

\[
f(U) = \sum_{a \in U} 10^a = \sum_{k=1}^{n} 10^{q_k}
\]

for all \( U = \{a_1, a_2, \ldots, a_n\} \in \mathcal{F}(\mathbb{N}) \). Put another way, \( f(U) \) is the natural number whose decimal expansion has a 1 in the \( r \)th position (counting from \( r = 0 \)) if \( r \in U \), and a 0 otherwise. For example

\[
f(\{0, 1, 3, 4, 8\}) = 100011011 \quad \text{and} \quad f(\emptyset) = 0
\]

Prove that \( f \) is injective, and use the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem to deduce that \( \mathcal{F}(\mathbb{N}) \) is countably infinite.

### Exercise 10.1.23

Let \( X, Y \) and \( Z \) be sets. Prove that if \( |X| = |Z| \) and \( X \subseteq Y \subseteq Z \), then \( |X| = |Y| = |Z| \).
</markdown><markdown>
## Section 10.2
Cardinal arithmetic

In this section we will define arithmetic operations for cardinal numbers, and then derive infinitary counting principles by analogy with Section 8.1. It is surprising how little additional work needs to be done for the results proved there to carry over.

We begin by defining the sum \(\kappa + \lambda\), product \(\kappa \cdot \lambda\) and power \(\lambda^\kappa\), for cardinal numbers \(\kappa\) and \(\lambda\).

### Cardinal addition

‚ú¶ **Definition 10.2.1**  
The (cardinal) sum of cardinal numbers \(\kappa\) and \(\lambda\) is the cardinal number \(\kappa + \lambda\) defined by

\[
\kappa + \lambda = |\,[\kappa] \sqcup [\lambda]\,|
\]

where for sets \(A\) and \(B\), the notation \(A \sqcup B\) denotes the disjoint union

\[
A \sqcup B = (A \times \{0\}) \cup (B \times \{1\})
\]

as discussed in Exercise 6.1.20.

Note that Definition 10.2.1 is compatible with addition for natural numbers by Exercise 6.1.20(b), which implies that \(|[m] \sqcup [n]| = m + n\) for all \(m, n \in \mathbb{N}\).

The following lemma makes cardinal addition easier to work with.

‚ú¢ **Lemma 10.2.2**  
Let \(X\) and \(Y\) be sets. If \(X \cap Y = \emptyset\), then \(|X \cup Y| = |X| + |Y|\).

**Proof**  
Let \(\kappa = |X|\) and \(\lambda = |Y|\). Note that \(|[\kappa] \times \{0\}| = \kappa = |X|\) and \(|[\lambda] \times \{1\}| = \lambda = |Y|\), so there are bijections \(f : [\kappa] \times \{0\} \to X\) and \(g : [\lambda] \times \{1\} \to Y\).

Define a function \(h : [\kappa] \sqcup [\lambda] \to X \cup Y\) by

\[
h(a, i) = 
\begin{cases} 
f(a) & \text{if } i = 0 \\
g(b) & \text{if } i = 1 
\end{cases}
\]

for all \((a, i) \in [\kappa] \sqcup [\lambda]\).
</markdown><markdown>
Then \( h \) is a bijection; to see this, define \( k : X \cup Y \to [\kappa] \bigsqcup [\lambda] \) by

\[
k(a) = 
\begin{cases} 
(f^{-1}(a), 0) & \text{if } a \in X \\
(g^{-1}(a), 1) & \text{if } a \in Y 
\end{cases}
\]

for all \( a \in X \cup Y \). Then \( k \) is well-defined since \( X \cap Y = \emptyset \) and \( f \) and \( g \) are bijections. And \( k \) can readily be seen to be an inverse for \( h \).

Since \( h : [\kappa] \bigsqcup [\lambda] \to X \cup Y \) is a bijection, we have

\[
|X \cup Y| = |[\kappa] \bigsqcup [\lambda]| = \kappa + \lambda
\]

as required.

### Example 10.2.3

We show that \(\aleph_0 + \aleph_0 = \aleph_0\). To see this, note that \(\mathbb{N} = E \cup O\), where \(E\) is the set of all even natural numbers and \(O\) is the set of all odd natural numbers. But \(E\) and \(O\) are disjoint, so that by Lemma 10.2.2 we have

\[
\aleph_0 + \aleph_0 = |E| + |O| = |E \cup O| = |\mathbb{N}| = \aleph_0
\]

as claimed.

Many of the basic properties enjoyed by addition of natural numbers carry over to cardinal numbers.

### Theorem 10.2.4 (Properties of cardinal addition)

(a) \(\kappa + (\lambda + \mu) = (\kappa + \lambda) + \mu\) for all cardinal numbers \(\kappa, \lambda, \mu\);

(b) \(\kappa + \lambda = \lambda + \kappa\) for all cardinal numbers \(\kappa, \lambda\);

(c) \(0 + \kappa = \kappa = \kappa + 0\) for all cardinal numbers \(\kappa\).

**Proof of (a)**

Let \(\kappa, \lambda\) and \(\mu\) be cardinal numbers, and define

\[
X = [\kappa] \times \{0\}, \quad Y = [\lambda] \times \{1\}, \quad Z = [\mu] \times \{2\}
\]

Note that \(|X| = \kappa\), \(|Y| = \lambda\) and \(|Z| = \mu\), and that \(X, Y\) and \(Z\) are pairwise disjoint. Therefore \(X\) and \(Y \cup Z\) are disjoint, and \(X \cup Y\) and \(Z\) are disjoint, so that

\[
\kappa + (\lambda + \mu) = |X| + (|Y| + |Z|) = |X| + |Y \cup Z| = |X \cup (Y \cup Z)|
\]

and likewise

\[
(\kappa + \lambda) + \mu = (|X| + |Y|) + |Z| = |X \cup Y| + |Z| = |(X \cup Y) \cup Z|
\]

But \(X \cup (Y \cup Z) = (X \cup Y) \cup Z\), so that \(\kappa + (\lambda + \mu) = (\kappa + \lambda) + \mu\), as required.
</markdown><markdown>
### Exercise 10.2.5
Prove parts (b) and (c) of [Theorem 10.2.4](#).

We can generalise the argument in [Example 10.2.3](#) to prove the following proposition.

### Proposition 10.2.6
\[
\kappa + \aleph_0 = \kappa \text{ for all cardinal numbers } \kappa \geq \aleph_0.
\]

**Proof**

Let \(\kappa \geq \aleph_0\) and let \(A = [\kappa]\). Note that by [Definition 10.1.7](#) there is an injection \(i : \mathbb{N} \to A\). Write \(a_n = i(n)\) for all \(n \in \mathbb{N}\), so that \(i[\mathbb{N}] = \{a_0, a_1, a_2, \ldots \} \subseteq A\).

Partition \(A\) as \(A = B \cup U \cup V\), where:

- \(B = A \setminus i[\mathbb{N}] = A \setminus \{a_0, a_1, a_2, a_3, \ldots \}\);
- \(U = i[E] = \{a_0, a_2, a_4, \ldots \}\); and
- \(V = i[O] = \{a_1, a_3, a_5, \ldots \}\).

Here \(E\) and \(O\) are the sets of even and odd natural numbers, respectively.

The sets \(U\), \(V\) and \(i[\mathbb{N}]\) are all countably infinite, since \(E\), \(O\) and \(\mathbb{N}\) are countably infinite and \(i\) is an injection.

Since \(U\) and \(i[\mathbb{N}]\) are countably infinite, there is a bijection \(f : U \to i[\mathbb{N}]\), which in turn yields a bijection \(g : B \cup U \to B \cup i[\mathbb{N}]\) defined by

\[
g(a) = 
\begin{cases} 
a & \text{if } a \in B \\
f(a) & \text{if } a \in U 
\end{cases}
\]

Note that \(g\) is well-defined since \(A \cap U = \emptyset\) and \(B \cap i[\mathbb{N}] = \emptyset\). And \(g\) is bijective: it has an inverse function, defined similarly but with \(f\) replaced by \(f^{-1}\).

Thus

\[
|A| = |B \cup i[\mathbb{N}]| = |B \cup U|
\]

Since \(U\) and \(V\) are countably infinite, we have \(|U| = |V| = \aleph_0\). It follows that

\[
\kappa + \aleph_0 = |A| + |V| = |B \cup U| + |V| = |B \cup U \cup V| = |A| = \kappa
\]

as required.
\end{proof}
</markdown><markdown>
# Cardinal multiplication

Products of cardinal numbers are defined by analogy with the result of Exercise 6.1.22, which says that \([m] \times [n] = mn\) for all \(m, n \in \mathbb{N}\).

‚ú¶ **Definition 10.2.7**  
The (cardinal) product of cardinal numbers \(\kappa\) and \(\lambda\) is the cardinal number \(\kappa \cdot \lambda\) defined by

\[
\kappa \cdot \lambda = |[\kappa] \times [\lambda]|
\]

That is, the cardinal product is the cardinality of the cartesian product.

Like Lemma 10.2.2, the following lemma allows us to replace \([\kappa]\) and \([\lambda]\) in Definition 10.2.7 by arbitrary sets of size \(\kappa\) and \(\lambda\), respectively.

‚ú£ **Lemma 10.2.8**  
Let \(X\) and \(Y\) be sets. Then \(|X \times Y| = |X| \times |Y|\).

**Proof**  
Let \(\kappa = |X|\) and \(\lambda = |Y|\), and fix bijections \(f : [\kappa] \to X\) and \(g : [\lambda] \to Y\). By Exercise 3.2.47, there is a bijection \([\kappa] \times [\lambda] \to X \times Y\), and so

\[
|X \times Y| = |[\kappa] \times [\lambda]| = \kappa \cdot \lambda
\]

as required.

‚úê **Example 10.2.9**  
We will prove that \(\aleph_0 \cdot \aleph_0 = \aleph_0\). Indeed, we proved in Example 10.1.20 that \(|\mathbb{N} \times \mathbb{N}| = \aleph_0\), and so by Lemma 10.2.8 we have

\[
\aleph_0 \cdot \aleph_0 = |\mathbb{N} \times \mathbb{N}| = \aleph_0
\]

as required.

‚úê **Example 10.2.10**  
We will prove that \(\aleph_0 \cdot \mathfrak{c} = \mathfrak{c}\).

Define \(f : \mathbb{Z} \times [0, 1) \to \mathbb{R}\) by \(f(n, r) = n + r\) for all \(n \in \mathbb{Z}\) and all \(0 \leq r < 1\). Then:

- **\(f\) is injective.** To see this, let \((n, r), (m, s) \in \mathbb{Z} \times [0, 1)\) and assume that \(n + r = m + s\). Then \(n - m = s - r\). Since \(s, r \in [0, 1)\), we have \(s - r \in (-1, 1)\), and so \(n - m \in (-1, 1)\). Since \(n - m \in \mathbb{Z}\), we must have \(n - m = 0\), and so \(m = n\). But then \(n + r = n + s\), and so \(r = s\). Thus \((n, r) = (m, s)\), as required.

- **\(f\) is surjective.** To see this, let \(x \in \mathbb{R}\). Let \(n \in \mathbb{Z}\) be such that \(n \leq x < n + 1\), and let \(r = x - n\). Note that \(0 \leq r < 1\), so that \((n, r) \in \mathbb{Z} \times [0, 1)\), and then \(x = n + r = f(n, r)\), as required.
</markdown><markdown>
Since \( f \) is a bijection, we have

\[
\aleph_0 \cdot c = | \mathbb{Z} | \times | [0, 1) | = | \mathbb{Z} \times [0, 1) | = | \mathbb{R} | = c
\]

as required.

### Exercise 10.2.11
Prove that \( c \cdot c = c \).

### Exercise 10.2.12
Let \( X \) be a set and let \(\sim\) be an equivalence relation on \( X \). Prove that if \( \kappa \) is a cardinal number such that \(|[a]_\sim| = \kappa\) for all \( a \in X \), then \(|X| = |X/\sim| \cdot \kappa\).

### Theorem 10.2.13 (Properties of cardinal multiplication)
(a) \(\kappa \cdot (\lambda \cdot \mu) = (\kappa \cdot \lambda) \cdot \mu\) for all cardinal numbers \(\kappa, \lambda, \mu\);

(b) \(\kappa \cdot \lambda = \lambda \cdot \kappa\) for all cardinal numbers \(\kappa, \lambda\);

(c) \(1 \cdot \kappa = \kappa = \kappa \cdot 1\) for all cardinal numbers \(\kappa\);

(d) \(\kappa \cdot (\lambda + \mu) = (\kappa \cdot \lambda) + (\kappa \cdot \mu)\) for all cardinal numbers \(\kappa, \lambda, \mu\).

**Proof of (a)**

Define \( f : [\kappa] \times ([\lambda] \times [\mu]) \to ([\kappa] \times [\lambda]) \times [\mu] \) by \( f(a, (b, c)) = ((a, b), c) \) for all \( a \in [\kappa], b \in [\lambda] \) and \( c \in [\mu] \). Then \( f \) is a bijection, since it has an inverse \( g : ([\kappa] \times [\lambda]) \times [\mu] \to [\kappa] \times ([\lambda] \times [\mu]) \) defined by \( g((a, b), c) = (a, (b, c)) \) for all \( a \in [\kappa], b \in [\lambda] \) and \( c \in [\mu] \).

But then by Lemma 10.2.8 we have

\[
\kappa \cdot (\lambda \cdot \mu) = |[\kappa] \times ([\lambda] \times [\mu])| = |([\kappa] \times [\lambda]) \times [\mu]| = (\kappa \cdot \lambda) \cdot \mu
\]

as required.

### Exercise 10.2.14
Prove parts (b), (c) and (d) of Theorem 10.2.13.

## Cardinal exponentiation

### Definition 10.2.15
Let \(\kappa\) and \(\lambda\) be cardinal numbers. The \(\kappa\)th (cardinal) power of \(\lambda\) is the cardinal number \(\lambda^\kappa\) defined by

\[
\lambda^\kappa = |[\lambda]^{[\kappa]}|
\]

where for sets \( A \) and \( B \), the notation \( B^A \) refers to the set of functions \( A \to B \).
</markdown><markdown>
Again, exponentiation of cardinal numbers agrees with that of natural numbers, since we proved in Exercise 8.1.16 that \(|[n]^{[m]}| = n^m\) for all \(m, n \in \mathbb{N}\).

Like with cardinal multiplication, the next lemma proves that we can replace the sets \([\kappa]\) and \([\lambda]\) in Definition 10.2.15 with arbitrary sets of cardinality \(\kappa\) and \(\lambda\), respectively.

### Lemma 10.2.16
Let \(X\) and \(Y\) be sets. Then \(|Y^X| = |Y|^{|X|}\).

**Proof**  
Let \(\kappa = |X|\) and \(\lambda = |Y|\), and fix bijections \(f : [\kappa] \to X\) and \(g : [\lambda] \to Y\).

Define \(H : [\lambda]^{[\kappa]} \to Y^X\) as follows. Given \(\theta \in [\lambda]^{[\kappa]}\), that is \(\theta : [\kappa] \to [\lambda]\), define \(h_\theta = g \circ \theta \circ f^{-1} : X \to Y\), and let \(H(\theta) = h_\theta \in Y^X\).

To see that \(H\) is a bijection, note that the function \(K : Y^X \to [\lambda]^{[\kappa]}\) defined by \(K(\varphi) = k_\varphi = g^{-1} \circ \varphi \circ f\) is a bijection, since for all \(\theta : [\kappa] \to [\lambda]\) we have

\[
K(H(\theta)) = K(g \circ \theta \circ f^{-1} = g^{-1} \circ g \circ \theta \circ f^{-1} \circ f = \theta
\]

and for all \(\varphi : X \to Y\), we have

\[
H(K(\varphi)) = H(g^{-1} \circ \varphi \circ f) = g \circ g^{-1} \circ \varphi \circ f \circ f^{-1} = \varphi
\]

Since \(H : [\lambda]^{[\kappa]} \to Y^X\) is a bijection, we have

\[
|Y^X| = |[ \lambda ]^{[\kappa]}| = \lambda^\kappa
\]

as required.

### Example 10.2.17
We prove that \(\kappa^2 = \kappa \cdot \kappa\) for all cardinal numbers \(\kappa\).

To see this, define \(f : [\kappa]^{\{0,1\}} \to [\kappa] \times [\kappa]\) by letting \(f(\theta) = (\theta(0), \theta(1))\) for all \(\theta : \{0,1\} \to [\kappa]\). Then

- \(f\) is injective. To see this, let \(\theta, \varphi : \{0,1\} \to [\kappa]\) and assume \(f(\theta) = f(\varphi)\). Then \((\theta(0), \theta(1)) = (\varphi(0), \varphi(1))\), so that \(\theta(0) = \varphi(0)\) and \(\theta(1) = \varphi(1)\). But then \(\theta = \varphi\) by function extensionality, as required.

- \(f\) is surjective. To see this, let \((a, b) \in [\kappa] \times [\kappa]\), and define \(\theta : \{0,1\} \to [\kappa]\) by letting \(\theta(0) = a\) and \(\theta(1) = b\). Then we have \(f(\theta) = (\theta(0), \theta(1)) = (a, b)\), as required.

Since \(f\) is a bijection, it follows that

\[
\kappa^2 = |[\kappa]^{\{0,1\}}| = |[\kappa]^{\{0,1\}}| = |[\kappa] \times [\kappa]| = \kappa \cdot \kappa
\]

as required.
</markdown><markdown>
Cardinal exponentiation gives us a convenient way of expressing the cardinalities of power sets.

### Theorem 10.2.18
Let \( X \) be a set. Then \( |\mathcal{P}(X)| = 2^{|X|} \).

**Proof**  
There is a bijection \( i : \mathcal{P}(X) \to \{0, 1\}^X \) defined for all \( U \subseteq X \) by letting \( i(U) = \chi_U \in \{0, 1\}^X \), where \( \chi_U : X \to \{0, 1\} \) is the characteristic function of \( U \) (see Definition 3.1.24). Note that \( i \) is a bijection by Theorem 3.1.26.

It follows by Lemma 10.2.16 that

\[
|\mathcal{P}(X)| = |\{0, 1\}^X| = |\{0, 1\}|^{|X|} = 2^{|X|}
\]

as required.

In light of Theorem 10.2.18, we can interpret Cantor‚Äôs theorem (Theorem 10.1.12) as saying that \( \kappa < 2^\kappa \) for all cardinal numbers \( \kappa \). Thus the cardinal numbers are unbounded.

Furthermore, Theorem 10.2.18 allows us to express the cardinality of the continuum \( \mathfrak{c} \) in terms of \( \aleph_0 \).

### Corollary 10.2.19
\( \mathfrak{c} = 2^{\aleph_0} \)

**Proof**  
We proved in Theorem 10.1.21 that \( |\mathcal{P}(\mathbb{N})| = \mathfrak{c} \), and so by Theorem 10.2.18 we have

\[
\mathfrak{c} = |\mathcal{P}(\mathbb{N})| = 2^{|\mathbb{N}|} = 2^{\aleph_0}
\]

as claimed.

### Exercise 10.2.20
Prove that for all cardinal numbers \( \kappa, \lambda, \mu \), if \( \lambda \leq \mu \), then \( \lambda^\kappa \leq \mu^\kappa \).

Many of the properties satisfied by exponentiation of natural numbers generalise to cardinal numbers.

### Theorem 10.2.21 (Properties of cardinal exponentiation)
(a) \( \mu^{\kappa + \lambda} = \mu^\kappa \cdot \mu^\lambda \) for all cardinal numbers \( \kappa, \lambda, \mu \);

(b) \( (\mu \cdot \lambda)^\kappa = \mu^\kappa \cdot \lambda^\kappa \) for all cardinal numbers \( \kappa, \lambda, \mu \);

(c) \( (\mu^\lambda)^\kappa = \mu^{\kappa \cdot \lambda} \) for all cardinal numbers \( \kappa, \lambda, \mu \).
</markdown><markdown>
**Proof of (a)**  
Let \(\kappa, \lambda, \mu\) be cardinal numbers and let \(X, Y\) and \(Z\) be sets with \(|X| = \kappa\), \(|Y| = \lambda\) and \(|Z| = \mu\). Assume furthermore that \(Y\) and \(Z\) are disjoint.

Given a function \(f : X \cup Y \to Z\), define \(f_X : X \to Z\) and \(f_Y : Y \to Z\) by \(f_X(a) = f(a)\) for all \(a \in X\), and \(f_Y(a) = f(a)\) for all \(y \in X\).

Define \(H : Z^{X \cup Y} \to Z^X \times Z^Y\) by \(H(f) = (f_X, f_Y)\). Then:

- \(H\) is injective. To see this, let \(f, g : X \cup Y \to Z\) and suppose that \(H(f) = H(g)\). Then \(f_X = g_X\) and \(f_Y = g_Y\). Now let \(a \in X \cup Y\). Then:
  - If \(a \in X\), then \(f(a) = f_X(a) = g_X(a) = g(a)\);
  - If \(a \in Y\), then \(f(a) = f_Y(a) = g_Y(a) = g(a)\).

  In both cases we have \(f(a) = g(a)\), so that \(f = g\) by function extensionality.

- \(H\) is surjective. To see this, let \((p, q) \in Z^X \times Z^Y\), so that we have \(p : X \to Z\) and \(q : Y \to Z\). Define \(f : X \cup Y \to Z\) by

  \[
  f(a) = 
  \begin{cases} 
  p(a) & \text{if } a \in X \\
  q(a) & \text{if } a \in Y 
  \end{cases}
  \]

  Then \(f\) is well-defined since \(X\) and \(Y\) are disjoint. Moreover for all \(a \in X\) we have \(f_X(a) = p(a)\) and for all \(a \in Y\) we have \(f_Y(a) = q(a)\), so that \((p, q) = (f_X, f_Y) = H(f)\), as required.

Since \(H\) is a bijection, it follows that

\[
\mu^{\kappa + \lambda} = |Z|^{|X| + |Y|} = |Z|^{|X \cup Y|} = |Z^{X \cup Y}| = |Z^X \times Z^Y| = |Z^X| \cdot |Z^Y| = \mu^\kappa \cdot \mu^\lambda
\]

as required.

‚úé **Exercise 10.2.22**  
Prove parts (b) and (c) of Theorem 10.2.21.

‚úê **Example 10.2.23**  
We prove that \(\aleph_0^{\aleph_0} = 2^{\aleph_0}\). Indeed:

- We know that \(\aleph_0^{\aleph_0} < 2^{\aleph_0^{\aleph_0}}\) by Cantor‚Äôs theorem (Theorem 10.1.12), so that:

  \[
  \aleph_0^{\aleph_0} < (2^{\aleph_0})^{\aleph_0} \quad \text{by Exercise 10.2.20}
  \]
  \[
  = 2^{\aleph_0 \cdot \aleph_0} \quad \text{by Theorem 10.2.21(c)}
  \]
  \[
  = 2^{\aleph_0} \quad \text{by Example 10.2.9}
  \]

- Since \(\aleph_0 > 2\), we have \(2^{\aleph_0} \leq \aleph_0^{\aleph_0}\) by Exercise 10.2.20.
</markdown><markdown>
It follows from the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem ([Theorem 10.1.14](#)) that \( \aleph_0^{\aleph_0} = 2^{\aleph_0} \).

### Exercise 10.2.24
Prove that \( c^c = 2^c \).

### Exercise 10.2.25
Prove that \( \aleph_0^{\aleph_0^{\aleph_0}} = 2^c \).

## Indexed cardinal sums and products

### Definition 10.2.26
Let \(\{A_i \mid i \in I\}\) be a family of sets indexed by a set \(I\). The (indexed) disjoint union of \(\{A_i \mid i \in I\}\) is the set \(\bigsqcup_{i \in I} A_i\) (LaTeX code: `\bigsqcup_{i \in I}`) defined by

\[
\bigsqcup_{i \in I} A_i = \bigcup_{i \in I} (\{i\} \times A_i) = \{(i, a) \mid i \in I, a \in A_i\}
\]

Note that for all \(i, j \in I\) with \(i \neq j\), the sets \(\{i\} \times A_i\) and \(\{j\} \times A_j\) are disjoint even if the sets \(A_i\) and \(A_j\) are not‚Äîhence the term disjoint union!

An element \((i, a) \in \bigsqcup_{i \in I} A_i\) can be thought of as simply being an element \(a \in A_i\), but keeping track of the label \(i\) of the set \(A_i\).

### Example 10.2.27
Given a set \(I\) and a set \(A\), we have

\[
\bigsqcup_{i \in I} A = \bigcup_{i \in I} (\{i\} \times A) = I \times A
\]

Thus the disjoint union of ‚Äò\(I\)-many‚Äô copies of a set \(A\) is simply \(I \times A\).

### Exercise 10.2.28
Let \(\{X_i \mid i \in I\}\) be a family of sets indexed by a set \(I\), and define a function

\[
q : \bigsqcup_{i \in I} X_i \to \bigcup_{i \in I} X_i
\]

by \(q(i, a) = a\) for all \(i \in I\) and \(a \in X_i\). Prove that if the sets \(X_i\) for \(i \in I\) are pairwise disjoint, then \(q\) is a bijection.
</markdown><markdown>
### Definition 10.2.29

Let \(\{\kappa_i \mid i \in I\}\) be an indexed family of cardinal numbers. The **(indexed) cardinal sum** of \(\kappa_i\) for \(i \in I\) is defined by

\[
\sum_{i \in I} \kappa_i = \left| \bigsqcup_{i \in I} [\kappa_i] \right|
\]

That is, the indexed cardinal sum is the cardinality of the indexed disjoint union.

As with the finitary operations, we should check that this agrees with the definition of addition for natural numbers. And indeed it does‚Äîgiven a finite set \(I\) and natural numbers \(n_i\) for each \(i \in I\), the fact that

\[
\left| \bigsqcup_{i \in I} [n_i] \right| = \sum_{i \in I} n_i
\]

is an immediate consequence of the addition principle (Theorem 8.1.24).

### Example 10.2.30

By Example 10.2.27 we have

\[
\sum_{i \in I} \kappa = |I \times [\kappa]| = |I| \cdot \kappa
\]

for all sets \(I\) and all cardinal numbers \(\kappa\).

### Example 10.2.31

We prove that \(\sum_{n \in \mathbb{N}} n = \aleph_0\).

Define a function \(f : \bigsqcup_{n \in \mathbb{N}} [n] \to \mathbb{N} \times \mathbb{N}\) by \(f(n, k) = (n, k)\) for all \(n \in \mathbb{N}\) and \(k \in [n]\).

Evidently \(f\) is injective, since it is the inclusion function of a subset. Therefore

\[
\sum_{n \in \mathbb{N}} n \leq |\mathbb{N} \times \mathbb{N}| = \aleph_0 \cdot \aleph_0 = \aleph_0
\]

Define a function \(g : \mathbb{N} \to \bigsqcup_{n \in \mathbb{N}} [n]\) by \(g(n) = (n+1, 1)\) for all \(n \in \mathbb{N}\). Then \(g\) is an injection, since given \(m, n \in \mathbb{N}\), if \(g(m) = g(n)\), then \((m+1, 1) = (n+1, 1)\), and so \(m+1 = n+1\). Thus \(m = n\), as required. So we have

\[
\aleph_0 = |\mathbb{N}| \leq \left| \bigsqcup_{n \in \mathbb{N}} [n] \right| = \sum_{n \in \mathbb{N}} n
\]

By the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem, we have \(\sum_{n \in \mathbb{N}} n = \aleph_0\).
</markdown><markdown>
### Exercise 10.2.32

Let \((a_n)_{n \in \mathbb{N}}\) be a sequence of natural numbers, and let \(I = \{n \in \mathbb{N} \mid a_n > 0\}\). Prove that

\[
\sum_{n \in \mathbb{N}} a_n = 
\begin{cases} 
\infty & \text{if } I \text{ is infinite} \\ 
\sum_{k=1}^{n} a_{n_k} & \text{if } I = \{n_k \mid k \in [n]\} \text{ is finite} 
\end{cases}
\]

### Lemma 10.2.33

Let \(\{X_i \mid i \in I\}\) be a family of pairwise disjoint sets, indexed by a set \(I\). Then

\[
\left| \bigcup_{i \in I} X_i \right| = \sum_{i \in I} |X_i|
\]

**Proof**

For each \(i \in I\) let \(\kappa_i = |X_i|\), and let \(f_i : [\kappa_i] \to X_i\) be a bijection. Then the function

\[
f : \bigsqcup_{i \in I} [\kappa_i] \to \bigsqcup_{i \in I} X_i
\]

defined by \(f(i, a) = f_i(a)\) for all \(i \in I\) and \(a \in [\kappa_i]\) is a bijection, since it has an inverse \(g\) given by \(g(i, a) = f_i^{-1}(a)\) for all \(i \in I\) and \(a \in X_i\).

Since the sets \(X_i\) are pairwise disjoint, we have by Exercise 10.2.28 that there is a bijection \(\bigsqcup_{i \in I} X_i \to \bigcup_{i \in I} X_i\). Hence

\[
\left| \bigcup_{i \in I} X_i \right| = \left| \bigsqcup_{i \in I} X_i \right| = \left| \bigsqcup_{i \in I} [\kappa_i] \right| = \sum_{i \in I} \kappa_i = \sum_{i \in I} |X_i|
\]

as required. \(\square\)

### Definition 10.2.34

Let \(\{X_i \mid i \in I\}\) be family of sets indexed by a set \(I\). The (indexed) cartesian product of the sets \(X_i\) for \(i \in I\) is defined by

\[
\prod_{i \in I} X_i = \left\{ f : I \to \bigcup_{i \in I} X_i \mid f(i) \in X_i \text{ for all } i \in I \right\}
\]

An element \(f \in \prod_{i \in I} X_i\) is called a **choice function** for the family \(\{X_i \mid i \in I\}\).

It is worth pointing out that the axiom of choice (Axiom 3.2.33) says precisely that the cartesian product of every family of inhabited sets is inhabited.
</markdown><markdown>
### Example 10.2.35

Given a set \( I \) and a set \( X \), a choice function for \(\{ X \mid i \in I \}\) is a function \( f : I \to \bigcup_{i \in I} X = X \) such that \( f(i) \in X \) for all \( i \in I \). But then every function \( f : I \to X \) is a choice function, and so

\[
\prod_{i \in I} X = X^I
\]

which is the set of functions \( I \to X \).

### Definition 10.2.36

Let \(\{ \kappa_i \mid i \in I \}\) be an indexed family of cardinal numbers. The (indexed) cardinal product of \(\kappa_i\) for \( i \in I \) is defined by

\[
\prod_{i \in I} \kappa_i = \left| \prod_{i \in I} [\kappa_i] \right|
\]

That is, the indexed cardinal product is the cardinality of the indexed cartesian product.

### Example 10.2.37

By Example 10.2.35, we have

\[
\prod_{i \in I} \kappa = |[\kappa]|^{|I|} = \kappa^{|I|}
\]

for all sets \( I \) and all cardinal numbers \( \kappa \).

### Exercise 10.2.38

Prove that \(\prod_{n \in \mathbb{N}} n = 2^{\aleph_0}\).

### Exercise 10.2.39

Prove that there do not exist cardinal numbers \(\{ \kappa_n \mid n \in \mathbb{N} \}\) such that \(\kappa_n \neq 1\) for all \( n \in \mathbb{N} \) and \(\prod_{n \in \mathbb{N}} \kappa_n = \aleph_0\).
</markdown><markdown>
# Section 10.E

## Chapter 10 exercises

### Countability

In Questions 10.1 to 10.4, prove that the set is countable.

10.1. The set \( D = \left\{ \frac{a}{2^n} \mid a \in \mathbb{Z}, n \in \mathbb{N} \right\} \) of dyadic rational numbers.

10.2. The set of all functions \([n] \to \mathbb{Z}\), where \(n \in \mathbb{N}\).

10.3. The set of all real numbers whose square is rational.

10.4. The following set:

\[
[(\mathbb{Z} \times \mathbb{Q}) \setminus (\mathbb{N} \times \mathbb{Z})] \cup \{ n \in \mathbb{N} \mid \exists u, v \in \mathbb{N}, n = 5u + 6v \} \cup \{ x \in \mathbb{R} \mid x - \sqrt{2} \in \mathbb{Q} \}
\]

In Questions 10.5 to 10.7, prove that the set is uncountable.

10.5. The set of all functions \(\mathbb{Z} \to \{0, 1\}\).

10.6. The set of all subsets \(U \subseteq \mathbb{N}\) such that neither \(U\) nor \(\mathbb{N} \setminus U\) is finite.

10.7. The set of all sequences of rational numbers that converge to 0.

In Questions 10.8 to 10.12, determine whether the set is countable or uncountable, and then prove it.

10.8. The set of all functions \(f : \mathbb{N} \to \mathbb{N}\) that are weakly decreasing‚Äîthat is, such that for all \(m, n \in \mathbb{N}\), if \(m \leq n\), then \(f(m) \geq f(n)\).

10.9. The set of all functions \(f : \mathbb{N} \to \mathbb{Z}\) that are weakly decreasing.

10.10. The set of all periodic functions \(f : \mathbb{Z} \to \mathbb{Q}\)‚Äîthat is, such that there is some integer \(p > 0\) such that \(f(x + p) = f(x)\) for all \(x \in \mathbb{Z}\).

10.11. The set of all periodic functions \(f : \mathbb{Q} \to \mathbb{Z}\)‚Äîthat is, such that there is some rational number \(p > 0\) such that \(f(x + p) = f(x)\) for all \(x \in \mathbb{Q}\).

10.12. The set of all real numbers \(x\) such that \(a_0 + a_1 x + \cdots + a_d x^d = 0\) for some rational numbers \(a_0, a_1, \ldots, a_d\) with \(a_d \neq 0\).

10.13. A subset \(D \subseteq \mathbb{R}\) is dense if \((a - \varepsilon, a + \varepsilon) \cap U\) is inhabited for all \(a \in \mathbb{R}\) and all \(\varepsilon > 0\)‚Äîintuitively, this means that there are elements of \(U\) arbitrarily close to any real number. Must a dense subset of \(\mathbb{R}\) be uncountable?
</markdown><markdown>
## Cardinality

In Questions 10.14 to 10.20, determine whether the cardinality of the set is equal to \(\aleph_0\), equal to \(c\), or greater than \(c\).

10.14. \(\mathcal{P}(\mathbb{R})\).

10.15. The set of all finite subsets of \(\mathbb{R}\).

10.16. The set of all countably infinite subsets of \(\mathbb{R}\).

10.17. \(\mathbb{R} \times \mathbb{R}\).

10.18. The set of all functions \(f : \mathbb{N} \rightarrow \mathbb{R}\) such that \(f(n) = 0\) for all but finitely many \(n \in \mathbb{N}\).

10.19. The set of all functions \(\mathbb{Q} \rightarrow \mathbb{R}\).

10.20. The set of all functions \(\mathbb{R} \rightarrow \mathbb{R}\).

10.21. Prove that there is a set \(\mathcal{C}\) of pairwise disjoint circles in \(\mathbb{R}^2\) such that \(|\mathcal{C}| = c\); formally, a circle is a subset \(C \subseteq \mathbb{R}^2\) of the form

\[
C = \{(x, y) \mid (x-a)^2 + (y-b)^2 = r^2\}
\]

for some \(a, b \in \mathbb{R}\) and some \(r > 0\).

10.22. Prove that there does not exist a set \(\mathcal{D}\) of pairwise disjoint discs in \(\mathbb{R}^2\) such that \(|\mathcal{D}| = c\); formally, a disc is a subset \(D \subseteq \mathbb{R}^2\) of the form

\[
D = \{(x, y) \mid (x-a)^2 + (y-b)^2 \leq r^2\}
\]

for some \(a, b \in \mathbb{R}\) and some \(r > 0\).

## Cardinal arithmetic

10.23. Prove that \(\kappa + 0 = \kappa \cdot 1 = \kappa^1 = \kappa\) for all cardinal numbers \(\kappa\).

10.24. Prove that \(\kappa^0 = 1^\kappa = 1\) for all cardinal numbers \(\kappa\).

10.25. Let \(\kappa\) be an infinite cardinal number such that \(\kappa \cdot \kappa = \kappa\). Prove that \(\kappa^\kappa = 2^\kappa\).

10.26. Prove that \(c^c = 2^{2^{\aleph_0}}\).

10.27. Let \(\kappa, \lambda\) and \(\mu\) be cardinal numbers.

(a) Prove that if \(\kappa \leq \lambda\), then \(\kappa + \mu \leq \lambda + \mu\).

(b) Suppose that \(\kappa + \mu \leq \lambda + \mu\). Must it be the case that \(\kappa \leq \lambda\)?
</markdown><markdown>
### Section 10.E. Chapter 10 exercises

**10.28.** Let \(\kappa, \lambda\) and \(\mu\) be cardinal numbers.

(a) Prove that if \(\kappa \leq \lambda\), then \(\kappa \cdot \mu \leq \lambda \cdot \mu\).

(b) Suppose that \(\kappa \cdot \mu \leq \lambda \cdot \mu\) and \(\mu > 0\). Must it be the case that \(\kappa \leq \lambda\)?

**10.29.** Let \(\kappa, \lambda\) and \(\mu\) be cardinal numbers.

(a) Prove that if \(\kappa \leq \lambda\), then \(\kappa^\mu \leq \lambda^\mu\).

(b) Suppose that \(\kappa^\mu \leq \lambda^\mu\) and \(\mu > 0\). Must it be the case that \(\kappa \leq \lambda\)?

**10.30.** Let \(\kappa, \lambda\) and \(\mu\) be cardinal numbers.

(a) Prove that if \(\kappa \leq \lambda\), then \(\mu^\kappa \leq \mu^\lambda\).

(b) Suppose that \(\mu^\kappa \leq \mu^\lambda\) and \(\mu > 1\). Must it be the case that \(\kappa \leq \lambda\)?

---

### Definition 10.E.1

Given cardinal numbers \(\kappa\) and \(\lambda\), define the **binomial coefficient** \(\binom{\kappa}{\lambda}\) by

\[
\binom{\kappa}{\lambda} = |\{ U \subseteq [\kappa] \mid |U| = \lambda \}|
\]

**10.31.** Let \(\kappa\) be a cardinal number. Prove that

\[
\binom{\aleph_0}{\kappa} = 
\begin{cases} 
1 & \text{if } \kappa = 0 \\
\aleph_0 & \text{if } \kappa \in \mathbb{N} \text{ and } \kappa > 0 \\
2^{\aleph_0} & \text{if } \kappa = \aleph_0 \\
0 & \text{if } \kappa \notin \mathbb{N} \cup \{\aleph_0\}
\end{cases}
\]

**10.32.** Find the values of \(\binom{c}{\kappa}\) for \(\kappa \in \mathbb{N} \cup \{\aleph_0, c\}\).

**10.33.** Prove that \(\sum_{\lambda < \kappa} \binom{\kappa}{\lambda} = 2^\kappa\) for all cardinal numbers \(\kappa\).

**10.34.** Define the factorial \(\kappa!\) of a cardinal number \(\kappa\) in a way that generalises the notion of factorial \(n!\) for a natural number \(n\). Find expressions for \(\aleph_0!\) and \(c!\).

---

### True‚ÄìFalse questions

In [Questions 10.35 to 10.45](#), determine (with proof) whether the statement is true or false.
</markdown><markdown>
10.35. For any two countably infinite sets \( X \) and \( Y \), there exists a bijection \( X \to Y \).

10.36. For any two uncountable sets \( X \) and \( Y \), there exists a bijection \( X \to Y \).

10.37. The product of countably many countable sets is countable.

10.38. Every countably infinite set can be partitioned into infinitely many infinite subsets.

10.39. There are countably many infinite subsets of \( \mathbb{N} \).

10.40. The set \( \mathbb{N} \times \mathbb{Z} \times \mathbb{Q} \) is countably infinite.

10.41. \( \aleph_0 \) is the smallest cardinal number.

10.42. There exist sets \( X \) and \( Y \) of different cardinalities that have the same number of finite subsets.

10.43. There exist sets \( X \) and \( Y \) of different cardinalities that have the same number of countably infinite subsets.

10.44. For all sets \( X \), if \( U \subsetneq X \), then \( |U| < |X| \).

10.45. \( 2^\kappa < 3^\kappa \) for all cardinal numbers \( \kappa > 0 \).

### Always‚ÄìSometimes‚ÄìNever questions

In Questions 10.46 to 10.51, determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

10.46. Let \( X \) be a set and suppose that there exists an injection \( \mathbb{N} \to X \). Then there exists an injection \( \mathbb{Q} \to X \).

10.47. Let \( X \) be a set. Then \( \mathcal{P}(X) \) is countably infinite.

10.48. Let \( X \) be a set, let \( \kappa \) and \( \lambda \) be cardinal numbers, and suppose that \( \lambda \leq \kappa = |X| \). Then \( X \) has a subset of size \( \lambda \).

10.49. Let \( X \) and \( Y \) be sets with \( |X| \leq |Y| \). Then \( |\mathcal{P}(X)| \leq |\mathcal{P}(Y)| \).

10.50. Let \( n \in \mathbb{N} \) with \( n \geq 2 \). Then \( \aleph_n > \aleph_0 \).

10.51. Let \( \kappa \) be a cardinal number with \( \kappa \geq \aleph_0 \). Then \( \aleph_0^\kappa > \aleph_0 \).

### Trick question

10.52. Does there exist a cardinal number \( \kappa \) such that \( \aleph_0 < \kappa < 2^{\aleph_0} \)?
</markdown><markdown>
# Chapter 11

## Discrete probability theory
</markdown><markdown>
## Section 11.1

# Discrete probability spaces

Probability theory is a field of mathematics which attempts to model randomness and uncertainty in the ‚Äòreal world‚Äô. The mathematical machinery it develops allows us to understand how this randomness behaves and to extract information which is useful for making predictions.

*Discrete* probability theory, in particular, concerns situations in which the possible outcomes form a *countable* set. This simplifies matters considerably: if there are only countably many outcomes, then the probability that any event occurs is determined entirely by the probabilities that the individual outcomes comprised by the event occur.

For example, the number \( N \) of words spoken by a child over the course of a year takes values in \( \mathbb{N} \), so is discrete. To each \( n \in \mathbb{N} \), we may assign a probability that \( N = n \), which can take positive values in a meaningful way, and from these probabilities we can compute the probabilities of more general events occurring (e.g. the probability that the child says under a million words). However, the height \( H \) grown by the child over the same period takes values in \([0, \infty)\), which is uncountable; for each \( h \in [0, \infty) \), the probability that \( H = h \) is zero, so these probabilities give us no information. We must study the behaviour of \( H \) through some other means.

In this chapter, we will concern ourselves only with the discrete setting.

It is important to understand from the outset that, although we use language like *outcome*, *event*, *probability* and *random*, and although we use real-world examples, everything we do concerns mathematical objects: sets, elements of sets, and functions. If we say, for example, ‚Äúthe probability that a roll of a fair six-sided die shows 3 or 4 is \( \frac{1}{3} \),‚Äù we are actually interpreting the situation mathematically‚Äîthe *outcomes* of the die rolls are interpreted as the elements of the set \([6]\); the *event* that the die shows 3 or 4 is interpreted as the subset \(\{3, 4\} \subseteq [6]\); and the *probability* that this event occurs is the value of a particular function \(\mathbb{P} : \mathcal{P}([6]) \rightarrow [0, 1]\) on input \(\{3, 4\}\). The mathematical interpretation is called a *model* of the real-world situation.
</markdown><markdown>
### Definition 11.1.1

A **discrete probability space** is a pair \((\Omega, \mathbb{P})\) (LaTeX code: \((\Omega, \mathbb{P})\)), consisting of a countable set \(\Omega\) and a function \(\mathbb{P} : \mathcal{P}(\Omega) \to [0, 1]\), such that

(i) \(\mathbb{P}(\Omega) = 1\); and

(ii) **(Countable additivity)** If \(\{A_i \mid i \in I\}\) is any family of pairwise disjoint subsets of \(\Omega\), indexed by a countable set \(I\), then

\[
\mathbb{P}\left(\bigcup_{i \in I} A_i\right) = \sum_{i \in I} \mathbb{P}(A_i)
\]

The set \(\Omega\) is called the **sample space**; the elements \(\omega \in \Omega\) are called **outcomes**; the subsets \(A \subseteq \Omega\) are called **events**; and the function \(\mathbb{P}\) is called the **probability measure**. Given an event \(A\), the value \(\mathbb{P}(A)\) is called the **probability of** \(A\).

<sup>a</sup>The symbols \(\Omega, \omega\) (LaTeX code: \(\omega, \omega\)) are the upper- and lower-case forms, respectively, of the Greek letter omega.

There is a general notion of a probability space, which does not require the sample space \(\Omega\) to be countable. This definition is significantly more technical, so we restrict our attention in this section to **discrete probability spaces**. Thus, whenever we say ‚Äòprobability space‚Äô in this section, the probability space can be assumed to be discrete. However, when our proofs do not specifically use countability of \(\Omega\), they typically are true of arbitrary probability spaces. As such, we will specify discreteness in the statement of results only when countability of the sample space is required.

### Example 11.1.2

We model the roll of a fair six-sided die.

The possible **outcomes** of the roll are 1, 2, 3, 4, 5 and 6, so we can take \(\Omega = [6]\) to be the sample space.

The **events** correspond with subsets of \([6]\). For example:

- \(\{4\}\) is the event that the die roll shows 4. This event occurs with probability \(\frac{1}{6}\).

- \(\{1, 3, 5\}\) is the event that the die roll is odd. This event occurs with probability \(\frac{1}{2}\).

- \(\{1, 4, 6\}\) is the event that the die roll is not prime. This event occurs with probability \(\frac{1}{2}\).

- \(\{3, 4, 5, 6\}\) is the event that the die roll shows a number greater than 2. This event occurs with probability \(\frac{2}{3}\).
</markdown><markdown>
- \(\{1, 2, 3, 4, 5, 6\}\) is the event that anything happens. This event occurs with probability 1.

- \(\emptyset\) is the event that nothing happens. This event occurs with probability 0.

More generally, since each outcome occurs with equal probability \(\frac{1}{6}\), we can define

\[
\mathbb{P}(A) = \frac{|A|}{6}
\]

for all events \(A\).

We will verify that \(\mathbb{P}\) defines a probability measure on [6] in Example 11.1.6.

### Example 11.1.3
Let \((\Omega, \mathbb{P})\) be a probability space. We prove that \(\mathbb{P}(\emptyset) = 0\).

Note that \(\Omega\) and \(\emptyset\) are disjoint, so by countable additivity, we have

\[
1 = \mathbb{P}(\Omega) = \mathbb{P}(\Omega \cup \emptyset) = \mathbb{P}(\Omega) + \mathbb{P}(\emptyset) = 1 + \mathbb{P}(\emptyset)
\]

Subtracting 1 throughout yields \(\mathbb{P}(\emptyset) = 0\), as required.

### Exercise 11.1.4
Let \((\Omega, \mathbb{P})\) be a probability space. Prove that

\[
\mathbb{P}(\Omega \setminus A) = 1 - \mathbb{P}(A)
\]

for all events \(A\).

Countable additivity of probability measures‚Äîthat is, condition (ii) in Definition 11.1.1‚Äîimplies that probabilities of events are determined by probabilities of individual outcomes. This is made precise in Proposition 11.1.5.

### Proposition 11.1.5
Let \(\Omega\) be a countable set and let \(\mathbb{P} : \mathcal{P}(\Omega) \to [0, 1]\) be a function such that \(\mathbb{P}(\Omega) = 1\). The following are equivalent:

(i) \(\mathbb{P}\) is a probability measure on \(\Omega\);

(ii) \(\sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \mathbb{P}(A)\) for all \(A \subseteq \Omega\).

**Proof**

Since \(\mathbb{P}(\Omega) = 1\), it suffices to prove that condition (ii) of Proposition 11.1.5 is equivalent to countable additivity of \(\mathbb{P}\).

- (i)‚áí(ii). Suppose \(\mathbb{P}\) is a probability measure on \(\Omega\). Let \(A \subseteq \Omega\).
</markdown><markdown>
Note that since \( A \subseteq \Omega \) and \(\Omega\) is countably infinite, it follows that \(\{ \{\omega\} \mid \omega \in A \}\) is a countable family of pairwise disjoint sets. By countable additivity, we have

\[
\mathbb{P}(A) = \mathbb{P} \left( \bigcup_{\omega \in A} \{\omega\} \right) = \sum_{\omega \in A} \mathbb{P}(\{\omega\})
\]

as required. Hence condition (ii) of the proposition is satisfied.

- (ii) \(\Rightarrow\) (i). Suppose that \(\sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \mathbb{P}(A)\) for all \(A \subseteq \Omega\). We prove that \(\mathbb{P}\) is a probability measure on \(\Omega\).

So let \(\{A_i \mid i \in I\}\) be a family of pairwise disjoint events, indexed by a countable set \(I\). Define \(A = \bigcup_{i \in I} A_i\). Since the sets \(A_i\) partition \(A\), summing over elements of \(A\) is the same as summing over each of the sets \(A_i\) individually, and then adding those results together; specifically, for each \(A\)-tuple \((p_\omega)_{\omega \in A}\), we have

\[
\sum_{\omega \in A} p_\omega = \sum_{i \in I} \sum_{\omega \in A_i} p_\omega
\]

Hence

\[
\begin{align*}
\mathbb{P}(A) &= \sum_{\omega \in A} \mathbb{P}(\{\omega\}) & \text{by condition (ii) of the proposition} \\
&= \sum_{i \in I} \sum_{\omega \in A_i} \mathbb{P}(\{\omega\}) & \text{by the above observation} \\
&= \sum_{i \in I} \mathbb{P}(A_i) & \text{by condition (ii) of the proposition}
\end{align*}
\]

So \(\mathbb{P}\) satisfies the countable additivity condition. Thus \(\mathbb{P}\) is a probability measure on \(\Omega\).

Hence the two conditions are equivalent.

üñâ **Example 11.1.6**

We prove that the function \(\mathbb{P}\) from Example 11.1.2 truly does define a probability measure. Indeed, let \(\Omega = [6]\) and let \(\mathbb{P} : \mathcal{P}(\Omega) \to [0, 1]\) be defined by

\[
\mathbb{P}(A) = \frac{|A|}{6} \quad \text{for all events } A
\]

Then \(\mathbb{P}(\Omega) = \frac{6}{6} = 1\), so condition (i) in Definition 11.1.1 is satisfied. Moreover, for each \(A \subseteq [6]\) we have

\[
\sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \sum_{\omega \in A} \frac{1}{6} = \frac{|A|}{6} = \mathbb{P}(A)
\]

so, by Proposition 11.1.5, \(\mathbb{P}\) defines a probability measure on \([6]\).
</markdown><markdown>
Proposition 11.1.5 makes defining probability measures much easier, since it implies that probability measures are determined entirely by their values on individual outcomes. This means that, in order to define a probability measure, we only need to specify its values on individual outcomes and check that the sum of these probabilities is equal to 1. This is significantly easier than defining \(\mathbb{P}(A)\) on all events \(A \subseteq \Omega\) and checking the two conditions of Definition 11.1.1.

This is made precise in Proposition 11.1.7 below.

### Proposition 11.1.7
Let \(\Omega\) be a countable set and, for each \(\omega \in \Omega\), let \(p_\omega \in [0, 1]\). If \(\sum_{\omega \in \Omega} p_\omega = 1\), then there is a unique probability measure \(\mathbb{P}\) on \(\Omega\) such that \(\mathbb{P}(\{\omega\}) = p_\omega\) for each \(\omega \in \Omega\).

**Proof**

We prove existence and uniqueness of \(\mathbb{P}\) separately.

- **Existence.** Define \(\mathbb{P} : \mathcal{P}(\Omega) \to [0, 1]\) be defined by

  \[
  \mathbb{P}(A) = \sum_{\omega \in A} p_\omega
  \]

  for all events \(A \subseteq \Omega\). Then condition (ii) of Proposition 11.1.5 is automatically satisfied, and indeed \(\mathbb{P}(\{\omega\}) = p_\omega\) for each \(\omega \in \Omega\). Moreover

  \[
  \mathbb{P}(\Omega) = \sum_{\omega \in \Omega} \mathbb{P}(\{\omega\}) = \sum_{\omega \in \Omega} p_\omega = 1
  \]

  and so condition (i) of Definition 11.1.1 is satisfied. Hence \(\mathbb{P}\) defines a probability measure on \(\Omega\).

- **Uniqueness.** Suppose that \(\mathbb{P}' : \mathcal{P}(\Omega) \to [0, 1]\) is another probability measure such that \(\mathbb{P}'(\{\omega\}) = p_\omega\) for all \(\omega \in \Omega\). For each event \(A \subseteq \Omega\), condition (ii) of Proposition 11.1.5 implies that

  \[
  \mathbb{P}'(A) = \sum_{\omega \in A} \mathbb{P}'(\{\omega\}) = \sum_{\omega \in A} p_\omega = \mathbb{P}(A)
  \]

  hence \(\mathbb{P}' = \mathbb{P}\).

So \(\mathbb{P}\) is uniquely determined by the values \(p_\omega\).

The assignments of \(p_\omega \in [0, 1]\) to each \(\omega \in \Omega\) in fact defines something that we will later define to be a **probability mass function** (Definition 11.2.5).

With Proposition 11.1.7 proved, we will henceforth specify probability measures \(\mathbb{P}\) on sample spaces \(\Omega\) by specifying only the values of \(\mathbb{P}(\{\omega\})\) for \(\omega \in \Omega\).
</markdown><markdown>
### Example 11.1.8

Let \( p \in [0, 1] \). A coin, which shows heads with probability \( p \), is repeatedly flipped until heads shows.

The outcomes of such a sequence of coin flips all take the form

\[
(\underbrace{\text{tails, tails,} \ldots, \text{tails}}_{n \text{ tails}}, \text{heads})
\]

for some \( n \in \mathbb{N} \). Identifying such a sequence with the number \( n \) of flips before heads shows, we can take \( \Omega = \mathbb{N} \) to be the sample space.

For each \( n \in \mathbb{N} \), we can define

\[
\mathbb{P}(\{n\}) = (1-p)^n p
\]

This will define a probability measure on \( \mathbb{N} \), provided these probabilities all sum to 1; and indeed by Theorem 9.3.8, we have

\[
\sum_{n \in \mathbb{N}} \mathbb{P}(\{n\}) = \sum_{n \in \mathbb{N}} (1-p)^n p = p \cdot \frac{1}{1-(1-p)} = p \cdot \frac{1}{p} = 1
\]

By Proposition 11.1.7, it follows that \( (\Omega, \mathbb{P}) \) is a probability space.

### Exercise 11.1.9

A fair six-sided die is rolled twice. Define a probability space \( (\Omega, \mathbb{P}) \) that models this situation.

### Exercise 11.1.10

Let \( (\Omega, \mathbb{P}) \) be a probability space and let \( A, B \) be events with \( A \subseteq B \). Prove that \( \mathbb{P}(A) \leq \mathbb{P}(B) \).

### Set operations on events

In the real world, we might want to talk about the probability that two events both happen, or the probability that an event doesn‚Äôt happen, or the probability that at least one of some collection of events happens. This is interpreted mathematically in terms of set operations.

### Example 11.1.11

Let \( (\Omega, \mathbb{P}) \) be the probability space modelling two rolls of a fair six-sided die‚Äîthat is, the sample space \( \Omega = [6] \times [6] \) with probability measure \( \mathbb{P} \) defined by \( \mathbb{P}(\{(a, b)\}) = \frac{1}{36} \) for each \( (a, b) \in \Omega \).
</markdown><markdown>
Let \( A \) be the event that the sum of the die rolls is even, that is

\[
A = \left\{ 
(1, 1), (1, 3), (1, 5), (2, 2), (2, 4), (2, 6), 
(3, 1), (3, 3), (3, 5), (4, 2), (4, 4), (4, 6), 
(5, 1), (5, 3), (5, 5), (6, 2), (6, 4), (6, 6) 
\right\}
\]

and let \( B \) be the event that the sum of the die rolls is greater than or equal to 9, that is

\[
B = \{(3, 6), (4, 5), (4, 6), (5, 4), (5, 5), (5, 6), (6, 3), (6, 4), (6, 5), (6, 6)\}
\]

Then

- Consider the event that the sum of the die rolls is even or greater than or equal to 9. An outcome \( \omega \) gives rise to this event precisely when either \( \omega \in A \) or \( \omega \in B \); so the event in question is \( A \cup B \);

- Consider the event that the sum of the die rolls is even and greater than or equal to 9. An outcome \( \omega \) gives rise to this event precisely when both \( \omega \in A \) and \( \omega \in B \); so the event in question is \( A \cap B \);

- Consider the event that the sum of the die rolls is not even. An outcome \( \omega \) gives rise to this event precisely when \( \omega \notin A \); so the event in question is \( ([6] \times [6]) \setminus A \).

Thus we can interpret ‚Äòor‚Äô as union, ‚Äòand‚Äô as intersection, and ‚Äònot‚Äô as relative complement in the sample space.

**Definition 11.1.12**  
Let \((\Omega, \mathcal{P})\) be a probability space. The complement of an event \( A \subseteq \Omega \) is the event \( \Omega \setminus A \subseteq \Omega \). We write \( A^c \) (LaTeX code: `A^\text{c}`) for \( \Omega \setminus A \).

That is, when we talk about the complement of an event, we really mean their relative complement inside the sample space.

**Exercise 11.1.13**  
Let \((\Omega, \mathcal{P})\) be a probability space, and let \( p(\omega), q(\omega) \) be logical formulae with free variable \( \omega \) ranging over \(\Omega\). Let

\[
A = \{ \omega \in \Omega \mid p(\omega) \} \quad \text{and} \quad B = \{ \omega \in \Omega \mid q(\omega) \}
\]

Prove that
</markdown><markdown>
- \(\{ \omega \in \Omega \mid p(\omega) \land q(\omega) \} = A \cap B\);
- \(\{ \omega \in \Omega \mid p(\omega) \lor q(\omega) \} = A \cup B\);
- \(\{ \omega \in \Omega \mid \lnot p(\omega) \} = A^c\).

For reference, in [Example 11.1.11](#), we had \(\Omega = [6] \times [6]\) and we defined \(p(a, b)\) to be ‚Äò\(a + b\) is even‚Äô and \(q(a, b)\) to be ‚Äò\(a + b \geq 7\)‚Äô.

With this in mind, it will be useful to know how set operations on events interact with probabilities. A useful tool in this investigation is that of an *indicator function*.

‚ú¶ **Definition 11.1.14**  
Let \(\Omega\) be a set and let \(A \subseteq \Omega\). The *indicator function* of \(A\) in \(\Omega\) is the function \(i_A : \Omega \to \{0, 1\}\) defined by

\[
i_A(\omega) = 
\begin{cases} 
1 & \text{if } \omega \in A \\
0 & \text{if } \omega \notin A 
\end{cases}
\]

‚ú£ **Proposition 11.1.15**  
Let \(\Omega\) be a set and let \(A, B \subseteq \Omega\). Then for all \(\omega \in \Omega\) we have

(i) \(i_{A \cap B}(\omega) = i_A(\omega)i_B(\omega)\);

(ii) \(i_{A \cup B}(\omega) = i_A(\omega) + i_B(\omega) - i_{A \cap B}(\omega)\); and

(iii) \(i_{A^c}(\omega) = 1 - i_A(\omega)\).

**Proof of (i)**  
Let \(\omega \in \Omega\). If \(\omega \in A \cap B\) then \(\omega \in A\) and \(\omega \in B\), so that \(i_{A \cap B}(\omega) = i_A(\omega) = i_B(\omega) = 1\). Hence

\[
i_A(\omega)i_B(\omega) = 1 = i_{A \cap B}(\omega)
\]

If \(\omega \notin A \cap B\) then either \(\omega \notin A\) or \(\omega \notin B\). Hence \(i_{A \cap B}(\omega) = 0\), and either \(i_A(\omega) = 0\) or \(i_B(\omega) = 0\). Thus

\[
i_A(\omega)i_B(\omega) = 0 = i_{A \cap B}(\omega)
\]

In both cases, we have \(i_{A \cap B}(\omega) = i_A(\omega)i_B(\omega)\), as required. ‚ñ°

‚úé **Exercise 11.1.16**  
Prove parts (ii) and (iii) of [Proposition 11.1.15](#).

‚úé **Exercise 11.1.17**  
Let \((\Omega, \mathbb{P})\) be a discrete probability space, and for each \(\omega \in \Omega\) let \(p_\omega = \mathbb{P}(\{\omega\})\). Prove that, for each event \(A\), we have

\[
\mathbb{P}(A) = \sum_{\omega \in \Omega} p_\omega i_A(\omega)
\]
</markdown><markdown>
### Theorem 11.1.18

Let \((\Omega, \mathbb{P})\) be a probability space and let \(A, B \subseteq \Omega\). Then

\[
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\]

**Proof**

For each \(\omega \in \Omega\), let \(p_\omega = \mathbb{P}(\{\omega\})\). Then

\[
\mathbb{P}(A \cup B) = \sum_{\omega \in \Omega} p_\omega i_{A \cup B}(\omega) \quad \text{by Exercise 11.1.17}
\]

\[
= \sum_{\omega \in \Omega} p_\omega (i_A(\omega) + i_B(\omega) - i_{A \cap B}(\omega)) \quad \text{by Proposition 11.1.15(ii)}
\]

\[
= \sum_{\omega \in \Omega} p_\omega i_A(\omega) + \sum_{\omega \in \Omega} p_\omega i_B(\omega) + \sum_{\omega \in \Omega} p_\omega i_{A \cap B}(\omega) \quad \text{rearranging}
\]

\[
= \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \quad \text{by Exercise 11.1.17}
\]

as required.

Although there are nice expressions for unions and complements of events, it is not always the case that intersection of events corresponds with multiplication of probabilities.

### Example 11.1.19

Let \(\Omega = [3]\) and define a probability measure \(\mathbb{P}\) on \(\Omega\) by letting

\[
\mathbb{P}(\{1\}) = \frac{1}{4}, \quad \mathbb{P}(\{2\}) = \frac{1}{2} \quad \text{and} \quad \mathbb{P}(\{3\}) = \frac{1}{4}
\]

Then we have

\[
\mathbb{P}(\{1, 2\} \cap \{2, 3\}) = \mathbb{P}(\{2\}) = \frac{1}{2} \neq \frac{9}{16} = \frac{3}{4} \cdot \frac{3}{4} = \mathbb{P}(\{1, 2\}) \cdot \mathbb{P}(\{2, 3\})
\]

This demonstrates that it is not always the case that \(\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)\) for events \(A, B\) in a given probability space. Pairs of events \(A, B\) for which this equation is true are said to be **independent**.

### Definition 11.1.20

Let \((\Omega, \mathbb{P})\) be a probability space and let \(A, B\) be events. We say \(A\) and \(B\) are **independent** if \(\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)\); otherwise, we say they are **dependent**. More generally, events \(A_1, A_2, \ldots, A_n\) are **pairwise independent** if \(\mathbb{P}(A_i \cap A_j)\) for all \(i \neq j\).

### Example 11.1.21

A fair six-sided die is rolled twice. Let \(A\) be the event that the first roll shows 4, and let
</markdown><markdown>
B be the event that the second roll is even. Then

\[ A = \{(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6)\} \]

so \(\mathbb{P}(A) = \frac{6}{36} = \frac{1}{6};\) and

\[ B = \{(a, 2), (a, 4), (a, 6) \mid a \in [6]\} \]

so \(\mathbb{P}(B) = \frac{18}{36} = \frac{1}{2}.\) Moreover \(A \cap B = \{(4, 2), (4, 4), (4, 6)\},\) so it follows that

\[
\mathbb{P}(A \cap B) = \frac{3}{36} = \frac{1}{12} = \frac{1}{6} \cdot \frac{1}{2} = \mathbb{P}(A)\mathbb{P}(B)
\]

so the events \(A\) and \(B\) are independent.

Let \(C\) be the event that the sum of the two dice rolls is equal to 5. Then

\[ C = \{(1, 4), (2, 3), (3, 2), (4, 1)\} \]

so \(\mathbb{P}(C) = \frac{4}{36} = \frac{1}{9}.\) Moreover \(A \cap C = \{(4, 1)\},\) so it follows that

\[
\mathbb{P}(A \cap C) = \frac{1}{36} \neq \frac{1}{54} = \frac{1}{6} \cdot \frac{1}{9} = \mathbb{P}(A)\mathbb{P}(C)
\]

so the events \(A\) and \(C\) are dependent.

üìÑ **Exercise 11.1.22**  
Let \((\Omega, \mathbb{P})\) be a probability space. Under what conditions is an event \(A\) independent from itself?

## Conditional probability

Suppose we model a random process, such as the roll of a die or the flip of a coin, using a probability space \((\Omega, \mathbb{P})\). When we receive new information, the situation might change, and we might want to model this new situation by updating our probabilities to reflect the fact that we know that \(B\) has occurred. This is done by defining a new probability measure \(\mathbb{P}\) on \(\Omega\). What follows is an example of this.

üìÑ **Example 11.1.23**  
Two cards are drawn at random, in order, without replacement, from a 52-card deck. We can model this situation by letting the sample space \(\Omega\) be the set of ordered pairs of distinct cards, and letting \(\mathbb{P}\) assign an equal probability (of \(\frac{1}{|\Omega|}\) to each outcome. Note that \(|\Omega| = 52 \cdot 51,\) and so

\[
\mathbb{P}(\{\omega\}) = \frac{1}{52 \cdot 51}
\]

for each outcome \(\omega\).

We will compute two probabilities:
</markdown><markdown>
- The probability that the second card drawn is a heart.
- The probability that the second card drawn is a heart *given that* the first card drawn is a diamond.

Let \( A \subseteq \Omega \) be the event that the second card drawn is a heart, and let \( B \subseteq \Omega \) be the event that the first card drawn is a diamond.

To compute \(\mathbb{P}(A)\), note first that \( A = A' \cup A'' \), where

- \( A' \) is the event that both cards are hearts, so that \(|A'| = 13 \cdot 12\); and
- \( A'' \) is the event that only the second card is a heart, so that \(|A''| = 39 \cdot 13\).

Since \( A' \cap A'' = \emptyset \), it follows from countable additivity that

\[
\mathbb{P}(A) = \mathbb{P}(A') + \mathbb{P}(A'') = \frac{13 \cdot 12 + 39 \cdot 13}{52 \cdot 51} = \frac{13 \cdot (12 + 39)}{52 \cdot 51} = \frac{1}{4}
\]

Now suppose we know that first card drawn is a diamond‚Äîthat is, event \( B \) has occurred‚Äîand we wish to update our probability that \( A \) occurs. We do this by defining a new probability measure

\[
\widetilde{\mathbb{P}} : \mathcal{P}(\Omega) \to [0, 1]
\]

such that:

(a) The outcomes that do not give rise to the event \( B \) are assigned probability zero; that is, \(\widetilde{\mathbb{P}}(\{\omega\}) = 0\) for all \(\omega \notin B\); and

(b) The outcomes that give rise to the event \( B \) are assigned probabilities proportional to their old probability; that is, there is some \( k \in \mathbb{R} \) such that \(\widetilde{\mathbb{P}}(\omega) = k\mathbb{P}(\omega)\) for all \(\omega \in B\).

In order for \(\widetilde{\mathbb{P}}\) to be a probability measure on \(\Omega\), we need condition (i) of Definition 11.1.1 to occur.

\[
\widetilde{\mathbb{P}}(\Omega) = \sum_{\omega \in \Omega} \widetilde{\mathbb{P}}(\{\omega\}) \quad \text{by condition (ii) of Proposition 11.1.5}
\]

\[
= \sum_{\omega \in B} \widetilde{\mathbb{P}}(\{\omega\}) \quad \text{since } \widetilde{\mathbb{P}}(\{\omega\}) = 0 \text{ for } \omega \notin B
\]

\[
= \sum_{\omega \in B} k\mathbb{P}(\{\omega\}) \quad \text{since } \widetilde{\mathbb{P}}(\{\omega\}) = k\mathbb{P}(\{\omega\}) \text{ for } \omega \in B
\]

\[
= k\mathbb{P}(B) \quad \text{by condition (ii) of Proposition 11.1.5}
\]

Since we need \(\widetilde{\mathbb{P}}(\Omega) = 1\), we must therefore take \( k = \frac{1}{\mathbb{P}(B)} \). (In particular, we need \(\mathbb{P}(B) > 0\) for this notion to be well-defined.)
</markdown><markdown>
Recall that, before we knew that the first card was a diamond, the probability that the second card is a heart was \( \frac{1}{4} \). We now calculate how this probability changes with the updated information that the first card was a diamond.

The event that the second card is a heart in the new probability space is precisely \( A \cap B \), since it is the subset of \( B \) consisting of all the outcomes \( \omega \) giving rise to the event \( A \). As such, the new probability that the second card is a heart is given by

\[
\tilde{\mathbb{P}}(A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\]

Now:

- \( A \cap B \) is the event that the first card is a diamond and the second is a heart. To specify such an event, we need only specify the ranks of the two cards, so \( |A \cap B| = 13 \cdot 13 \) and hence \( \mathbb{P}(A \cap B) = \frac{13 \cdot 13}{52 \cdot 51} \).

- \( B \) is the event that the first card is a diamond. A similar procedure as with \( A \) yields \( \mathbb{P}(B) = \frac{1}{4} \).

Hence

\[
\tilde{\mathbb{P}}(A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{13 \cdot 13 \cdot 4}{52 \cdot 51} = \frac{13}{51}
\]

Thus the knowledge that the first card drawn is a diamond very slightly increases the probability that the second card is a heart from \( \frac{1}{4} = \frac{13}{52} \) to \( \frac{13}{51} \).

**Example 11.1.23** suggests the following schema: upon discovering that an event \( B \) occurs, the probability that event \( A \) occurs should change from \( \mathbb{P}(A) \) to \( \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \). This motivates the following definition of *conditional probability*.

### Definition 11.1.24

Let \( (\Omega, \mathbb{P}) \) be a probability space and let \( A, B \) be events such that \( \mathbb{P}(B) > 0 \). The conditional probability of \( A \) given \( B \) is the number \( \mathbb{P}(A \mid B) \) (LaTeX code: `\mathbb{P}(A \mid B)`) defined by

\[
\mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\]

### Example 11.1.25

A fair six-sided die is rolled twice. We compute the probability that the first roll showed a 2 given that the sum of the die rolls is less than 5.

We can model this situation by taking the sample space to be \([6] \times [6]\), with each outcome having an equal probability of \( \frac{1}{36} \).
</markdown><markdown>
Let \( A \) be the event that the first die roll shows a 2, that is

\[ 
A = \{(2,1), (2,2), (2,3), (2,4), (2,5), (2,6)\} 
\]

and let \( B \) be the event that the sum of the die rolls is less than 5, that is

\[ 
B = \{(1,1), (1,2), (1,3), (2,1), (2,2), (3,1)\} 
\]

We need to compute \( \mathbb{P}(A \mid B) \). Well,

\[ 
A \cap B = \{(2,1), (2,2)\} 
\]

so \( \mathbb{P}(A \cap B) = \frac{2}{36} \); and \( \mathbb{P}(B) = \frac{6}{36} \). Hence

\[
\mathbb{P}(A \mid B) = \frac{\frac{2}{36}}{\frac{6}{36}} = \frac{2}{6} = \frac{1}{3}
\]

### Exercise 11.1.26

A fair six-sided die is rolled three times. What is the probability that the sum of the die rolls is less than or equal to 12, given that each die roll shows a power of 2?

### Exercise 11.1.27

Let \((\Omega, \mathbb{P})\) be a probability space and let \( A, B \) be events with \( \mathbb{P}(B) > 0 \). Prove that

\[
\mathbb{P}(A \mid B) = \mathbb{P}(A \cap B \mid B)
\]

### Exercise 11.1.28

Let \((\Omega, \mathbb{P})\) be a probability space and let \( A, B \) be events such that \( \mathbb{P}(B) > 0 \). Prove that \( \mathbb{P}(A \mid B) = \mathbb{P}(A) \) if and only if \( A \) and \( B \) are independent.

We will soon see some useful real-world applications of probability theory using Bayes‚Äôs theorem (Theorem 11.1.33). Before we do so, some technical results will be useful in our proofs.

### Proposition 11.1.29

Let \((\Omega, \mathbb{P})\) be a probability space and let \( A, B \) be events with \( 0 < \mathbb{P}(B) < 1 \). Then

\[
\mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c)
\]

**Proof**

Note first that we can write

\[
A = A \cap \Omega = A \cap (B \cup B^c) = (A \cap B) \cup (A \cap B^c)
\]
</markdown><markdown>
and moreover the events \(A \cap B\) and \(A \cap B^c\) are mutually exclusive. Hence

\[
\mathbb{P}(A) = \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^c)
\]

by countable additivity. The definition of conditional probability ([Definition 11.1.24](#)) then gives

\[
\mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c)
\]

as required.

‚úé **Example 11.1.30**

An animal rescue centre houses a hundred animals, sixty of which are dogs and forty of which are cats. Ten of the dogs and ten of the cats hate humans. We compute the probability that a randomly selected animal hates humans.

Let \(A\) be the event that a randomly selected animal hates humans, and let \(B\) be the event that the animal is a dog. Note that \(B^c\) is precisely the event that the animal is a cat. The information we are given says that:

- \(\mathbb{P}(B) = \frac{60}{100}\), since 60 of the 100 animals are dogs;
- \(\mathbb{P}(B^c) = \frac{40}{100}\), since 40 of the 100 animals are cats;
- \(\mathbb{P}(A \mid B) = \frac{10}{60}\), since 10 of the 60 dogs hate humans;
- \(\mathbb{P}(A \mid B^c) = \frac{10}{40}\), since 10 of the 40 cats hate humans.

By [Proposition 11.1.29](#), it follows that the probability that a randomly selected animal hates humans is

\[
\mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c) = \frac{60}{100} \cdot \frac{10}{60} + \frac{40}{100} \cdot \frac{10}{40} = \frac{20}{100} = \frac{1}{5}
\]

The following example generalises [Proposition 11.1.29](#) to arbitrary partitions of a sample space into events with positive probabilities.

‚úé **Example 11.1.31**

The animal rescue centre from **Example 11.1.30** acquires twenty additional rabbits, of whom sixteen hate humans. We compute the probability that a randomly selected animal hates humans, given the new arrivals.

A randomly selected animal must be either a dog, a cat or a rabbit, and each of these occurs with positive probability. Thus, letting \(D\) be the event that the selected animal is a dog, \(C\) be the event that the animal is a cat, and \(R\) be the event that the animal is a rabbit, we see that the sets \(D, C, R\) form a partition of the sample space.
</markdown><markdown>
Letting \( A \) be the event that the selected animal hates humans. Then

\[
\mathbb{P}(A) = \mathbb{P}(A \mid D)\mathbb{P}(D) + \mathbb{P}(A \mid C)\mathbb{P}(C) + \mathbb{P}(A \mid R)\mathbb{P}(R)
\]

\[
= \frac{10}{60} \cdot \frac{60}{120} + \frac{10}{40} \cdot \frac{40}{120} + \frac{16}{20} \cdot \frac{20}{120}
\]

\[
= \frac{3}{10}
\]

**Proposition 11.1.32** below is a technical result which proves that conditional probability truly does yield a new probability measure on a given sample space.

‚ú£ **Proposition 11.1.32**  
Let \((\Omega, \mathbb{P})\) be a probability space and let \( B \) be an event such that \(\mathbb{P}(B) > 0\). The function \(\tilde{\mathbb{P}} : \mathcal{P}(\Omega) \to [0, 1]\) defined by

\[
\tilde{\mathbb{P}}(A) = \mathbb{P}(A \mid B) \text{ for all } A \subseteq \Omega
\]

defines a probability measure on \(\Omega\).

**Proof**  
First note that

\[
\tilde{\mathbb{P}}(\Omega) = \mathbb{P}(\Omega \mid B) = \frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1
\]

so condition (i) of Definition 11.1.1 is satisfied.

Moreover, for each \( A \subseteq \Omega \) we have

\[
\tilde{\mathbb{P}}(A) = \mathbb{P}(A \mid B) \quad \text{by definition of } \tilde{\mathbb{P}}
\]

\[
= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \quad \text{by Definition 11.1.24}
\]

\[
= \frac{1}{\mathbb{P}(B)} \sum_{\omega \in A \cap B} \mathbb{P}(\{\omega\}) \quad \text{by Proposition 11.1.5}
\]

\[
= \sum_{\omega \in A \cap B} \mathbb{P}(\{\omega\} \mid B) \quad \text{by Definition 11.1.24}
\]

\[
= \sum_{\omega \in A} \tilde{\mathbb{P}}(\{\omega\}) \quad \text{since } \mathbb{P}(\{\omega\} \mid B) = 0 \text{ for } \omega \in A \setminus B
\]

\[
= \sum_{\omega \in A} \tilde{\mathbb{P}}(\{\omega\}) \quad \text{by definition of } \tilde{\mathbb{P}}
\]

so condition (ii) of Proposition 11.1.5 is satisfied. Hence \(\tilde{\mathbb{P}}\) defines a probability measure on \(\Omega\).
</markdown><markdown>
Proposition 11.1.32 implies that we can use all the results we‚Äôve proved about probability measures to conditional probability given a fixed event \( B \). For example, Theorem 11.1.18 implies that

\[
\mathbb{P}(A \cup A' \mid B) = \mathbb{P}(A \mid B) + \mathbb{P}(A' \mid B) - \mathbb{P}(A \cap A' \mid B)
\]

for all events \( A, A', B \) in a probability space \((\Omega, \mathbb{P})\) such that \(\mathbb{P}(B) > 0\).

The next theorem we prove has a very short proof, but is extremely important in applied probability theory.

### Theorem 11.1.33 (Bayes‚Äôs theorem)
Let \((\Omega, \mathbb{P})\) be a probability space and let \( A, B \) be events with positive probabilities. Then

\[
\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A)}
\]

**Proof**

Definition 11.1.24 gives

\[
\mathbb{P}(A \mid B) \mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B \cap A) = \mathbb{P}(B \mid A) \mathbb{P}(A)
\]

Dividing through by \(\mathbb{P}(A)\) yields the desired equation.

As stated, Bayes‚Äôs theorem is not necessarily particularly enlightening, but its usefulness increases sharply when combined with Proposition 11.1.29 to express the denominator of the fraction in another way.

### Corollary 11.1.34
Let \((\Omega, \mathbb{P})\) be a probability space and let \( A, B \) be events such that \(\mathbb{P}(A) > 0\) and \(0 < \mathbb{P}(B) < 1\). Then

\[
\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A \mid B) \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \mathbb{P}(B^c)}
\]

**Proof**

Bayes‚Äôs theorem tells us that

\[
\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A)}
\]

By Proposition 11.1.29 we have

\[
\mathbb{P}(A) = \mathbb{P}(A \mid B) \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \mathbb{P}(B^c)
\]

Substituting for \(\mathbb{P}(A)\) therefore yields

\[
\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A \mid B) \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \mathbb{P}(B^c)}
\]

as required.
</markdown><markdown>
The following example is particularly counterintuitive.

### Example 11.1.35

A town has 10,000 people, 30 of whom are infected with Disease X. Medical scientists develop a test for Disease X, which is accurate 99% of the time. A person takes the test, which comes back positive. We compute the probability that the person truly is infected with Disease X.

Let \( A \) be the event that the person tests positive for Disease X, and let \( B \) be the event that the person is infected with Disease X. We need to compute \( \mathbb{P}(B \mid A) \).

By Corollary 11.1.34, we have

\[
\mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A \mid B) \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \mathbb{P}(B^c)}
\]

It remains to compute the individual probabilities on the right-hand side of this equation. Well,

- \( \mathbb{P}(A \mid B) \) is the probability that the person tests positive for Disease X, given that they are infected. This is equal to \( \frac{99}{100} \), since the test is accurate with probability 99%.

- \( \mathbb{P}(A \mid B^c) \) is the probability that the person tests positive for Disease X, given that they are not infected. This is equal to \( \frac{1}{100} \), since the test is inaccurate with probability 1%.

- \( \mathbb{P}(B) = \frac{30}{10000} \), since 30 of the 10,000 inhabitants are infected with Disease X.

- \( \mathbb{P}(B^c) = \frac{9970}{10000} \), since 9,970 of the 10,000 inhabitants are not infected with Disease X.

Piecing this together gives

\[
\mathbb{P}(B \mid A) = \frac{\frac{99}{100} \cdot \frac{30}{10000}}{\frac{99}{100} \cdot \frac{30}{10000} + \frac{1}{100} \cdot \frac{9970}{10000}} = \frac{297}{1294} \approx 0.23
\]

Remarkably, the probability that the person is infected with Disease X given that the test is positive is only 23%, even though the test is accurate 99% of the time!

The following result generalises Corollary 11.1.34 to arbitrary partitions of the sample space into sets with positive probabilities.

### Corollary 11.1.36

Let \((\Omega, \mathbb{P})\) be a probability space, let \( A \) be an event with \( \mathbb{P}(A) > 0 \), and let \(\{B_i \mid i \in I\}\) be a family of mutually exclusive events indexed by a countable set \( I \) such that

\[
\mathbb{P}(B_i) > 0 \text{ for all } i \in I \quad \text{and} \quad \bigcup_{i \in I} B_i = \Omega
\]
</markdown><markdown>
Then

\[
\mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i)\mathbb{P}(B_i)}{\sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i)}
\]

for each \( i \in I \).

**Proof**

Bayes‚Äôs theorem tells us that

\[
\mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i)\mathbb{P}(B_i)}{\mathbb{P}(A)}
\]

By countable additivity, we have

\[
\mathbb{P}(A) = \mathbb{P}\left(\bigcup_{i \in I} A \cap B_i\right) = \sum_{i \in I} \mathbb{P}(A \cap B_i) = \sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i)
\]

Substituting for \(\mathbb{P}(A)\) therefore yields

\[
\mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i)\mathbb{P}(B_i)}{\sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i)}
\]

as required.

‚ú¶ **Example 11.1.37**

A small car manufacturer, Cars N‚ÄôAt, makes three models of car: the Allegheny, the Monongahela and the Ohio. It made 3000 Alleghenys, 6500 Monongahelas and 500 Ohios. In a given day, an Allegheny breaks down with probability \(\frac{1}{100}\), a Monongahela breaks down with probability \(\frac{1}{200}\), and the notoriously unreliable Ohio breaks down with probability \(\frac{1}{20}\). An angry driver calls Cars N‚ÄôAt to complain that their car has broken down. We compute the probability that the driver was driving an Ohio.

Let \( A \) be the event that the car is an Allegheny, let \( B \) be the event that the car is a Monongahela, and let \( C \) be the event that the car is an Ohio. Then

\[
\mathbb{P}(A) = \frac{3000}{10000}, \quad \mathbb{P}(B) = \frac{6500}{10000}, \quad \mathbb{P}(C) = \frac{500}{10000}
\]

Let \( D \) be the event that the car broke down. Then

\[
\mathbb{P}(D \mid A) = \frac{1}{100}, \quad \mathbb{P}(D \mid B) = \frac{1}{200}, \quad \mathbb{P}(D \mid C) = \frac{1}{20}
\]

We need to compute \(\mathbb{P}(C \mid D)\). Since the events \( A, B, C \) partition the sample space and have positive probabilities, we can use Corollary 11.1.36, which tells us that

\[
\mathbb{P}(C \mid D) = \frac{\mathbb{P}(D \mid C)\mathbb{P}(C)}{\mathbb{P}(D \mid A)\mathbb{P}(A) + \mathbb{P}(D \mid B)\mathbb{P}(B) + \mathbb{P}(D \mid C)\mathbb{P}(C)}
\]
</markdown><markdown>
Substituting the probabilities that we computed above, it follows that

\[
\mathbb{P}(C \mid D) = \frac{\frac{1}{100} \cdot \frac{1}{20} \cdot \frac{500}{10000}}{\frac{1}{100} \cdot \frac{3000}{10000} + \frac{1}{200} \cdot \frac{6500}{10000} + \frac{1}{20} \cdot \frac{500}{10000}} = \frac{2}{7} \approx 0.29
\]

### Exercise 11.1.38

In Example 11.1.37, find the probabilities that the car was an Allegheny and that the car was a Monongahela.
</markdown><markdown>
## Section 11.2

# Discrete random variables

Events in a probability space are sometimes unenlightening when looked at in isolation. For example, suppose we roll a fair six-sided die twice. The outcomes are elements of the set \([6] \times [6]\), each occurring with equal probability \(\frac{1}{36}\). The event that the die rolls sum to 7 is precisely the subset

\[
\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\} \subseteq [6] \times [6]
\]

and so we can say that the probability that the two rolls sum to 7 is

\[
\mathbb{P}(\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}) = \frac{1}{6}
\]

However, it is not at all clear from the expression \(\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}\) that, when we wrote it down, what we had in mind was the event that the sum of the die rolls is 7. Moreover, the expression of the event in this way does not make it clear how to generalise to other possible sums of die rolls.

Note that the sum of the die rolls defines a function \(S : [6] \times [6] \rightarrow [12]\), defined by

\[
S(a,b) = a + b \text{ for all } (a,b) \in [6] \times [6]
\]

The function \(S\) allows us to express our event in a more enlightening way: indeed,

\[
\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\} = \{(a,b) \in [6] \times [6] \mid a + b = 7\} = S^{-1}[\{7\}]
\]

(Recall the definition of preimage in Definition 3.1.38.) Thus the probability that the sum of the two die rolls is 7 is equal to \(\mathbb{P}(S^{-1}[\{7\}])\).

If we think of \(S\) not as a function \([6] \times [6] \rightarrow [12]\), but as a \([12]\)-valued random variable, which varies according to a random outcome in \([6] \times [6]\), then we can informally say

\[
\mathbb{P}\{S = 7\} = \frac{1}{6} \quad \text{which formally means} \quad \mathbb{P}(S^{-1}[\{7\}]) = \frac{1}{6}
\]

This affords us much more generality. Indeed, we could ask what the probability is that the die rolls sum to a value greater than or equal to 7. In this case, note that the die rolls \((a,b)\) sum to a number greater than or equal to 7 if and only if \(a + b \in \{7,8,9,10,11,12\}\), which occurs if and only if \((a,b) \in S^{-1}[\{7,8,9,10,11,12\}]\). Thus, we might informally say

\[
\mathbb{P}\{S \geq 7\} = \frac{7}{12} \quad \text{which formally means} \quad \mathbb{P}(S^{-1}[\{7,8,9,10,11,12\}]) = \frac{7}{12}
\]
</markdown><markdown>
We might also ask what the probability is that the sum of the die rolls is prime. In this case, we might informally say

\[
\mathbb{P}\{S \text{ is prime}\} = \frac{5}{12}
\]

which formally means

\[
\mathbb{P}(S^{-1}[\{2, 3, 5, 7, 11\}]) = \frac{5}{12}
\]

and so on. In each of these cases, we‚Äôre defining events‚Äîwhich are subsets of the sample space‚Äîin terms of conditions on the values of a random variable (which is, formally, a function).

We make the above intuition formal in Definition 11.2.1.

‚ú¶ **Definition 11.2.1**

Let \((\Omega, \mathbb{P})\) be a probability space and let \(E\) be a set. An **E-valued random variable** on \((\Omega, \mathbb{P})\) is a function \(X : \Omega \to E\) such that the image

\[
X[\Omega] = \{X(\omega) \mid \omega \in \Omega\}
\]

is countable. The set \(E\) is called the **state space** of \(X\). A random variable with countable state space is called a **discrete random variable**.

Before we proceed with examples, some notation for events regarding values of random variables will be particularly useful.

‚ú¶ **Notation 11.2.2**

Let \((\Omega, \mathbb{P})\) be a probability space, let \(E\) be a set and let \(X\) be an **E-valued random variable** on \((\Omega, \mathbb{P})\). For each \(e \in E\), write

\[
\{X = e\} = \{\omega \in \Omega \mid X(\omega) = e\} = X^{-1}[\{e\}]
\]

to denote the event that \(X\) takes the value \(e\). More generally, for each logical formula \(p(x)\) with free variable \(x\) ranging over \(E\), we write

\[
\{p(X)\} = \{\omega \in \Omega \mid p(X(\omega))\} = X^{-1}[\{e \in E \mid p(e)\}]
\]

for the event that the value of \(X\) satisfies \(p(x)\).

We will usually write \(\mathbb{P}\{X = e\}\) instead of \(\mathbb{P}(\{X = e\})\) for the probability that a random variable \(X\) takes a value \(e\), and so on.

‚úèÔ∏è **Example 11.2.3**

We can model a sequence of three coin flips using the probability space \((\Omega, \mathbb{P})\), where \(\Omega = \{H, T\}^3\) and \(\mathbb{P}(\{\omega\}) = \frac{1}{8}\) for all \(\omega \in \Omega\).

Let \(N\) be the real-valued random variable representing number of heads that show. This is formalised as a function

\[
N : \Omega \to \mathbb{R} \quad \text{where} \quad N(i_1, i_2, i_3) = \text{the number of heads amongst } i_1, i_2, i_3
\]
</markdown><markdown>
for example, \( N(H, T, H) = 2 \). Now

- The probability that exactly two heads show is

  \[
  \mathbb{P}\{N = 2\} = \mathbb{P}(N^{-1}[\{2\}]) = \mathbb{P}(\{(H, H, T), (H, T, H), (T, H, H)\}) = \frac{3}{2^3} = \frac{3}{8}
  \]

  by Notation 11.2.2 evaluating event \( N^{-1}[\{2\}] \)

- The probability that at least two heads show is

  \[
  \mathbb{P}\{N \geq 2\} = \mathbb{P}(\{\omega \in \Omega \mid N(\omega) \geq 2\}) = \mathbb{P}\left( \left\{ (H, H, T), (H, T, H), (T, H, H), (H, H, H) \right\} \right) = \frac{4}{2^3} = \frac{1}{2}
  \]

  by Notation 11.2.2 evaluating event

### Exercise 11.2.4
With probability space \((\Omega, \mathbb{P})\) and random variable \(N\) defined as in Example 11.2.3, compute \(\mathbb{P}\{N \text{ is odd}\}\) and \(\mathbb{P}\{N = 4\}\).

Each random variable comes with an associated probability mass function, which allows us to ‚Äòforget‚Äô the underlying probability space for the purposes of studying only the random variable.

### Definition 11.2.5
Let \((\Omega, \mathbb{P})\) be a probability space, let \(X : \Omega \to E\) be an \(E\)-valued random variable. The probability mass function of \(X\) is the function \(f_X : E \to [0, 1]\) defined by

\[
f_X(e) = \mathbb{P}\{X = e\} \text{ for all } e \in E
\]

### Example 11.2.6
The probability mass function of the random variable \(N\) from Example 11.2.3 is the function \(f_N : \mathbb{R} \to [0, 1]\) defined by

\[
f_N(e) = \mathbb{P}\{N = e\} = \frac{1}{8} \binom{3}{e}
\]

for all \(e \in \{0, 1, 2, 3\}\), and \(f_N(e) = 0\) otherwise. Indeed, there are \(2^3 = 8\) possible outcomes, each equally likely, and \(\binom{3}{e}\) of those outcomes show exactly \(e\) heads for \(e \in \{0, 1, 2, 3\}\).
</markdown><markdown>
### Exercise 11.2.7

Let \((\Omega, \mathcal{P})\) be a probability space, let \(E\) be a set, let \(X\) be an \(E\)-valued random variable and let \(U \subseteq E\). Prove that the event \(\{X \in U\}\) is equal to the preimage \(X^{-1}[U]\). Deduce that

\[
\mathbb{P}\{X \in U\} = \sum_{e \in U} f_X(e)
\]

In Example 11.2.6, we could have just specified the value of \(f_N\) on \(\{0, 1, 2, 3\}\), with the understanding that \(N\) does not take values outside of this set and hence that \(\mathbb{P}\{N = e\} = 0\) for all \(e \notin \{0, 1, 2, 3\}\). This issue arises frequently when dealing with real-valued discrete random variables, and it will be useful to ignore most (or all) of those real numbers which are not values of the random variable.

As such, for \(E \subseteq \mathbb{R}\), we will from now on blur the distinction between the following concepts:

(i) \(E\)-valued random variables;

(ii) Real-valued random variables \(X\) such that \(\mathbb{P}\{X = x\} = 0\) for all \(x \notin E\).

### Example 11.2.8

The probability mass function of the random variable \(N\) from Example 11.2.3 can be taken to be the function \(f_X : \{0, 1, 2, 3\} \to [0, 1]\) defined by

\[
f_X(k) = \frac{1}{8} \binom{3}{k} \quad \text{for all } k \in \{0, 1, 2, 3\}
\]

### Lemma 11.2.9

Let \((\Omega, \mathcal{P})\) be a probability space, let \(E\) be a set and let \(X\) be an \(E\)-valued random variable. The events \(\{X = e\}\) for \(e \in E\) are mutually exclusive, and their union is \(\Omega\).

**Proof**

If \(e, e' \in E\), then for all \(\omega \in \Omega\) we have

\[
\omega \in \{X = e\} \cap \{X = e'\} \Leftrightarrow \omega \in X^{-1}[\{e\}] \cap X^{-1}[\{e'\}] \quad \text{by Notation 11.2.2}
\]
\[
\Leftrightarrow X(\omega) = e \text{ and } X(\omega) = e' \quad \text{by definition of preimage}
\]
\[
\Rightarrow e = e'
\]

so if \(e \neq e'\) then \(\{X = e\} \cap \{X = e'\} = \emptyset\). So the events are mutually exclusive.

Moreover, if \(\omega \in \Omega\), then \(\omega \in \{X = X(\omega)\}\). Hence

\[
\Omega = \bigcup_{e \in E} \{X = e\}
\]
</markdown><markdown>
### Theorem 11.2.10

Let \((\Omega, \mathbb{P})\) be a probability space, let \(E\) be a countable set, and let \(X\) be an \(E\)-valued random variable. Then

\[
\sum_{e \in E} f_X(e) = 1
\]

**Proof**

Since \(f_X(e) = \mathbb{P}\{X = e\}\) for all \(e \in E\), we need to check that

\[
\sum_{e \in E} \mathbb{P}\{X = e\} = 1
\]

By Lemma 11.2.9, we have

\[
\sum_{e \in E} \mathbb{P}\{X = e\} = \mathbb{P}\left( \bigcup_{e \in E} \{X = e\} \right) = \mathbb{P}(\Omega) = 1
\]

as required.

The following corollary follows immediately.

### Corollary 11.2.11

Let \((\Omega, \mathbb{P})\) be a probability space, let \(E\) be a countable set, and let \(X\) be an \(E\)-valued random variable. The function \(X_* \mathbb{P} : \mathcal{P}(E) \to [0, 1]\) defined by

\[
(X_*, \mathbb{P})(A) = \sum_{e \in A} f_X(e) = \mathbb{P}\{X \in A\}
\]

for all \(A \subseteq E\) defines a probability measure on \(E\). The space \((E, X_* \mathbb{P})\) is called the **pushforward probability measure** of \(X\).

Corollary 11.2.11 implies that any statement about probability measures can be applied to the pushforward measure. For example,

\[
\mathbb{P}\{X \in A \cup B\} = \mathbb{P}\{X \in A\} + \mathbb{P}\{X \in B\} - \mathbb{P}\{X \in A \cap B\}
\]

for all subsets \(A, B \subseteq E\).

As with events, there is a notion of independence for random variables.

### Definition 11.2.12

Let \((\Omega, \mathbb{P})\) be a discrete probability space and let \(X, Y : \Omega \to E\) be discrete random variables on \((\Omega, \mathbb{P})\). We say \(X\) and \(Y\) are **independent** if, for all \(e, e' \in E\), the events \(\{X = e\}\) and \(\{Y = e'\}\) are independent. More generally, random variables \(X_1, X_2, \ldots, X_n\) are **pairwise independent** if, for all \(e_1, e_2, \ldots, e_n \in E\), the events \(\{X_i = e_i\}\) are pairwise independent.
</markdown><markdown>
### Example 11.2.13

A fair six-sided die is rolled twice. Let \( X \) be the value shown on the first roll and \( Y \) be the value shown on the second roll.

We can model this situation by letting \( \Omega = [6] \times [6] \) with \( \mathbb{P}(\{(a,b)\}) = \frac{1}{36} \) for all \( (a,b) \in \Omega \). The random variables \( X, Y \) can thus be taken to be functions \( \Omega \to [6] \) defined by

\[
X(a,b) = a \quad \text{and} \quad Y(a,b) = b \quad \text{for all } (a,b) \in \Omega
\]

So let \( e, e' \in [6] \). Note first that

\[
\{X = e\} \cap \{Y = e'\} = \{(a,b) \in \Omega \mid a = e\} \cap \{(a,b) \in \Omega \mid b = e'\} \quad \text{by Notation 11.2.2}
\]

\[
= \{(a,b) \in \Omega \mid a = e \text{ and } b = e'\} = \{(e, e')\}
\]

Hence

\[
\mathbb{P}(\{X = e\} \cap \{Y = e'\}) = \mathbb{P}(\{(e,e')\}) = \frac{1}{36} = \frac{1}{6} \cdot \frac{1}{6} = \mathbb{P}\{X = e\}\mathbb{P}\{Y = e'\}
\]

The events \(\{X = e\}\) and \(\{Y = e'\}\) are independent, and so \( X \) and \( Y \) are independent.

### Exercise 11.2.14

A coin which shows heads with probability \( p \in [0, 1] \), and tails otherwise, is flipped five times. For each \( i \in [5] \), let

\[
X_i = 
\begin{cases} 
0 & \text{if the } i\text{th flip shows tails} \\
1 & \text{if the } i\text{th flip shows heads}
\end{cases}
\]

Prove that the random variables \( X_1, X_2, X_3, X_4, X_5 \) are pairwise independent.

One final technicality that we mention before continuing concerns performing arithmetic with random variables which assume real values.

### Notation 11.2.15

Let \( (\Omega, \mathbb{P}) \) be a probability space, and let \( X, Y \) be real-valued random variables on \( (\Omega, \mathbb{P}) \). Then we can define a new real-valued random variable \( X + Y \) by

\[
(X + Y)(\omega) = X(\omega) + Y(\omega) \quad \text{for all } \omega \in \Omega
\]

Likewise for multiplication, scalar multiplication and constants: for each \( \omega \in \Omega \), define

\[
(XY)(\omega) = X(\omega)Y(\omega), \quad (aX)(\omega) = a \cdot X(\omega), \quad a(\omega) = a
\]

where \( a \in \mathbb{R} \). Note that the random variables \( X + Y, XY, aX, a \) are all supported on a countable set.
</markdown><markdown>
### Example 11.2.16

A coin which shows heads with probability \( p \in [0, 1] \), and tails otherwise, is flipped \( n \) times. For each \( i \in [n] \), let

\[
X_i = 
\begin{cases} 
0 & \text{if the } i\text{th flip shows tails} \\
1 & \text{if the } i\text{th flip shows heads}
\end{cases}
\]

Then each \( X_i \) is a \(\{0, 1\}\)-valued random variable.

Define \( X = X_1 + X_2 + \cdots + X_n \). Then \( X \) is a \(\{0, 1, \ldots, n\}\)-valued random variable representing the number of heads that show in total after the coin is flipped \( n \) times.

### Probability distributions

Most of the random variables we are interested in are characterised by one of a few **probability distributions**. We won‚Äôt define the term ‚Äòprobability distribution‚Äô precisely‚Äîindeed, its use in the mathematical literature is often ambiguous and informal‚Äîinstead, we will take it to mean any description of the random behaviour of a probability space or random variable.

The **uniform distribution** models the real-world situation in which any of a fixed number of outcomes occurs with equal probability.

### Definition 11.2.17 (Uniform distribution)

Let \((\Omega, \mathbb{P})\) be a probability space, let \( E \) be a finite set, and let \( X : \Omega \to E \) be a random variable. We say \( X \) follows the **uniform distribution** on \( E \), or \( X \) is **uniformly distributed** on \( E \), if \( f_X \) is constant‚Äîthat is, if

\[
f_X(e) = \frac{1}{|E|} \quad \text{for all } e \in E
\]

If \( X \) is uniformly distributed on \( E \), we write \( X \sim \text{Unif}(E) \) (\(\LaTeX\) code: \texttt{\textbackslash sim}).

### Example 11.2.18

Let \((\Omega, \mathbb{P})\) be the probability space modelling the roll of a fair six-sided die, and let \( X \) be the \([6]\)-valued random variable representing the number shown. Then for each \( k \in [6] \) we have

\[
f_X(k) = \mathbb{P}\{X = k\} = \mathbb{P}(\{k\}) = \frac{1}{6}
\]

so \( X \) is uniformly distributed on \([6]\).

### Exercise 11.2.19

Let \((\Omega, \mathbb{P})\) be the probability space modelling the roll of a fair six-sided die, and let
</markdown><markdown>
Let \( X \) be the \(\{0, 1\}\)-valued random variable which is equal to 0 if the die shows an even number and 1 if the die shows an odd number. Prove that \( X \sim \text{Unif}(\{0, 1\}) \).

Before we continue, we prove that the notion of ‚Äòuniform distribution‚Äô does not make sense for countably infinite sets.

### Theorem 11.2.20
Let \((\Omega, \mathcal{P})\) be a probability space and let \( E \) be a countably infinite set. There is no notion of a uniformly \( E \)-valued random variable \( X \)‚Äîthat is, there is no \( p \in [0, 1] \) such that \( f_X(e) = p \) for all \( e \in E \).

**Proof**

We may assume \( E = \mathbb{N} \); otherwise, re-index the sums accordingly.

Let \( p \in [0, 1] \). Note that

\[
\sum_{n \in \mathbb{N}} f_X(n) = \sum_{n \in \mathbb{N}} p = \lim_{N \to \infty} \sum_{n=0}^{N} p = \lim_{N \to \infty} (N+1)p
\]

If \( p = 0 \) then

\[
\lim_{N \to \infty} (N+1)p = \lim_{N \to \infty} 0 = 0
\]

If \( p > 0 \) then, for all \( K > 0 \), letting \( N = \frac{K}{p} \) yields \( (N+1)p = K + p > K \), and hence

\[
\lim_{N \to \infty} (N+1)p = \infty
\]

Thus \(\sum_{n \in \mathbb{N}} p \neq 1\) for all \( p \in [0, 1] \).

In both cases, we have contradicted Theorem 11.2.10. As such, there can be no random variable \( X : \Omega \to \mathbb{N} \) such that \( f_X \) is constant.

The **Bernoulli distribution** models real-world situations in which one of two outcomes occurs, but not necessarily with the same probability.

### Definition 11.2.21 (Bernoulli distribution)
Let \((\Omega, \mathcal{P})\) be a probability space. A \(\{0, 1\}\)-valued random variable \( X \) follows the **Bernoulli distribution** with parameter \( p \) if its probability mass function \( f_X : \{0, 1\} \to [0, 1] \) satisfies

\[
f_X(0) = 1 - p \quad \text{and} \quad f_X(1) = p
\]

If \( X \) follows the Bernoulli distribution with parameter \( p \), we write \( X \sim B(1, p) \).
</markdown><markdown>
The reason behind the notation \( B(1, p) \) will become clear soon‚Äîthe Bernoulli distribution is a specific instance of a more general distribution, which we will see in Definition 11.2.24.

### Example 11.2.22
A coin shows ‚Äòheads‚Äô with probability \( p \) and ‚Äòtails‚Äô with probability \( 1 - p \). Let \( X \) be the random variable which takes the value 0 if the coin shows tails and 1 if the coin shows heads. Then \( X \sim B(1, p) \).

### Exercise 11.2.23
Let \( X \) be a \(\{0, 1\}\)-valued random variable. Prove that \( X \sim U(\{0, 1\}) \) if and only if \( X \sim B(1, \frac{1}{2}) \).

Suppose that, instead of flipping a coin just once, as in Example 11.2.22, you flip it \( n \) times. The total number of heads that show must be an element of \(\{0, 1, \ldots, n\}\), and each such element occurs with some positive probability. The resulting probability distribution is called the **binomial distribution**.

### Definition 11.2.24 (Binomial distribution)
Let \((\Omega, \mathbb{P})\) be a probability space. A \(\{0, 1, \ldots, n\}\)-valued random variable \( X \) follows the **binomial distribution with parameters** \( n, p \) if its probability mass function \( f_X : \{0, 1, \ldots, n\} \to [0, 1] \) satisfies

\[
f_X(k) = \binom{n}{k} p^k (1-p)^{n-k}
\]

for all \( k \in \{0, 1, \ldots, n\} \). If \( X \) follows the binomial distribution with parameters \( n, p \), we write \( X \sim B(n, p) \).

### Example 11.2.25
A coin which shows heads with probability \( p \in [0, 1] \), and tails otherwise, is flipped \( n \) times. We will prove that the number of heads that show is binomially distributed.

We can model this situation with probability space \((\Omega, \mathbb{P})\) defined by taking \(\Omega = \{H, T\}^n\), and letting \(\mathbb{P}(\{\omega\}) = p^h (1-p)^t\) for all \(\omega \in \Omega\), where \( h \) is the number of heads that show and \( t \) is the number of tails that show in outcome \(\omega\). For example, if \( n = 5 \) then

\[
\mathbb{P}(\{HTHHT\}) = p^3 (1-p)^2 \quad \text{and} \quad \mathbb{P}(\{TTTTT\}) = (1-p)^5
\]

Note in particular that \( h + t = n \).

Let \( X \) be the random variable which counts the number of heads that show. Formally, we can define \( X : \{H, T\}^n \to \{0, 1, \ldots, n\} \) by letting \( X(\omega) \) be the number of heads that show in outcome \(\omega\). For example if \( n = 5 \) then

\[
X(HTHHT) = 3 \quad \text{and} \quad X(TTTTT) = 0
\]
</markdown><markdown>
The event \(\{X = k\}\) is the set of \(n\)-tuples of ‚ÄòH‚Äôs and ‚ÄòT‚Äôs which contain exactly \(k\) ‚ÄòH‚Äô. Hence \(|\{X = k\}| = \binom{n}{k}\), since such an \(n\)-tuple can be specified by choosing the \(k\) positions of the ‚ÄòH‚Äôs, and putting ‚ÄòT‚Äôs in the remaining positions. Since each outcome in this event occurs with equal probability \(p^k (1-p)^{n-k}\), it follows that

\[
f_X(k) = \binom{n}{k} p^k (1-p)^{n-k}
\]

for all \(k \in \{0, 1, \ldots, n\}\). Hence \(X \sim \text{B}(n, p)\).

The following theorem proves that the sum of Bernoulli random variables follows the binomial distribution.

### Theorem 11.2.26
Let \((\Omega, \mathbb{P})\) be a probability space, let \(p \in [0, 1]\) and let \(X_1, X_2, \ldots, X_n : \Omega \to \{0, 1\}\) be independent random variables such that \(X_i \sim \text{B}(1, p)\). Then

\[
X_1 + X_2 + \cdots + X_n \sim \text{B}(n, p)
\]

**Proof**

Let \(X = X_1 + X_2 + \cdots + X_n\). For each outcome \(\omega\) and each \(k \in \{0, 1, \ldots, n\}\), we have \(X(\omega) = k\) if and only if exactly \(k\) of the values \(X_1(\omega), X_2(\omega), \ldots, X_n(\omega)\) are equal to 1.

For each specification \(S\) of which of the random variables \(X_i\) is equal to 1, let \(A_S \subseteq \Omega\) be the event that this occurs. Formally, this is to say that, for each \(S \subseteq [n]\), we define

\[
A_S = \{\omega \in \Omega \mid X_i(\omega) = 0 \text{ for all } i \notin S \text{ and } X_i(\omega) = 1 \text{ for all } i \in S\}
\]

Then \(\mathbb{P}(A_S) = p^k (1-p)^{n-k}\), since the random variables \(X_1, X_2, \ldots, X_n\) are mutually independent.

As argued above sets \(\{A_S \mid U \subseteq [n], |S| = k\}\) form a partition of \(\{X = k\}\), and hence

\[
f_X(k) = \sum_{S \in \binom{[n]}{k}} \mathbb{P}(A_S) = \sum_{S \in \binom{[n]}{k}} p^k (1-p)^{n-k} = \binom{n}{k} p^k (1-p)^{n-k}
\]

which is to say that \(X \sim \text{B}(n, p)\).

We will make heavy use of Theorem 11.2.26 when we will study the expectation of binomially distributed random variables (Definition 11.2.34). First, let‚Äôs will look at a couple of scenarios in which a binomially distributed random variable is expressed as a sum of independent Bernoulli random variables.
</markdown><markdown>
### Example 11.2.27

In Example 11.2.25, we could have defined \(\{0,1\}\)-valued random variables \(X_1, X_2, \ldots, X_n\) by letting

\[
X_i(\omega) = 
\begin{cases} 
0 & \text{if the } i\text{th coin flip shows tails} \\ 
1 & \text{if the } i\text{th coin flip shows heads} 
\end{cases}
\]

Then the number of heads shown in total is the random variable \(X = X_1 + X_2 + \cdots + X_n\). Note that each random variable \(X_i\) follows the Bernoulli distribution with parameter \(p\), and they are independent, so that \(X \sim B(n, p)\) by [Theorem 11.2.26](#).

In Example 11.2.25, we flipped a coin a fixed number of times and counted how many heads showed. Now suppose that we flip a coin repeatedly until heads show, and then stop. The number of times the coin was flipped before heads shows could, theoretically, be any natural number. This situation is modelled by the **geometric distribution**.

### Definition 11.2.28 (Geometric distribution on \(\mathbb{N}\))

Let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space. An \(\mathbb{N}\)-valued random variable \(X\) follows the **geometric distribution with parameter** \(p\) if its probability mass function \(f_X : \mathbb{N} \to [0, 1]\) satisfies

\[
f_X(k) = (1 - p)^k p \quad \text{for all } k \in \mathbb{N}
\]

If \(X\) follows the geometric distribution with parameter \(p\), we write \(X \sim \text{Geom}(p)\).

### Example 11.2.29

A coin which shows heads with probability \(p \in [0, 1]\), and tails otherwise, is flipped repeatedly until heads shows.

### Exercise 11.2.30

Let \(p \in [0, 1]\) and let \(X \sim \text{Geom}(p)\). Prove that

\[
\mathbb{P}\{X \text{ is even}\} = \frac{1}{2 - p}
\]

What is the probability that \(X\) is odd?

Occasionally, it will be useful to consider geometrically distributed random variables which are valued in the set

\[
\mathbb{N}^+ = \{1, 2, 3, \ldots\}
\]

of all **positive** natural numbers. The probability mass function of such a random variable is slightly different.
</markdown><markdown>
### Definition 11.2.31 (Geometric distribution on \( \mathbb{N}^+ \))

Let \( (\Omega, \mathbb{P}) \) be a probability space. An \( \mathbb{N}^+ \)-valued random variable \( X \) follows the **geometric distribution with parameter** \( p \) if its probability mass function \( f_X : \mathbb{N}^+ \to [0, 1] \) satisfies

\[
f_X(k) = (1 - p)^{k-1} p \quad \text{for all } k \in \mathbb{N}^+
\]

If \( X \) follows the geometric distribution with parameter \( p \), we write \( X \sim \text{Geom}(p) \).

It is to be understood from context whether a given geometric random variable is \( \mathbb{N} \)-valued or \( \mathbb{N}^+ \)-valued.

### Example 11.2.32

An urn contains \( n \geq 1 \) distinct coupons. Each time you draw a coupon that you have not drawn before, you get a stamp. When you get all \( n \) stamps, you win. Let \( X \) be the number of coupons drawn up to, and including, a winning draw.

For each \( k \in [n] \), let \( X_k \) be the random variable representing the number of draws required to draw the \( k \)th new coupon, after \( k - 1 \) coupons have been collected. Then the total number of times a coupon must be drawn is \( X = X_1 + X_2 + \cdots + X_n \).

After \( k - 1 \) coupons have been collected, there are \( n - k + 1 \) uncollected coupons remaining in the urn, and hence on any given draw, an uncollected coupon is drawn with probability \( \frac{n-k+1}{n} \), and a coupon that has already been collected is drawn with probability \( \frac{k-1}{n} \). Hence for each \( r \in \mathbb{N}^+ \) we have

\[
\mathbb{P}\{X_k = r\} = \left( \frac{k-1}{n} \right)^{r-1} \left( \frac{n-k+1}{n} \right)
\]

That is to say, \( X_k \) is geometrically distributed on \( \mathbb{N}^+ \) with parameter \( \frac{n-k+1}{n} \).

We will use this in Example 11.2.47 to compute the number of times a person should expect to have to draw coupons from the urn until they win.

### Expectation

We motivate the definition of **expectation** (Definition 11.2.34) with the following example.

### Example 11.2.33

For each \( n \geq 1 \), let \( X_n \) be the average value shown when a fair six-sided die is rolled \( n \) times.

When \( n \) is small, the value of \( X_n \) is somewhat unpredictable. For example, \( X_1 \) is uni-
</markdown><markdown>
formly distributed, since it takes each of the values 1, 2, 3, 4, 5, 6 with equal probability. This is summarised in the following table:

\[
\begin{array}{c|cccccc}
e & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
\mathbb{P}\{X_1 = e\} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} \\
\end{array}
\]

The distribution of \(X_2\) is shown in the following table:

\[
\begin{array}{c|cccccccccc}
e & 1 & 1.5 & 2 & 2.5 & 3 & 3.5 & 4 & 4.5 & 5 & 5.5 & 6 \\
\hline
\mathbb{P}\{X_2 = e\} & \frac{1}{36} & \frac{2}{36} & \frac{3}{36} & \frac{4}{36} & \frac{5}{36} & \frac{6}{36} & \frac{5}{36} & \frac{4}{36} & \frac{3}{36} & \frac{2}{36} & \frac{1}{36} \\
\end{array}
\]

As can be seen, the probabilities increase towards the middle of the table; the extreme values occur with low probability. This effect is exaggerated as \(n\) increases. Indeed,

\[
\mathbb{P}\{X_n = 1\} = \mathbb{P}\{X_n = 6\} = \frac{1}{6^n}
\]

so that \(\mathbb{P}\{X_n = 1\} \to 0\) and \(\mathbb{P}\{X_n = 6\} \to 0\) as \(n \to \infty\); however, it can be shown that for all \(\epsilon > 0\), we have

\[
\mathbb{P}\{3.5 - \epsilon < X_n < 3.5 + \epsilon\}
\]

so that \(\mathbb{P}\{|X_n - 3.5| < \epsilon\} \to 1\) as \(n \to \infty\).

Thus when we roll a die repeatedly, we can expect its value to eventually be as close to 3.5 as we like. This is an instance of a theorem called the **law of large numbers**.

The value 3.5 in Example 11.2.33 is special because it is the average of the numbers 1, 2, 3, 4, 5, 6. More generally, assignments of different probabilities to different values of a random variable \(X\) yields a **weighted average** of the possible values. This weighted average, known as the **expectation** of the random variable, behaves in the same way as the number 3.5 did in Example 11.2.33.

### Definition 11.2.34
Let \((\Omega, \mathbb{P})\) be a probability space, let \(E \subseteq \mathbb{R}\) be countable, and let \(X\) be an \(E\)-valued random variable on \((\Omega, \mathbb{P})\). The **expectation** (or **expected value**) of \(X\), if it exists, is the real number \(\mathbb{E}[X]\) (LaTeX code: \texttt{\textbackslash mathbb\{E\}}) defined by

\[
\mathbb{E}[X] = \sum_{e \in E} e f_X(e)
\]

### Example 11.2.35
Let \(X\) be a random variable representing the value shown when a fair six-sided die is
</markdown><markdown>
rolled. Then \( X \sim \mathcal{U}([6]) \), so that \( f_X(k) = \frac{1}{6} \) for all \( k \in [6] \), and hence

\[
\mathbb{E}[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5
\]

so the expected value of the die roll is 3.5.

### Example 11.2.36
Let \( p \in [0, 1] \) and let \( X \sim \mathcal{B}(1, p) \). Then

\[
\mathbb{E}[X] = 0 \cdot (1 - p) + 1 \cdot p = p
\]

So the expected value of a Bernoulli random variable is equal to the parameter.

### Exercise 11.2.37
Let \((\Omega, \mathbb{P})\) be a probability space and let \( c \in \mathbb{R} \). Thinking of \( c \) as a constant real-valued random variable,[a] prove that \(\mathbb{E}[c] = c\).

The following lemma provides an alternative method for computing the expectation of a random variable. It will be useful for proving that expectation is linear in [Theorem 11.2.43](#).

### Lemma 11.2.38
Let \((\Omega, \mathbb{P})\) be a probability space, let \( E \) be a countable set and let \( X \) be an \( E \)-valued random variable on \((\Omega, \mathbb{P})\). Then

\[
\mathbb{E}[X] = \sum_{\omega \in \Omega} X(\omega) \mathbb{P}(\{\omega\})
\]

**Proof**

Recall from [Lemma 11.2.9](#) that

\[
\Omega = \bigcup_{e \in E} \{X = e\}
\]

and the events \(\{X = e\}\) are mutually exclusive. Hence

\[
\sum_{\omega \in \Omega} X(\omega) \mathbb{P}(\{\omega\}) = \sum_{e \in E} \sum_{\omega \in \{X = e\}} X(\omega) \mathbb{P}(\{\omega\}) \quad \text{by Lemma 11.2.9}
\]

\[
= \sum_{e \in E} e \mathbb{P}\{X = e\} \quad \text{by (ii) in Proposition 11.1.5}
\]

\[
= \sum_{e \in E} e f_X(e) \quad \text{by Definition 11.2.5}
\]

as required.

[a] Formally, we should define \( X : \Omega \to \mathbb{R} \) by letting \( X(\omega) = c \) for all \( \omega \in \Omega \); then compute \(\mathbb{E}[X]\).
</markdown><markdown>
### Proposition 11.2.39

Let \( n \in \mathbb{N} \) and \( p \in [0, 1] \), and suppose that \( X \) is a random variable such that \( X \sim B(n, p) \). Then \(\mathbb{E}[X] = np\).

**Proof**

Since \( X \sim B(n, p) \), we have \( f_X(k) = \binom{n}{k} p^k (1-p)^{n-k} \) for all \( 0 \leq k \leq n \). Hence

\[
\mathbb{E}[X] = \sum_{k=0}^{n} k \cdot \binom{n}{k} p^k (1-p)^{n-k} \quad \text{by definition of expectation}
\]

\[
= \sum_{k=1}^{n} k \cdot \binom{n}{k} p^k (1-p)^{n-k} \quad \text{since the } k = 0 \text{ term is zero}
\]

\[
= \sum_{k=1}^{n} n \cdot \binom{n-1}{k-1} p^k (1-p)^{n-k} \quad \text{by Proposition 8.1.38}
\]

\[
= \sum_{\ell=0}^{n-1} n \cdot \binom{n-1}{\ell} p^{\ell+1} (1-p)^{(n-1)-\ell} \quad \text{writing } \ell = k+1
\]

\[
= np \cdot \sum_{\ell=0}^{n-1} \binom{n-1}{\ell} p^\ell (1-p)^{(n-1)-\ell} \quad \text{pulling out constant factors}
\]

\[
= np(p + (1-p))^{n-1} \quad \text{by the binomial theorem}
\]

\[
= np \quad \text{since } p + (1-p) = 1
\]

as required.

### Example 11.2.40

A coin which shows heads with probability \( \frac{1}{3} \), and tails otherwise, is tossed 12 times. Letting \( X \) be the random variable representing the number of heads that show, we see that \( X \sim B(12, \frac{1}{3}) \), and hence the expected number of heads that show is equal to

\[
\mathbb{E}[X] = 12 \cdot \frac{1}{3} = 4
\]

### Exercise 11.2.41

Use Exercise 9.3.50 to prove that the expectation of a \(\mathbb{N}\)-valued random variable which is geometrically distributed with parameter \( p \in [0, 1] \) is equal to \(\frac{1-p}{p}\). Use this to compute the expected number of times a coin must be flipped before the first time heads shows, given that heads shows with probability \(\frac{2}{7}\).

### Exercise 11.2.42

Prove that the expectation of a \(\mathbb{N}^+\)-valued random variable which is geometrically distributed with parameter \( p \in [0, 1] \) is equal to \(\frac{1}{p}\).
</markdown><markdown>
### Theorem 11.2.43 (Linearity of expectation)

Let \((\Omega, \mathbb{P})\) be a probability space, let \(E \subseteq \mathbb{R}\) be countable, let \(X\) and \(Y\) be \(E\)-valued random variables on \((\Omega, \mathbb{P})\), and let \(a, b \in \mathbb{R}\). Then

\[
\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]
\]

**Proof**

This follows directly from the fact that summation is linear. Indeed,

\[
\begin{align*}
\mathbb{E}[aX + bY] &= \sum_{\omega \in \Omega} (aX + bY)(\omega)\mathbb{P}(\{\omega\}) & \text{by Lemma 11.2.38} \\
&= \sum_{\omega \in \Omega} \left(aX(\omega)\mathbb{P}(\{\omega\}) + bY(\omega)\mathbb{P}(\{\omega\})\right) & \text{expanding} \\
&= a \sum_{\omega \in \Omega} X(\omega)\mathbb{P}(\{\omega\}) + b \sum_{\omega \in \Omega} Y(\omega)\mathbb{P}(\{\omega\}) & \text{by linearity of summation} \\
&= a\mathbb{E}[X] + b\mathbb{E}[Y] & \text{by Lemma 11.2.38}
\end{align*}
\]

as required.

### Example 11.2.44

Let \(X\) be a random variable representing the sum of the numbers shown when a fair six-sided die is rolled twice. We can write \(X = Y + Z\), where \(Y\) is the value of the first die roll and \(Z\) is the value of the second die roll. By Example 11.2.35, we have \(\mathbb{E}[Y] = \mathbb{E}[Z] = 3.5\). Linearity of expectation then yields

\[
\mathbb{E}[X] = \mathbb{E}[Y] + \mathbb{E}[Z] = 3.5 + 3.5 = 7
\]

so the expected value of the sum of the two die rolls is 7.

### Example 11.2.45

A coin, when flipped, shows heads with probability \(p \in [0, 1]\). The coin is flipped. If it shows heads, I gain $10; if it shows tails, I lose $20. We compute the least value of \(p\) that ensures that I do not expect to lose money.

Let \(X\) be the random variable which is equal to 0 if tails shows, and 1 if heads shows. Then \(X \sim \text{B}(1, p)\), so that \(\mathbb{E}[X] = p\) by Example 11.2.36. Let \(Y\) be the amount of money I gain. Then

\[
Y = 10X - 20(1 - X) = 30X - 20
\]

Hence my expected winnings are

\[
\mathbb{E}[Y] = 30\mathbb{E}[X] - 20 = 30p - 20
\]

In order for this number to be non-negative, we require \(p \geq \frac{2}{3}\).

Theorem 11.2.43 generalises by induction to linear combinations of countably many random variables; this is proved in the following exercise.
</markdown><markdown>
### Exercise 11.2.46

Let \((\Omega, \mathcal{P})\) be a probability space, let \(E \subseteq \mathbb{R}\) be countable, let \(\{X_i \mid i \in I\}\) be a family of \(E\)-valued random variables on \((\Omega, \mathcal{P})\), indexed by some countable set \(I\), and let \(\{a_n \mid n \in \mathbb{N}\}\) be an \(I\)-indexed family of real numbers. Prove that

\[
\mathbb{E} \left[ \sum_{i \in I} a_i X_i \right] = \sum_{i \in I} a_i \mathbb{E}[X_i]
\]

### Example 11.2.47

Recall Example 11.2.32: an urn contains \(n \geq 1\) distinct coupons. Each time you draw a coupon that you have not drawn before, you get a stamp. When you get all \(n\) stamps, you win. We find the expected number of times you need to draw a coupon from the urn in order to win.

For each \(k \in [n]\), let \(X_k\) be the random variable representing the number of draws required to draw the \(k\)th new coupon, after \(k-1\) coupons have been collected. Then the total number of times a coupon must be drawn is \(X = X_1 + X_2 + \cdots + X_n\).

We already saw that \(X_k \sim \text{Geom} \left( \frac{n-k+1}{n} \right)\) for each \(k \in [n]\). By Exercise 11.2.42, we have \(\mathbb{E}[X_k] = \frac{n}{n-k+1}\) for all \(k \in [n]\). By linearity of expectation, it follows that

\[
\mathbb{E}[X] = \sum_{k=1}^{n} \mathbb{E}[X_k] = \sum_{k=1}^{n} \frac{n}{n-k+1} = n \sum_{i=1}^{n} \frac{1}{i}
\]
</markdown><markdown>
## Chapter 11 exercises

### True‚ÄìFalse questions

In [Questions 11.1 to 11.5](#), determine (with proof) whether the statement is true or false.

11.1. In a probability space \((\Omega, \mathbb{P})\), we have \(\mathbb{P}(A \mid \Omega) = \mathbb{P}(A)\) for all events \(A\).

11.2. The function \(\mathbb{P} : \mathcal{P}(\mathbb{N}) \to [0, 1]\) defined by \(\mathbb{P}(A) = \sum_{n \in A} 2^{-n}\) is a probability measure on \(\mathbb{N}\).

11.3. The function \(\mathbb{P} : \mathcal{P}(\mathbb{Z}) \to [0, 1]\) defined by \(\mathbb{P}(A) = \sum_{n \in A} 2^{-n}\) is a probability measure on \(\mathbb{Z}\).

11.4. The function \(\mathbb{P} : \mathcal{P}(\mathbb{N}) \to [0, 1]\), defined by letting \(\mathbb{P}(\emptyset) = \emptyset\) and \(\mathbb{P}(A) = 2^{-\min(A)}\) for all \(A \neq \emptyset\), is a probability measure on \(\mathbb{N}\).

11.5. A random variable can have infinite expectation.

### Always‚ÄìSometimes‚ÄìNever questions

In [Questions 11.6 to 11.10](#), determine (with proof) whether the conclusion is always, sometimes or never true under the given hypotheses.

11.6. Let \((\Omega, \mathbb{P})\) be a probability space. The only event whose probability is zero is the empty event \(\emptyset \subseteq \Omega\).

11.7. Let \((\Omega, \mathbb{P})\) be a probability space and let \(A\) be an event. Then \(\mathbb{P}(A)^2 - \mathbb{P}(A) = 2\).

11.8. Let \((\Omega, \mathbb{P})\) be a probability space and let \(A\) and \(B\) be events with \(\mathbb{P}(B) > 0\). Then \(\mathbb{P}(A \mid B) \geq \mathbb{P}(A)\).

11.9. Let \((\Omega, \mathbb{P})\) be a probability space and let \(A\) and \(B\) be mutually exclusive events with \(\mathbb{P}(B) > 0\). Then \(\mathbb{P}(A \mid B) = 0\).

11.10. Let \((\Omega, \mathbb{P})\) be a probability space and let \(A, B\) and \(C\) be events with \(\mathbb{P}(B) > 0\) and \(\mathbb{P}(C) > 0\). Then \(\mathbb{P}(A \mid B)\mathbb{P}(B \mid C) = \mathbb{P}(A \mid C)\).
</markdown><markdown>
# Chapter 12

## Additional topics
</markdown><markdown>
## Section 12.1

# Orders and lattices

We saw in [Section 5.1](#) how equivalence relations behave like ‚Äò=‚Äô, in the sense that they are reflexive, symmetric and transitive.

This section explores a new kind of relation which behaves like ‚Äò‚â§‚Äô. This kind of relation proves to be extremely useful for making sense of mathematical structures, and has powerful applications throughout mathematics, computer science and even linguistics.

‚ú¶ **Definition 12.1.1**  
A relation \( R \) on a set \( X \) is a **partial order** if \( R \) is reflexive, antisymmetric and transitive. That is, if:

- (Reflexivity) \( x \, R \, x \) for all \( x \in X \);
- (Antisymmetry) For all \( x, y \in X \), if \( x \, R \, y \) and \( y \, R \, x \), then \( x = y \);
- (Transitivity) For all \( x, y, z \in X \), if \( x \, R \, y \) and \( y \, R \, z \), then \( x \, R \, z \).

A set \( X \) together with a partial order \( R \) on \( X \) is called a **partially ordered set**, or **poset** for short, and is denoted \( (X, R) \).

When we talk about partial orders, we usually use a suggestive symbol like ‚Äò\(\preceq\)‚Äô (\(\LaTeX\) code: `\preceq`) or ‚Äò\(\sqsubseteq\)‚Äô (\(\LaTeX\) code: `\sqsubseteq`).

‚úèÔ∏è **Example 12.1.2**  
We have seen many examples of posets so far:

- Any of the sets \( \mathbb{N}, \mathbb{Z}, \mathbb{Q} \) or \( \mathbb{R} \), with the usual order relation \(\leq\).
- Given a set \( X \), its power set \( \mathcal{P}(X) \) is partially ordered by \(\subseteq\). Indeed:
  - **Reflexivity.** If \( U \in \mathcal{P}(X) \) then \( U \subseteq U \).
  - **Antisymmetry.** If \( U, V \in \mathcal{P}(X) \) with \( U \subseteq V \) and \( V \subseteq U \), then \( U = V \) by definition of set equality.
  - **Transitivity.** If \( U, V, W \in \mathcal{P}(X) \) with \( U \subseteq V \) and \( V \subseteq W \), then \( U \subseteq W \) by [Proposition 2.1.20](#).

- The set \( \mathbb{N} \) of natural numbers is partially ordered by the divisibility relation‚Äîsee [Exercises 5.1.23, 5.1.33 and 5.1.40](#). However, as noted in [Exercise 5.1.33](#), the set \( \mathbb{Z} \) of integers is not partially ordered by divisibility, since divisibility is not antisymmetric on \( \mathbb{Z} \).
</markdown><markdown>
- Any set \( X \) is partially ordered by its equality relation. This is called the **discrete order** on \( X \).

Much like the difference between the relations \(\preceq\) and \(<\) on \(\mathbb{N}\), or between \(\subseteq\) and \(\subset\) on \(\mathcal{P}(X)\), every partial order can be strictified, in a precise sense outlined in the following definition and proposition.

### ‚ú¶ Definition 12.1.3
A relation \( R \) on a set \( X \) is a **strict partial order** if it is irreflexive, asymmetric and transitive. That is, if:

- (Irreflexivity) \(\neg(x \, R \, x)\) for all \( x \in X \);
- (Asymmetry) For all \( x, y \in X \), if \( x \, R \, y \), then \(\neg(y \, R \, x)\);
- (Transitivity) For all \( x, y, z \in X \), if \( x \, R \, y \) and \( y \, R \, z \), then \( x \, R \, z \).

### ‚ú£ Proposition 12.1.4
Let \( X \) be a set. Partial orders \(\preceq\) on \( X \) are in natural correspondence with strict partial orders \( \prec \) on \( X \), according to the rule:

\[
x \preceq y \iff (x \prec y \lor x = y) \quad \text{and} \quad x \prec y \iff (x \preceq y \land x \neq y)
\]

**Proof**  
Let \( P \) be the set of all partial orders on \( X \) and let \( S \) be the set of all strict partial orders on \( X \). Define functions

\[
f : P \to S \quad \text{and} \quad g : S \to P
\]

as in the statement of the proposition, namely:

- Given a partial order \(\preceq\), let \( f(\preceq) \) be the relation \(\prec\) defined for \( x, y \in X \) by letting \( x \prec y \) be true if and only if \( x \preceq y \) and \( x \neq y \);
- Given a strict partial order \(\prec\), let \( g(\prec) \) be the relation \(\preceq\) defined for \( x, y \in X \) by letting \( x \preceq y \) be true if and only if \( x \prec y \) or \( x = y \).

We‚Äôll prove that \( f \) and \( g \) are mutually inverse functions. Indeed:

- \( f \) is well-defined. To see this, fix \(\preceq\) and \(\prec = f(\preceq)\) and note that:
  - \(\prec\) is irreflexive, since for \( x \in X \) if \( x \prec x \) then \( x \neq x \), which is a contradiction.
  - \(\prec\) is asymmetric. To see this, let \( x, y \in X \) and suppose \( x \prec y \). Then \( x \preceq y \) and \( x \neq y \). If also \( y \prec x \), then we‚Äôd have \( y \preceq x \), so that \( x = y \) by antisymmetry of \(\preceq\). But \( x \neq y \), so this is a contradiction.
</markdown><markdown>
‚óä ‚â∫ is transitive. To see this, let \(x, y, z \in X\) and suppose \(x \prec y\) and \(y \prec z\). Then \(x \nprec y\) and \(y \nprec z\), so that \(x \nprec z\). Moreover, if \(x = z\) then we‚Äôd also have \(x \nprec x\) by reflexivity of \(\nprec\), so \(z \nprec y\) by transitivity of \(\nprec\), and hence \(y = z\) by antisymmetry of \(\nprec\). But this contradicts \(y \prec z\).

So \(\prec\) is a strict partial order on \(X\).

- \(g\) is well-defined. To see this, fix \(\prec\) and \(\nprec = g(\prec)\) and note that:
  ‚óä \(\nprec\) is reflexive. This is built into the definition of \(\nprec\).
  ‚óä \(\nprec\) is symmetric. To see this, fix \(x, y \in X\) and suppose \(x \nprec y\) and \(y \nprec x\). Now if \(x \neq y\) then \(x \prec y\) and \(y \prec x\), but this contradicts asymmetry of \(\prec\). Hence \(x = y\).
  ‚óä \(\nprec\) is transitive. To see this, fix \(x, y, z \in X\) and suppose \(x \nprec y\) and \(y \nprec z\). Then one of the following four cases must be true:
    * \(x = y = z\). In this case, \(x = z\), so \(x \nprec z\).
    * \(x = y \prec z\). In this case, \(x \prec z\), so \(x \nprec z\).
    * \(x \prec y = z\). In this case, \(x \prec z\), so \(x \nprec z\).
    * \(x \prec y \prec z\). In this case, \(x \prec z\) by transitivity of \(\prec\), so \(x \nprec z\).

In any case, we have that \(x \nprec z\).

So \(\nprec\) is a partial order on \(X\).

- \(g \circ f = \text{id}_P\). To see this, let \(\prec = f(\nprec)\) and \(\sqsubseteq = g(\prec)\). For \(x, y \in X\), we have \(x \sqsubseteq y\) if and only if \(x \prec y\) or \(x = y\), which in turn occurs if and only if \(x = y\) or both \(x \nprec y\) and \(x \neq y\). This is equivalent to \(x \nprec y\), since if \(x = y\) then \(x \nprec y\) by reflexivity. Hence \(\sqsubseteq\) and \(\nprec\) are equal relations, so \(g \circ f = \text{id}_P\).

- \(f \circ g = \text{id}_S\). To see this, let \(\nprec = g(\prec)\) and \(\sqsubseteq = f(\nprec)\). For \(x, y \in X\), we have \(x \sqsubseteq y\) if and only if \(x \nprec y\) and \(x \neq y\), which in turn occurs if and only if \(x \neq y\) and either \(x \prec y\) or \(x = y\). Since \(x \neq y\) precludes \(x = y\), this is equivalent to \(x \prec y\). Hence \(\prec\) and \(\sqsubseteq\) are equal relations, so \(f \circ g = \text{id}_S\).

So \(f\) and \(g\) are mutually inverse functions, and we have established the required bijection.

In light of Proposition 12.1.4, we will freely translate between partial orders and strict partial orders wherever necessary. When we do so, we will use \(\prec\) to denote the ‚Äòstrict‚Äô version, and \(\nprec\) to denote the ‚Äòweak‚Äô version. (Likewise for \(\sqsubseteq\).)

‚ú¶ **Definition 12.1.5**

Let \((X, \nprec)\) be a poset. A \(\nprec\)-least element of \(X\) (or a least element of \(X\) with respect to \(\nprec\)) is an element \(\bot \in X\) such that \(\bot \nprec x\) for all \(x \in X\). A \(\nprec\)-greatest element of \(X\) (or a greatest element of \(X\) with respect to \(\nprec\)) is an element \(\top \in X\) such that \(x \nprec \top\) for all \(x \in X\).
</markdown><markdown>
### Example 12.1.6

Some examples of least and greatest elements that we have already seen are:

- In \( (\mathbb{N}, \preceq) \), 0 is a least element; there is no greatest element.
- Let \( n \in \mathbb{N} \) with \( n > 0 \). Then 1 is a least element of \( ([n], \preceq) \), and \( n \) is a greatest element.
- \( (\mathbb{Z}, \preceq) \) has no greatest or least elements.

Proposition 12.1.7 says that least and greatest elements of posets are unique, if they exist. This allows us to talk about ‚Äòthe‚Äô least or ‚Äòthe‚Äô greatest element of a poset.

### Proposition 12.1.7

Let \( (X, \preceq) \) be a poset. If \( X \) has a least element, then it is unique; and if \( X \) has a greatest element, then it is unique.

**Proof**

Suppose \( X \) has a least element \( \ell \). We prove that if \( \ell' \) is another least element, then \( \ell' = \ell \).

So take another least element \( \ell' \). Since \( \ell \) is a least element, we have \( \ell \preceq \ell' \). Since \( \ell' \) is a least element, we have \( \ell' \preceq \ell \). By antisymmetry of \( \preceq \), it follows that \( \ell = \ell' \).

Hence least elements are unique. The proof for greatest elements is similar, and is left as an exercise.

### Exercise 12.1.8

Let \( X \) be a set. The poset \( (\mathcal{P}(X), \subseteq) \) has a least element and a greatest element; find both.

### Exercise 12.1.9

Prove that the least element of \( \mathbb{N} \) with respect to divisibility is 1, and the greatest element is 0.

### Definition 12.1.10 (Supremum)

Let \( (X, \preceq) \) be a poset and let \( A \subseteq X \). A \( \preceq \)-supremum of \( A \) is an element \( s \in X \) such that

- \( a \preceq s \) for each \( a \in A \); and
- If \( s' \in X \) with \( a \preceq s' \) for all \( a \in A \), then \( s \preceq s' \).

A \( \preceq \)-infimum of \( A \) is an element \( i \in X \) such that

- \( i \preceq a \) for each \( a \in A \); and
- If \( i' \in X \) with \( i' \preceq a \) for all \( a \in A \), then \( i' \preceq i \).
</markdown><markdown>
### Example 12.1.11

The well-ordering principle states that if \( U \subseteq \mathbb{N} \) is inhabited then \( U \) has a \(\leq\)-infimum, and moreover the infimum of \( U \) is an element of \( U \).

### Exercise 12.1.12

Let \( X \) be a set, and let \( U, V \in \mathcal{P}(X) \). Prove that the \(\subseteq\)-supremum of \(\{U, V\}\) is \( U \cup V \), and the \(\subseteq\)-infimum of \(\{U, V\}\) is \( U \cap V \).

### Exercise 12.1.13

Let \( a, b \in \mathbb{N} \). Show that \(\gcd(a, b)\) is an infimum of \(\{a, b\}\) and that \(\mathrm{lcm}(a, b)\) is a supremum of \(\{a, b\}\) with respect to divisibility.

### Example 12.1.14

Define \( U = [0, 1) = \{ x \in \mathbb{R} \mid 0 \leq x < 1 \} \). We prove that \( U \) has both an infimum and a supremum in the poset \((\mathbb{R}, \leq)\).

- **Infimum.** 0 is an infimum for \( U \). Indeed:

  (i) Let \( x \in U \). Then \( 0 \leq x \) by definition of \( U \).

  (ii) Let \( y \in \mathbb{R} \) and suppose that \( y \leq x \) for all \( x \in U \). Then \( y \leq 0 \), since \( 0 \in U \), so 0 is as required.

- **Supremum.** 1 is a supremum for \( U \). Indeed:

  (i) Let \( x \in U \). Then \( x < 1 \) by definition of \( U \), so certainly \( x \leq 1 \).

  (ii) Let \( y \in \mathbb{R} \) and suppose that \( x \leq y \) for all \( x \in U \). We prove that \( 1 \leq y \) by contradiction. So suppose it is not the case that \( 1 \leq y \). Then \( y < 1 \). Since \( x \leq y \) for all \( x \in U \), we have \( 0 \leq y \). But then

  \[
  0 \leq y = \frac{y + y}{2} < \frac{y + 1}{2} < \frac{1 + 1}{2} = 1
  \]

  But then \(\frac{y + 1}{2} \in U\) and \( y < \frac{y + 1}{2} \). This contradicts the assumption that \( x \leq y \) for all \( x \in U \). So it must in fact have been the case that \( 1 \leq y \), so 1 is as required.

The following proposition proves that suprema and infima are unique, provided they exist.

### Proposition 12.1.15

Let \((X, \preceq)\) is a poset, and let \( A \subseteq X \).

(i) If \( s, s' \in X \) are suprema of \( A \), then \( s = s' \);

(ii) If \( i, i' \in X \) are infima of \( A \), then \( i = i' \).
</markdown><markdown>
## Section 12.1. Orders and lattices

**Proof**  
Suppose \( s, s' \) are suprema of \( A \). Then:

- \( a \preceq s' \) for all \( a \in A \), so \( s' \preceq s \) since \( s \) is a supremum of \( A \);
- \( a \preceq s \) for all \( a \in A \), so \( s \preceq s' \) since \( s' \) is a supremum of \( A \).

Since \(\preceq\) is antisymmetric, it follows that \( s = s' \). This proves (i).

The proof of (ii) is almost identical and is left as an exercise to the reader.

### Notation 12.1.16
Let \( (X, \preceq) \) be a poset and let \( U \subseteq X \). Denote the \(\preceq\)-infimum of \( U \), if it exists, by \(\bigwedge U\) (LaTeX code: `\bigwedge`); and denote the \(\preceq\)-supremum of \( U \), if it exists, by \(\bigvee U\) (LaTeX code: `\bigvee`). Moreover, for \( x, y \in X \), write

\[
\bigwedge \{x, y\} = x \wedge y \quad (\text{LaTeX code: } \wedge), \quad \bigvee \{x, y\} = x \vee y \quad (\text{LaTeX code: } \vee)
\]

### Example 12.1.17
Some examples of Notation 12.1.16 are as follows.

- Let \( X \) be a set. In \( (\mathcal{P}(X), \subseteq) \) we have \( U \wedge V = U \cap V \) and \( U \vee V = U \cup V \) for all \( U, V \in \mathcal{P}(X) \).

- We have seen that, in \( (\mathbb{N}, |) \), we have \( a \wedge b = \gcd(a, b) \) and \( a \vee b = \mathrm{lcm}(a, b) \) for all \( a, b \in \mathbb{N} \).

- In \( (\mathbb{R}, \leq) \), we have \( a \wedge b = \min\{a, b\} \) and \( a \vee b = \max\{a, b\} \).

### Definition 12.1.18
A **lattice** is a poset \( (X, \preceq) \) such that every pair of elements of \( X \) has a \(\preceq\)-supremum and a \(\preceq\)-infimum.

### Example 12.1.19
We have seen that \( (\mathcal{P}(X), \subseteq) \), \( (\mathbb{R}, \leq) \) and \( (\mathbb{N}, |) \) are lattices.

### Proposition 12.1.20 (Associativity laws for lattices)
Let \( (X, \preceq) \) be a lattice, and let \( x, y, z \in X \). Then

\[
x \wedge (y \wedge z) = (x \wedge y) \wedge z \quad \text{and} \quad x \vee (y \vee z) = (x \vee y) \vee z
\]

**Proof**  
We prove \( x \wedge (y \wedge z) = (x \wedge y) \wedge z \); the other equation is dual and is left as an exercise.
</markdown><markdown>
We prove that the sets \(\{x, y \land z\}\) and \(\{x \land y, z\}\) have the same sets of lower bounds, and hence the same infima. So let

\[ 
L_1 = \{i \in X \mid i \preceq x \text{ and } i \preceq y \land z\} \quad \text{and} \quad L_2 = \{i \in X \mid i \preceq x \land y \text{ and } i \preceq z\} 
\]

We prove \(L_1 = L = L_2\), where

\[ 
L = \{i \in X \mid i \preceq x, \, i \preceq y \text{ and } i \preceq z\} 
\]

First we prove \(L_1 = L\). Indeed:

- \(L_1 \subseteq L\). To see this, suppose \(i \in L_1\). Then \(i \preceq x\) by definition of \(L_1\). Since \(i \preceq y \land z\), and \(y \land z \preceq y\) and \(y \land z \preceq z\), we have \(i \preceq y\) and \(i \preceq z\) by transitivity of \(\preceq\).

- \(L \subseteq L_1\). To see this, suppose \(i \in L\). Then \(i \preceq x\) by definition of \(L\). Moreover, \(i \preceq y\) and \(i \preceq z\) by definition of \(L\), so that \(i \preceq y \land z\) by definition of \(\land\). Hence \(i \in L\).

The proof that \(L_2 = L\) is similar. Hence \(L_1 = L_2\). But \(x \land (y \land z)\) is, by definition of \(\land\), the \(\preceq\)-greatest element of \(L_1\), which exists since \((X, \preceq)\) is a lattice. Likewise, \((x \land y) \land z\) is the \(\preceq\)-greatest element of \(L_2\).

Since \(L_1 = L_2\), it follows that \(x \land (y \land z) = (x \land y) \land z\), as required.

In the next exercise, you will prove some properties satisfied by suprema and infima in addition to associativity.

### Exercise 12.1.21 (Properties of suprema and infima)
Let \((X, \preceq)\) be a lattice. Prove that, for all \(x, y \in X\), we have:

(a) **(Idempotence)** \(x \land x = x\) and \(x \lor x = x\);

(b) **(Commutativity)** \(x \land y = y \land x\) and \(x \lor y = y \lor x\);

(c) **(Absorption)** \(x \lor (x \land y) = x\) and \(x \land (x \lor y) = x\).

### Example 12.1.22
It follows from what we‚Äôve proved that if \(a, b, c \in \mathbb{Z}\) then

\[
\gcd(a, \gcd(b, c)) = \gcd(\gcd(a, b), c)
\]

For example, take \(a = 882\), \(b = 588\) and \(c = 252\). Then

- \(\gcd(b, c) = 84\), so \(\gcd(a, \gcd(b, c)) = \gcd(882, 84) = 42\);

- \(\gcd(a, b) = 294\), so \(\gcd(\gcd(a, b), c) = \gcd(294, 252) = 42\).

These are indeed equal.
</markdown><markdown>
## Distributive lattices and Boolean algebras

One particularly important class of lattice is that of a **distributive lattice**, in which suprema and infima interact in a particularly convenient way. This makes algebraic manipulations of expressions involving suprema and infima particularly simple.

### Definition 12.1.23
A lattice \((X, \preceq)\) is **distributive** if

\[
x \land (y \lor z) = (x \land y) \lor (x \land z) \quad \text{and} \quad x \lor (y \land z) = (x \lor y) \land (x \lor z)
\]

for all \(x, y, z \in X\).

### Example 12.1.24
For any set \(X\), the power set lattice \((\mathcal{P}(X), \subseteq)\) is distributive. That is to say that for all \(U, V, W \subseteq X\) we have

\[
U \cap (V \cup W) = (U \cap V) \cup (U \cap W) \quad \text{and} \quad U \cup (V \cap W) = (U \cup V) \cap (U \cup W)
\]

This was the content of Example 2.2.16 and Exercise 2.2.17.

### Exercise 12.1.25
Prove that \((\mathbb{N}, |)\) is a distributive lattice.

### Definition 12.1.26
Let \((X, \preceq)\) be a lattice with a greatest element \(\top\) and a least element \(\bot\), and let \(x \in X\). A **complement** for \(x\) is an element \(y\) such that

\[
x \land y = \bot \quad \text{and} \quad x \lor y = \top
\]

### Example 12.1.27
Let \(X\) be a set. We show that every element \(U \in \mathcal{P}(X)\) has a complement.

### Exercise 12.1.28
Let \((X, \preceq)\) be a distributive lattice with a greatest element and a least element, and let \(x \in X\). Prove that, if a complement for \(x\) exists, then it is unique; that is, prove that if \(y, z \in X\) are complements for \(x\), then \(y = z\).

Exercise 12.1.28 justifies the following notation.

### Notation 12.1.29
Let \((X, \preceq)\) be a distributive lattice with greatest and least elements. If \(x \in X\) has a complement, denote it by \(\neg x\).
</markdown><markdown>
### Proposition 12.1.30

Let \((X, \preceq)\) be a distributive lattice and let \(x \in X\). If \(x\) has a complement \(\neg x\), then \(\neg x\) has a complement, and \(\neg(\neg x) = x\).

*Proof*

Assume that \(x\) has a complement \(\neg x\). Then by commutativity of \(\land\) and \(\lor\) and by definition of \(\neg\), we have

\[
(\neg x) \land x = x \land (\neg x) = \bot \quad \text{and} \quad (\neg x) \lor x = x \lor (\neg x) = \top
\]

so \(x\) satisfies the definition of what it means to be a complement of \(\neg x\). By uniqueness of complements ([Exercise 12.1.28](#)), we have \(x = \neg(\neg x)\).

### Definition 12.1.31

A lattice \((X, \preceq)\) is *complemented* if every element \(x \in X\) has a complement. A *Boolean algebra* is a complemented distributive lattice with a greatest element and a least element.

The many preceding examples and exercises concerning \((\mathcal{P}(X), \subseteq)\) piece together to provide a proof of the following theorem.

### Theorem 12.1.32

Let \(X\) be a set. Then \((\mathcal{P}(X), \subseteq)\) is a Boolean algebra.

Another extremely important example of a Boolean algebra is known as the *Lindenbaum‚ÄìTarski algebra*, which we define in [Definition 12.1.35](#). In order to define it, we need to prove that the definition will make sense. First of all, we fix some notation.

### Definition 12.1.33

Let \(P\) be a set, thought of as a set of propositional variables. Write \(L(P)\) to denote the set of propositional formulae with propositional variables in \(P\)‚Äîthat is, the elements of \(L(P)\) are strings built from the elements of \(P\), using the operations of conjunction (\(\land\)), disjunction (\(\lor\)) and negation (\(\neg\)).

### Lemma 12.1.34

Logical equivalence \(\equiv\) is an equivalence relation on \(L(P)\).

*Proof*

This is immediate from definition of equivalence relation, since for \(s, t \in L(P)\), \(s \equiv t\) is defined to mean that \(s\) and \(t\) have the same truth values for all assignments of truth values to their propositional variables.
</markdown><markdown>
In what follows, the set \( P \) of propositional variables is fixed; we may moreover take it to be countably infinite, since all strings in \( L(P) \) are finite.

### Definition 12.1.35
The **Lindenbaum‚ÄìTarski algebra** (for propositional logic) over \( P \) is the pair \( (A, \vdash) \), where \( A = L(P)/\equiv \) and \( \vdash \) is the relation on \( A \) defined by \([s]_{\equiv} \vdash [t]_{\equiv}\) if and only if \( s \Rightarrow t \) is a tautology.

In what follows, we will simply write \([\,]\) for \([\,]_{\equiv}\).

### Theorem 12.1.36
The Lindenbaum‚ÄìTarski algebra is a Boolean algebra.

**Proof Sketch**

There is lots to prove here! Indeed, we must prove:

- \( \vdash \) is a well-defined relation on \( A \); that is, if \( s \equiv s' \) and \( t \equiv t' \) then we must have \([s] \vdash [t]\) if and only if \([s'] \vdash [t']\).

- \( \vdash \) is a partial order on \( A \); that is, it is reflexive, antisymmetric and transitive.

- The poset \( (A, \vdash) \) is a lattice; that is, it has suprema and infima.

- The lattice \( (A, \vdash) \) is distributive, has a greatest element and a least element, and is complemented.

We will omit most of the details, which are left as an exercise; instead, we outline what the components involved are.

The fact that \( \vdash \) is a partial order can be proved as follows.

- **Reflexivity** of \( \vdash \) follows from the fact that \( s \Rightarrow s \) is a tautology for all propositional formulae \( s \).

- **Symmetry** of \( \vdash \) follows from the fact that, for all propositional formulae \( s, t \), if \( s \Leftrightarrow t \) is a tautology then \( s \) and \( t \) are logically equivalent.

- **Transitivity** of \( \vdash \) follows immediately from transitivity of \( \Rightarrow \).

The fact that \( (A, \vdash) \) is a lattice can be proved by verifying that:

- Given \([s], [t] \in A\), the infimum \([s] \land [t]\) is given by conjunction, namely \([s] \land [t] = [s \land t]\).

- Given \([s], [t] \in A\), the supremum \([s] \lor [t]\) is given by disjunction, namely \([s] \lor [t] = [s \lor t]\).
</markdown><markdown>
Finally, distributivity of suprema and infima in \( (A, \models) \) follows from the corresponding properties of conjunction and disjunction; \( (A, \models) \) has greatest element \([p \Rightarrow p]\) and least element \([\neg(p \Rightarrow p)]\), where \( p \) is some fixed propositional variable; and the complement of \([s] \in A\) is given by \([\neg s]\).

## De Morgan‚Äôs laws revisited

We finish this section on orders and lattices with a general version of de Morgan‚Äôs laws for Boolean algebras, which by Theorems [Theorems 12.1.32](#) and [12.1.36](#) implies the versions we proved for logical formulae ([Theorem 1.3.24](#)) and for sets ([Theorem 2.2.31(a)‚Äì(b)](#)).

### Theorem 12.1.37 (De Morgan‚Äôs laws)
Let \( (X, \preceq) \) be a Boolean algebra, and let \( x, y \in X \). Then

\[
\neg(x \land y) = (\neg x) \lor (\neg y) \quad \text{and} \quad \neg(x \lor y) = (\neg x) \land (\neg y)
\]

**Proof of (a)**  
We prove that \((\neg x) \lor (\neg y)\) satisfies the definition of a complement for \( x \land y \); then we‚Äôll have \((\neg x) \lor (\neg y) = \neg(x \land y)\) by [Exercise 12.1.28](#).

So let \( z = (\neg x) \lor (\neg y) \). Then

\[
(x \land y) \land z
\]
\[
= (x \land y) \land ((\neg x) \lor (\neg y)) \quad \text{by definition of } z
\]
\[
= [(x \land y) \land (\neg x)] \lor [(x \land y) \land (\neg y)] \quad \text{by distributivity}
\]
\[
= [((x \land (\neg x)) \land y] \lor [x \land (y \land (\neg y))] \quad \text{by associativity and commutativity}
\]
\[
= [\bot \land y] \lor [x \land \bot] \quad \text{by definition of complements}
\]
\[
= \bot \lor \bot \quad \text{by definition of } \bot \text{ and } \land
\]
\[
= \bot \quad \text{by idempotence}
\]

Likewise we have

\[
(x \land y) \lor z
\]
\[
= (x \land y) \lor ((\neg x) \lor (\neg y)) \quad \text{by definition of } z
\]
\[
= [x \lor ((\neg x) \lor (\neg y))] \land [y \lor ((\neg x) \lor (\neg y))] \quad \text{by distributivity}
\]
\[
= [(x \lor (\neg x)) \lor (\neg y)] \land [(\neg x) \lor (y \lor (\neg y))] \quad \text{by associativity and commutativity}
\]
\[
= [\top \lor (\neg y)] \land [(\neg x) \lor \top] \quad \text{by definition of complements}
\]
\[
= \top \land \top \quad \text{by definition of } \top \text{ and } \lor
\]
\[
= \top \quad \text{by idempotence}
\]
</markdown><markdown>
Since \((x \land y) \land z = \bot\) and \((x \land y) \lor z = \top\), we have

\[
(\neg x) \lor (\neg y) = z = \neg (x \land y)
\]

by definition of complements.

üìå **Exercise 12.1.38**  
Prove part (b) of [Theorem 12.1.37](#).
</markdown><markdown>
## Section 12.2

# Inductively defined sets

In [Section 4.1](#), we formalised the idea that the set of natural numbers should be what is obtained by starting with zero and repeating the successor (‚Äòplus one‚Äô) operation‚Äîthis was done using Peano‚Äôs axioms ([Definition 4.1.1](#)). From these axioms we were able to derive the weak and strong induction principles, which turned out to be extremely powerful for proving results about natural numbers.

We now generalise this idea to other so-called *inductively defined sets*. The definition ([Definition 12.2.9](#)) is a little tricky to digest, but the idea is relatively simple: an inductively defined set \( X \) is one whose elements are built out of some specified *basic elements* (such as 0) by iterating some specified operations, called *constructors* (such as the successor operation)‚Äîevery element of \( X \) should either be a basic element, or should be built in a unique way out of simpler elements of \( X \) by using a constructor.

Each inductively defined set \( X \) will have its own induction principle: which says that if a property is true of all of the basic elements of \( X \), and if all of its constructors preserve the truth of the property, then the property is true of all of the elements of \( X \). We will prove this in [Theorem 12.2.27](#).

Before jumping into the definition of an inductively defined set, it is helpful to see some examples. The first example is familiar.

‚úê **Example 12.2.1**  
The set \( \mathbb{N} \) of natural numbers is *the* canonical example of an inductively defined set. The Peano axioms ([Definition 4.1.1](#)) tell us that every element of \( \mathbb{N} \) can be obtained by starting from 0 and applying the successor operation (‚Äòplus one‚Äô) some finite number of times. In particular, every natural number is either 0, or is the successor of a unique natural number.

We can think of \( \mathbb{N} \) as being the set *generated by* the following rules:

1. \( 0 \in \mathbb{N} \); and
2. If \( n \in \mathbb{N} \), then \( n + 1 \in \mathbb{N} \).

Certainly these rules are *true*, but that is not enough to fully characterise the set of natural numbers: both rules are also true with \( \mathbb{N} \) replaced by \( \mathbb{Z} \) or \( \mathbb{R} \), for example.

But what it means to say that \( \mathbb{N} \) is ‚Äògenerated by‚Äô these rules is that:

- Each time one of the rules is applied, we obtain a *new* natural number. In particular:
</markdown><markdown>
Since \(0 \in \mathbb{N}\) and rule (ii) must give us a new natural number every time we apply it, we must have \(n + 1 \neq 0\) for all \(n \in \mathbb{N}\). This is exactly what Definition 4.1.1(i) says; note that this fails with \(\mathbb{N}\) replaced by \(\mathbb{Z}\) or \(\mathbb{R}\) since we have \((-1) + 1 = 0\).

If we apply rule (ii) to two different natural numbers, we must obtain two different results. Thus for all \(m, n \in \mathbb{N}\), if \(m + 1 = n + 1\), then \(m = n\). This is exactly what Definition 4.1.1(ii) says.

Only those things that can be obtained by applying rules (i) and (ii) some finite number of times are natural numbers. This is exactly what Definition 4.1.1(iii) says.

Thus we can think of the natural numbers as being precisely those things that are given to us by applying rules (i) and (ii).

The next example concerns words over an alphabet, which we studied in Section 6.2 in the context of determining when a set is countable‚Äîsee Definition 6.2.26.

### Example 12.2.2
Let \(\Sigma\) be a fixed alphabet. The set \(\Sigma^*\) of words over \(\Sigma\) is built by starting from the empty word \(\varepsilon\) by appending elements \(a \in \Sigma\). Thus \(\Sigma^*\) is generated by the following rules:

1. \(\varepsilon \in \Sigma^*\); and

2. For all \(a \in \Sigma\), if \(w \in \Sigma^*\), then \(wa \in \Sigma^*\).

The fact that \(\Sigma^*\) is generated by these rules means that:

- Each time we append an element \(a \in \Sigma\) to the end of a word \(w \in \Sigma^*\), the resulting word \(wa\) is a new element of \(\Sigma^*\); and
- Only those things obtained by appending elements \(a \in \Sigma\) to the end of the empty word \(\varepsilon\) some finite number of times are elements of \(\Sigma^*\).

We can actually think of rule (ii) as being a family of rules, one rule for each \(a \in \Sigma\). For fixed \(a\), the rule says ‚Äòfor all \(w\), if \(w \in \Sigma^*\), then \(wa \in \Sigma^*\)‚Äô. This viewpoint is useful because it allows us to restrict our attention to rules that only have variable elements of the set being inductively defined in the hypothesis.

We alluded to the inductive character of propositional formulae in Section 1.1. Although we were not able to make the idea precise at the time, we are now well on our way to doing so.

### Example 12.2.3
Given a set \(P\), consider the set \(L(P)\) of all propositional formulae with propositional variables from \(P\) and logical operators from the set \(\{\land, \lor, \Rightarrow, \lnot\}\). Then \(L(P)\) is built out of the elements of \(P\) using these logical operators.
</markdown><markdown>
Specifically, \( L(P) \) is generated by the following rules:

(i) For each \( p \in P \), we have \( p \in L(P) \);

(ii) If \( \varphi, \psi \in L(P) \), then \( \varphi \land \psi \in L(P) \);

(iii) If \( \varphi, \psi \in L(P) \), then \( \varphi \lor \psi \in L(P) \);

(iv) If \( \varphi, \psi \in L(P) \), then \( \varphi \Rightarrow \psi \in L(P) \);

(v) If \( \varphi \in L(P) \), then \( \neg \varphi \in L(P) \).

To say that \( L(P) \) is generated by these rules means that every propositional formula is (exclusively) either: a propositional variable, the conjunction of two simpler formulae, the disjunction of two simpler formulae, the implication formed by two simpler formulae, or the negation of a simpler formula.

We can interpret (i) as being a family of rules, one for each \( p \in P \), where for fixed \( p \), the rule simply says ‚Äò\( p \in L(P) \)‚Äô. Just like we said in Example 12.2.2, this is useful because it removes variable elements of sets other than \( L(P) \) from the hypotheses of the rule.

Keeping these examples in mind, we will now work towards defining the notion of an inductively defined set. First we formalise the notion of a rule.

‚ú¶ **Definition 12.2.4**

A (finitary) rule for an inductive definition is an expression of the form

\[
(x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r))
\]

where \( r \in \mathbb{N} \), \( x_1, x_2, \ldots, x_r \) are variables, and \( \sigma(x_1, x_2, \ldots, x_r) \) is some expression involving the variables \( x_1, x_2, \ldots, x_r \).

The number of variables \( r \in \mathbb{N} \) is called the *arity* of the rule, and is denoted by \( \text{ar}(\sigma) \).

A quick note on terminology: a constructor of arity \( r \in \mathbb{N} \) is called an *r*-ary constructor. For \( r = 0, 1, 2 \), we may say *nullary*, *unary* and *binary*, respectively.

We interpret the rule \( (x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r)) \) as meaning ‚Äòif \( x_1, x_2, \ldots, x_r \) are elements of the set being defined, then \( \sigma(x_1, x_2, \ldots, x_r) \) is an element of the set being defined‚Äô.

Note that nullary rules take the form \( ( \mid a) \), where \( a \) is some expression with no variables; we would interpret such a rule as saying ‚Äò\( a \) is an element of the set being defined‚Äô, with no hypotheses.
</markdown><markdown>
### Example 12.2.5

The rules describing the natural numbers are \( ( \mid 0) \) and \( (n \mid n + 1) \). The rule \( ( \mid 0) \) can be interpreted to mean ‚Äò0 is a natural number‚Äô, and the rule \( (n \mid n + 1) \) can be interpreted to mean ‚Äòif \( n \) is a natural number, then \( n + 1 \) is a natural number‚Äô. In the context of Example 12.2.1, this means that \( ( \mid 0) \) corresponds with rule (i) and \( (n \mid n + 1) \) corresponds with rule (ii).

### Example 12.2.6

Let \( P \) be a set of propositional variables. The rules that describe the set \( L(P) \) of logical formulae over \( P \), as described in Example 12.2.3, are given by

\[
\begin{align*}
( \mid p) & \quad \text{one for each } p \in P \\
(\varphi, \psi \mid \varphi \land \psi) & \\
(\varphi, \psi \mid \varphi \lor \psi) & \\
(\varphi, \psi \mid \varphi \Rightarrow \psi) & \\
(\varphi \mid \lnot \varphi) &
\end{align*}
\]

The first of these rules says that every propositional variable \( p \in P \) is a propositional formula. The next three say that if \( \varphi \) and \( \psi \) are propositional formulae, then so are \( \varphi \land \psi \), \( \varphi \lor \psi \) and \( \varphi \Rightarrow \psi \). The last rule says that if \( \varphi \) is a propositional formula, then so is \( \lnot \varphi \).

### Exercise 12.2.7

Fix an alphabet \( \Sigma \). Following Example 12.2.2, define rules that describe the set \( \Sigma^* \) of words over \( \Sigma \). How would your rules need to be changed if the empty word \( \varepsilon \) were not allowed?

We can represent a rule \( \sigma = (x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r)) \) diagrammatically by drawing a node with \( r \) inputs, one for each variable in the rule, and one output, representing the expression \( \sigma(x_1, x_2, \ldots, x_r) \). (Note that a nullary rule has no inputs.) For example:

Thus the rules \( 0 = ( \mid 0) \) and \( s = (n \mid n + 1) \) that describe the natural numbers can be expressed as follows:
</markdown><markdown>
These diagrams can be pieced together to represent the result of applying a rule to the outputs of other rules. For example:

\[
\begin{align*}
x_1 & \quad x_2 & \quad x_3 \\
& \sigma & \\
& \sigma(x_1, x_2, x_3) & \\
& \alpha & \\
& \alpha(\sigma(x_1, x_2, x_3), \tau(y_1, y_2)) & \\
y_1 & \quad y_2 \\
& \tau & \\
& \tau(y_1, y_2) & \\
\end{align*}
\]

This can be useful for seeing how an element of a set is obtained by applying the rules. For example, the following diagram shows how the natural number 3 is obtained by applying the rules \( (| 0) \) and \( (n | n + 1) \)

\[
0 \rightarrow 0 \rightarrow s \rightarrow 1 \rightarrow s \rightarrow 2 \rightarrow s \rightarrow 3
\]

The following diagram shows how the logical formula \( (p \Rightarrow q) \land (\neg r) \) is obtained by applying the rules described in Example 12.2.6.
</markdown><markdown>
### Exercise 12.2.8

Let \( \Sigma = \{a, b, c, d\} \). Draw a diagram to represent how the word \( cbadbd \in \Sigma^* \) can be obtained by applying the rules you defined in Exercise 12.2.7.

We are now ready to define an inductively defined set.

### Definition 12.2.9

An **inductively defined set** is a set \( A \) equipped with a set \( R \) of rules and, for each rule \( \sigma = (x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r)) \in R \), a function \( f_\sigma : A^r \rightarrow A \), such that:

(a) For each \( a \in A \), there is a unique rule \( \sigma \in R \) and unique elements \( a_1, a_2, \ldots, a_r \in A \) such that \( a = f_\sigma(a_1, a_2, \ldots, a_r) \); and

(b) For every set \( X \), if \( f_\sigma(a_1, a_2, \ldots, a_r) \in X \) for all \( \sigma \in R \) and all \( a_1, a_2, \ldots, a_r \in A \), then \( A \subseteq X \).

Given a rule \( \sigma \) of arity \( r \), the function \( f_\sigma : A^r \rightarrow A \) is called the **constructor** associated with \( \sigma \); the natural number \( r \) is called the **arity** of the constructor.

Note that nullary constructors are the same thing as elements of \( A \). Indeed, \( A^0 = \{()\} \), where \( () \) is the empty list of elements of \( A \), and so specifying a function \( f_\sigma : A^0 \rightarrow A \) is equivalent to specifying an element \( f_\sigma(()) \in A \). In this sense, we may regard nullary constructors as being the elements of \( A \)‚Äîwe call such elements **basic elements**.

### Definition 12.2.10

A **basic element** of an inductively defined set \( A \) is an element of \( A \) that is the value of a nullary constructor \( f_\sigma : A^0 \rightarrow A \). If \( \sigma = ( \mid a) \) is a nullary rule, we will denote this element by \( a \)‚Äîthus we have \( a = f_\sigma(()) \in A \) for all nullary rules \( \sigma = ( \mid a) \).
</markdown><markdown>
Considering basic elements separately from constructors of positive arity has its pros and cons, so we will take whichever approach is most convenient for us at any given point in time. Unless otherwise specified, we will not separate nullary constructors from the others.

We have already seen some examples of inductively defined sets‚Äîlet‚Äôs prove that they truly are inductively defined sets!

### Proposition 12.2.11
The set \(\mathbb{N}\) of natural numbers is inductively defined by the rules \((\,|\,0)\) and \((n\,|\,n+1)\).

**Proof**  
Since \((\,|\,0)\) is a nullary rule, it will correspond to a basic element of \(\mathbb{N}\)‚Äîit may be no surprise that we take this element to be the natural number 0.

The rule \(s = (n\,|\,n+1)\) induces a function \(f_s : \mathbb{N} \rightarrow \mathbb{N}\); we take this to be the successor function, defined by \(f_s(n) = n + 1\) for all \(n \in \mathbb{N}\).

Now we must verify conditions (a) and (b) of Definition 12.2.9.

(a) Let \(n \in \mathbb{N}\).

- If \(n = 0\), then since 0 is a basic element and \(0 \neq n + 1 = f_s(n)\) for any \(n \in \mathbb{N}\), we have that ‚Äò0‚Äô is the unique expression of 0 as a (nullary) constructor applied to (no) elements of \(\mathbb{N}\).

- If \(n > 0\), then \(n - 1 \in \mathbb{N}\) and \(n = (n - 1) + 1 = f_s(n - 1)\). Moreover if \(m \in \mathbb{N}\) and \(n = f_s(m)\), then \(n = m + 1\), so that \(m = n - 1\). Moreover \(n \neq 0\), meaning that there is a unique rule (namely \(s\)) and a unique natural number \(m\) (namely \(m = n - 1\)) such that \(n = f_s(m)\).

(b) Let \(X\) be a set, and assume that \(0 \in X\) and \(f_s(n) \in X\) for all \(n \in \mathbb{N}\). Then condition (iii) of Definition 4.1.1 ensures that \(\mathbb{N} \subseteq X\).

Thus \(\mathbb{N}\) is inductively defined by \((\,|\,0)\) and \((n\,|\,n+1)\), as required. \(\square\)

### Example 12.2.12
Let \(P\) be a set of propositional variables. In order to exhibit \(L(P)\) as an inductively defined set, we should be slightly more precise about the role of brackets in propositional formulae than we have been so far.

For example, if we take the rules discussed in Example 12.2.3 literally, then \(p \land q \lor r\) would be a valid logical formula. This is problematic for two reasons: first, does it mean \((p \land q) \lor r\), or \(p \land (q \lor r)\)? These are not logically equivalent, so the distinction matters. Second, this causes the ‚Äòuniqueness‚Äô part of Definition 12.2.9 to fail, since \(p \land q \lor r = f_\land (p, f_\lor (q, r)) = f_\lor (f_\land (p, q), r)\).

To remedy this, we will require parentheses to be added whenever we introduce a logical
</markdown><markdown>
Thus the rules defining a logical formula are:

\[
\begin{align*}
( | p ) & \quad \text{one for each } p \in P \\
(\varphi, \psi \mid (\varphi \land \psi)) & \\
(\varphi, \psi \mid (\varphi \lor \psi)) & \\
(\varphi, \psi \mid (\varphi \Rightarrow \psi)) & \\
(\varphi \mid (\neg \varphi)) &
\end{align*}
\]

Thus \( p \land q \lor r \) is not a valid element of \( L(P) \), but \((p \land q) \lor r\) and \((p \land (q \lor r))\) are.

**Exercise 12.2.13**  
Let \(\Sigma\) be an alphabet. Prove that \(\Sigma^*\) is inductively defined by the rules \(( | \varepsilon )\) and \(\sigma_a = (w \mid wa)\) for \(a \in \Sigma\).

**Exercise 12.2.14**  
Prove that \(\mathbb{N}\) is inductively defined by the rules \(( | 0 )\), \(( | 1 )\) and \((n \mid n + 2)\).

### Defining new inductively defined sets

Now that we have seen what it means for an already defined set to be inductively defined, it would be useful to be able to make definitions of inductively defined sets. Based on what we have seen so far, in order to define an inductively defined set, all we should need to do is specify a set of rules that tell us how to generate its elements.

We now prove that it really is that simple!

‚ú¶ **Definition 12.2.15**  
Let \( R \) be a set of rules. The set generated by \( R \) is defined by \( A = \bigcup_{n \in \mathbb{N}} A_n \), where the sets \( A_n \) for \( n \in \mathbb{N} \) are defined recursively by:

- \( A_0 = \{ a \mid ( | a ) \in R \} \); and
- \( A_{n+1} = \{ \sigma(a_1, a_2, \ldots, a_r) \mid a_1, a_2, \ldots, a_r \in A_n, (x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r)) \in R \} \).

That is, \( A_0 \) is the set of symbols to the right of the bar ‚Äò|‚Äô in the nullary rules in \( R \), and \( A_{n+1} \) is the set of all expressions obtained by substituting the elements of \( A_n \) symbol-by-symbol for the variables in the expressions to the right of the bar the other rules.

‚ú¶ **Example 12.2.16**  
Let \( N \) be the set generated by the rules \(( | z )\) and \((n \mid s(n))\). Then

- \( N_0 = \{ z \} \), since \(( | z )\) is the only nullary rule.
- \( N_1 \) is the result of applying the rules to the elements of \( N_0 \), of which there is only one, namely \( z \). Applying the rule \(( | z )\) (to no elements, since it is nullary) gives \( z \); applying the rule \((n \mid s(n))\) to \( z \) gives \( s(z) \). So \( N_1 = \{ z, s(z) \} \).
</markdown><markdown>
- Continuing, we get \( N_2 = \{ z, s(z), s(s(z)) \} \), and so on.

We thus have

\[
N = \bigcup_{n \in \mathbb{N}} N_n = \{ z, s(z), s(s(z)), s(s(s(z))), \ldots \}
\]

This looks an awful lot like the set of all natural numbers; and indeed it is, provided we interpret \( z = 0 \) and \( s(n) = n + 1 \) (like we did in Section 4.1).

### Example 12.2.17
Let \( A \) be the set generated by the rules \( ( \mid \star ) \) and \( (x, y \mid [x, y]) \). Then

- \( A_0 = \{ \star \} \);
- \( A_1 = \{ \star, [\star, \star] \} \);
- \( A_2 = \{ \star, [\star, \star], [\star, [\star, \star]], [[\star, \star], \star], [[\star, \star], [\star, \star]] \} \);
- \ldots and so on.

Thus for each \( n \in \mathbb{N} \), the set \( A_n \) consists of all parenthesised lists of \(\star\)'s, where the list has length between 1 and \( 2^n \) (inclusive). This can be proved by induction on \( n \).

Hence \( A = \bigcup_{n \in \mathbb{N}} A_n \) is the set of all (finite) such lists.

### Exercise 12.2.18
Let \( R \) be a set of rules for an inductive definition and let \( A \) be the set generated by \( R \). Prove that if \( R \) has no nullary rules, then \( A \) is empty.

### Exercise 12.2.19
Let \( R \) be a set of rules for an inductive definition, and let \( A \) be the set generated by \( R \). Prove that if \( R \) is countable, then \( A \) is countable.

### Exercise 12.2.20
Let \( R \) be a set of rules. Prove that the set \( A \) generated by \( R \) is inductively defined; for each rule \( \sigma = (x_1, x_2, \ldots, x_r \mid \sigma(x_1, x_2, \ldots, x_r)) \), the constructor \( f_\sigma : A' \rightarrow A \) is defined by

\[
f_\sigma(a_1, a_2, \ldots, a_r) = \sigma(a_1, a_2, \ldots, a_r)
\]

where \( \sigma(a_1, a_1, \ldots, a_r) \) denotes the result of substituting \( a_i \) for the variable \( x_i \) for each \( i \in [r] \).

## Structural recursion

The recursion theorem for the natural numbers (Theorem 4.1.2) says that we can define a function \( h : \mathbb{N} \rightarrow X \) by specifying the value of \( h(0) \), and for each \( n \in \mathbb{N} \), specifying
</markdown><markdown>
the value of \( h(n + 1) \) in terms of the value of \( h(n) \). This makes intuitive sense: since every natural number is obtained from 0 by adding one some finite number of times, if we know the value of \( h(0) \), and we know how the value of \( h \) changes when we add one to its argument, then we should know the value of \( h(n) \) for all natural numbers \( n \).

It turns out that there is nothing special about \( \mathbb{N} \) here: exactly the same argument demonstrates that for any inductively defined set \( A \), if we know what a function \( h : A \to X \) does to its basic elements, and we know how its values change when we apply a constructor to its arguments, then we should know the value of \( h(a) \) for all \( a \in A \).

The proof of the structural recursion theorem is one of the times where treating basic elements (\(\equiv\) nullary constructors) separately from constructors of positive arity makes the proof more difficult; so we will phrase it entirely in terms of constructors.

### Theorem 12.2.21 (Structural recursion theorem)
Let \( A \) be an inductively defined set, let \( X \) be a set, and for each rule \( \sigma \), let \( h_\sigma : A^r \to X \) be a function, where \( r = \text{ar}(\sigma) \). Then there is a unique function \( h : A \to X \) such that

\[
h(f_\sigma(a_1, a_2, \ldots, a_r)) = h_\sigma(a_1, a_2, \ldots, a_r)
\]

for all rules \( \sigma \) and all \( a_1, a_2, \ldots, a_r \in A \).

**Proof**  
Given \( a \in A \), we know by Definition 12.2.9(a) that there is a unique rule \( \sigma \) and unique \( a_1, a_2, \ldots, a_r \in A \) such that \( a = f_\sigma(a_1, a_2, \ldots, a_r) \). Thus the specification

\[
h(a) = h_\sigma(a_1, a_2, \ldots, a_r)
\]

uniquely determines a function \( h : A \to X \), satisfying the required condition. \(\square\)

### Strategy 12.2.22 (Defining functions by structural recursion)
Let \( A \) be an inductively defined set and let \( X \) be a set. In order to specify a function \( h : A \to X \), it suffices to define \( h(a) \) for all basic elements \( a \in A \), and to define \( h(f_\sigma(a_1, a_2, \ldots, a_r)) \) in terms of the values of \( h(a_i) \) for each \( i \in [r] \).

### Example 12.2.23
Let \( L(P) \) be the inductively defined set of propositional formulae over a set \( P \) of propositional variables. Define \( h : L(P) \to \mathbb{N} \) recursively as follows:

(i) Let \( h(p) = 0 \) for all \( p \in P \);

(ii) For all \( \varphi, \psi \in L(P) \), let \( h(\varphi \land \psi) = h(\varphi) + h(\psi) + 1 \);

(iii) For all \( \varphi, \psi \in L(P) \), let \( h(\varphi \lor \psi) = h(\varphi) + h(\psi) + 1 \);
</markdown><markdown>
(iv) For all \( \varphi, \psi \in L(P) \), let \( h(\varphi \Rightarrow \psi) = h(\varphi) + h(\psi) + 1 \);

(v) For all \( \varphi \in L(P) \), let \( h(\neg \varphi) = h(\varphi) + 1 \).

By the structural recursion theorem, this completely determines the function \( h \).

For example

\[
h((p \Rightarrow q) \land (\neg r)) 
= h(p \Rightarrow q) + h(\neg r) + 1 \quad \text{by (ii)} \\
= [h(p) + h(q) + 1] + [h(r) + 1] + 1 \quad \text{by (iv) and (v)} \\
= [0 + 0 + 1] + [0 + 1] + 1 \quad \text{by (i)} \\
= 3
\]

More generally, for each \( \varphi \in L(P) \), the value \( h(\varphi) \) is the number of logical operators that appear in \( \varphi \). We can prove this by structural induction‚Äîmore on this soon.

### Definition 12.2.24

Let \( A \) be an inductively defined set. The rank \( \mathrm{rk}(a) \) of an element \( a \in A \) is defined by recursively by letting \( \mathrm{rk}(a) = 0 \) for all basic elements \( a \), and

\[
\mathrm{rk}(\sigma(a_1, a_2, \ldots, a_r)) = \max\{\mathrm{rk}(a_1), \mathrm{rk}(a_2), \ldots, \mathrm{rk}(a_r)\} + 1
\]

for all rules \( \sigma \) of arity \( r > 0 \) and all elements \( a_1, a_2, \ldots, a_r \in A \).

Intuitively, the rank of an element \( a \in A \) tells us how many constructors (of positive arity) appear in the unique expression of \( a \) as constructors applied to basic elements.

### Example 12.2.25

The rank of a natural number \( n \) is \( n \). Indeed we have \( \mathrm{rk}(0) = 0 \) since 0 is a basic element, and for all \( n \in \mathbb{N} \) we have \( \mathrm{rk}(n + 1) = \max\{\mathrm{rk}(n)\} + 1 = \mathrm{rk}(n) + 1 \). By the recursion theorem (take your pick from Theorem 4.1.2 or Theorem 12.2.21), we have \( \mathrm{rk}(n) = n \) for all \( n \in \mathbb{N} \).

### Exercise 12.2.26

Let \( \Sigma \) be an alphabet and let \( \Sigma^* \) be the inductively defined set of words over \( \Sigma \). Prove that for all \( w \in \Sigma \), the rank \( \mathrm{rk}(w) \) is equal to the length of \( W \). [We regard the empty string \( \varepsilon \) to have length 0, despite the fact that the placeholder character \( \varepsilon \) is used to denote it.]
</markdown><markdown>
## Structural induction

We now derive the structural induction principle, which is used for proving that a property \( p(x) \) is true for all elements \( x \) of an inductively defined set \( A \).

The idea behind structural induction is the same as the idea behind weak induction: since every element \( a \in A \) is of the form \( f_\sigma(a_1, a_2, \ldots, a_r) \) for some (possibly nullary) rule \( \sigma \), if we can prove that the truth of \( p \) is preserved by applying each constructor \( f_\sigma \), then \( p(x) \) must be true for all \( x \in A \). [In the nullary case, this amounts to checking that \( p(a) \) is true for each basic element \( a \in A \).]

Thus in order to prove \( \forall x \in A, \, p(x) \), we need to prove that for every rule \( \sigma \ldots \)

\[
\begin{array}{c}
a_1 \quad a_2 \quad \ldots \quad a_r \\
\downarrow \quad \downarrow \quad \ldots \quad \downarrow \\
f_\sigma(a_1, a_2, \ldots, a_r)
\end{array}
\]

\[
\text{if } p(x) \text{ is true for these elements, then } p(x) \text{ is true for this element.}
\]

Let's prove that this intuition is valid.

### Theorem 12.2.27 (Structural induction principle)

Let \( A \) be an inductively defined set and let \( p(x) \) be a logical formula with free variable \( x \in A \). If for all rules \( \sigma \) and all \( a_1, \ldots, a_r \in A \) we have

\[
[\forall i \in [r], \, p(a_i)] \implies p(f_\sigma(a_1, a_2, \ldots, a_r))
\]

then \( p(x) \) is true for all \( x \in A \).

**Proof**

Assume that for all rules \( \sigma \) and all \( a_1, \ldots, a_r \in A \) we have

\[
[\forall i \in [r], \, p(a_i)] \implies p(f_\sigma(a_1, a_2, \ldots, a_r))
\]

Let \( X = \{ a \in A \mid p(a) \} \subseteq A \). Then for all \( a \in A \) we have \( a \in X \) if and only if \( p(a) \) is true. Thus we have

\[
[\forall i \in [r], \, a_i \in X] \implies f_\sigma(a_1, a_2, \ldots, a_r) \in X
\]

But this is exactly the hypothesis of condition (b) of Definition 12.2.9, and so \( A \subseteq X \).

Hence \( A = X \), so that \( p(a) \) is true for all \( a \in A \), as required. \(\square\)
</markdown><markdown>
Let's digest the statement of Theorem 12.2.27.

For a nullary rule \( \sigma = ( \mid a) \), the arity \( r \) is 0, so \(\forall i \in [r], \, p(a_i)\) is vacuously true (see Exercise 2.1.32), so that

\[
[\forall i \in [r], \, p(a_i)] \Rightarrow p(f_\sigma(a_1, a_2, \ldots, a_r)) \equiv p(a)
\]

Therefore, saying that \([\forall i \in [r], \, p(a_i)] \Rightarrow p(f_\sigma(a_1, a_2, \ldots, a_r))\) is true for all rules \( \sigma \) and all elements \( a_1, a_2, \ldots, a_r \in A \) is equivalent to saying:

- \( p(a) \) is true for all basic elements \( a \); and
- \([\forall i \in [r], \, p(a_i)] \Rightarrow p(f_\sigma(a_1, a_2, \ldots, a_r))\) is true for all rules \( \sigma \) of positive arity \( r \), and all elements \( a_1, a_2, \ldots, a_r \).

This suggests the following proof strategy.

### Strategy 12.2.28 (Proof by structural induction)
In order to prove a proposition of the form \(\forall a \in A, \, p(a)\), where \( A \) is an inductively defined set, it suffices to prove for all rules \( \sigma \) that

\[
[\forall i \in [r], \, p(a_i)] \Rightarrow p(f_\sigma(a_1, a_2, \ldots, a_r))
\]

Equivalently, it suffices to:

- For each basic element \( a \in A \), prove \( p(a) \)‚Äîthis is the **base case** for \( a \);
- For each constructor \( \sigma \) of arity \( r > 0 \), let \( a_1, a_2, \ldots, a_r \in A \) and assume that \( p(a_i) \) is true for all \( i \in [r] \), and prove that \( p(f_\sigma(a_1, a_2, \ldots, a_r)) \) is true‚Äîthis is the **induction step** for \( \sigma \).

The assumption \(\forall i \in [r], \, p(a_i)\) in the induction step for \( \sigma \) is called the **induction hypothesis** for \( \sigma \).

‚úèÔ∏è **Example 12.2.29**
The structural induction principle for the inductively defined set \( \mathbb{N} \) is exactly the same as the weak induction principle (Theorem 4.2.1). It says that to prove \(\forall n \in \mathbb{N}, \, p(n)\) is true, it suffices to:

- Prove \( p(0) \) is true (since 0 is the only basic element); and
- Let \( n \in \mathbb{N} \), assume that \( p(n) \) is true, and prove that \( p(n+1) \) is true (since the successor operation is the only constructor of positive arity).

Thus we recover weak induction as a special case of structural induction.
</markdown><markdown>
### Example 12.2.30

Fix a set \( P \) of propositional variables and let \( L(P) \) be the inductively defined set of (properly parenthesised) propositional formulae over \( P \), as in Example 12.2.12.

We prove that every propositional formula \( \varphi \in L(P) \) has the same number of open brackets ‚Äò(‚Äô as closed brackets ‚Äò)‚Äô.

- **(Base cases)** The basic elements of \( L(P) \) are the propositional variables \( p \in P \). So let \( p \in P \); this is a logical formula with no open brackets and no closed brackets, so the number of open brackets is equal to the number of closed brackets.

- **(Induction step for \(\land\))** Let \( \varphi, \psi \in L(P) \) and assume that \( \varphi \) and \( \psi \) each have the same number of open brackets as closed brackets. Say \( \varphi \) has \( a \) open brackets and \( a \) closed brackets, and \( \psi \) has \( b \) open brackets and \( b \) closed brackets, where \( a, b \in \mathbb{N} \). Now

  \[
  f_\land(\varphi, \psi) = (\varphi \land \psi)
  \]

  Since \( \varphi \) contributes \( a \) open brackets and \( \psi \) contributes \( b \) open brackets, this has \( a + b + 1 \) open brackets; likewise it has \( a + b + 1 \) closed brackets.

  This completes the induction step for \(\land\).

- **(Induction steps for \(\lor\) and \(\Rightarrow\))** These are identical to the induction step for \(\land\)‚Äîjust replace \(\land\) by the necessary logical operator throughout.

- **(Induction step for \(\lnot\))** Let \( \varphi \in L(P) \) and assume that \( \varphi \) has the same number of open brackets as closed brackets‚Äîsay it has \( a \) of each, where \( a \in \mathbb{N} \). Then

  \[
  f_\lnot(\varphi) = (\lnot \varphi)
  \]

  Since \( \varphi \) contributes \( a \) open brackets, this has \( a + 1 \) open brackets; likewise it has \( a + 1 \) closed brackets.

  This completes the induction step for \(\lnot\).

So by structural induction, it follows that every propositional formula \( \varphi \in L(P) \) has the same number of open brackets as closed brackets.

### Exercise 12.2.31

Let \( P \) be a set of propositional variables. Prove by structural induction that the rank of a propositional formula \( \varphi \in L(P) \) is equal to the number of logical operators in \( \varphi \).

A nice application of structural induction is to provide a formula for the totient of an integer \( n \) (Definition 7.3.26). We proved such a formula in Section 7.3, but that proof relied on the heavy machinery of the Chinese remainder theorem (Theorem 7.3.46)‚Äîhere we give a proof without it.
</markdown><markdown>
### Theorem 7.3.59 (Formula for Euler's totient function)

Let \( n \) be a nonzero integer. Then

\[
\varphi(n) = |n| \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right)
\]

where the product is indexed over positive primes \( p \) dividing \( n \).

**Proof**

Recall that \(\varphi(-n) = \varphi(n)\) for all \( n \in \mathbb{Z} \), so we may assume without loss of generality that \( n \geq 0 \)‚Äîotherwise just replace \( n \) by \(-n\) throughout. Moreover

\[
\varphi(0) = 0 = 0 \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right)
\]

so for the rest of the proof, assume that \( n > 0 \).

Let \( P \) be the set of all positive prime numbers, and let

\[
A = \bigcup_{n \in \mathbb{N}} P^n = \{(p_1, p_2, \ldots, p_k) \mid k \in \mathbb{N}, \, p_1, p_2, \ldots, p_k \in P\}
\]

The elements of \( A \) are precisely lists of positive primes of finite length. Note that \( A \) is inductively defined by the rules

\[
() \quad \text{and} \quad \sigma_q = (x \mid (x, q)) \quad \text{for each } q \in P
\]

That is, the empty list \(()\) is an element of \( A \), and for each \((p_1, p_2, \ldots, p_k) \in A\) and \( q \in P \), we have \((p_1, p_2, \ldots, p_k, q) \in A\).

By the fundamental theorem of arithmetic ([Theorem 7.2.12](#)), there is a surjection \(\Pi : A \to \{n \in \mathbb{Z} \mid n > 0\}\) defined by

\[
\Pi(p_1, p_2, \ldots, p_k) = \prod_{i=1}^{k} p_i = p_1 \times p_2 \times \cdots \times p_k
\]

for all \( k \in \mathbb{N} \) and \( p_1, p_2, \ldots, p_k \in P \). Note in particular that \(\Pi(()) = 1\), where \(()\) is the empty list.

We prove by structural induction on \((p_1, p_2, \ldots, p_k) \in A\) that the integer \( n = \Pi(p_1, p_2, \ldots, p_k) > 0 \) satisfies the formula in the statement of the theorem.

- **(Base case)** The unique basic element of \( A \) is the empty list \(()\). Since \(\Pi(()) = 1\), we must prove that the equation in the statement of the theorem is satisfied when \( n = 1 \).

  Well there are no primes \( p \) such that \( p \mid 1 \), and so the product \(\prod_{p \mid 1} \left( 1 - \frac{1}{p} \right)\) is the
</markdown><markdown>
empty product, which is equal to 1. Thus

\[
\varphi(1) = 1 = 1 \cdot 1 = 1 \cdot \prod_{p \mid 1} \left( 1 - \frac{1}{p} \right)
\]

as required.

- **(Induction step for \( q \in P \))** Fix \( (p_1, p_2, \ldots, p_k) \in A \), let \( n = \Pi(p_1, p_2, \ldots, p_k) \) and assume that

\[
\varphi(n) = n \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right)
\]

Note that \(\Pi(p_1, p_2, \ldots, p_k, q) = qn\), and so we need to prove that

\[
\varphi(qn) = qn \cdot \prod_{p \mid qn} \left( 1 - \frac{1}{p} \right)
\]

Now either \( q \mid n \) or \( q \nmid n \).

- Assume \( q \mid n \). Then \( n \) and \( qn \) have the same prime divisors, and so for all \( p \in P \) we have \( p \mid n \) if and only if \( p \mid qn \). Therefore:

\[
\varphi(qn) = q \varphi(n) \quad \text{by Exercise 7.3.28}
\]

\[
= qn \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right) \quad \text{by induction hypothesis}
\]

\[
= qn \cdot \prod_{p \mid qn} \left( 1 - \frac{1}{p} \right) \quad \text{as observed above}
\]

as required.

- Assume \( q \nmid n \). Then for all \( p \in P \) we have \( p \mid qn \) if and only if \( p \mid n \) or \( p = q \). Therefore:

\[
\varphi(qn) = (q - 1) \varphi(n) \quad \text{by Exercise 7.3.29}
\]

\[
= (q - 1) n \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right) \quad \text{by induction hypothesis}
\]

\[
= qn \left( 1 - \frac{1}{q} \right) \cdot \prod_{p \mid n} \left( 1 - \frac{1}{p} \right) \quad \text{rearranging}
\]

\[
= qn \cdot \prod_{p \mid qn} \left( 1 - \frac{1}{p} \right) \quad \text{absorbing } 1 - \frac{1}{q} \text{ into the product}
\]

as required.

In both cases, the formula in the statement of the theorem is satisfied by the integer \( qn \).

By structural induction, the result follows.
</markdown><markdown>
## Uniqueness of inductive definitions

It would be nice if an inductive definition completely characterises the set that it defines. But this is slightly too much to ask; for example, the sets

\[
\{0, 1, 2, 3, \ldots \} \quad \text{and} \quad \{\varepsilon, \bullet, \bullet\bullet, \bullet\bullet\bullet, \ldots \}
\]

are both inductively defined by the rules \( (| \, z) \) and \( (n \, | \, s(n)) \). In the first we take the basic element \( z \) to be the natural number 0, and for each natural number \( n \) we take \( s(n) \) to be the natural number \( n + 1 \); in the second, we take the basic element \( z \) to be the empty word \( \varepsilon \), and for each string \( n \) we take \( s(n) \) to be the string ‚Äò\( n\bullet \)‚Äô (so for example \( s(\bullet\bullet) = \bullet\bullet\bullet \)).

But there is evidently a way of translating between these sets: we can define a function from the first to the second by sending each natural number \( n \) to the string \( \bullet\bullet\cdots\bullet \), where there are \( n \) ‚Äò\( \bullet \)‚Äôs in the string.

Thus the sets \(\{0, 1, 2, 3, \ldots \}\) and \(\{\varepsilon, \bullet, \bullet\bullet, \bullet\bullet\bullet, \ldots \}\) are ‚Äòessentially the same‚Äô‚Äîthey are inductively defined by the same rules, and so the only real difference is how we label their elements.

The next theorem demonstrates that the same is true of inductively defined sets in general. That is, if two sets \( A \) and \( B \) are inductively defined by the same rules, then the only way that they can differ is in the labels we use to denote their elements. Formally, this ‚Äòrelabelling‚Äô establishes a bijection \( h : A \to B \) between the two sets.

In fact, we prove something stronger: not only is there a bijection between the two sets, but the bijection respects the constructors‚Äîthat is, if we apply a constructor in \( A \) to some elements and relabel the result, that is equivalent to relabelling the elements first and then applying the corresponding constructor in \( B \). Even better, this bijection is the unique bijection that does so.

Thus to sum up what the following theorem tells us: inductively defined sets are uniquely determined by their defining rules, up to a unique bijection that is compatible with the constructors‚Äîthis is as close to absolute uniqueness as we could possibly hope to get!

### Theorem 12.2.32 (Uniqueness of inductively defined sets)

Let \( A \) and \( B \) be two sets that are inductively defined by the same set of rules. There is a unique bijection \( h : A \to B \) such that

\[
h(f_\sigma(a_1, a_2, \ldots, a_r)) = g_\sigma(h(a_1), h(a_2), \ldots, h(a_r))
\]

for all rules \( \sigma \), where \( f_\sigma : A^r \to A \) and \( g_\sigma : B^r \to B \) are the corresponding constructors.
</markdown><markdown>
## Section 12.2. Inductively defined sets

**Proof**  
Define \( h : A \to B \) by structural recursion as follows: given a rule \( \sigma \) with arity \( r \in \mathbb{N} \) and given \( a_1, a_2, \ldots, a_r \), such that \( h(a_i) \) has been defined for all \( i \in [r] \), define

\[
h(f_\sigma(a_1, a_2, \ldots, a_r)) = g_\sigma(h(a_1), h(a_2), \ldots, h(a_r))
\]

We just need to prove that \( h \) is a bijection, since evidently the other condition on \( h \) is satisfied by construction.

So define \( k : B \to A \) by structural recursion on the same way; note that now we have

\[
k(g_\sigma(b_1, b_2, \ldots, b_r)) = f_\sigma(k(b_1), k(b_2), \ldots, k(b_r))
\]

We prove that \( k(h(a)) = a \) for all \( a \in A \) by structural induction. To this end, let \( \sigma \) be a rule of arity \( r \in \mathbb{N} \), let \( a_1, a_2, \ldots, a_r \in A \) and suppose that \( k(h(a_i)) = a_i \) for all \( i \in [r] \). Let \( a = f_\sigma(a_1, a_2, \ldots, a_r) \). Then

\[
\begin{align*}
k(h(a)) &= k(h(f_\sigma(a_1, a_2, \ldots, a_r))) & \text{by definition of } a \\
&= k(g_\sigma(h(a_1), h(a_2), \ldots, h(a_r))) & \text{by construction} \\
&= f_\sigma(k(h(a_1)), k(h(a_2)), \ldots, k(h(a_r))) & \text{by construction} \\
&= f_\sigma(a_1, a_2, \ldots, a_r) & \text{by induction hypothesis} \\
&= a & \text{by definition of } a
\end{align*}
\]

This completes the induction step. So we have \( k(h(a)) = a \) for all \( a \in A \).

A similar proof by structural induction reveals that \( h(k(b)) = b \) for all \( b \in B \).

Thus \( k \) is an inverse for \( h \), so that \( h \) is a bijection.

The fact that \( h \) is the unique such bijection is immediate from the fact that it is defined by structural recursion. \(\square\)

### Example 12.2.33

Let \( \Sigma \) be an alphabet. Then leaf-labelled rooted planar binary trees over \( \Sigma \) are essentially the same as parenthesizations of lists of elements of \( \Sigma \) of positive length.

Indeed, let \( R \) be the set of rules defined by

\[
R = \{ (| \ a) \mid a \in \Sigma \} \cup \{ (t_1, t_2 \mid [t_1, t_2]) \}
\]

Like in Example 12.2.17, the inductively defined set \( A \) generated by \( R \) is given by the set of all parenthesizations of lists of elements of \( \Sigma \). For example

\[
[[[a_1, a_2], a_3], [a_4, a_5]] \in A
\]
</markdown><markdown>
where \( a_1, a_2, a_3, a_4, a_5 \in \Sigma \).

But the set \( T \) of all leaf-labelled rooted planar binary trees over \( \Sigma \) is also inductively defined by the rules in \( R \). The basic element \( a \) corresponding to the rule \( (| a) \) is precisely the tree consisting of just its root, which is labelled by the element \( a \in \Sigma \):

\[
\begin{array}{c}
a
\end{array}
\]

and given trees \( t_1, t_2 \), the tree corresponding to \([t_1, t_2]\) is given by forming a tree with a root and two branches, pasting \( t_1 \) onto the left branch, and pasting \( t_2 \) onto the right branch:

\[
\begin{array}{c}
t_1 \quad t_2
\end{array}
\]

By Theorem 12.2.32, there is a unique bijection \( A \to T \) that is compatible with the constructors in each set. For example, the element \([[[a_1, a_2], a_3], [a_4, a_5]] \in A\) corresponds with the following tree:

\[
\begin{array}{c}
a_1 \quad a_2 \\
a_3 \quad a_4 \quad a_5
\end{array}
\]

## ‚òÖ Quotient-inductive sets

Some sets appear to be inductively defined, in the sense that its elements can be constructed from some basic elements by applying some constructors, except there may be more than one way of expressing each of its elements as a constructor applied to some elements of the set.
</markdown><markdown>
For example, consider the set \(\mathbb{Z}\) of integers. Every integer \(n\) can be obtained from 0 by either adding 1 or subtracting 1 some number of times. Thus we‚Äôd like to say that \(\mathbb{Z}\) is inductively defined by the rules

\[
\begin{align*}
(| \ 0) & \quad s = (x \mid x + 1) \\
& \quad p = (x \mid x - 1)
\end{align*}
\]

Here \(s\) stands for ‚Äòsuccessor‚Äô and \(p\) stands for ‚Äòpredecessor‚Äô. However, strictly speaking, \(\mathbb{Z}\) is not inductively defined by these rules; for example

\[
2 = 1 + 1 = f_s(1) \quad \text{and} \quad 2 = 3 - 1 = f_p(3)
\]

and so 2 does not have a *unique* expression as a constructor applied to an integer.

In fact, the inductively defined set \(A\) generated by the three rules above consists of all strings of the form

\[
0 \pm 1 \pm 1 \pm \cdots \pm 1
\]

with each \(\pm\) sign being either \(+\) or \(-\). We recover the corresponding integer by evaluating the string; this evaluation function defines a surjection \(q : A \to \mathbb{Z}\); for example

\[
q(0 + 1 + 1) = 2 \quad \text{and} \quad q(0 + 1 + 1 + 1 - 1 + 1 - 1 - 1 + 1) = 2
\]

As made precise in Theorem 5.2.35, the fact that there is a surjection \(A \to \mathbb{Z}\) means that we can think of \(\mathbb{Z}\) as being a quotient of \(A\) by a suitable equivalence relation.

To see what this equivalence relation is, note that \((n + 1) - 1 = n\) and \((n - 1) + 1 = n\) for all \(n \in \mathbb{Z}\). Thus in a string \(0 \pm 1 \pm \cdots \pm 1\), we can add or remove instances of ‚Äò\(+1 - 1\)‚Äô or ‚Äò\(-1 + 1\)‚Äô in the string as we please, and we will obtain the same integer. So define an equivalence relation \(\sim\) on \(A\) by letting \(a \sim b\) if and only if the string \(b\) can be obtained from \(a\) by adding or removing ‚Äò\(+1 - 1\)‚Äô or ‚Äò\(-1 + 1\)‚Äô some number of times; for example

\[
0 + 1 + 1 \sim 0 + 1 - 1 + 1 + 1 \sim 0 + 1 + 1 + 1 - 1 + 1 - 1 - 1 + 1
\]

Then \(\sim\) is an equivalence relation on \(A\), and the evaluation function \(q : A \to \mathbb{Z}\) descends to a bijection \(\tilde{q} : A / \sim \to \mathbb{Z}\).

We will call sets obtained in this way *quotient-inductive sets*.

### Definition 12.2.34
Let \(A\) be an inductively defined set and let \(\sigma\) be a rule. A function \(h : A \to X\) with domain \(A\) respects \(\sigma\) if, for all \(a_1, a_2, \ldots, a_r, b_1, b_2, \ldots, b_r \in A\), we have

\[
[\forall i \in [r], h(a_i) = h(b_i)] \implies h(f_\sigma(a_1, a_2, \ldots, a_r)) = h(f_\sigma(b_1, b_2, \ldots, b_r))
\]

**To do:** Example, exercise
</markdown><markdown>
### Definition 12.2.35

A **quotient-inductive set** is a set \( Q \) equipped with a surjection \( q : A \to Q \) for some inductively defined set \( A \), such that \( q \) respects all of the rules of \( A \).

The function \( q \) is called the **quotient map**; the **basic elements**, **constructors** and **rules** of \( Q \) are the images under \( q \) of those of \( A \).

We will abuse notation slightly in the following way. Given a rule \( \sigma \) of \( A \) of arity \( r \in \mathbb{N} \) and elements \( u_1, u_2, \ldots, u_r \in Q \), we will write \( f_\sigma(u_1, u_2, \ldots, u_r) \) instead of \( q(f_\sigma(a_1, a_2, \ldots, a_r)) \), where \( a_1, a_2, \ldots, a_r \in A \) are such that \( q(a_i) = u_i \) for each \( i \in [r] \). Note that this is well-defined: the elements \( a_1, a_2, \ldots, a_r \) exist since \( q \) is surjective, and the value of \( q(f_\sigma(a_1, a_2, \ldots, a_r)) \) is independent on the choices of \( a_i \) picked since \( q \) respects the rules of \( A \).

### Example 12.2.36

As we discussed, the rules \( ( \mid 0) \), \( s = (x \mid x + 1) \) and \( p = (x \mid x - 1) \) give rise to an inductively defined set \( A \) whose elements are strings of the form \( 0 \pm 1 \pm 1 \pm \cdots \pm 1 \), with each ‚Äò\(+\)‚Äô sign replaced with either \( + \) or \( - \).

The function \( q : A \to \mathbb{Z} \) given by evaluating the string is evidently a surjection: indeed, given \( n \in \mathbb{Z} \), if \( n \geq 0 \) then \( n = q(0 + 1 + \cdots + 1) \), and if \( n < 0 \) then \( n = q(0 - 1 - \cdots - 1) \).

Thus \( \mathbb{Z} \) is a quotient-inductive set, with basic element 0 and constructors ‚Äò\(+1\)‚Äô and ‚Äò\(-1\)‚Äô.

### Exercise 12.2.37

Prove that the set \( \mathbb{Z}^+ \) of all positive integers is a quotient-inductive set given by the rules \( ( \mid 1) \) and \( (x \mid x \cdot p) \) for each positive prime \( p \in \mathbb{Z} \). Describe the corresponding inductively defined set \( A \) and the quotient map \( q : A \to \mathbb{Z}^+ \) explicitly.

The reason we generalise the notion of an inductively defined set to that of a quotient-inductive set is that we can generalise the structural induction principle to such sets.

Note, however, that the structural recursion theorem depends in an essential way on the uniqueness of the representation of elements in terms of constructors, so we cannot expect the structural recursion theorem to generalise to quotient-inductive sets.
</markdown><markdown>
### Theorem 12.2.38 (Structural induction principle for quotient-inductive sets)

Let \( Q \) be a quotient-inductive set and let \( p(x) \) be a logical formula with free variable \( x \in Q \). If for all rules \( \sigma \) and all \( x_1, \ldots, x_r \in Q \) we have

\[
[\forall i \in [r], \, p(x_i)] \Rightarrow p(f_\sigma(x_1, x_2, \ldots, x_r))
\]

then \( p(x) \) is true for all \( x \in Q \).

**Proof**

Let \( q : A \rightarrow Q \) be the quotient map from the inductively defined set \( A \), and let \( \overline{p}(x) \) be the logical formula with free variable \( x \in A \) defined by letting \( \overline{p}(x) \) mean \( p(q(x)) \).

Since \( q \) is surjective, if \( \overline{p}(x) \) is true for all \( x \in A \), then \( p(x) \) is true for all \( x \in Q \). But the assumptions in the statement of the theorem imply that for all rules \( \sigma \) we have

\[
\forall x_1, \ldots, x_r \in A, \, [\forall i \in [r], \, \overline{p}(x_i)] \Rightarrow \overline{p}(f_\sigma(x_1, x_2, \ldots, x_r))
\]

and so \( \overline{p}(x) \) is true for all \( x \in A \) by (the usual version of) structural induction on \( A \). ‚ñ°
</markdown><markdown>
## Section 12.E

# Chapter 12 exercises
</markdown><markdown>
# Appendices
</markdown>It seems the image is blank. Could you please try uploading it again?<markdown>
# Appendix A

## Proof-writing
</markdown><markdown>
## Section A.1

# Elements of proof-writing

Prior to taking a first course in pure mathematics, most people‚Äôs exposure to the subject has a *computational* focus: the problems are questions that have a single correct answer, and the solutions consist of supplying the correct answer together with some explanation of how it was obtained (‚Äòshowing your work‚Äô). Typically this amounts to a step-by-step sequence of calculations, algebraic manipulations and applications of formulae that result in the desired answer.

Pure mathematics has a different flavour: the task we face is to use the knowledge we have already established in order to discover new properties of the objects we are studying, and then communicate our discoveries in the form of a proof.

A given (true) statement may have many possible proofs, and the proofs may be qualitatively different from one another in many ways. Factors that may differ from one proof to another include length, verbosity, detail, motivation, proof strategies used, balance of words and notation, . . . ‚Äîthe list goes on.

To complicate matters further, it might be that a proof is suitable in one setting but not another. For example, a detailed proof that \(2 + 2 = 4\) using the definitions of ‚Äò2‚Äô, ‚Äò4‚Äô and ‚Äò+‚Äô is appropriate when studying the set of natural numbers from an axiomatic perspective (see Proposition 4.1.5), but this level of detail would not be appropriate when using the fact that \(2 + 2 = 4\) in a counting argument (as in Section 8.1).

With all of this going on, it is difficult to know what is expected of us when we are asked to prove something.

The goal of this section is to shed some light on the following question:

*What makes for an effective proof?*

Learning how to write a proof is like learning any new style of writing: it is an iterative process with a cycle of practice, feedback and reflection. Writing a good proof requires patience and sincere effort, but the ends very much justify the means.

This section should not be read in isolation: it should be read either *after* working through a few chapters in the main part of the book, or as a reference for proof-writing in parallel with the rest of the book.
</markdown><markdown>
## Proofs as prose

Someone with little mathematical background might look at a calculus textbook and ask ‚Äòwhere are all the numbers?‚Äô, surprised by the sheer quantity of letters and symbols that appear. In the same vein, someone with little exposure to pure mathematics might look at this book and ask ‚Äòwhere are all the equations?‚Äô‚Äîit can be surprising to look at a mathematical text and see so many‚Ä¶ *words*.

This is because we‚Äîpure mathematicians‚Äîuse proofs as a tool for communicating our ideas, and we simply *need* to use words in order to make ourselves comprehensible to one another. Furthermore, in order to convince the wider mathematical community that our results are correct, another mathematician should be able to read and understand the proofs that we write, and then be able to communicate our arguments to other mathematicians.

This brings us to the first, and perhaps most important, writing principle for proofs.

### ‚ú• Writing Principle A.1.1
Mathematical proofs should be written (and read) as prose‚Äîin particular, it should be possible to read a proof aloud using full sentences.

To illustrate, consider the following proof that the composite of two injective functions is injective.

### ‚ùù Extract A.1.2
\( X, Y, Z \) sets, \( f : X \to Y \), \( g : Y \to Z \), \( f \) injective, \( g \) injective, \( a, b \in X \):

\[
\begin{align*}
g(f(a)) &= g(f(b)) & \text{def } \circ \\
f(a) &= f(b) & \because g \text{ injective} \\
a &= b & \because f \text{ injective}
\end{align*}
\]

‚à¥ \( g \circ f \) injective

This proof has many assets: it is correct, all of the variables used are quantified, and the steps are justified. But try reading it aloud. You will soon see that this proof does not read as prose‚Äîat least not easily.

For a short proof like this, that may be inconsequential, but for a more extended proof, or in a fully fledged mathematical paper (or textbook), it will not do. It is the duty of the proof-writer‚Äîthat‚Äôs you!‚Äîto make the reader feel as if they are being spoken to.

With this in mind, compare Extract A.1.2 with Extract A.1.3 below.
</markdown><markdown>
**Extract A.1.3**

Let \( X, Y \) and \( Z \) be sets, let \( f : X \to Y \) and \( g : Y \to Z \), and assume that \( f \) and \( g \) are injective. Let \( a, b \in X \) and assume that \( (g \circ f)(a) = (g \circ f)(b) \). Then

\[
g(f(a)) = g(f(b)) \quad \text{by definition of } \circ
\]

\[
\Rightarrow f(a) = f(b) \quad \text{by injectivity of } g
\]

\[
\Rightarrow a = b \quad \text{by injectivity of } f
\]

Hence \( g \circ f \) is injective, as required.

Despite the fact that Extract A.1.3 is not written exclusively using words, it is much easier to read it as prose, substituting phrases for the notation where needed‚Äîwe will see more of this in Writing Principle A.1.8.

Here is a transcription of how Extract A.1.3 might be read aloud, with all the variables‚Äô letter names spelt out in italics.

Let *ex*, *wye* and *zed* be sets, let *ef* be a function from *ex* to *wye* and *gee* be a function from *wye* to *zed*, and assume that *ef* and *gee* are injective. Let *a* and *bee* be elements of *ex* and assume that *gee ef* of *a* is equal to *gee ef* of *bee*. Then *gee ef* of *a* equals *gee ef* of *bee* by definition of function composition, so *ef* of *a* equals *ef* of *bee* by injectivity of *gee*, and so *a* equals *bee* by injectivity of *ef*. Hence *gee ef* is injective, as required.

**Exercise A.1.4**

Consider the following proof that the product of two odd integers is odd.

\( a, b \in \mathbb{Z} \) both odd \(\Rightarrow a = 2k + 1, b = 2\ell + 1, k, \ell \in \mathbb{Z} \)

\[
ab = (2k + 1)(2\ell + 1) = 4k\ell + 2k + 2\ell + 1 = 2(2k\ell + k + \ell) + 1
\]

\( 2k\ell + k + \ell \in \mathbb{Z} \therefore ab \) odd

Rewrite the proof so that it is easier to read it as prose. Read it aloud, and transcribe what you read.

**Us, ourselves and we**

Different fields of study have different conventions about how their results should be communicated, and these conventions change over time. One such convention in mathematics, at least in recent decades, is the use of the first person plural (‚Äòwe show that‚Ä¶‚Äô).
</markdown><markdown>
There are several theories for why this practice has developed; for example, it is more modest than the first person singular (‚ÄòI show that‚Ä¶‚Äô), and it implies that the reader is included in the proof process. But the *reasons* for using the first person plural are largely irrelevant for us‚Äîwhat matters is that this is the convention that prevails.

Some texts, particularly older ones, may deviate from this practice, such as by using the third person impersonal (‚Äòone shows that‚Ä¶‚Äô) or the passive voice (‚Äòit is shown that‚Ä¶‚Äô).

But although not universal, the first person plural is certainly the most commonplace, and so we shall adopt it.

> **Writing Principle A.1.5**  
> It is customary for mathematical proofs to be written in the first person plural.

Open almost any page in this textbook and you will see the first person plural everywhere. The next extract was taken almost at random to illustrate.

> **Extract A.1.6 (taken from Theorem 7.1.23)**  
> We proved in Theorem 7.1.12 that a greatest common divisor of \( a \) and \( b \) is a least element of the set  
> \[
> X = \{ au + bv \mid u, v \in \mathbb{Z}, \, au + bv > 0 \}
> \]
> So let \( u, v \in \mathbb{Z} \) be such that \( au + bv = d \). Then  
> \[
> a(ku) + b(kv) = k(au + bv) = kd = c
> \]
> and so letting \( x = ku \) and \( y = kv \), we see that the equation \( ax + by = c \) has a solution \( (x, y) \in \mathbb{Z} \times \mathbb{Z} \).

Some publishers require that mathematical variables not appear immediately after a punctuation mark, such as a comma or full stop. The first person plural comes to the rescue in the form of the phrase ‚Äòwe have‚Äô, which can usually be inserted after the offending punctuation mark in order to separate it from a statement that begins with a variable.

> **Extract A.1.7 (taken from Example 5.1.21)**  
> Let \( R \) be the relation on \( \mathbb{R} \) defined for \( a, b \in \mathbb{R} \) by \( a \, R \, b \) if and only if \( b - a \in \mathbb{Q} \). Then \( R \) is reflexive, since for all \( a \in \mathbb{R} \), we have \( a - a = 0 \in \mathbb{Q} \), so that \( a \, R \, a \).

Without the phrase ‚Äòwe have‚Äô, this would have read as follows, violating the convention that variables should not follow punctuation marks.

‚Ä¶ for all \( a \in \mathbb{R}, a - a = 0 \in \mathbb{Q}, \) so ‚Ä¶
</markdown><markdown>
# Mathematical notation

What sets mathematics apart from most other areas of study is its heavy use of notation. Even in this introductory book, so much new notation is introduced that it can quickly become overwhelming. When writing a proof, it is sometimes important to take a step back and remember the reason why it is used.

> **Writing Principle A.1.8**  
> Mathematical notation should be used in moderation with the goal of improving readability and precision.

The material covered in this textbook introduces a huge quantity of new notation, and it can be tempting to overuse it in proofs. Of course, how much notation is too much or too little is largely a matter of taste, but there is a balance to be struck.

To illustrate, what follows are three proofs that for all \( a \in \mathbb{Z} \), if \( a^2 \) is divisible by 3, then \( a \) is divisible by 3.

The first proof can be read as prose, as long as you‚Äôre willing to try hard enough to translate notation on the fly, but it is very heavy on the notation. Quantifiers and logical operators are everywhere.

> **Extract A.1.9**  
> Let \( a \in \mathbb{Z} \) and assume 3 | \( a^2 \). Then \(\exists k \in \mathbb{Z}, a^2 = 3k\) by [Definition 7.1.4](#). Also \(\exists q, r \in \mathbb{Z}, 0 \leq r < 3 \land a = 3q + r\) by [Theorem 7.1.1](#). Now
> 
> - \( r = 0 \Rightarrow a^2 = (3q)^2 = 3(3q^2) \Rightarrow \exists k \in \mathbb{Z}, a^2 = 3k \)
> - \( r = 1 \Rightarrow a^2 = (3q + 1)^2 = 3(3q^2 + 2q) + 1 \Rightarrow \nexists k \in \mathbb{Z}, a^2 = 3k \) ‚Äî contradiction by [Theorem 7.1.1](#)
> - \( r = 2 \Rightarrow a^2 = (3q + 2)^2 = 3(3q^2 + 4q + 1) + 1 \Rightarrow \nexists k \in \mathbb{Z}, a^2 = 3k \) ‚Äî contradiction by [Theorem 7.1.1](#)
> 
> Now \((r = 0 \lor r = 1 \lor r = 2) \land (r \neq 1 \land r \neq 2) \Rightarrow r = 0 \Rightarrow 3 | a\), as required.

On the other extreme, the next proof is very easy to read as prose, but its sheer verbosity makes the actual mathematics harder to follow.

> **Extract A.1.10**  
> Suppose the square of an integer \( a \) is divisible by three. Then \( a^2 \) can be expressed as three multiplied by another integer \( k \) by definition of division. Furthermore, by the division theorem, \( a \) can itself be expressed as three multiplied by the quotient \( q \) of \( a \).
</markdown><markdown>
upon division by three, plus the remainder \( r \) upon division by three; and the remainder \( r \) is greater than or equal to zero and is less than three.

Now if the remainder \( r \) is zero, then \( a^2 = (3q)^2 = 3(3q^2) \), which is consistent with our assumption that \( a^2 \) is three multiplied by an integer, since \( 3q^2 \) is an integer. If the remainder \( r \) is one, then \( a^2 = (3q+1)^2 = 3(3q^2+2q)+1 \), which implies by the division theorem that \( a^2 \) cannot be expressed as three multiplied by an integer, contradicting our assumption. If the remainder \( r \) is equal to two, then \( a^2 = (3q+2)^2 = 3(3q^2+4q+1)+1 \), which again implies by the division theorem that \( a^2 \) cannot be expressed as three multiplied by an integer, contradicting our assumption again.

Since the only case that did not lead to a contradiction was that in which the remainder \( r \) was equal to zero, it follows that \( a \) is divisible by three.

The next extract strikes a better balance. It uses enough words to make reading it as prose easier than in Extract A.1.9, but it uses enough notation to keep it much more concise and navigable than Extract A.1.10.

**Extract A.1.11**  
Let \( a \in \mathbb{Z} \) and assume that \( 3 \mid a^2 \). Then \( a^2 = 3k \) for some \( k \in \mathbb{Z} \) by Definition 7.1.4. Moreover, by the division theorem (Theorem 7.1.1), there exist \( q, r \in \mathbb{Z} \) with \( 0 \leq r < 3 \) such that \( a = 3q + r \). Now

- If \( r = 0 \), then \( a^2 = (3q)^2 = 3(3q^2) \). Letting \( k = 3q^2 \), we see that this is consistent with our assumption that \( 3 \mid a \).

- If \( r = 1 \), then \( a^2 = (3q+1)^2 = 3(3q^2+2q) + 1 \). By the division theorem, it follows that \( 3 \nmid a^2 \), contradicting our assumption.

- If \( r = 2 \), then \( a^2 = (3q+2)^2 = 3(3q^2+4q) + 1 \). By the division theorem again, it follows that \( 3 \nmid a^2 \), contradicting our assumption.

Since \( r = 0 \) in the only case that is consistent with our assumption, it follows that \( a = 3q \), and so \( 3 \mid a \), as required.

Observe that one of the main differences between the notation-heavy Extract A.1.9 and the more reasonable Extract A.1.11 is that the latter does not use logical operators or quantifiers‚Äîthey are replaced by their corresponding English translations.

While logical operators and quantifiers‚Äîparticularly \( \Rightarrow, \iff \) and \( \forall \)‚Äîdo have their place in proofs, it is often best to tread lightly. The role of symbolic logic is to help you to figure out how to prove a proposition, and how to word its proof (more in this later); but the proof in the end should not typically contain much in the way of logical notation.
</markdown><markdown>
Think of logical notation as being like an engine in a car, a circuit board in a computer, or an internal organ in a guinea pig‚Äîthey make it work, but you don‚Äôt want to see them!

In line with Writing Principle A.1.1, when we use mathematical notation, we should be careful to make sure that what we write can still be read as prose. To illustrate, let‚Äôs recall Extract A.1.3.

**Extract A.1.3**  
Let \( X, Y \) and \( Z \) be sets, let \( f : X \to Y \) and \( g : Y \to Z \), and assume that \( f \) and \( g \) are injective. Let \( a, b \in X \) and assume that \( (g \circ f)(a) = (g \circ f)(b) \). Then

\[
g(f(a)) = g(f(b)) \quad \text{by definition of } \circ
\]

\[
\Rightarrow f(a) = f(b) \quad \text{by injectivity of } g
\]

\[
\Rightarrow a = b \quad \text{by injectivity of } f
\]

Hence \( g \circ f \) is injective, as required.

The uses of notation are highlighted in the following transcription.

Let \( ex, wye \) and \( zed \) be sets, let \( ef \) be a function from \( ex \) to \( wye \) and \( gee \) be a function from \( wye \) to \( zed \), and assume that \( ef \) and \( gee \) are injective. Let \( a \) and \( bee \) be elements of \( ex \) and assume that \( gee \, ef \) of \( a \) is equal to \( gee \, ef \) of \( bee \). Then \( gee \, ef \) of \( a \) equals \( gee \, ef \) of \( bee \) by definition of function composition, so \( ef \) of \( a \) equals \( ef \) of \( bee \) by injectivity of \( gee \), and so \( a \) equals \( bee \) by injectivity of \( ef \). Hence \( gee \, ef \) is injective, as required.

Observe that the kind of phrase that is read aloud depends on how the notation is being used within a larger sentence. For example, consider the following extract.

**Extract A.1.12**  
Let \( n \in \mathbb{Z} \) and suppose that \( n \) is even. Then \( n = 2k \) for some \( k \in \mathbb{Z} \).

Read aloud, we would say something like:

Let \( en \) be an integer and suppose that \( en \) is even. Then \( en \) equals two \( kay \) for some integer \( kay \).

Despite the fact that ‚Äò\( n \in \mathbb{N} \)‚Äô and ‚Äò\( k \in \mathbb{N} \)‚Äô here differ only in the letter that is used, they are read aloud differently because they are playing different roles in the sentence‚Äîthe former is used as a verb phrase, and the latter as a noun phrase.
</markdown><markdown>
### Exercise A.1.13

Consider the following notation-heavy proof that \( X \cap (Y \cup Z) \subseteq (X \cap Y) \cup (X \cap Z) \) for all sets \( X, Y \) and \( Z \).

Let \( X, Y \) and \( Z \) be sets and let \( a \in X \cap (Y \cup Z) \). Then

\[ a \in X \land a \in Y \cup Z \Rightarrow a \in X \land (a \in Y \lor a \in Z) \]

- **Case 1**: \( a \in Y \Rightarrow (a \in X \land a \in Y) \Rightarrow a \in X \cap Y \Rightarrow a \in (X \cap Y) \cup (X \cap Z) \).

- **Case 2**: \( a \in Z \Rightarrow (a \in X \land a \in Z) \Rightarrow a \in X \cap Z \Rightarrow a \in (X \cap Y) \cup (X \cap Z) \).

\[
\therefore \forall a, a \in X \cap (Y \cup Z) \Rightarrow a \in (X \cap Y) \cup (X \cap Z)
\]

\[
\therefore X \cap (Y \cup Z) \subseteq (X \cap Y) \cup (X \cap Z).
\]

Read the proof aloud and transcribe what you said. Then rewrite the proof with a more appropriate balance of notation and text.

### How much detail to provide

**To do**: How much detail to provide

**To do**: Citing definitions and previously-proved results

### The Greek alphabet

Greek letters are often used as variables in mathematical texts‚Äîsometimes the 26 letters in the Latin alphabet just aren‚Äôt enough! The Greek alphabet has 24 letters and, like the Latin alphabet, it has two cases.

| Name   | Upper | Lower | Name   | Upper | Lower |
|--------|-------|-------|--------|-------|-------|
| Alpha  | A     | \(\alpha\) | Nu     | N     | \(\nu\) |
| Beta   | B     | \(\beta\) | Xi     | \(\Xi\) | \(\xi\) |
| Gamma  | \(\Gamma\) | \(\gamma\) | Omicron | O     | o     |
| Delta  | \(\Delta\) | \(\delta\) | Pi     | \(\Pi\) | \(\pi\) |
| Epsilon| E     | \(\varepsilon\) or \(\epsilon\) | Rho    | P     | \(\rho\) |
| Zeta   | Z     | \(\zeta\) | Sigma  | \(\Sigma\) | \(\sigma\) |
| Eta    | H     | \(\eta\) | Tau    | T     | \(\tau\) |
| Theta  | \(\Theta\) | \(\theta\) | Upsilon| \(\Upsilon\) | \(\upsilon\) |
| Iota   | I     | \(\iota\) | Phi    | \(\Phi\) | \(\phi\) or \(\varphi\) |
| Lambda | \(\Lambda\) | \(\lambda\) | Chi    | X     | \(\chi\) |
| Kappa  | K     | \(\kappa\) | Psi    | \(\Psi\) | \(\psi\) |
| Mu     | M     | \(\mu\) | Omega  | \(\Omega\) | \(\omega\) |
</markdown><markdown>
Note that several of the upper-case Greek letters are identical to upper-case Latin letters‚Äîin mathematics, these are not typically distinguished so that, for example, the letter ‚ÄòH‚Äô will always be interpreted as an upper-case Latin letter aitch, rather than an upper-case Greek letter eta. For the same reason, the lower-case Greek letter omicron is not distinguished from the lower-case Latin letter o.

### \LaTeX\ tip

In order to use Greek letters as mathematical variables using \LaTeX:

- For the upper-case letters that are identical to a letter in the Latin alphabet, use the `\mathrm` command together with the Latin letter. For example, upper-case rho can be input as `\mathrm{P}`, even though rho corresponds phonemically with the Latin letter R.

- For the upper-case letters that are not identical to a letter in the Latin alphabet, the \LaTeX\ command is given by their Greek name with an upper-case first letter. For example, the command `\Gamma` produces the ouput \(\Gamma\).

- For the lower-case letters (except epsilon and phi‚Äîsee below), the \LaTeX\ command is given by their Greek name in lower case. For example, the command `\eta` produces the output \(\eta\).

The variant forms of epsilon and phi, respectively \(\varepsilon\) (`\varepsilon`) and \(\varphi\) (`\varphi`), are preferred over \(\epsilon\) (`\epsilon`) and \(\phi\) (`\phi`). This is to better distinguish them from the symbols \(\in\) (element symbol) and \(\emptyset\) (empty set), respectively.

### Handwriting

**To do:**
</markdown><markdown>
# Section A.2

## Vocabulary for proofs

The focus of [Chapter 1](#) was on examining the logical structure of a proposition and using this to piece together a proof.

For example, in order to prove that every prime number greater than two is odd, we observe that ‚Äòevery prime number greater than two is odd‚Äô takes the form

\[
\forall n \in \mathbb{Z}, [(n \text{ is prime} \land n > 2) \Rightarrow n \text{ is odd}]
\]

By piecing together the proof strategies in [Sections 1.1](#) and [1.2](#), we can see what a proof of this must look like:

- By [Strategy 1.2.10](#), we must assume \( n \in \mathbb{Z} \) and, without assuming anything about \( n \) other than that it is an integer, derive ‚Äò\( (n \text{ is prime} \land n > 2) \Rightarrow n \text{ is odd} \)‚Äô;
- By [Strategy 1.1.22](#), we must assume ‚Äò\( n \text{ is prime} \land n > 2 \)‚Äô and derive that \( n \) is odd;
- By [Strategy 1.1.9](#), we may separately assume that \( n \) is prime and \( n > 2 \).

Thus a proof that every prime number greater than two is odd would assume \( n \in \mathbb{Z} \), assume that \( n \) is prime and \( n > 2 \), and then derive that \( n \) is odd.

All of this tells us how to *structure* a proof, but it does not tell us what to *write* in such a proof‚Äîthat is the goal of this section.

This section provides some basic vocabulary and templates that can be used in proofs. We will use some notation conventions for introducing these templates:

- [Square \| brackets \| and \| bars] will be used where one of several choices can be made. For example, if you see

  \[
  [\text{then} \mid \text{therefore} \mid \text{so} \mid \text{hence}]
  \]

  it means that any of the words ‚Äòthen‚Äô, ‚Äòtherefore‚Äô, ‚Äòso‚Äô or ‚Äòhence‚Äô can be used.

- (Round brackets) will be used where a word or phrase is optional. For example if you see

  \[
  \text{Let } x \in X \text{ (be arbitrary)}
  \]

  it means that either ‚ÄòLet \( x \in X \)‚Äô or ‚ÄòLet \( x \in X \) be arbitrary‚Äô can be used.

- ‚ü®Angle brackets‚ü© will be used to provide instructions. For example if you see

  \[
  \langle \text{insert proof of } p \text{ here} \rangle
  \]

  then you should write out a proof of the proposition \( p \) being referred to.
</markdown><markdown>
## Breaking down a proof

As we discussed in [Section 1.1](#), at every stage in a proof, there is some set of *assumptions* and some set of *goals*. The assumptions are the propositions that we may take to be true, either because we already proved them or because they are being temporarily assumed; and the goals are the propositions that remain to be deduced in order for the proof to be complete.

The words we use indicate to the reader how the assumptions and goals are changing. Thus the words we use allow the reader to follow our logical reasoning and verify our correctness.

For the next few pages, we will examine the proof strategies governing logical operators and quantifiers, as discussed in [Chapter 1](#), and identify some words and phrases that can be used in a proof to indicate which strategy is being used.

### Deductive reasoning

At its core, a proof is a sequence of deductions: starting with a proposition that is already known or assumed to be true, we deduce something new, and continue deducing new things until the proposition we deduce is the result we are trying to prove.

Each time we make a deduction, it should be clear why that deduction is valid, so it is good practice to justify each deduction we make by either *stating* or *citing* the reason.

‚ú¶ **Vocabulary A.2.1**  
The following construction can be used to indicate that an assumption \( p \) is being used to deduce a goal \( q \).

\[
\begin{align*}
&[\text{then | therefore | so (that) | hence} \langle \text{state } q \text{ here} \rangle \\
&\quad [\text{by} \langle \text{cite } p \text{ here} \rangle \text{ | since} \langle \text{state } p \text{ here} \rangle]] \\
&\quad \text{‚Äî or ‚Äî} \\
&\text{we know that} \langle \text{state } p \text{ here} \rangle, \text{ and so} \langle \text{state } q \text{ here} \rangle \\
&\quad \text{‚Äî or ‚Äî} \\
&\text{it follows from} \langle \text{cite } p \text{ here} \rangle \text{ that} \langle \text{state } q \text{ here} \rangle
\end{align*}
\]

If \( p \) was the last thing to be proved in the proof, it may not be necessary to cite it or state it again explicitly‚Äîthat can be inferred.

Here are a couple of examples of [Vocabulary A.2.1](#) in action.

‚ú∂ **Extract A.2.2** (taken from Proposition 2.2.6)  
... Then \( a \in Y \) since \( X \subseteq Y \), so that \( a \in X \cap Y \) by definition of intersection ...
</markdown><markdown>
### Section A.2. Vocabulary for proofs

Notice the choice of words used to **state** versus to **cite** assumptions; in both the previous and next example, we used ‚Äòsince‚Äô to state, and ‚Äòby‚Äô to cite.

#### Extract A.2.3 (taken from Theorem 9.1.19)
If \( a > 0 \), then by Example 9.1.14 and Exercise 9.1.15, we have

\[
\vec{x} \cdot \left( \frac{b}{a} \vec{x} \right) = \frac{b}{a} \frac{1}{\|\vec{x}\|^2}
\]

which is non-negative if and only if \( b \geq 0 \), since we are assuming that \( a \geq 0 \).

#### Exercise A.2.4
In the following proof that every multiple of four is even, identify all instances where an assumption is stated or cited in order to justify a step in the proof.

> Let \( n \in \mathbb{Z} \) and suppose that \( n \) is divisible by 4. Then by definition of divisibility we have \( n = 4k \) for some \( k \in \mathbb{Z} \). But then \( n \) is even, since \( n = 4k = 2(2k) \) and \( 2k \in \mathbb{Z} \).

#### Assuming implications: reducing a problem to another problem

One of the ways that an assumption of the form \( p \Rightarrow q \) is useful is that it ‚Äòreduces‚Äô the problem of proving \( q \) to that of proving \( p \).

#### Vocabulary A.2.5
The following construction can be used to indicate to a reader that you are invoking an assumption of the form \( p \Rightarrow q \) to prove a goal \( q \) by instead proving \( p \).

\[
\text{[Since \(\langle\text{state } p \Rightarrow q \text{ here}\rangle\) | By \(\langle\text{cite } p \Rightarrow q \text{ here}\rangle\)], (in order to prove \(\langle\text{state } q \text{ here}\rangle\)), it suffices to prove \(\langle\text{state } p \text{ here}\rangle\).}
\]

We used this construction in the proof of the strong induction principle:

#### Extract A.2.6 (taken from Theorem 4.3.2)
Notice that \( q(n) \) implies \( p(n) \) for all \( n \geq n_0 \), since given \( n \geq n_0 \), if \( p(k) \) is true for all \( n_0 \leq k \leq n \), then in particular \( p(k) \) is true when \( k = n \).

So it suffices to prove \( q(n) \) is true for all \( n \geq n_0 \).

In the proof of Proposition 8.1.37, we used Vocabulary A.2.5 in the very first sentence to guide the rest of the proof.
</markdown><markdown>
### Extract A.2.7 (taken from Proposition 8.1.37)

First note that \( \binom{n}{k} = \left( \binom{[n]}{k} \right) \) and \( \binom{n}{n-k} = \left( \binom{[n]}{n-k} \right) \), so in order to prove \( \binom{n}{k} = \binom{n}{n-k} \),

it suffices by Strategy 6.1.16 to find a bijection \( f : \left( \binom{[n]}{k} \right) \to \left( \binom{[n]}{n-k} \right) \).

---

### Proving implications: introducing assumptions

Several kinds of logical formulae are proved by introducing new assumptions into a proof. For example:

- **Strategy 1.1.22** says that an implication \( p \Rightarrow q \) can be proved by assuming \( p \) and deriving \( q \).

- **Strategy 1.2.10** says that a universally quantified proposition \( \forall x \in X, \, p(x) \) can be proved by introducing a new variable \( x \), assuming \( x \in X \), and deriving \( p(x) \).

- **Strategy 1.1.38** says that a negation \( \neg p \) can be proved by assuming \( p \) and deriving a contradiction.

---

### Vocabulary A.2.8

The words **assume** and **suppose** can be used to introduce a new assumption \( p \) into a proof.

\[ \text{[assume | suppose]} \, (\text{state } p \text{ here}). \]

The proposition \( p \) may then be used in the proof.

---

In the following extract, observe how the introduced assumption is used later in the proof.

### Extract A.2.9 (taken from Theorem 3.1.26)

Assume \( U = V \) and let \( a \in X \). Then

\[
\chi_U(a) = 1 \Leftrightarrow a \in U \quad \text{by definition of } \chi_U
\]
\[
\Leftrightarrow a \in V \quad \text{since } U = V
\]
\[
\Leftrightarrow \chi_V(a) = 1 \quad \text{by definition of } \chi_V
\]

---

### Proving conjunctions: breaking into steps

Often a goal in a proof has the form \( p \land q \)‚Äîfor example, in order to prove a function \( f : X \to Y \) is a bijection, we can prove that \( f \) is injective **and** \( f \) is surjective; and in order
</markdown><markdown>
to prove that a relation \(\sim\) is an equivalence relation, we can prove that \(\sim\) is reflexive and symmetric and transitive.

In these cases, we can split into steps.

‚ú¶ **Vocabulary A.2.10**  
To indicate to a reader that you are proving a conjunction \(p \land q\) by proving \(p\) and \(q\) individually, you can say that you are breaking into **steps**. For example:

- **Step 1**: (\(\langle\)state \(p\) here\(\rangle\)) \(\langle\)insert proof of \(p\) here\(\rangle\).
- **Step 2**: (\(\langle\)state \(q\) here\(\rangle\)) \(\langle\)insert proof of \(q\) here\(\rangle\).

This can be generalised to conjunctions of more than two propositions. Explicitly enumerated steps are not usually necessary, as long as it is clear what you are aiming to achieve in each step.

A common example of where steps are used is in proving propositions of the form \(p \iff q\), which is shorthand for \((p \Rightarrow q) \land (q \Rightarrow p)\). In these cases, the two steps are the proofs of \(p \Rightarrow q\) and \(q \Rightarrow p\); the steps can then be labelled as \((\Rightarrow)\) and \((\Leftarrow)\), respectively.

‚ú§ **Extract A.2.11** (taken from Example 1.1.33)  
Our goal is now to prove that **8 divides \(n\) if and only if 8 divides \(n'\)**.

- \((\Rightarrow)\) Suppose 8 divides \(n\). Since 8 divides \(n''\), it follows from Exercise 0.16 that 8 divides \(an + bn''\) for all \(a, b \in \mathbb{Z}\). But

  \[
  n' = n - (n - n') = n - n'' = 1 \cdot n + (-1) \cdot n''
  \]

  so indeed 8 divides \(n'\), as required.

- \((\Leftarrow)\) Suppose 8 divides \(n'\). Since 8 divides \(n''\), it follows from Exercise 0.16 that 8 divides \(an' + bn''\) for all \(a, b \in \mathbb{Z}\). But

  \[
  n = n' + (n - n') = n' + n'' = 1 \cdot n' + 1 \cdot n''
  \]

  so indeed 8 divides \(n\), as required.

Proofs of set equality by double containment also follow this format; in Section 2.1 we denoted the steps by \((\subseteq)\) and \((\supseteq)\), respectively.
</markdown><markdown>
## Assuming disjunctions: breaking into cases

By Strategy 1.1.16, in order to use an assumption of the form \( p \lor q \) (‚Äò\( p \) or \( q \)‚Äô) to deduce a goal \( r \), it suffices to show that \( r \) may be deduced from each of \( p \) and \( q \)‚Äîthe idea here is that we may not know which of \( p \) or \( q \) is true, but that is fine since we derive \( r \) in both cases.

‚ú¶ **Vocabulary A.2.12**  
To indicate to a reader that you are using an assumption \( p \lor q \) to prove a goal \( r \), you can say that you are breaking into **cases**. For example:

> We know that either ‚ü®state \( p \) here‚ü© or ‚ü®state \( q \) here‚ü©.
> 
> - **Case 1**: Assume that ‚ü®state \( p \) here‚ü©. ‚ü®insert proof of \( r \) here‚ü©.
> - **Case 2**: Assume that ‚ü®state \( q \) here‚ü©. ‚ü®insert proof of \( r \) here‚ü©.
> 
> In both cases we see that ‚ü®state \( r \) here‚ü©, as required.

Like with proofs involving steps (Vocabulary A.2.10), the explicit enumeration of cases is not usually necessary.

In the following extract, the assumption made is \( (k \leq n) \lor (k > n) \), which is valid by the law of excluded middle (Strategy 1.1.45), and the goal is \( 2^k y \in D \).

‚ú∂ **Extract A.2.13** (taken from Example 3.2.18)  
Since \( y \in D \), we must have \( y = \frac{a}{2^n} \) for some \( n \in \mathbb{N} \).

- If \( k \leq n \) then \( n - k \in \mathbb{N} \) and so \( 2^k y = \frac{a}{2^{n-k}} \in D \).

- If \( k > n \) then \( k - n > 0 \) and \( 2^k y = 2^{k-n} a \in \mathbb{Z} \); but \(\mathbb{Z} \subseteq D\) since if \( a \in \mathbb{Z} \) then \( a = \frac{a}{2^0} \).  
  So again we have \( 2^k y \in D \).

In both cases we have \( 2^k y \in D \); and \( f(2^k y) = y \), so that \( f \) is surjective.

Sometimes proofs by cases are extremely compact‚Äîso much so that you might not notice it‚Äîlike in the next extract.

‚ú∂ **Extract A.2.14** (taken from Proposition 7.2.7)  
Since \( a \mid p \), we have \( a \in \{1, -1, p, -p\} \). If \( a = \pm 1 \), then \( a \) is a unit; if \( a = \pm p \), then \( b = \pm 1 \), so that \( b \) is a unit. In any case, either \( a \) or \( b \) is a unit, and hence \( p \) is irreducible.
</markdown><markdown>
Note that cases were not explicitly labelled, but it was clear from context that cases were what were being used.

## Proving negations: proof by contradiction

### Vocabulary A.2.15

The following construction can be used in order to indicate that an assumption \( p \) is being introduced with the view of deriving a contradiction, thereby proving that \( p \) is false (that is, \(\neg p\) is true).

- **Towards a contradiction, assume** ‚ü®state \( p \) here‚ü©.
  
  ‚Äî or ‚Äî

- **Assume** ‚ü®state \( p \) here‚ü©. **We will derive a contradiction.**

The following construction can be used in order to indicate that a contradiction to an assumption \( q \) has been reached from a contradictory assumption \( p \).

- **This contradicts** ‚ü®cite \( q \) here‚ü©. **Therefore** ‚ü®state \(\neg p\) here‚ü©.
  
  ‚Äî or ‚Äî

- \(\ldots\), **contrary to** ‚ü®cite \( q \) here‚ü©, **so that** ‚ü®state \(\neg p\) here‚ü©.

Explicit reference to the proposition being contradicted is not always necessary if it is clear from context.

- **This is** [a contradiction | nonsense | absurd | impossible]. **Therefore** ‚ü®state \(\neg p\) here‚ü©.

### Extract A.2.16 (taken from Theorem 6.1.25)

We proceed by contradiction. Suppose \(\mathbb{N}\) is finite. Then \(|\mathbb{N}| = n\) for some \(n \in \mathbb{N}\), and hence \(\mathbb{N}\) is either empty (nonsense, since \(0 \in \mathbb{N}\)) or, by Lemma 6.1.24, it has a greatest element \(g\). But \(g + 1 \in \mathbb{N}\) since every natural number has a successor, and \(g + 1 > g\), so this contradicts maximality of \(g\). Hence \(\mathbb{N}\) is infinite.

### Extract A.2.17 (taken from Theorem 9.3.55)

Towards a contradiction, suppose that \(e \in \mathbb{Q}\).

[. . . many lines of proof omitted . . .]

But this implies that \(0 < c < 1\), which is nonsense since \(c \in \mathbb{Z}\).

We have arrived at a contradiction, so it follows that \(e\) is irrational.
</markdown><markdown>
Proofs by contradiction need not be long and drawn out‚Äîsometimes you just need to say that something is false and give a brief justification. This is illustrated in the next extract.

**Extract A.2.18 (taken from Theorem 7.1.1)**  
... it remains to show that \( r < b \). Well, if \( r \geq b \) then \( r - b \geq 0 \), but \( r - b = r_{k+1} \), so this would imply \( r_{k+1} \in R \), contradicting minimality of \( r \). Hence \( r < b \) ...

### Proving universally quantified statements: introducing variables

As discussed in Strategy 1.2.10, proving that a property \( p(x) \) hold for all elements \( x \) of a set \( X \) is done by introducing a new variable \( x \in X \) and, assuming nothing about that variable other than that it is an element of \( X \), deriving the truth of \( p(x) \).

This sounds scary, but introducing the variable \( x \) is very easy to do.

‚ú¶ **Vocabulary A.2.19**  
The following constructions can be used to introduce a new variable \( x \), referring to an arbitrary element of a set \( X \).

- Let \( x \in X \) (be arbitrary).  
  ‚Äî or ‚Äî  
- [Take | Fix] (an (arbitrary) element) \( x \in X \).  
  ‚Äî or ‚Äî  
- Given \( x \in X \), ...

Explicit use of the word ‚Äòarbitrary‚Äô can be useful to drive home the point that nothing is assumed about \( x \) other than that it is an element of \( X \). In practice, it is optional.

If you turn to almost any page in the book, you will see an instance of Vocabulary A.2.19.

For example, in the next extract, the goal was to prove the universally quantified proposition meaning that \( g \circ f \) is injective, that is \(\forall a, b \in X, [(g \circ f)(a) = (g \circ f)(b) \Rightarrow a = b]\).

**Extract A.2.20 (taken from Proposition 3.2.4)**  
... let \( a, b \in X \). We need to prove that

\[
(g \circ f)(a) = (g \circ f)(b) \Rightarrow a = b
\]

...

Variables can also be introduced mid-sentence using the word ‚Äògiven‚Äô, like in the next extract.
</markdown><markdown>
### Extract A.2.21 (taken from Example 6.1.12)

We need to prove \( f(k) \in X \) for all \( k \in [2n + 1] \). Well given \( k \in [2n + 1] \), we have \( 1 \leq k \leq 2n + 1 \), and so

\[
-n = 1 - (n + 1) \leq k - (n + 1) = f(k) \leq (2n + 1) - (n + 1) = n
\]

so that \( f(k) \in X \) as claimed.

### Proving existentially quantified statements: making definitions

When proving a statement of the form \(\exists x \in X, \, p(x)\), we usually have to define an element \( a \in X \) that makes \( p(a) \) true, and then prove that \( p(a) \) is true.

#### Vocabulary A.2.22

The following construction can be used to indicate that you are proving that there exists an element \( x \) of a set \( X \) such that \( p(x) \) is true.

- \([ \text{Define} \mid \text{Let} ] \langle \text{define } a \text{ here} \rangle. \langle \text{insert proof of } p(a) \text{ here} \rangle.\)
- \([ \text{It follows that} \mid \text{So} \mid \text{Therefore} ] \langle \text{state } \exists x \in X, \, p(x) \text{ here} \rangle.\)

Proofs of divisibility (see Definition 7.1.4) often use this construction, as in the following example.

### Extract A.2.23 (taken from Theorem 7.2.11)

Define \( q = ku + bv \); then

\[
b = abu + pbv = pku + pbv = p(ku + bv) = qp
\]

so \( p \mid b \), as required.

Here is another example, which uses the word ‚Äòlet‚Äô.

### Extract A.2.24 (taken from Theorem 9.1.16)

If \( \vec{y} \neq \vec{0} \) then let \( a = \|\vec{y}\|^2 \) and \( b = \vec{x} \cdot \vec{y} \); otherwise, let \( a = 0 \) and \( b = 1 \). In both cases, we have \( a\vec{x} = b\vec{y} \) and \( a, b \) are not both zero.

### Assuming existentially quantified statements: choosing elements

Another reason why variables might be introduced is because an assumption is existentially quantified. For example, if we know that 8 is even, then we know that \( 8 = 2k \) for some \( k \in \mathbb{Z} \).
</markdown><markdown>
### Vocabulary A.2.25

The following construction can be used to indicate that you are invoking an assumption of the form \(\exists x \in X, \, p(x)\).

\[
\text{Let } a \in X \text{ be such that } \langle \text{state } p(a) \text{ here} \rangle.
\]

Again, proofs involving division yield several examples of when this construction is used.

### Extract A.2.26 (taken from Theorem 7.1.35)

... so by Proposition 7.1.32, we have \(a \mid y_0 - y\) and \(b \mid x - x_0\). Let \(k, \ell \in \mathbb{Z}\) be such that

\[
x - x_0 = kb \quad \text{and} \quad y_0 - y = \ell a.
\]

Here is an example from Section 9.2 on the convergence of sequences; here, the existentially quantified assumption allowing us to introduce the variables \(a\) and \(b\) was cited.

### Extract A.2.27 (taken from Theorem 9.2.58)

Let \((x_n)\) be a sequence of real numbers and let \(a, b \in \mathbb{R}\) be such that \(a < x_n < b\) for each \(n \geq 0\)‚Äîthe numbers \(a\) and \(b\) exist since the sequence \((x_n)\) is bounded.

### ‚ÄòWithout loss of generality‚Äô

Sometimes we need to prove a result in one of several cases, but the proofs in each case turn out to be very similar. This might be because the proofs are the same but with two variables swapped; or it might be because one case easily reduces to the other.

In such cases, instead of writing out the whole proof again with the minor changes made, we can inform the reader that this is happening and tell them how to recover the cases we do not prove.

### Vocabulary A.2.28

When invoking an assumption of the form \(p \lor q\), the phrase **without loss of generality** can be helpful to avoid splitting into cases when a proof in each case is essentially identical:

\[
(\text{We may}) \, \text{assume } \langle \text{state } p \text{ here} \rangle \, (\text{without loss of generality}) \text{‚Äîotherwise} \langle \text{say how to modify the proof if } q \text{ were true instead of } p \rangle.
\]

The phrase ‚Äòwithout loss of generality‚Äô is so widespread that it is sometimes abbreviated to **wlog** (or **WLOG**), but this is best reserved for informal, hand-written proofs.
</markdown><markdown>
We only used the phrase ‚Äòwithout loss of generality‚Äô explicitly once in this book so far; this is recalled in the next extract.

> **Extract A.2.29 (taken from proof of Theorem 7.3.59 in Section 12.2)**  
> Recall that \( \varphi(-n) = \varphi(n) \) for all \( n \in \mathbb{Z} \), so we may assume without loss of generality that \( n \geq 0 \)‚Äîotherwise just replace \( n \) by \( -n \) throughout.

Nonetheless, we used the construction in Vocabulary A.2.28 at other times, as illustrated in the next extract, where we used it *twice*!

> **Extract A.2.30 (taken from Theorem 7.1.1)**  
> We may assume that \( b > 0 \): if not, replace \( b \) by \( -b \) and \( q \) by \( -q \). We may also assume that \( a \geq 0 \). Otherwise, replace \( a \) by \( -a \), \( q \) by \( -(q+1) \) and \( r \) by \( b - r \).

## Keeping track of everything

It can be difficult when writing a proof to keep track of what you have proved and what you have left to prove, particularly if you partially prove a result and then come back to it after a break.

On the other hand, it can be very difficult when *reading* a proof to keep track of what has been proved and what is left to prove!

As such, when writing a proof, it is of benefit both to you and to your future readers to insert language into your proofs that clarifies what has been done and what is left to do.

‚ú¶ **Vocabulary A.2.31**  
The following phrases can be used to indicate that the next goal in a proof is to prove a proposition \( p \).

\[
\begin{array}{|l|}
\hline
\text{[We want | Our goal (now) is | It remains] to [show | prove] \(\langle\text{state } p \text{ here}\rangle\)} \\
\text{‚Äî or ‚Äî} \\
\text{To see that \(\langle\text{state } p \text{ here}\rangle\), note that \ldots} \\
\hline
\end{array}
\]

The first extract we look at is taken from the middle of a very long proof in Section 9.3. The entire highlighted sentence could be deleted and the proof would still be correct, but it would be much harder to follow‚Äîthe sentence beginning ‚ÄòSo let \( K \geq N \ldots \)‚Äô might look as if it came out of nowhere.
</markdown><markdown>
## Extract A.2.32 (taken from Exercise 9.3.41)

It remains to prove that

\[
\sum_{n=0}^{K} a_{\sigma(n)} - A < \varepsilon \text{ for all } K \geq N.
\]

So let \( K \geq N, \ldots \)

Here is an example where we reiterate what we are about to prove before we prove it. Again, the highlighted text could be deleted without making the proof any less correct, but including it makes the proof easier to navigate.

## Extract A.2.33 (taken from Lemma 10.2.16)

To see that \( H \) is a bijection, note that the function \( K : Y^X \to [\lambda]^{[k]} \) defined by \( K(\varphi) = k_{\varphi} = g^{-1} \circ \varphi \circ f \) is a bijection \ldots

When a key goal in a proof is reached, it is useful to say so.

### Vocabulary A.2.34

The following phrases can be used to reiterate that a goal \( p \) has been proved.

\[ \text{[Hence | So (that) | Therefore | It follows that] } \langle \text{state } p \text{ here}\rangle (\text{, as required}). \]

The next proof extract is a very typical example.

## Extract A.2.35 (taken from Theorem 4.1.2)

By condition (iii) of Definition 4.1.1, we have \( \mathbb{N} \subseteq D \), so that \( f(n) \) is defined for all \( n \in \mathbb{N}, \text{ as required}. \)

Sometimes it might not be obvious that we have achieved the goal that we set out to achieve, in which case saying why the conclusion is ‚Äòas required‚Äô is also important.

This is illustrated in the next extract, where the goal was to prove that \( |X \cup Y| = |X| + |Y| - |X \cap Y| \).

## Extract A.2.36 (taken from Proposition 6.1.19)

Hence \( |X \cup Y| = m + n = |X| + |Y| \), which is as required since \( |X \cap Y| = 0 \).

## Case study: proofs by induction

Many of the strategies for structuring and writing proofs are illustrated in proofs by induction. The strategy of proof by weak induction is described by the weak induction principle, recalled next.
</markdown><markdown>
### Theorem 4.2.1 (Weak induction principle)

Let \( p(n) \) be a logical formula with free variable \( n \in \mathbb{N} \), and let \( n_0 \in \mathbb{N} \). If

1. \( p(n_0) \) is true; and
2. For all \( n \geq n_0 \), if \( p(n) \) is true, then \( p(n+1) \) is true;

then \( p(n) \) is true for all \( n \geq n_0 \).

Expressed as a single logical formula, the weak induction principle says that

\[
[p(n_0) \land (\forall n \geq n_0, (p(n) \Rightarrow p(n+1)))] \Rightarrow [\forall n \geq n_0, p(n)]
\]

Let's pretend we know nothing about proofs by induction, and use the vocabulary in this section to construct a template for ourselves.

The goal is to prove \( \forall n \geq n_0, p(n) \). We are going to invoke the weak induction principle (Theorem 4.2.1), and so using Vocabulary A.2.5 our proof should look something like this:

\[
\begin{align*}
&\text{{insert proof of }} p(n_0) \land (\forall n \geq n_0, (p(n) \Rightarrow p(n+1))) \text{{ here}} \\
&\text{It follows by the weak induction principle that } p(n) \text{ is true for all } n \geq n_0.
\end{align*}
\]

Since our goal is the conjunction of two formulae, using Vocabulary A.2.10 tells us that we should break up the proof into steps:

- **Step 1.** {insert proof of \( p(n_0) \) here}

- **Step 2.** {insert proof of \( \forall n \geq n_0, p(n) \Rightarrow p(n+1) \) here}  
  It follows by the weak induction principle that \( p(n) \) is true for all \( n \geq n_0 \).

The goal in Step 2 is universally quantified, so we now need to introduce a variable \( n \) satisfying the condition that \( (n \text{ is a natural number and }) n \geq n_0 \). We can do so using Vocabulary A.2.19.

- **Step 1.** {insert proof of \( p(n_0) \) here}

- **Step 2.** Fix \( n \geq n_0 \). {insert proof of \( p(n) \Rightarrow p(n+1) \) here}  
  It follows by the weak induction principle that \( p(n) \) is true for all \( n \geq n_0 \).

The goal in Step 2 is an implication, so using Vocabulary A.2.8, we obtain the following.
</markdown><markdown>
- **Step 1.** ‚ü®insert proof of \( p(n_0) \) here‚ü©

- **Step 2.** Fix \( n \ge n_0 \) and assume ‚ü®state \( p(n) \) here‚ü©.

  ‚ü®insert proof of \( p(n+1) \) here‚ü©

  It follows by the weak induction principle that \( p(n) \) is true for all \( n \ge n_0 \).

To help the reader navigate the proof, we can use Vocabulary A.2.31 to reiterate the goal that we want to prove in Step 2.

- **Step 1.** ‚ü®insert proof of \( p(n_0) \) here‚ü©

- **Step 2.** Fix \( n \ge n_0 \) and assume ‚ü®state \( p(n) \) here‚ü©.

  We need to prove ‚ü®state \( p(n+1) \) here‚ü©.

  ‚ü®insert proof of \( p(n+1) \) here‚ü©

  It follows by the weak induction principle that \( p(n) \) is true for all \( n \ge n_0 \).

This is now a perfectly good template for a proof of \( \forall n \ge n_0, \, p(n) \). But this is exactly what a proof by induction looks like: ‚ÄòStep 1‚Äô is the base case, ‚ÄòStep 2‚Äô is the induction step, the assumption \( p(n) \) in Step 2 is the induction hypothesis, and the goal \( p(n+1) \) in Step 2 is the induction goal!
</markdown><markdown>
## Appendix B

# Mathematical miscellany

There have been a number of times in the book where we have avoided delving too deeply into the more technical or obscure aspects of a definition or proof. Usually this was because exploring these aspects was not central to the topic at hand, or because the details involved were sufficiently messy that providing all the details would obfuscate the main ideas being discussed.

This appendix provides a home for the comments we didn‚Äôt make, the theorems we didn‚Äôt prove, the details we didn‚Äôt provide and the obscurities we didn‚Äôt explore.

We begin with a quick glance at the *foundations of mathematics* in Section B.1. We will provide the axioms for Zermelo‚ÄìFraenkel set theory (ZF), which encodes all mathematical objects as sets and allows us to derive all mathematical objects from a collection of axioms. We will also demonstrate how to encode natural numbers, integers, rational numbers and complex numbers within this framework.
</markdown><markdown>
## Section B.1
# Set theoretic foundations

### Na√Øve set theory and Russell‚Äôs paradox

*Na√Øve set theory* formalises what we mean when we say ‚Äòa set is a collection of objects‚Äô. It consists of just two axioms:

- **Extensionality**: If two sets contain exactly the same elements, then they are equal.

- **Comprehension**: For any logical formula \( p(x) \) with free variable \( x \), there is a set whose elements are precisely those objects \( x \) such that \( p(x) \) is true.

Note that the comprehension axiom implies that anything that we can define using set-builder notation \(\{x \mid p(x)\}\) is a set.

Unfortunately, na√Øve set theory is not consistent‚Äîthe most famous proof of this is due to Bertrand Russell, stated next.

#### Theorem B.1.1 (Russell‚Äôs paradox)
Under na√Øve set theory, the set \( R = \{x \mid x \notin x\} \) satisfies neither \( R \in R \) nor \( R \notin R \). Thus na√Øve set theory is inconsistent.

**Proof**  
The definition of \( R \) implies that \( R \in R \) if and only if \( R \notin R \), so that both the assertion that \( R \in R \) and the assertion that \( R \notin R \) lead to a contradiction.

Several resolutions to Russell‚Äôs paradox were suggested in the first half of the 20th century. Some did this by restricting how set-builder notation can be used, so that not all expressions of the form \(\{x \mid p(x)\}\) define sets; others did this by assigning a *type* to each object and restricting how objects of one type may interact with objects of another type.

The foundation that was eventually adopted by the mathematical community at large is *Zermelo‚ÄìFraenkel set theory* (ZF), these days usually with the addition of the axiom of choice (ZFC). As such, this is the foundational system presented here.

It is worth noting, however, that other systems continue to be studied by mathematicians, logicians, philosophers and computer scientists. In particular, *Martin‚ÄìL√∂f type theory* is a popular mathematical foundation used by computer scientists, as it much more closely mimics how programming languages work.
</markdown><markdown>
## Zermelo‚ÄìFraenkel set theory

Zermelo‚ÄìFraenkel set theory overcomes Russell‚Äôs paradox by deleting the axiom of comprehension and replacing it with axioms of two kinds: axioms that posit the existence of some sets (such as the empty set), and axioms that describe how to construct new sets from sets already known to exist (such as their power set). In this theory, everything is a set, and so all mathematical objects‚Äînumbers, functions, relations, and so on‚Äîare encoded as sets of some kind. This is in contrast to type theories, which describe mathematical objects in terms of their intrinsic properties.

The first two axioms of ZF set theory that we introduce are the **axiom of extensionality** and the **axiom of foundation**. They concern the behaviour of the set elementhood relation \(\in\), with the axiom of extensionality describing how it relates to equality of sets (as in Axiom 2.1.22), and axiom of foundation.

### Axiom B.1.2 (Axiom of extensionality)
If two sets have the same elements, then they are equal.

\[
\forall X, \forall Y, \left( \forall a, a \in X \iff a \in Y \right) \Rightarrow X = Y
\]

A consequence of the axiom of extensionality is that two sets can be proved to be equal by proving that they contain the same elements.

### Axiom B.1.3 (Axiom of foundation)
Every inhabited set has an \(\in\)-minimal element.

\[
\forall X, \left( \exists y, y \in X \right) \Rightarrow \exists x, \left( x \in X \land \forall u \in X, u \notin x \right)
\]

The axiom of foundation states that \(\in\) is a well-founded relation.

The axiom of foundation is mysterious at first sight, but it captures the idea that every set should build up from \(\emptyset\) using set theoretic operations.

### Lemma B.1.4
Under the axiom of foundation, we have \(X \notin X\) for all sets \(X\).

**Proof**  
Let \(X\) be a set. The set \(\{X\}\) is inhabited since \(X \in \{X\}\), so by the axiom of foundation there is some \(x \in \{X\}\) such that \(u \notin x\) for all \(u \in \{X\}\). But the only element of \(\{X\}\) is \(X\) itself, so this says exactly that \(X \notin X\).

### Exercise B.1.5
Use the axiom of foundation to prove that there is no sequence of sets \(X_0, X_1, X_2, \ldots\) such that \(X_{n+1} \in X_n\) for all \(n \in \mathbb{N}\).
</markdown><markdown>
The next few axioms of ZF set theory posit the existence of certain sets or constructions of sets.

### Axiom B.1.6 (Empty set axiom)
There is a set with no elements.

\[
\exists X, \forall x, x \notin X
\]

The empty set axiom asserts the existence of \(\emptyset\).

### Axiom B.1.7 (Pairing axiom)
For any two sets \(x\) and \(y\), there is a set containing only \(x\) and \(y\).

\[
\forall x, \forall y, \exists X, \forall u, [u \in X \iff (u = x \lor u = y)]
\]

The axiom of pairing asserts the existence of sets of the form \(\{x, y\}\).

### Axiom B.1.8 (Union axiom)
The union of any family of sets exists and is a set.

\[
\forall F, \exists U, \forall x, [x \in U \iff \exists X, (x \in X \land X \in F)]
\]

The axiom of union asserts that if \(F = \{X_i \mid i \in I\}\) is a family of sets then the set \(U = \bigcup_{i \in I} X_i\) exists.

### Axiom B.1.9 (Power set axiom)
The set of all subsets of a set is a set.

\[
\forall X, \exists P, \forall U, [U \in P \iff \forall u, (u \in U \Rightarrow u \in X)]
\]

The axiom of power set asserts the existence of \(\mathcal{P}(X)\) for all sets \(X\).

Assuming only the previously stated axioms, it is entirely plausible that every set be finite. This isn‚Äôt good news for us, since we want to be able to reason about infinite sets, such as the set \(\mathbb{N}\) of natural numbers. The axiom of infinity asserts the existence of an infinite set using a clever set theoretic construction called the successor set operation.

### Definition B.1.10
Given a set \(X\), the successor set of \(X\) is the set \(X^+\) defined by

\[
X^+ = X \cup \{X\}
\]
</markdown><markdown>
### Lemma B.1.11
Let \( X \) and \( Y \) be sets. If \( X^+ = Y^+ \), then \( X = Y \).

**Proof**  
Assume \( X^+ = Y^+ \). Then

- We have \( X \in X^+ \), so \( X \in Y^+ = Y \cup \{Y\} \), and so either \( X = Y \) or \( X \in Y \);
- We have \( Y \in Y^+ \), so \( Y \in X^+ = X \cup \{X\} \), and so either \( Y = X \) or \( Y \in X \).

If \( X = Y \) then we‚Äôre done. Otherwise, we must have \( X \in Y \) and \( Y \in X \). But then we can define a sequence of sets by letting

\[
X_n = 
\begin{cases} 
X & \text{if } n \text{ is even} \\
Y & \text{if } n \text{ is odd}
\end{cases}
\]

for all \( n \in \mathbb{N} \). This sequence satisfies \( X_{n+1} \in X_n \) for all \( n \in \mathbb{N} \), since if \( n \) is even then

\[
X_{n+1} = Y \in X = X_n
\]

and if \( n \) is odd then

\[
X_{n+1} = X \in Y = X_n
\]

This contradicts Exercise B.1.5, so we must have \( X = Y \), as claimed.

### Axiom B.1.12 (Axiom of infinity)
There is an inhabited set containing successor sets of all of its elements.

\[
\exists X, \left( \exists u, u \in X \right) \wedge \forall x, \left( x \in X \Rightarrow x^+ \in X \right)
\]

Intuitively, the axiom of infinity tells us that there is a set \( X \) which contains (at least) a family of elements of the form \( u, u^+, u^{++}, u^{+++}, \) and so on‚Äîeach of these elements must be distinct by Lemma B.1.11, so that \( X \) must be infinite.

### Axiom B.1.13 (Axiom of replacement)
The image of any set under any function is a set. That is, for each logical formula \( p(x,y) \) with two free variables \( x,y \), we have

\[
\forall X, \left( \forall x \in X, \exists! y, p(x,y) \right) \Rightarrow \exists Y, \forall y, y \in Y \Leftrightarrow \exists x \in X, p(x,y)
\]

### Axiom B.1.14 (Axiom of separation)
For any logical formula \( p(x) \) with one free variable, and any set \( X \), there is a set consisting of the elements of \( X \) satisfying \( p(x) \).

\[
\forall X, \exists U, \forall x, [x \in U \Leftrightarrow (x \in X \land p(x))]
\]

The axiom of separation asserts the existence of sets of the form \(\{x \in X \mid p(x)\}\).
</markdown><markdown>
### Exercise B.1.15

Prove that the axioms of infinity and separation imply the existence of an empty set.

In light of Exercise B.1.15, the empty set axiom (Axiom B.1.6) is in fact redundant, in the presence of the other axioms. We keep it around for the sake of convenience.

### Grothendieck universes

In [Section 2.1](#) one of the first things we defined was a universal set, which we promptly forgot about and mentioned as little as possible. In this short subsection we briefly introduce the notion of a **Grothendieck universe**, named after the interesting (and influential) mathematician Alexander Grothendieck.

#### Definition B.1.16

A **Grothendieck universe** is a set \(\mathcal{U}\) satisfying the following properties:

(i) The elements of \(\mathcal{U}\) are sets;

(ii) For all \(X \in \mathcal{U}\), if \(x \in X\), then \(x \in \mathcal{U}\);

(iii) \(\mathbb{N} \cup \mathbb{N} \in \mathcal{U}\) (see Construction B.2.5);

(iv) For all \(X \in \mathcal{U}\), we have \(\mathcal{P}(X) \in \mathcal{U}\);

(v) For all \(I \in \mathcal{U}\) and all \(\{X_i \mid i \in I\} \subseteq \mathcal{U}\), we have \(\bigcup_{i \in I} X_i \in \mathcal{U}\).

The existence of a Grothendieck universe is not implied by the axioms of Zermelo‚ÄìFraenkel set theory (with or without the axiom of choice)‚Äîif it were, it would violate G√∂del‚Äôs incompleteness theorem, a result that is even further beyond the scope of this book than Grothendieck universes are!

#### Theorem B.1.17

Let \(\mathcal{U}\) be a Grothendieck universe. The axioms of Zermelo‚ÄìFraenkel set theory are satisfied relative to \(\mathcal{U}\). If the axiom of choice is assumed, then that is also satisfied relative to \(\mathcal{U}\).

The upshot of Theorem B.1.17 is that although there is no universal set, if we assume the existence of a Grothendieck universe \(\mathcal{U}\), then for the purposes of this book, we may relativise everything we do to \(\mathcal{U}\) and pretend that \(\mathcal{U}\) is indeed a universal set. And this is exactly what we did in [Section 2.1](#).
</markdown><markdown>
## Section B.2

### Constructions of the number sets

In the most commonly used foundation of mathematics, Zermelo‚ÄìFraenkel set theory with the axiom of choice (discussed at length in Section B.1), every mathematical object is a set. This raises the question of: what about the other mathematical objects? For example, if numbers are sets, what sets are they? What about functions and relations?

For functions and relations, there is an easy cop-out: we simply identify them with their graphs. For example, we can pretend that the function \( f : \mathbb{R} \to \mathbb{R} \) given by \( f(x) = x^2 \) for all \( x \in \mathbb{R} \) ‚Äòis‚Äô the set \(\{(x,y) \in \mathbb{R} \times \mathbb{R} \mid y = x^2\}\).

For numbers, however, the answer is not quite so simple. For example, what set should the number number 3 be? And does it matter if we consider 3 to be a natural number or a real number?

This section presents one of many possible ways of encoding numbers‚Äîthat is, natural numbers, integers, rational numbers, real numbers and complex numbers‚Äîas sets.

#### The natural numbers

We can use the framework provided by Zermelo‚ÄìFraenkel set theory (Section B.1) to provide set theoretic constructions of the number sets \(\mathbb{N}, \mathbb{Z}, \mathbb{Q}, \mathbb{R}\) and \(\mathbb{C}\). Indeed, if we want to reason about mathematics within the confines of ZF, we must encode everything (including numbers) as sets!

We will begin with a set theoretic construction of the natural numbers‚Äîthat is, we will construct a notion of natural numbers in the sense of Definition 4.1.1. We will encode the natural numbers as sets, called **von Neumann natural numbers**. We will identify the natural number 0 with the empty set \(\emptyset\), and we will identify the successor operation \(s\) with an operation involving sets.

> **Definition B.2.1**  
> A **von Neumann natural number** is any set obtainable from \(\emptyset\) by repeatedly taking successor sets (see Definition B.1.10). Write \(0_{\mathbb{N}} = \emptyset\) and \((n+1)_{\mathbb{N}} = (n_{\mathbb{N}})^+\); that is
> 
> \[
> 0_{\mathbb{N}} = \emptyset, \quad 1_{\mathbb{N}} = \emptyset^+, \quad 2_{\mathbb{N}} = \emptyset^{++}, \quad 3_{\mathbb{N}} = \emptyset^{+++}, \quad 4_{\mathbb{N}} = \emptyset^{++++}, \quad \ldots
> \]

> **Example B.2.2**  
> The first three von Neumann natural numbers are:
</markdown><markdown>
- \(0_{\mathbb{N}} = \emptyset\);
- \(1_{\mathbb{N}} = \emptyset^+ = \emptyset \cup \{\emptyset\} = \{\emptyset\}\);
- \(2_{\mathbb{N}} = \emptyset^{++} = \{\emptyset\}^+ = \{\emptyset\} \cup \{\{\emptyset\}\} = \{\emptyset, \{\emptyset\}\}\).

### Exercise B.2.3
Write out the elements of \(3_{\mathbb{N}} (= \emptyset^{+++})\) and of \(4_{\mathbb{N}}\).

### Exercise B.2.4
Recall the definition of von Neumann natural numbers from Definition B.2.1. Prove that \(|n_{\mathbb{N}}| = n\) for all \(n \in \mathbb{N}\).

### Construction B.2.5
We construct the set \(\mathbb{N}_{\mathbb{N}}\) of all von Neumann natural numbers as follows. Let \(X\) be an arbitrary set satisfying the axiom of infinity (Axiom B.1.12), and then define \(\mathbb{N}_{\mathbb{N}}\) to be the intersection of all subsets of \(X\) that also satisfy the axiom of infinity‚Äîthat is:

\[
\mathbb{N}_{\mathbb{N}} = \{x \in X \mid \forall U \in \mathcal{P}(X), [U \text{ satisfies the axiom of infinity} \Rightarrow x \in U]\}
\]

The existence of \(\mathbb{N}_{\mathbb{N}}\) follows from the axioms of power set (Axiom B.1.9) and separation (Axiom B.1.14).

### Theorem B.2.6
The set \(\mathbb{N}_{\mathbb{N}}\), zero element \(0_{\mathbb{N}}\) and successor function \(s : \mathbb{N}_{\mathbb{N}} \to \mathbb{N}_{\mathbb{N}}\) defined by \(s(n_{\mathbb{N}}) = n_{\mathbb{N}}^+\) for all \(n_{\mathbb{N}} \in \mathbb{N}_{\mathbb{N}}\), define a notion of natural numbers.

#### Proof
We must verify Peano‚Äôs axioms, which are conditions (i)‚Äì(iii) of Definition 4.1.1.

To prove (i), observe that for all sets \(X\) we have \(X^+ = X \cup \{X\}\), so that \(X \subset X^+\). In particular, we have \(n_{\mathbb{N}} \in n_{\mathbb{N}}^+\) for all \(n_{\mathbb{N}} \in \mathbb{N}_{\mathbb{N}}\), and hence \(n_{\mathbb{N}}^+ \neq \emptyset = 0_{\mathbb{N}}\).

For (ii), let \(m_{\mathbb{N}}, n_{\mathbb{N}} \in \mathbb{N}_{\mathbb{N}}\) and assume that \(m_{\mathbb{N}}^+ = n_{\mathbb{N}}^+\). Then \(m_{\mathbb{N}} = n_{\mathbb{N}}\) by Lemma B.1.11.

For (iii), let \(X\) be a set and suppose that \(0_{\mathbb{N}} \in X\) and, for all \(n_{\mathbb{N}} \in \mathbb{N}_{\mathbb{N}}\), if \(n_{\mathbb{N}} \in X\), then \(n_{\mathbb{N}}^+ \in X\). Then \(X\) satisfies the axiom of infinity (Axiom B.1.12), and so by Construction B.2.5 we have \(\mathbb{N}_{\mathbb{N}} \subseteq X\).

In light of Theorem B.2.6, we may declare ‚Äòthe natural numbers‚Äô to be the von Neumann natural numbers, and have done with it. As such, you can‚Äîif you want‚Äîthink of
</markdown><markdown>
all natural numbers in these notes as *being* their corresponding von Neumann natural number. With this in mind, we now omit the subscript ‚ÄòvN‚Äô, leaving implicit the fact that we are referring to von Neumann natural numbers.

However, there are many other possible notions of natural numbers. In [Theorem B.2.8](#), we prove that any two notions of natural numbers are essentially the same, and so the specifics of how we actually define \(\mathbb{N}\), the zero element and successor operation, are irrelevant for most purposes.

First we will prove the following handy lemma, which provides a convenient means of proving when a function is the identity function ([Definition 3.1.13](#)).

### Lemma B.2.7
Let \((\mathbb{N}, z, s)\) be a notion of natural numbers, and let \(j : \mathbb{N} \to \mathbb{N}\) be a function such that \(j(z) = 0\) and \(j(s(n)) = s(j(n))\) for all \(n \in \mathbb{N}\). Then \(j = \text{id}_{\mathbb{N}}\).

**Proof.** By [Theorem 4.1.2](#), there is a unique function \(i : \mathbb{N} \to \mathbb{N}\) such that \(i(z) = 0\) and \(i(s(n)) = s(i(n))\) for all \(n \in \mathbb{N}\). But then:

- \(j = i\) by uniqueness of \(i\), since \(j\) satisfies the same conditions as \(i\); and
- \(\text{id}_{\mathbb{N}} = i\) by uniqueness of \(i\), since \(\text{id}_{\mathbb{N}}(z) = z\) and \(\text{id}_{\mathbb{N}}(s(n)) = s(n) = s(\text{id}_{\mathbb{N}}(n))\) for all \(n \in \mathbb{N}\).

Hence \(j = \text{id}_{\mathbb{N}}\), as required.

### Theorem B.2.8
Any two notions of natural numbers are essentially the same, in a very strong sense. More precisely, if \((\mathbb{N}_1, z_1, s_1)\) and \((\mathbb{N}_2, z_2, s_2)\) are notions of natural numbers, then there is a unique bijection \(f : \mathbb{N}_1 \to \mathbb{N}_2\) such that \(f(z_1) = z_2\) and \(f(s_1(n)) = s_2(f(n))\) for all \(n \in \mathbb{N}_1\).

**Proof.** By applying the recursion theorem ([Theorem 4.1.2](#)) for \((\mathbb{N}_1, z_1, s_1)\), with \(X = \mathbb{N}_2, a = z_2\) and \(h : \mathbb{N}_1 \times \mathbb{N}_2 \to \mathbb{N}_2\) defined by \(h(m, n) = s_2(n)\) for all \(m \in \mathbb{N}_1\) and \(n \in \mathbb{N}_2\), we obtain a function \(f : \mathbb{N}_1 \to \mathbb{N}_2\) such that \(f(z_1) = z_2\) and \(f(s_1(n)) = s_2(f(n))\) for all \(n \in \mathbb{N}_1\). This also gives us uniqueness of \(f\), so it remains only to prove that \(f\) is a bijection.

Likewise, by applying [Theorem 4.1.2](#) to \((\mathbb{N}_2, z_2, s_2)\), with \(X = \mathbb{N}_1, a = z_1\) and \(h : \mathbb{N}_2 \times \mathbb{N}_1 \to \mathbb{N}_1\) defined by \(h(m, n) = s_1(n)\) for all \(m \in \mathbb{N}_2\) and \(n \in \mathbb{N}_1\), we obtain a (unique!) function \(g : \mathbb{N}_2 \to \mathbb{N}_1\) such that \(g(z_2) = z_1\) and \(g(s_2(n)) = s_1(g(n))\) for all \(n \in \mathbb{N}_2\).

But then \(g(f(z_1)) = g(z_2) = z_1\) and, for all \(n \in \mathbb{N}_1\), we have
\[
g(f(s_1(n))) = g(s_2(f(n))) = s_2(g(f(n)))
\]
</markdown><markdown>
and so \( g \circ f = \text{id}_{\mathbb{N}_1} \) by Lemma B.2.7. Likewise \( f \circ g = \text{id}_{\mathbb{N}_2} \). Hence \( g \) is an inverse for \( f \), so that \( f \) is a bijection, as required.

With this settled, we will now simply work with a fixed notion of natural numbers \((\mathbb{N}, z, s)\), and not worry too much about how it is constructed‚Äîif you like, take \(\mathbb{N} = \mathbb{N}_{\mathbb{N}}\), \( z = \emptyset \) and \( s(n) = n^+ = n \cup \{ n \} \) for all \( n \in \mathbb{N} \).

The arithmetic operations that we know and love‚Äîaddition, multiplication and exponentiation‚Äîare defined using recursion in Section 4.1; so are factorials and binomial coefficients.

We can define more familiar notions involving natural numbers. For example, we can define the relation \(\leq\) on \(\mathbb{N}\) by defining \( m \leq n \) to mean \(\exists k \in \mathbb{N}, m + k = n\). An alternative approach would be to define ‚Äò\( m \leq n \)‚Äô for \( m, n \in \mathbb{N} \) by an iterated recursion on \( m \) and \( n \)‚Äîspecifically:

- \( 0 \leq 0 \) is true;
- For all \( m \in \mathbb{N}, m + 1 \leq 0 \) is false;
- For all \( n \in \mathbb{N}, 0 \leq n + 1 \) is true; and
- For all \( m, n \in \mathbb{N}, m + 1 \leq n + 1 \) is true if and only if \( m \leq n \) is true.

**Exercise B.2.9**  
Prove that the recursive definition of \(\leq\) is equivalent to the definition of ‚Äò\( m \leq n \)‚Äô as ‚Äò\(\exists k \in \mathbb{N}, m + k = n\)‚Äô.

### Integers

Now that we have constructed the natural numbers, it is time to construct the integers. The intuition behind the definition we are about to give is that an integer tells us the difference between two natural numbers. Thus ‚Äò3 as an integer‚Äô refers to increase of 3 from one natural number to another (say 0 to 3, or 7 to 10); and ‚Äò‚àí4 as an integer‚Äô refers to a decrease of 4 from one natural number to another (say 4 to 0, or 10 to 6).

A first attempt might be to define \(\mathbb{Z}\) to be \(\mathbb{N} \times \mathbb{N}\), and interpret \((a, b) \in \mathbb{N} \times \mathbb{N}\) to represent the integer \( b - a \). Unfortunately this doesn‚Äôt quite work, since for instance the integer 3 would be represented as \((0, 3)\) and as \((7, 10)\)‚Äîand, more generally, as \((n, n + 3)\) for all \( n \in \mathbb{N} \).

So instead, we will declare two pairs \((a, b), (c, d) \in \mathbb{N} \times \mathbb{N}\) to represent the same integer whenever \( b - a = d - c \). To make this formal, a couple of things need to happen:
</markdown><markdown>
(1) We need to make precise what we mean by ‚Äòdeclare two pairs to represent the same integer‚Äô‚Äîthis will be done by defining an equivalence relation on \( \mathbb{N} \times \mathbb{N} \) and then passing to the quotient (see Section 5.2).

(2) We cannot use subtraction in our definition of the equivalence relation, or in our proof that it is an equivalence relation, since we have not yet defined a notion of subtraction for \( \mathbb{N} \).

Fortunately for us, (2) is easy to resolve: we can state the equation \( b - a = d - c \) for \( m, n \in \mathbb{N} \) equivalently as \( a + d = b + c \); this is the equation that we will use to define the equivalence relation on \( \mathbb{N} \times \mathbb{N} \) in (1).

### Lemma B.2.10
The relation \( \sim \) on \( \mathbb{N} \times \mathbb{N} \), defined for \( (a, b), (c, d) \in \mathbb{N} \times \mathbb{N} \) by \( (a, b) \sim (c, d) \) if and only if \( a + d = b + c \), is an equivalence relation.

**Proof**

- **(Reflexivity)** Let \( (a, b) \in \mathbb{N} \times \mathbb{N} \). Then \( a + b = b + a \), so that \( (a, b) \sim (a, b) \).

- **(Symmetry)** Let \( (a, b), (c, d) \in \mathbb{N} \times \mathbb{N} \) and assume that \( (a, b) \sim (c, d) \). Then \( a + d = b + c \) so \( c + b = d + a \), and so \( (c, d) \sim (a, b) \).

- **(Transitivity)** Let \( (a, b), (c, d), (e, f) \in \mathbb{N} \times \mathbb{N} \) and assume that \( (a, b) \sim (c, d) \) and \( (c, d) \sim (e, f) \). Then \( a + d = b + c \) and \( c + f = d + e \). But then

  \[
  (a + f) + (c + d) = (a + d) + (c + f) = (b + c) + (d + e) = (b + e) + (c + d)
  \]

  so cancelling \( c + d \) from both sides gives \( a + f = b + e \), and so \( (a, b) \sim (e, f) \), as required.

We should be careful to note that we did not assume the existence of a subtraction operation when we cancelled \( c + d \) from both sides of the equation: the fact that \( u + w = v + w \) implies \( u = v \) for all \( u, v, w \in \mathbb{N} \) can be proved by induction on \( w \) using only Peano‚Äôs axioms.

Now that we have an equivalence relation on \( \mathbb{N} \times \mathbb{N} \), we can take \( \mathbb{Z} \) to be the resulting set of equivalence classes (that is, the quotient).

### Construction B.2.11
The set of integers is the set \( \mathbb{Z} \) defined by

\[
\mathbb{Z} = (\mathbb{N} \times \mathbb{N}) / \sim
\]

where \( \sim \) is the equivalence relation on \( \mathbb{N} \times \mathbb{N} \) defined by

\[
(a, b) \sim (c, d) \text{ if and only if } a + d = b + c
\]

for all \( (a, b), (c, d) \in \mathbb{N} \times \mathbb{N} \).
</markdown><markdown>
We interpret the element \([(a,b)]_\sim \in \mathbb{Z}\) to be the integer \(b-a\). So for example we have

\[ 3 = [(0,3)]_\sim = [(7,10)]_\sim \quad \text{and} \quad -4 = [(4,0)]_\sim = [(10,6)]_\sim \]

A couple of remarks are now in order.

The first remark is that we can define addition and multiplication operations on \(\mathbb{Z}\) using those on \(\mathbb{N}\). Formally, we define:

- \([(a,b)]_\sim + [(c,d)]_\sim = [(a+c,b+d)]_\sim\) ‚Äî the intuition here is that for all \(a,b,c,d \in \mathbb{N}\) we have \((b-a) + (d-c) = (b+d) - (a+c)\).

- \([(a,b)]_\sim \cdot [(c,d)]_\sim = [(ad+bc,ac+bd)]_\sim\) ‚Äî the intuition here is that for all \(a,b,c,d \in \mathbb{N}\) we have \((b-a)(d-c) = (ac+bd) - (ad+bc)\).

Since the operations \(+\) and \(\cdot\) are defined in terms of representatives of equivalence classes, we should (but won‚Äôt) check that these operations are well-defined‚Äîfor example, we need to verify that if \([(a,b)]_\sim = [(a',b')]\_\sim\) and \([(c,d)]_\sim = [(c',d')]\_\sim\), then \([(a+c,b+d)]_\sim = [(a'+c',b'+d')]\_\sim\).

What makes the integers special is that we can also negate them, which in turn allows us to subtract them. With this in mind, we may define negation and subtraction operations on \(\mathbb{Z}\) as follows:

- \(-[(a,b)]_\sim = [(b,a)]_\sim\) ‚Äî the intuition here is that for all \(a,b \in \mathbb{N}\) we have \(-(b-a) = a-b\).

- \([(a,b)]_\sim - [(c,d)]_\sim = [(a+d,b+c)]_\sim\) ‚Äî the intuition here is that for all \(a,b,c,d \in \mathbb{N}\) we have \((b-a) - (d-c) = (b+c) - (a+d)\).

The negation and subtraction operations may be defined in terms of one another: they are related by the identities

\[-[(a,b)]_\sim = [(0,0)]_\sim - [(a,b)]_\sim \quad \text{and} \quad [(a,b)]_\sim - [(c,d)]_\sim = [(a,b)]_\sim + ( -[(c,d)]_\sim )\]

Again, we should but will not prove that the negation and subtraction operations are well-defined.

The second and perhaps most alarming remark is that this construction of \(\mathbb{Z}\) means that we do not have \(\mathbb{N} \subseteq \mathbb{Z}\): the elements of \(\mathbb{Z}\) are equivalence classes of pairs of natural numbers, meaning that the natural number ‚Äò3‚Äô is not equal to the integer ‚Äò3‚Äô (which is really equal to \([(0,3)]_\sim\)). However, we will regard \(\mathbb{N}\) as being a subset of \(\mathbb{Z}\) by identifying each natural number \(n\) with the element \([(0,n)]_\sim \in \mathbb{Z}\). What justifies this identification is the following exercise.
</markdown><markdown>
### Exercise B.2.12

Prove that every element of \(\mathbb{Z}\), as defined in Construction B.2.11, is equal to exactly one of the following: \([(0,0)]_\sim\), or \([(0,n)]_\sim\) for some \(n > 0\), or \([(n,0)]_\sim\) for some \(n > 0\).

Exercise B.2.12 implies that the ‚Äòinclusion‚Äô function \(i : \mathbb{N} \to \mathbb{Z}\) defined by \(i(n) = [(0,n)]_\sim\) for all \(n \in \mathbb{N}\) is injective. When \(n \in \mathbb{N}\), we will usually just write ‚Äò\(n\)‚Äô to denote the integer \(i(n) = [(0,n)]_\sim\).

Note that \(i(m+n) = i(m) + i(n)\) and \(i(mn) = i(m)i(n)\), so when we write something like ‚Äò\(m+n\)‚Äô for \(m,n \in \mathbb{N}\), it doesn‚Äôt matter whether we‚Äôre first adding \(m\) and \(n\) as natural numbers and then interpreting the result as an integer, or first interpreting \(m\) and \(n\) as integers and then adding the result using the addition operation for integers.

Note also that the identification of \(n \in \mathbb{N}\) with \(i(n) = [(0,n)]_\sim \in \mathbb{Z}\) gives \(-n = [(n,0)]_\sim\) for all \(n \in \mathbb{N}\), and so Exercise B.2.12 implies that every integer is equal to exactly one of \(0\), \(n\) or \(-n\) for some positive natural number \(n\).

The next result proves that \(\mathbb{Z}\) is a ring‚Äîwhat this means in essence is that the operations of addition, multiplication and negation satisfy the basic properties that we take for granted when doing arithmetic with integers.

### Theorem B.2.13 (\(\mathbb{Z}\) is a ring)

(a) **(Associativity of addition)** \((a+b)+c = a+(b+c)\) for all \(a,b,c \in \mathbb{Z}\).

(b) **(Commutativity of addition)** \(a+b = b+a\) for all \(a,b \in \mathbb{Z}\).

(c) **(Zero)** \(a+0 = a\) for all \(a \in \mathbb{Z}\).

(d) **(Negation)** \(a+(-a) = 0\) for all \(a \in \mathbb{Z}\).

(e) **(Associativity of multiplication)** \((a \cdot b) \cdot c = a \cdot (b \cdot c)\) for all \(a,b,c \in \mathbb{Z}\).

(f) **(Commutativity of multiplication)** \(a \cdot b = b \cdot a\) for all \(a,b \in \mathbb{Z}\).

(g) **(One)** \(a \cdot 1 = a\) for all \(a \in \mathbb{Z}\).

(h) **(Distributivity)** \(a \cdot (b+c) = (a \cdot b) + (a \cdot c)\) for all \(a,b,c \in \mathbb{Z}\).

**Proof**

Many pages would be wasted by writing out the proof of this theorem in full detail. Instead we will just prove part (h); any particularly masochistic readers are invited to prove parts (a)‚Äì(g) on their own.

To see that the distributivity law holds, let \(a,b,c \in \mathbb{Z}\), and let \(m,n,p,q,r,s \in \mathbb{N}\) be such that \(a = [(m,n)]_\sim\), \(b = [(p,q)]_\sim\) and \(c = [(r,s)]_\sim\). (We will omit the subscript \(\sim\) in what follows.)
</markdown><markdown>
Then:

\[ 
a \cdot (b + c) 
\]
\[
= [(m, n)] \cdot ([(p, q)] + [(r, s)]) 
\]
\[
= [(m, n)] \cdot [(p \cdot r, q + s)] 
\]
\[
= [(m(q + s) + n(p + r), m(p + r) + n(q + s))] 
\]
\[
= [(mq + ms + np + nr, mp + mr + nq + ns)] 
\]
\[
= [(mq + np, mp + nq)] + [(ms + nr, mr + ns)] 
\]
\[
= [(m, n)] \cdot [(p, q)] + [(m, n)] \cdot [(r, s)] 
\]
\[
= (a \cdot b) + (a \cdot c) 
\]

as required.

## Rational numbers

Just as we constructed the integers from the natural numbers by formalising what we mean by the ‚Äòdifference‚Äô of two natural numbers, we will construct the rational numbers from the integers by formalising what we mean by the ‚Äòratio‚Äô of two integers.

Since rational numbers should take the form \(\frac{a}{b}\) with \(a, b \in \mathbb{Z}\) and \(b \neq 0\), a first attempt at constructing \(\mathbb{Q}\) might be to take \(\mathbb{Q} = \mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\), and letting the pair \((a, b)\) represent the fraction \(\frac{a}{b}\). But just as an integer cannot be expressed uniquely as the difference of two natural numbers, a rational number cannot be expressed uniquely as the ratio of two integers‚Äîfor example, we have \(\frac{1}{2} = \frac{2}{4}\).

This means that we must identify \((a, b) \in \mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\) with \((c, d) \in \mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\) whenever \(\frac{a}{b} = \frac{c}{d}\). But wait! We have not yet defined a division operation, so we cannot use it in our construction of \(\mathbb{Q}\)‚Äîso instead, we will identify \((a, b)\) with \((c, d)\) whenever \(ad = bc\), noting that this will be equivalent to \(\frac{a}{b} = \frac{c}{d}\) once we have defined a division operation.

### Lemma B.2.14

The relation \(\sim\) on \(\mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\), defined for \((a, b), (c, d) \in \mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\) by letting \((a, b) \sim (c, d)\) if and only if \(ad = bc\), is an equivalence relation.

**Sketch of proof**

The proof is essentially the same as that of Lemma B.2.10, but with addition replaced by multiplication; the proof of transitivity uses the fact that for \(u, v, w \in \mathbb{Z}\) with \(u \neq 0\), we have \(uv = uw \Rightarrow v = w\), which can be proved by induction on \(|u| \geq 1\) without using a division operation.
</markdown><markdown>
### Construction B.2.15

The set of **rational numbers** is the set \(\mathbb{Q}\) defined by

\[
\mathbb{Q} = (\mathbb{Z} \times (\mathbb{Z} \setminus \{0\}))/\sim
\]

where \(\sim\) is the equivalence relation on \(\mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\) defined by

\[
(a, b) \sim (c, d) \text{ if and only if } ad = bc
\]

for all \((a, b), (c, d) \in \mathbb{Z} \times (\mathbb{Z} \setminus \{0\})\).

We will write \(\frac{a}{b}\) to denote the element \([(a, b)]_\sim \in \mathbb{Q}\), noting that \(\frac{a}{b} = \frac{c}{d}\) if and only if \(ad = bc\). We can now define addition, multiplication, negation and subtraction operations on \(\mathbb{Q}\) in terms of those on \(\mathbb{Z}\)‚Äînamely, given \(a, b, c, d \in \mathbb{Z}\) with \(b, d \neq 0\), we define

\[
\frac{a}{b} + \frac{c}{d} = \frac{ad + bc}{bd}, \quad \frac{a}{b} \cdot \frac{c}{d} = \frac{ac}{bd}, \quad -\frac{a}{b} = \frac{-a}{b}, \quad \frac{a}{b} - \frac{c}{d} = \frac{ad - bc}{bd}
\]

As always, we should (but won‚Äôt) verify that these operations are well-defined.

Again, we do not have \(\mathbb{Z} \subseteq \mathbb{Q}\), but we can embed \(\mathbb{Z}\) into \(\mathbb{Q}\) in a way that respects the arithmetic operations of addition, multiplication, negation and subtraction.

#### Exercise B.2.16

Prove that the function \(i : \mathbb{Z} \to \mathbb{Q}\) defined by \(i(a) = \frac{a}{1}\) is an injection, and for all \(a, b \in \mathbb{Z}\) we have \(i(a + b) = i(a) + i(b)\), \(i(ab) = i(a)i(b)\), \(i(-a) = -i(a)\) and \(i(a - b) = i(a) - i(b)\).

In light of this, given \(a \in \mathbb{Z}\), we will typically just write ‚Äò\(a\)‚Äô for the rational number \(i(a) = \frac{a}{1} \in \mathbb{Q}\), so we may pretend that \(\mathbb{Z} \subseteq \mathbb{Q}\) even though this is not strictly the case.

The construction of \(\mathbb{Q}\) also allows for reciprocal and division operations to be defined. Note that \(\frac{c}{d} = 0\) if and only if \(c = c \cdot 1 = 0 \cdot d = 0\); so if \(a, b, c, d \in \mathbb{Q}\) with \(b, c, d \neq 0\), then we define

\[
\left(\frac{c}{d}\right)^{-1} = \frac{d}{c} \quad \text{and} \quad \frac{a}{b} \div \frac{c}{d} = \frac{ad}{bc}
\]

The usual comment about well-definedness applies. Note that for all \(a, b \in \mathbb{Z}\) with \(b \neq 0\) we have

\[
a \div b = \frac{a}{1} \div \frac{b}{1} = \frac{a \cdot 1}{1 \cdot b} = \frac{a}{b}
\]

So we will dispense with the ‚Äò\(\div\)‚Äô symbol and simply use the fraction notation.
</markdown><markdown>
Note also that the reciprocal and division operations may be defined in terms of one another: they are related by the identities

\[ y^{-1} = \frac{1}{y} \quad \text{and} \quad \frac{x}{y} = x \cdot y^{-1} \]

for all \( x, y \in \mathbb{Q} \) with \( y \neq 0 \).

**To do:** \(\mathbb{Q}\) is a field

## Real numbers

**To do:**

‚ú¶ **Definition B.2.17 (Dedekind‚Äôs construction of the real numbers)**  
The set of (Dedekind) real numbers is the set \(\mathbb{R}\) defined by

\[
\mathbb{R} = \{ D \subseteq \mathbb{Q} \mid D \text{ is bounded above and downwards-closed} \}
\]

**To do:** Arithmetic operations, order

**To do:** Motivate Cauchy reals

‚ú¶ **Definition B.2.18 (Cauchy‚Äôs construction of the real numbers)**  
The set of (Cauchy) real numbers is the set \(\mathbb{R}\) defined by

\[
\mathbb{R} = \{ (x_n) \in \mathbb{Q}^{\mathbb{N}} \mid (x_n) \text{ is Cauchy} \} / \sim
\]

where \(\sim\) is the equivalence relation defined by

\[
(x_n) \sim (y_n) \text{ if and only if } (x_n - y_n) \to 0
\]

for all Cauchy sequences \((x_n), (y_n)\) of rational numbers.

**To do:** Arithmetic operations, order

**To do:** Motivate definition of complex numbers

‚ú¶ **Definition B.2.19**  
The set of complex numbers is the set \(\mathbb{C} = \mathbb{R} \times \mathbb{R}\).

**To do:** Arithmetic operations
</markdown><markdown>
## Algebraic structures

To do: Monoids, groups, rings

## Axiomatising the real numbers

To do:

### Axioms B.2.20 (Field axioms)

Let \( X \) be a set equipped with elements 0 (‚Äòzero‚Äô) and 1 (‚Äòunit‚Äô), and binary operations \( + \) (‚Äòaddition‚Äô) and \( \cdot \) (‚Äòmultiplication‚Äô). The structure \( (X, 0, 1, +, \cdot) \) is a **field** if it satisfies the following axioms:

- **Zero and unit**

  (F1) \( 0 \neq 1 \).

- **Axioms for addition**

  (F2) (Associativity) \( x + (y + z) = (x + y) + z \) for all \( x, y, z \in X \).

  (F3) (Identity) \( x + 0 = x \) for all \( x \in X \).

  (F4) (Inverse) For all \( x \in X \), there exists \( y \in X \) such that \( x + y = 0 \).

  (F5) (Commutativity) \( x + y = y + x \) for all \( x, y \in X \).

- **Axioms for multiplication**

  (F6) (Associativity) \( x \cdot (y \cdot z) = (x \cdot y) \cdot z \) for all \( x, y, z \in X \).

  (F7) (Identity) \( x \cdot 1 = x \) for all \( x \in X \).

  (F8) (Inverse) For all \( x \in X \) with \( x \neq 0 \), there exists \( y \in X \) such that \( x \cdot y = 1 \).

  (F9) (Commutativity) \( x \cdot y = y \cdot x \) for all \( x, y \in X \).

- **Distributivity**

  (F10) \( x \cdot (y + z) = (x \cdot y) + (x \cdot z) \) for all \( x, y, z \in X \).

### Example B.2.21

The rationals \( \mathbb{Q} \) and the reals \( \mathbb{R} \) both form fields with their usual notions of zero, unit, addition and multiplication. However, the integers \( \mathbb{Z} \) do not, since for example 2 has no multiplicative inverse.

### Example B.2.22

Let \( p > 0 \) be prime. The set \( \mathbb{Z}/p\mathbb{Z} \) (see Definition 5.2.18) is a field, with zero element.
</markdown><markdown>
\[ [0]_p \] and unit element \([1]_p\), and with addition and multiplication defined by

\[
[a]_p + [b]_p = [a + b]_p \quad \text{and} \quad [a]_p \cdot [b]_p = [ab]_p
\]

for all \(a, b \in \mathbb{Z}\). Well-definedness of these operations is immediate from Theorem 5.2.11 and the modular arithmetic theorem (Theorem 7.3.3).

The only axiom which is not easy to verify is the multiplicative inverse axiom (F8). Indeed, if \([a]_p \in \mathbb{Z}/p\mathbb{Z}\) then \([a]_p \neq [0]_p\) if and only if \(p \nmid a\). But if \(p \nmid a\) then \(a \perp p\), so \(a\) has a multiplicative inverse \(u\) modulo \(p\). This implies that \([a]_p \cdot [u]_p = [au]_p = [1]_p\). So (F8) holds.

‚úé **Exercise B.2.23**  
Let \(n > 0\) be composite. Prove that \(\mathbb{Z}/n\mathbb{Z}\) is not a field, where zero, unit, addition and multiplication are defined as in Example B.2.22.

Axioms B.2.20 tell us that every element of a field has an additive inverse, and every nonzero element of a field has a multiplicative inverse. It would be convenient if inverses were unique whenever they exist. Proposition B.2.24 proves that this is the case.

‚ú£ **Proposition B.2.24 (Uniqueness of inverses)**  
Let \((X, 0, 1, +, \cdot)\) be a field and let \(x \in X\). Then

(a) Suppose \(y, z \in X\) are such that \(x + y = 0\) and \(x + z = 0\). Then \(y = z\).

(b) Suppose \(x \neq 0\) and \(y, z \in X\) are such that \(x \cdot y = 1\) and \(x \cdot z = 1\). Then \(y = z\).

**Proof of (a)**  
By calculation, we have

\[
\begin{align*}
y &= y + 0 & \text{by (F3)} \\
  &= y + (x + z) & \text{by definition of } z \\
  &= (y + x) + z & \text{by associativity (F2)} \\
  &= (x + y) + z & \text{by commutativity (F5)} \\
  &= 0 + z & \text{by definition of } y \\
  &= z + 0 & \text{by commutativity (F5)} \\
  &= z & \text{by (F3)}
\end{align*}
\]

so indeed \(y = z\).

The proof of (b) is essentially the same and is left as an exercise.

Since inverses are unique, it makes sense to have notation to refer to them.

‚ú¶ **Notation B.2.25**
</markdown><markdown>
Let \((X, 0, 1, +, \cdot)\) be a field and let \(x \in X\). Write \(-x\) for the (unique) additive inverse of \(x\) and, if \(x \neq 0\) write \(x^{-1}\) for the (unique) multiplicative inverse of \(x\).

### Example B.2.26
In the fields \(\mathbb{Q}\) and \(\mathbb{R}\), the additive inverse \(-x\) of an element \(x\) is simply its negative, and the multiplicative inverse \(x^{-1}\) of some \(x \neq 0\) is simply its reciprocal \(\frac{1}{x}\).

### Example B.2.27
Let \(p > 0\) be prime and let \([a]_p \in \mathbb{Z}/p\mathbb{Z}\). Then \(-[a]_p = [-a]_p\) and, if \(p \nmid a\), then \([a]_p^{-1} = [u]_p\), where \(u\) is any integer satisfying \(au \equiv 1 \mod p\).

### Exercise B.2.28
Let \((X, 0, 1, +, \cdot)\) be a field. Prove that \(-(-x) = x\) for all \(x \in X\), and that \((x^{-1})^{-1} = x\) for all nonzero \(x \in X\).

### Example B.2.29
Let \((X, 0, 1, +, \cdot)\) be a field. We prove that if \(x \in X\) then \(x \cdot 0 = 0\). Well, \(0 = 0 + 0\) by (F3). Hence \(x \cdot 0 = x \cdot (0 + 0)\). By distributivity (F10), we have \(x \cdot (0 + 0) = (x \cdot 0) + (x \cdot 0)\). Hence

\[
x \cdot 0 = (x \cdot 0) + (x \cdot 0)
\]

Let \(y = -(x \cdot 0)\). Then

\[
\begin{align*}
0 &= x \cdot 0 + y & \text{by (F4)} \\
  &= ((x \cdot 0) + (x \cdot 0)) + y & \text{as above} \\
  &= (x \cdot 0) + ((x \cdot 0) + y) & \text{by associativity (F2)} \\
  &= (x \cdot 0) + 0 & \text{by (F4)} \\
  &= x \cdot 0 & \text{by (F3)}
\end{align*}
\]

so indeed we have \(x \cdot 0 = 0\).

### Exercise B.2.30
Let \((X, 0, 1, +, \cdot)\) be a field. Prove that \((-1) \cdot x = -x\) for all \(x \in X\), and that \((-x)^{-1} = -(x^{-1})\) for all nonzero \(x \in X\).

What makes the real numbers useful is not simply our ability to add, subtract, multiply and divide them; we can also compare their size‚Äîindeed, this is what gives rise to the informal notion of a *number line*. [Axioms B.2.31](#) make precise exactly what it means for the elements of a field to be assembled into a ‚Äònumber line‚Äô.
</markdown><markdown>
### Axioms B.2.31 (Ordered field axioms)

Let \( X \) be a set, \( 0, 1 \in X \) be elements, \( +, \cdot \) be binary operations, and \(\leq\) be a relation on \( X \). The structure \((X, 0, 1, +, \cdot, \leq)\) is an **ordered field** if it satisfies the field axioms (F1)‚Äì(F10) (see Axioms B.2.20) and, additionally, it satisfies the following axioms:

- **Linear order axioms**

  (PO1) (Reflexivity) \( x \leq x \) for all \( x \in X \).

  (PO2) (Antisymmetry) For all \( x, y \in X \), if \( x \leq y \) and \( y \leq x \), then \( x = y \).

  (PO3) (Transitivity) For all \( x, y, z \in X \), if \( x \leq y \) and \( y \leq z \), then \( x \leq z \).

  (PO4) (Linearity) For all \( x, y \in X \), either \( x \leq y \) or \( y \leq x \).

- **Interaction of order with arithmetic**

  (OF1) For all \( x, y, z \in X \), if \( x \leq y \), then \( x + z \leq y + z \).

  (OF2) For all \( x, y \in X \), if \( 0 \leq x \) and \( 0 \leq y \), then \( 0 \leq xy \).

### Example B.2.32

The field \(\mathbb{Q}\) of rational numbers and the field \(\mathbb{R}\) of real numbers, with their usual notions of ordering, can easily be seen to form ordered fields.

### Example B.2.33

We prove that, in any ordered field, we have \( 0 \leq 1 \). Note first that either \( 0 \leq 1 \) or \( 1 \leq 0 \) by linearity (PO4). If \( 0 \leq 1 \) then we‚Äôre done, so suppose \( 1 \leq 0 \). Then \( 0 \leq -1 \); indeed:

\[
\begin{align*}
0 &= 1 + (-1) & \text{by (F4)} \\
&\leq 0 + (-1) & \text{by (OF1), since } 1 \leq 0 \\
&= (-1) + 0 & \text{by commutativity (F5)} \\
&= -1 & \text{by (F3)}
\end{align*}
\]

By (OF2), it follows that \( 0 \leq (-1)(-1) \). But \((-1)(-1) = 1\) by Exercise B.2.30, and hence \( 0 \leq 1 \). Since \( 1 \leq 0 \) and \( 0 \leq 1 \), we have \( 0 = 1 \) by antisymmetry (PO2). But this contradicts axiom (F1). Hence \( 0 \leq 1 \). In fact, \( 0 < 1 \) since \( 0 \neq 1 \).

We have seen that \(\mathbb{Q}\) and \(\mathbb{R}\) are ordered fields (Examples B.2.26 and B.2.32), and that \(\mathbb{Z}/p\mathbb{Z}\) is a field for \( p > 0 \) prime (Example B.2.22). The following proposition is an interesting result proving that there is no notion of ‚Äòordering‚Äô under which the field \(\mathbb{Z}/p\mathbb{Z}\) can be made into an ordered field!

### Proposition B.2.34

Let \( p > 0 \) be prime. There is no relation \(\leq\) on \(\mathbb{Z}/p\mathbb{Z}\) which satisfies the ordered field axioms.

**Proof**

We just showed that \([0] \leq [1]\). It follows that, for all \( a \in \mathbb{Z} \), we have \([a] \leq [a] + [1]\);
</markdown><markdown>
indeed:

\[
\begin{align*}
[a] &= [a] + [0] & \text{by (F3)} \\
&\leq [a] + [1] & \text{by (OF1), since } [0] \leq [1] \\
&= [a + 1] & \text{by definition of } + \text{ on } \mathbb{Z}/p\mathbb{Z}
\end{align*}
\]

It is a straightforward induction to prove that \([a] \leq [a + n]\) for all \(n \in \mathbb{N}\). But then we have

\[
[1] \leq [1 + (p - 1)] = [p] = [0]
\]

so \([0] \leq [1]\) and \([1] \leq [0]\). This implies \([0] = [1]\) by antisymmetry (PO2), contradicting axiom (F1).

**Exercise B.2.35**  
Let \((X, 0, 1, +, \cdot)\) be a field. Prove that if \(X\) is finite, then there is no relation \(\leq\) on \(X\) such that \((X, 0, 1, +, \cdot, \leq)\) is an ordered field.

**Theorem B.2.36**  
Let \((X, 0, 1, +, \cdot, \leq)\) be an ordered field. Then

(a) For all \(x, y \in X\), \(x \leq y\) if and only if \(0 \leq y - x\);

(b) For all \(x \in X\), \(-x \leq 0 \leq x\) or \(x \leq 0 \leq -x\);

(c) For all \(x, x', y, y' \in X\), if \(x \leq x'\) and \(y \leq y'\), then \(x + y \leq x' + y'\);

(d) For all \(x, y, z \in X\), if \(0 \leq x\) and \(y \leq z\), then \(xy \leq xz\);

(e) For all nonzero \(x \in X\), if \(0 < x\), then \(0 < x^{-1}\).

(f) For all nonzero \(x, y \in X\), if \(x \leq y\), then \(y^{-1} \leq x^{-1}\).

**Proof of (a), (b) and (e)**

(a) (\(\Rightarrow\)) Suppose \(x \leq y\). Then by additivity (OF1), \(x + (-x) \leq y + (-x)\), that is \(0 \leq y - x\). (\(\Leftarrow\)) Suppose \(0 \leq y - x\). By additivity (OF1), \(0 + x \leq (y - x) + x\); that is, \(x \leq y\).

(b) We know by linearity (PO4) that either \(0 \leq x\) or \(x \leq 0\). If \(0 \leq x\), then by (OF1) we have \(0 + (-x) \leq x + (-x)\), that is \(-x \leq 0\). Likewise, if \(x \leq 0\) then \(0 \leq -x\).
</markdown><markdown>
(e) Suppose \(0 \leq x\). By linearity (PO4), either \(0 \leq x^{-1}\) or \(x^{-1} \leq 0\). If \(x^{-1} \leq 0\), then by (d) we have \(x^{-1} \cdot x \leq 0 \cdot x\), that is \(1 \leq 0\). This contradicts Example B.2.33, so we must have \(0 \leq x^{-1}\).

The proofs of the remaining properties are left as an exercise.

We wanted to characterise the reals completely, but so far we have failed to do so‚Äîindeed, Example B.2.32 showed that both \(\mathbb{Q}\) and \(\mathbb{R}\) are ordered fields, so the ordered field axioms do not suffice to distinguish \(\mathbb{Q}\) from \(\mathbb{R}\). The final piece in the puzzle is completeness. This single additional axiom distinguishes \(\mathbb{Q}\) from \(\mathbb{R}\), and in fact completely characterises \(\mathbb{R}\) (see Theorem B.2.38).

### Axioms B.2.37 (Complete ordered field axioms)
Let \(X\) be a set, \(0, 1 \in X\) be elements, \(+, \cdot\) be binary operations, and \(\leq\) be a relation on \(X\). The structure \((X, 0, 1, +, \cdot, \leq)\) is a **complete ordered field** if it is an ordered field‚Äîthat is, it satisfies axioms (F1)‚Äì(F10), (PO1)‚Äì(PO4) and (OF1)‚Äì(OF2) (see Axioms B.2.20 and B.2.31)‚Äîand, in addition, it satisfies the following **completeness axiom**:

(C1) Let \(A \subseteq X\). If \(A\) has an upper bound, then it has a least upper bound. Specifically, if there exists \(u \in X\) such that \(a \leq u\) for all \(a \in A\), then there exists \(s \in X\) such that
- \(a \leq s\) for all \(a \in A\); and
- If \(s' \in X\) is such that \(a \leq s'\) for all \(a \in A\), then \(s \leq s'\).

We call such a value \(s \in X\) a **supremum** for \(A\).

### Theorem B.2.38
The real numbers \((\mathbb{R}, 0, 1, +, \cdot, \leq)\) form a complete ordered field. Moreover, any two complete ordered fields are essentially the same.

The notion of ‚Äòsameness‚Äô alluded to in Theorem B.2.38 is more properly called **isomorphism**. A proof of this theorem is intricate and far beyond the scope of this book, so is omitted. What it tells us is that it doesn‚Äôt matter exactly how we define the reals, since any complete ordered field will do. We can therefore proceed with confidence that, no matter what notion of ‚Äòreal numbers‚Äô we settle on, everything we prove will be true of that notion. This is for the best, since we haven‚Äôt actually defined the set \(\mathbb{R}\) of real numbers at all!

The two most common approaches to constructing a set of real numbers are:

- **Dedekind reals.** In this approach, real numbers are identified with particular subsets
</markdown><markdown>
of \(\mathbb{Q}\)‚Äîinformally speaking, \(r \in \mathbb{R}\) is identified with the set of rational numbers less than \(r\).

- **Cauchy reals.** In this approach, real numbers are identified with equivalence classes of sequences of rational numbers‚Äîinformally speaking, \(r \in \mathbb{R}\) is identified with the set of sequences of rational numbers which converge to \(r\) (in the sense of [Definition 9.2.15](#)).

**To do:**
</markdown><markdown>
## Section B.3

# Limits of functions

At the end of [Section 9.1](#) we mentioned the use of *limits* of functions without properly defining what we meant. This admittedly brusque section is dedicated to making what we meant mathematically precise.

### Limits

‚ú¶ **Definition B.3.1**  
Let \( D \subseteq \mathbb{R} \). A *limit point* of \( D \) is a real number \( a \) such that, for all \( \delta > 0 \), there exists some \( x \in D \) such that \( 0 < |x - a| < \delta \).

üîπ **Lemma B.3.2**  
Let \( D \subseteq \mathbb{R} \). A real number \( a \) is a limit point of \( D \) if and only if there is a sequence \( (x_n) \) of elements of \( D \), which is not eventually constant, such that \( (x_n) \to a \).

**Proof**  
(‚áí) Let \( a \in \mathbb{R} \) and assume that \( a \) is a limit point of \( D \). For each \( n \geq 1 \), let \( x_n \) be some element of \( D \) such that \( 0 < |x_n - a| < \frac{1}{n} \).

Evidently \( (x_n) \to a \): indeed, given \( \varepsilon > 0 \), letting \( N \geq \max\{1, \frac{1}{\varepsilon}\} \) gives \( |x_n - a| < \varepsilon \) for all \( n \geq N \).

Moreover, the sequence \( (x_n) \) is not eventually constant: if it were, there would exist \( N \geq 1 \) and \( b \in \mathbb{R} \) such that \( x_n = b \) for all \( n \geq N \). But then by the squeeze theorem ([Theorem 9.2.38](#)), we‚Äôd have

\[
0 \leq \lim_{n \to \infty} |x_n - a| = |b - a| \leq \lim_{n \to \infty} \frac{1}{n} = 0
\]

and so \( b = a \). But this contradicts the fact that \( |x_n - a| > 0 \) for all \( n \geq 1 \).

(‚áê) Let \( a \in \mathbb{R} \) and assume that there is a sequence \( (x_n) \) of elements of \( D \), which is not eventually constant, such that \( (x_n) \to a \). Then for all \( \delta > 0 \) there exists some \( N \in \mathbb{N} \) such that \( |x_n - a| < \varepsilon \) for all \( n \geq N \). Since \( (x_n) \) is not eventually constant, there is some \( n \geq N \) such that \( |x_n - a| > 0 \)‚Äîotherwise \( (x_n) \) would be eventually constant with value \( a \)! But then \( x_n \in D \) and \( 0 < |x_n - a| < \delta \), so that \( a \) is a limit point of \( D \).
</markdown><markdown>
### Definition B.3.3

Let \( D \subseteq \mathbb{R} \). The **closure** of \( D \) is the set \( \overline{D} \) defined by

\[
\overline{D} = D \cup \{ a \in \mathbb{R} \mid a \text{ is a limit point of } D \}
\]

That is, \( \overline{D} \) is given by \( D \) together with its limit points.

### Example B.3.4

We have \( (0, 1) = [0, 1] \). Indeed, \( (0, 1) \subseteq (0, 1] \subseteq [0, 1] \) since \( D \subseteq \overline{D} \) for all \( D \subseteq \mathbb{R} \). Moreover, the sequences \( \left( \frac{1}{n} \right) \) and \( \left( 1 - \frac{1}{n} \right) \) are non-constant, take values in \( (0, 1) \), and converge to 0 and 1 respectively, so that 0 and 1 are in \( (0, 1] \). Hence \( [0, 1] \subseteq (0, 1] \).

Given \( a \in \mathbb{R} \), if \( a > 1 \), then letting \( \delta = 1 - a > 0 \) reveals that \( |x - a| \geq \delta \) for all \( x \in D \); and likewise, if \( a < 0 \), then letting \( \delta = -a > 0 \) reveals that \( |x - a| \geq \delta \) for all \( x \in D \). Hence no element of \( \mathbb{R} \setminus [0, 1] \) is an element of \( D \), so that \( (0, 1) = [0, 1] \).

### Exercise B.3.5

Let \( a, b \in \mathbb{R} \) with \( a < b \). Prove that \( (a, b) = (a, b] = [a, b) = [a, b] \).

### Convention B.3.6

For the rest of this section, whenever we declare \( f : D \to \mathbb{R} \) to be a function, it will be assumed that the domain \( D \) is a subset of \( \mathbb{R} \), and that every point of \( D \) is a limit point of \( D \). In other words, \( D \) has no **isolated points**, which are points separated from all other elements of \( D \) by a positive distance. For instance, in the set \( (0, 1] \cup \{2\} \), the element 2 is an isolated point.

### Definition B.3.7

Let \( f : D \to \mathbb{R} \) be a function, let \( a \in \overline{D} \), and let \( \ell \in \mathbb{R} \). We say \( \ell \) is a **limit** of \( f(x) \) as \( x \) approaches \( a \) if

\[
\forall \varepsilon > 0, \exists \delta > 0, \forall x \in D, \, 0 < |x - a| < \delta \implies |f(x) - \ell| < \varepsilon
\]

In other words, for values of \( x \in D \) near \( a \) (but not equal to \( a \)), the values of \( f(x) \) become arbitrarily close to \( \ell \).

We write \( f(x) \to \ell \text{ as } x \to a \) to denote the assertion that \( \ell \) is a limit of \( f(x) \) as \( x \) approaches \( a \).

### Example B.3.8

Define \( f : \mathbb{R} \to \mathbb{R} \) by \( f(x) = x \) for all \( x \in \mathbb{R} \). Then \( f(x) \to 0 \) as \( x \to 0 \). To see this, let \( \varepsilon > 0 \), and define \( \delta = \varepsilon > 0 \). Then for all \( x \in \mathbb{R} \), if \( 0 < |x - a| < \delta = \varepsilon \), then

\[
|f(x) - f(a)| = |x - a| < \varepsilon
\]

as required.
</markdown><markdown>
### Exercise B.3.9
Let \( f : D \to \mathbb{R} \) be a function, let \( a \in D \) and let \( \ell \in \mathbb{R} \). Fix some sequence \( (x_n) \) of elements of \( D \), not eventually constant, such that \( (x_n) \to a \). Prove that if \( f(x) \to \ell \) as \( x \to a \), then the sequence \( (f(x_n)) \) converges to \( \ell \).

The next exercise tells us that limits of functions are unique, provided that they exist. Its proof looks much like the analogous result we proved for sequences in Theorem 9.2.41.

### Exercise B.3.10
Let \( f : D \to \mathbb{R} \) be a function, let \( a \in D \), and \( \ell_1, \ell_2 \in \mathbb{R} \). Prove that if \( f(x) \to \ell_1 \) as \( x \to a \), and \( f(x) \to \ell_2 \) as \( x \to a \), then \( \ell_1 = \ell_2 \).

When the domain \( D \) of a function \( f : D \to \mathbb{R} \) is unbounded, we might also be interested in finding out how the values of \( f(x) \) behave as \( x \in D \) gets (positively or negatively) larger and larger.

### Definition B.3.11
Let \( f : D \to \mathbb{R} \) be a function and let \( \ell \in \mathbb{R} \). If \( D \) is unbounded above‚Äîthat is, for all \( p \in \mathbb{R} \), there exists \( x \in D \) with \( x > p \)‚Äîthen we say \( \ell \) is a **limit of \( f(x) \) as \( x \) increases without bound** if

\[
\forall \varepsilon > 0, \exists p \in \mathbb{R}, \forall x \in D, x > p \implies |f(x) - \ell| < \varepsilon
\]

We write \( f(x) \to \ell \) as \( x \to \infty \) to denote the assertion that \( \ell \) is a limit of \( f(x) \) as \( x \) increases without bound.

Likewise, if \( D \) is unbounded below‚Äîthat is, for all \( p \in \mathbb{R} \), there exists \( x \in D \) with \( x < p \)‚Äîthen we say \( \ell \) is a **limit of \( f(x) \) as \( x \) decreases without bound** if

\[
\forall \varepsilon > 0, \exists p \in \mathbb{R}, \forall x \in D, x < p \implies |f(x) - \ell| < \varepsilon
\]

We write \( f(x) \to \ell \) as \( x \to -\infty \) to denote the assertion that \( \ell \) is a limit of \( f(x) \) as \( x \) decreases without bound.

### Example B.3.12
Let \( f : \mathbb{R} \to \mathbb{R} \) be the function defined by \( f(x) = \frac{x}{1 + |x|} \) for all \( x \in \mathbb{R} \). Then:

- \( f(x) \to 1 \) as \( x \to \infty \). To see this, let \( \varepsilon > 0 \), and define \( p = \max\{1, \frac{1}{\varepsilon}\} \). Then for all \( x > p \), we have \( x > 0 \), so that \( f(x) = \frac{x}{1 + x} \), and \( x > \frac{1}{\varepsilon} - 1 \). Hence:

\[
\left| \frac{x}{1 + x} - 1 \right| = \left| \frac{-1}{1 + x} \right| = \frac{1}{1 + x} < \frac{1}{1 + \left( \frac{1}{\varepsilon} - 1 \right)} = \varepsilon
\]

as required.
</markdown><markdown>
- \( f(x) \to -1 \) as \( x \to \infty \). To see this, let \(\varepsilon > 0\) and define \( p = \min\{-1, -\frac{1}{\varepsilon}\} \). Then for all \( x < p \), we have \( x < 0 \), so that \( f(x) = \frac{1}{1-x} \), and \( x < -\frac{1}{\varepsilon} + 1 \). Hence:

\[
\left| \frac{x}{1-x} - (-1) \right| = \left| \frac{1}{1-x} \right| = \frac{1}{1-x} < \frac{1}{1 - \left( -\frac{1}{\varepsilon} + 1 \right)} = \varepsilon
\]

as required.

So \( f(x) \to 1 \) as \( x \to \infty \) and \( f(x) \to -1 \) as \( x \to -\infty \).

‚úé **Exercise B.3.13**

Let \( f : D \to \mathbb{R} \) be a function and \(\ell_1, \ell_2 \in \mathbb{R}\). Prove that if \( D \) is unbounded above, and if \( f(x) \to \ell_1 \) as \( x \to \infty \) and \( f(x) \to \ell_2 \) as \( x \to \infty \), then \(\ell_1 = \ell_2\). Prove the analogous result for limits as \( x \to -\infty \) in the case when \( D \) is unbounded below.

The results of [Exercises B.3.10](#) and [B.3.13](#) justify the following definition.

‚ú¶ **Definition B.3.14**

Let \( f : D \to \mathbb{R} \) and let \( a \in [-\infty, \infty] \). Assuming the limits in question are well-defined and exist, we write \(\lim_{x \to a} f(x)\) to denote the unique real number \(\ell \in \mathbb{R}\) such that \( f(x) \to \ell \) as \( x \to a \).
</markdown>The page appears to be blank. There is no text to extract.<markdown>
## Appendix C

# Hints for selected exercises

**Chapter 0**

**Chapter 0 exercises**

### Section 1.1

**Hint for Exercise 1.1.19**  
Mimic the proof of [Proposition 1.1.18](#).

**Hint for Exercise 1.1.34**  
Suppose \( n = d_r \cdot 10^r + \cdots + d_1 \cdot 10 + d_0 \) and let \( s = d_r + \cdots + d_1 + d_0 \). Start by proving that \( 3 \mid n - s \).

**Hint for Exercise 1.1.48**  
Use the law of excluded middle according to whether the proposition ‚Äò\( \sqrt{2}^{\sqrt{2}} \) is rational‚Äô is true or false.

### Section 1.2

**Hint for Exercise 1.2.15**  
Consider the sum of \( x + y \) and \( x - y \).

**Hint for Exercise 1.2.22**  
Look carefully at the definition of divisibility ([Definition 0.12](#)).
</markdown><markdown>
## Section 1.3

**Hint for Exercise 1.3.9**  
Note that you may need to use the law of excluded middle (Axiom 1.1.44) and the principle of explosion (Axiom 1.1.49).

**Hint for Exercise 1.3.22**  
Express this statement as ‚àÄn ‚àà ‚Ñ§, (n is even) ‚áî (n¬≤ is even), and note that the negation of ‚Äòx is even‚Äô is ‚Äòx is odd‚Äô.

**Hint for Exercise 1.3.31**  
Find a rational number all of whose representations as a ratio of two integers have an even denominator.

**Hint for Exercise 1.3.37**  
Start by expressing ‚áî in terms of ‚áí and ‚àß, as in Definition 1.1.28.

## Chapter 1 exercises

### Section 2.1

**Hint for Exercise 2.1.3**  
Think about what ‚Äòa ‚àà X‚Äô *really* means‚Äîdon‚Äôt let your intuition fool you.

**Hint for Exercise 2.1.32**  
Recall from the beginning of Section 2.1 that ‚àÄx ‚àà X, p(x) is equivalent to ‚àÄx, (x ‚àà X ‚áí p(x)) and ‚àÉx ‚àà X, p(x) is equivalent to ‚àÉx, (x ‚àà X ‚àß p(x)). What can be said about the truth value of x ‚àà E when E is empty?

### Section 2.2

**Hint for Exercise 2.2.25**  
You need to find a family of subsets of ‚Ñï such that (i) any two of the subsets have infinitely many elements in common, but (ii) given any natural number, you can find one of the subsets that it is *not* an element of.

## Chapter 2 exercises
</markdown><markdown>
## Hint for Question 2.12

The temptation is to write a long string of equations, but it is far less painful to prove this by double containment, splitting into cases where needed. An even less painful approach is to make a cunning use of truth tables.

## Hint for Question 2.13

The hint for Question 2.12 applies here too.

## Section 3.1

### Hint for Exercise 3.1.17

What is the value of \( i(z) \) if \( z \in X \cap Y \)?

### Hint for Exercise 3.1.23

Look closely at Definition 3.1.18.

## Section 3.2

### Hint for Exercise 3.2.13

Recall Definition 3.1.32.

### Hint for Exercise 3.2.15

If \( Z \) were a subset of \( Y \), then we could easily define an injection \( i : Z \to Y \) by \( i(z) = z \) for all \( z \in Z \). Are there any subsets of \( Y \) that are associated with a function whose codomain is \( Y \)?

### Hint for Exercise 3.2.16

Note that \( B \in \mathcal{P}(X) \). Prove that there does not exist \( a \in X \) such that \( B = f(a) \).

### Hint for Exercise 3.2.28

This can be proved in a single sentence; if you find yourself writing a long proof, then there is an easier way.

### Hint for Exercise 3.2.31

The proof is almost identical to Exercise 3.2.28.

### Hint for Exercise 3.2.39

For part (c), don‚Äôt try to write a formula for the inverse of \( h \); instead, use the fundamental theorem of arithmetic.

### Hint for Exercise 3.2.45

Use Exercise 3.2.40.
</markdown><markdown>
## Hint for Exercise 3.2.47
Define \( h : X \times Y \to A \times B \) by \( h(x, y) = (f(x), g(y)) \) for all \( x \in X \) and all \( y \in Y \); find an inverse for \( h \) in terms of the inverses of \( f \) and \( g \).

## Chapter 3 exercises

### Hint for Question 3.2

### Hint for Question 3.3
Given a function \( f : X \to \emptyset \), we must have \( f(a) \in \emptyset \) for each \( a \in X \).

### Hint for Question 3.7
Consider \( f(x) + f(-x) \) and \( f(x) - f(-x) \) for \( x \in \mathbb{R} \).

### Hint for Question 3.8
Fix \( n \in \mathbb{N} \) and consider how \( \theta_n \) interacts with functions \( f : [1] \to [n] \).

### Hint for Question 3.14
Begin by observing that each \( h \in f[\mathbb{R}] \) is an even function, in the sense of Definition 3.E.1.

### Hint for Question 3.19
This problem is very fiddly. First prove that conditions (i)‚Äì(iii) are satisfied when \( A = f[X] \) and \( p \) and \( i \) are chosen appropriately. Then condition (iii) in each case (for \( A \) and for \( f[X] \)) defines functions \( v : A \to f[X] \) and \( w : f[X] \to A \), and gives uniqueness of \( v \). You can prove that these functions are mutually inverse using the ‚Äòuniqueness‚Äô part of condition (iii).

### Hint for Question 3.22
Avoid the temptation to prove either part of this question by contradiction. For (a), a short proof is available directly from the definitions of ‚Äòinjection‚Äô and ‚Äòsurjection‚Äô. For (b), find as simple a counterexample as you can.

### Hint for Question 3.25
Start by proving that 
\[
\binom{m}{2} < \binom{m+1}{2}
\]
for all \( m \geq 1 \). Deduce that, for all \( n \in \mathbb{N} \), there is a unique natural number \( k \) such that 
\[
\binom{k+1}{2} \leq n < \binom{k+2}{2}
\]
. Can you see what this has to do with the function \( f \)?

### Hint for Question 3.26
Consider the set of fixed points of \( e \)‚Äîthat is, elements \( x \in X \) such that \( e(x) = x \).
</markdown><markdown>
**Hint for Question 3.45**  
First prove that \( f \) is bijective if and only if \( ad \neq bc \).

## Section 4.1

## Section 4.2

**Hint for Exercise 4.2.12**  
To define the bijection, think about what the elements of the two sets look like: The elements of \(\prod_{k=1}^{n+1} X_k\) look like \((a_1, a_2, \ldots, a_n, a_{n+1})\), where \(a_k \in X_k\) for each \(1 \leq k \leq n+1\).

On the other hand, the elements of \(\left(\prod_{k=1}^{n} X_k\right) \times X_{n+1}\) look like \(((a_1, a_2, \ldots, a_n), a_{n+1})\).

**Hint for Exercise 4.2.19**  
Note that \((-1)^i \binom{n}{i} = \binom{n}{i} \cdot (-1)^i \cdot 1^{n-i}\).

## Section 4.3

**Hint for Exercise 4.3.10**  
Observe that if \( n \) dubloons can be obtained using only 3 and 5 dubloon coins, then so can \( n + 3 \). See how you might use this fact to exploit strong induction with multiple base cases.

**Hint for Exercise 4.3.15**  
Prove first that if \( a \in \mathbb{Z} \) and \( a^2 \) is divisible by 3, then \( a \) is divisible by 3.

## Chapter 4 exercises

**Hint for Question 4.6**  
For example, if there are exactly 3 elements of \( X \) making \( p(x) \) true, then that means that there is some \( a \in X \) such that \( p(a) \) is true, and there are exactly two elements \( x \in X \) other than \( a \) making \( p(x) \) true.

**Hint for Question 4.8**  
The number of trailing zeros in the base-\( b \) expansion of a natural number \( n \) is the
</markdown><markdown>
greatest natural number \( r \) such that \( b^r \) divides \( n \). How many times does 10 go into 41!? How many times does 2 go into 41!?

**Hint for Question 4.11**  
Proving this by induction on \( x \) only demonstrates that it is true for **integers** \( x \geq -1 \), not **real numbers** \( x \geq -1 \). Try proving a more general fact by induction on a different variable, and deducing this as a special case. (Do you *really* think there is anything special about the natural number 123 456 789?)

**Hint for Question 4.14**  
To find the value of \( a \) in part (a), try substituting some small values of \( x \) and \( y \) into the equation \( f(x+y) = f(x) + f(y) \). For part (b), use the fact that \( n + (-n) = 0 \) for all \( n \in \mathbb{N} \). For part (c), consider the number \( b f\left(\frac{a}{b}\right) \) when \( a, b \in \mathbb{Z} \) and \( b \neq 0 \).

**Hint for Question 4.15**  
Like in Question 4.14, prove this first for \( x \in \mathbb{N} \) (by induction), then for \( x \in \mathbb{Z} \), and finally for \( x \in \mathbb{Q} \).

**Hint for Question 4.17**  
Observe that \( 7^{n+1} - 2 \cdot 4^{n+1} + 1 = 4(7^n - 2 \cdot 4^n + 1) + 3(7^n - 1) \) for all \( n \in \mathbb{N} \). You may need to do a separate proof by induction in your induction step.

**Hint for Question 4.22**  
You will need to use the angle addition formulae

\[
\sin(\alpha \pm \beta) = \sin \alpha \cos \beta \pm \cos \alpha \sin \beta
\]

and

\[
\cos(\alpha \pm \beta) = \cos \alpha \cos \beta \mp \sin \alpha \sin \beta
\]

**Hint for Question 4.23**  
For (a) use integration by parts; you do not need induction. For parts (b) and (c), use part (a).

## Section 5.1

## Section 5.2

**Hint for Exercise 5.2.36**  
Given a partition \(\mathcal{U}\) of a set \( X \), find a surjection \( q : X \to \mathcal{U} \). Then prove that, for every surjection \( p : X \to A \), there is a unique partition \(\mathcal{U}_p\) of \( X \) and a unique bijection \( f : \mathcal{U}_p \to A \) such that, for all \( U \in \mathcal{U}_p \), we have \( p(x) = f(U) \) for all \( x \in U \). The structure of the proof will be similar to that of Theorem 5.2.35.
</markdown><markdown>
## Chapter 5 exercises

### Section 6.1

**Hint for Exercise 6.1.8**  
Part (b) has a proof by induction that looks much like the one in part (a). In the induction step, given a surjection \( g : [m+1] \to [n] \), observe that we must have \( n \geq 1 \), and construct a surjection \( \overline{g} : [m+1] \setminus \{a\} \to [n-1] \) for some suitable \( a \in [m+1] \). Then invoke Lemma 6.1.6, now using the fact that \([m] = [m+1] \setminus \{m+1\}\).

**Hint for Exercise 6.1.17**  
Given \( U \subseteq X \), find an injection \( U \to X \) and apply Theorem 6.1.14(a).

**Hint for Exercise 6.1.18**  
Recall that \( X \cap Y \subseteq X \) for all sets \( X \) and \( Y \).

**Hint for Exercise 6.1.21**  
Apply Proposition 6.1.19 with \( Y = U \).

**Hint for Exercise 6.1.22**  
Prove by induction on \( n \in \mathbb{N} \) that, for all \( m, n \in \mathbb{N} \), there is a bijection \([mn] \to [m] \times [n]\).

### Section 6.2

**Hint for Exercise 6.2.6**  
The fundamental theorem of arithmetic (Theorem 7.2.12) implies that, for all \( n \in \mathbb{N} \), we can express \( n+1 \) uniquely as a power of 2 multiplied by an odd number.

**Hint for Exercise 6.2.8**  
Use Exercise 3.2.47, together with the definition of countably infinite sets, to construct a bijection \( \mathbb{N} \times \mathbb{N} \to X \times Y \). Then apply Exercise 6.2.6 and Proposition 6.2.7.

**Hint for Exercise 6.2.11**  
For (ii)\(\Rightarrow\)(i), use the fact that the composite of two injections is injective. Likewise for (iii)\(\Rightarrow\)(i).

**Hint for Exercise 6.2.13**  
Suppose \( X = \mathbb{N} \). By Proposition 6.2.9, the set \( \mathbb{N}^k \) is countable. By Theorem 6.2.10(c), it suffices to find an injection \(\binom{\mathbb{N}}{k} \to \mathbb{N}^k\).

**Hint for Exercise 6.2.25**  
We have already proved this when \( X \) is finite. When \( X \) is countably infinite, use The-
</markdown><markdown>
When \( X \) is uncountably infinite, find an injection \( X \to \mathcal{P}(X) \) and find a way to apply Exercise 6.2.11.

**Hint for Exercise 6.2.28**  
How many ‚Äò√∑‚Äô symbols can a string from \(\Sigma^*\) have if the string is to represent a rational number? Where in a word over \(\Sigma\) can a √∑ symbol appear?

**Hint for Exercise 6.2.29**  
Prove by induction that \(\Sigma^n\) is countable for all \( n \in \mathbb{N} \), and then apply Theorem 6.2.14.

**Hint for Exercise 6.2.38**  
Is \(\Sigma\) really finite? Is \(D\) really well-defined?

**Hint for Exercise 6.2.40**  
Start by proving that there is a finite description of the elements of \(\mathbb{N}^*\) over a finite alphabet \(\Phi\). This defines an injection \(\mathbb{N}^* \to \Phi^*\). Now given a countable alphabet \(\Sigma\), use the injection \(\Sigma \to \mathbb{N}\) given by Theorem 6.2.10 to construct an injection \(\Sigma^* \to \mathbb{N}^*\), and therefore an injection \(\Sigma^* \to \Phi^*\). Finally, use this to turn finite descriptions over \(\Sigma\) into finite descriptions over \(\Phi\).

## Chapter 8 exercises

### Section 7.1

**Hint for Exercise 7.1.11**  
Remember that negative integers can be greatest common divisors too.

**Hint for Exercise 7.1.13**  
Start by proving that \(d\) and \(d'\) must divide each other.

**Hint for Exercise 7.1.25**  
Example 7.1.22 would be a good starting point.

**Hint for Exercise 7.1.39**  
This is essentially the same as Exercise 7.1.13.

**Hint for Exercise 7.1.41**  
Define \( m = \frac{ab}{\gcd(a,b)} \) and prove that \( m \) satisfies the definition of being a least common multiple of \( a \) and \( b \) (Definition 7.1.38). Then apply Exercise 7.1.39.
</markdown><markdown>
## Section 7.2

### Hint for Exercise 7.2.5
Use the factorial formula for binomial coefficients (Theorem 4.2.14).

### Hint for Exercise 7.2.9
Assume \( p = mn \) for some \( m, n \in \mathbb{Z} \). Prove that \( m \) or \( n \) is a unit.

### Hint for Exercise 7.2.23
What are the prime factors of \( n! - 1 \)?

## Section 7.3

### Hint for Exercise 7.3.18
Consider the list \( a^0, a^1, a^2, \ldots \). Since there are only finitely many remainders modulo \( n \), we must have \( a^i \equiv a^j \mod n \) for some \( 0 \leq i < j \).

### Hint for Exercise 7.3.25
First find the remainder of 244886 when divided by 12.

### Hint for Exercise 7.3.28
Find a bijection \([p] \times C_n \to C_{pn}\), where \( C_n = \{ k \in [|n|] \mid k \perp n \} \). You will need to use the techniques of Section 6.1 in your proof.

### Hint for Exercise 7.3.29
Start by proving that \( k \in [pn] \) is not coprime to \( pn \) if and only if either \( p \mid k \) or \( k \) is not coprime to \( n \). You will need to use the techniques of Section 6.1 in your proof.

### Hint for Exercise 7.3.34
Recall that \( \varphi(100) = 40 \)‚Äîthis was Example 7.3.30.

### Hint for Exercise 7.3.38
You need to use the fact that \( p \) is prime at some point in your proof.

### Hint for Exercise 7.3.39
Pair as many elements of \([p - 1]\) as you can into multiplicative inverse pairs modulo \( p \).

### Hint for Exercise 7.3.49
This generalisation will be tricky! You may need to generalise the definitions and results about greatest common divisors and least common multiples that we have seen so far, including B√©zout‚Äôs lemma. You might want to try proving this first in the case that \( n_i \perp n_j \) for all \( i \neq j \).
</markdown><markdown>
**Hint for Exercise 7.3.50**  
Observe that if \( a, k \in \mathbb{Z} \) and \( k \mid a \), then \( k \mid a + k \).

## Chapter 7 exercises

**Hint for Question 7.4**  
Consider the linear Diophantine equation \( (a+b)x + (c+d)y = 1 \).

**Hint for Question 7.9**  
Use the ‚Äòuniqueness‚Äô part of the fundamental theorem of arithmetic.

**Hint for Question 7.10**  
Use appropriate restrictions of the functions from Question 7.9. Apply the ‚Äòexistence‚Äô and ‚Äòuniqueness‚Äô parts of the fundamental theorem of arithmetic to prove surjectivity and injectivity, respectively.

**Hint for Question 7.11**  
A sequence of \( k \) integers \( (n_1, n_2, \ldots, n_k) \in \mathbb{Z}^k \) can be encoded as a sequence of \( 2k \) natural numbers \( (i_1, |n_1|, i_2, |n_2|, \ldots, i_k, |n_k|) \in \mathbb{N}^{2k} \), where for each \( j \in [k] \), either \( i_j = 0 \) or \( i_j = 1 \) according to the sign of \( n_j \). For example we can encode \( (10, -32, 81) \in \mathbb{Z}^3 \) as \( (0, 10, 1, 32, 0, 81) \in \mathbb{N}^6 \).

**Hint for Question 7.13**  
Start by proving that, for all \( n \in \mathbb{N} \) and all \( k \in \mathbb{N} \), there are exactly \( \left\lfloor \frac{n}{5^k} \right\rfloor \) natural numbers \(\leq n\) that are divisible by \( 5^k \). Then consider how many zeros are contributed by each factor of \( n! \) to the decimal expansion of \( n! \).

**Hint for Question 7.14**  
This will be similar to Question 7.13 in the end; to get started, consider the greatest prime factor of \( b \).

## Section 8.1

**Hint for Exercise 8.1.16**  
Any function \( f : X \to Y \) with finite domain can be specified by listing its values. For each \( x \in X \), how many choices do you have for the value \( f(x) \)?

**Hint for Exercise 8.1.22**  
The image (Definition 3.1.32) of an injection \([3] \to [4]\) must be a subset of \([4]\) of size three.
</markdown><markdown>
## Hint for Exercise 8.1.40

How many ways can you select \( k + 1 \) animals from a set containing \( n \) cats and one dog?

## Hint for Exercise 8.1.43

Find two procedures for counting the number of pairs \((U, u)\), such that \( U \subseteq [n] \) is a \( k \)-element subset and \( u \in U \). Equivalently, count the number of ways of forming a committee of size \( k \) from a population of size \( n \), and then appointing one member of the committee to be the chair.

## Hint for Exercise 8.1.45

Find an expression for \((a + b + c)!\) in terms of \(a!\), \(b!\), \(c!\) and \(\binom{a+b+c}{a,b,c}\), following the pattern of Theorem 8.1.42.

## Section 8.2

### Hint for Exercise 8.2.1

You will need to use the recursive definition of binomial coefficients.

### Hint for Exercise 8.2.27

Consider selecting a committee from a population of size \( n \), with a subcommittee of size exactly \(\ell\); toggle whether the oldest member of the population that is not on the subcommittee is or is not on the committee. If you prefer mathematical objects, consider the set of pairs \((A, B)\), where \( A \subseteq B \subseteq [n] \) and \(|A| = \ell\); toggle the least element of \([n] \setminus A\) in the set \( B \).

## Chapter 8 exercises

### Hint for Question 8.8

Prove that \(|X/\sim| \cdot k = n\) using the multiplication principle: find a two-step procedure for specifying an element of \( X \) by first considering its \(\sim\)-equivalence class.

### Hint for Question 8.17

Find a way to apply Lemma 8.2.21.

## Section 9.1

## Section 9.2

### Hint for Exercise 9.2.28
</markdown><markdown>
If \((x_n) \to a \neq 0\), show that \(|x_n - a|\) is eventually small enough that no \(x_n\) can be equal to zero after a certain point in the sequence. On the other hand, there are plenty of sequences, *all* of whose terms are nonzero, which converge to zero‚Äîfind one!

**Hint for Exercise 9.2.40**  
Divide the numerator and denominator by \(n^r\) and apply Theorem 9.2.34 and Example 9.2.39.

**Hint for Exercise 9.2.47**  
You might want to begin by solving ??.

**Hint for Exercise 9.2.61**  
In the definition of a Cauchy sequence, observe that \(x_m - x_n = (x_m - a) - (x_n - a)\), and apply the triangle inequality (Theorem 9.1.9).

## Section 9.3

**Hint for Exercise 9.3.5**  
Begin by observing that

\[
\binom{n}{3}^{-1} = \frac{3}{n-2} - \frac{6}{n-1} + \frac{3}{n}.
\]

**Hint for Exercise 9.3.13**  
Proceed by contraposition: suppose \(j \in \mathbb{N}\) is least such that \(u_j \neq v_j\)‚Äîwithout loss of generality \(u_j < v_j\)‚Äîthen

\[
0 = \sum_{i \geq 0} \frac{v_i}{b^i} - \sum_{i \geq 0} \frac{u_i}{b^i} = \frac{v_j - u_j}{b^j} + \sum_{i > j} \frac{v_i - u_i}{b^i}
\]

Prove that this is nonsense. You will use condition (iii) in Theorem 9.3.12 somewhere in your proof.

**Hint for Exercise 9.3.32**  
Read the hypotheses of Theorem 9.3.29 very carefully.

**Hint for Exercise 9.3.36**  
We‚Äôve already proved that such a series exists‚Äîgo find it!

**Hint for Exercise 9.3.46**  
This exercise looks harder than it is. Write out the definitions of \(\sum_{i \in I} a_{f(i)}\) and \(\sum_{j \in J} a_j\) and apply Theorem 9.3.42.

**Hint for Exercise 9.3.50**
</markdown><markdown>
We know that 

\[
\frac{1}{(1-x)^2} = \left( \frac{1}{1-x} \right)^2 = \left( \sum_{n \geq 0} x^n \right)^2
\]

by [Theorem 9.3.8](#). Multiply out this sum and see what happens.

## Chapter 9 exercises

**Hint for Question 9.5**  
Consider what the Cauchy‚ÄìSchwarz inequality has to say about the vectors \((a, b)\) and \((c, d)\). Note also that \(a + 2b = (a, b) \cdot (1, 2)\) and \(c + 2d = (c, d) \cdot (1, 2)\).

**Hint for Question 9.8**  
Begin by comparing the harmonic and arithmetic means of the numbers \(a, b\) and \(c\).

**Hint for Question 9.9**  
Begin by seeing what the QM‚ÄìAM inequality says about the numbers \(a^2, b^2\) and \(c^2\). Then apply the Cauchy‚ÄìSchwarz inequality, noting that \(ab + bc + ca\) is the scalar product of two vectors.

## Section 10.1

**Hint for Exercise 10.1.6**  
Start by finding a bijection \((-1, 1) \to (a, b)\) and apply [Exercise 10.1.5](#).

## Section 10.2

**Hint for Exercise 10.2.11**  
A nice trick is to construct an injection \([0, 1) \times [0, 1) \to [0, 1)\) using decimal expansions, and then invoke the Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem.

## Chapter 10 exercises

**Hint for Question 10.22**  
Start by proving that any disc \(D \subseteq \mathbb{R}^2\) contains a point whose coordinates are both rational.

**Hint for Question 10.34**  
Think combinatorially. You should not try to generalise the recursive definition of
</markdown><markdown>
factorials (Definition 4.1.14); and you *very much should not* try to generalise the informal definition ‚Äò\(n! = 1 \times 2 \times \cdots \times n\)‚Äô.

**Hint for Question 10.52**  
Don‚Äôt waste too much time on this question‚Äîit has been proved to be independent of the usual axioms of mathematics (see Section B.1), meaning that neither an answer of ‚Äòtrue‚Äô nor an answer of ‚Äòfalse‚Äô can be proved (so neither answer leads to a contradiction). An answer of ‚Äòfalse‚Äô is known as the *continuum hypothesis*.

## Section 11.1

## Section 11.2

## Chapter 11 exercises

**Hint for Exercise 12.1.25**  
Use the characterisation of gcd and lcm in terms of prime factorisation.

**Hint for Exercise 12.1.28**  
Use distributivity, together with the fact that \(y = y \lor \bot\) and \(y = y \land \top\).

**Hint for Exercise 12.1.38**  
This can be proved by swapping \(\land\) with \(\lor\) and \(\top\) with \(\bot\) in the proof of (a). But there is a shorter proof which uses the result of part (a) together with Proposition 12.1.30.

## Chapter 12 exercises

## Section B.1

**Hint for Exercise B.1.15**  
Let \(X\) be the set whose existence is asserted by the axiom of infinity, and take \(p(x)\) to be the formula \(x \neq x\) in the axiom of separation.

## Section B.2

**Hint for Exercise B.2.9**  
Define two relations \(\preceq\) and \(\preceq'\) on \(\mathbb{N}\), one using the recursive definition and the other
</markdown><markdown>
using the definition as a logical formula; then use induction to prove that \( m \leq n \iff m \leq' n \) for all \( m, n \in \mathbb{N} \).

**Hint for Exercise B.2.28**  
Prove that \( x \) is an additive inverse for \(-x\) (in the sense of Axioms B.2.20(F4)) and use uniqueness of additive inverses. Likewise for \( x^{-1} \).

## Section B.3
</markdown>The page appears to be blank. There is no text to extract.<markdown>
# Appendix D

## Typesetting mathematics with LaTeX

Being able to type up your mathematical writing is a beneficial skill to have, and is one that is expected of anyone working in a mathematical field. Unfortunately, most Office-style WYSIWYG (‚Äòwhat you see is what you get‚Äô) text editors are not designed for this task‚Äîit becomes quickly cumbersome to deal with complicated mathematical notation, and fast alternations between notation and prose.

LaTeX is a markup language that allows you to input both regular text and mathematical notation, using code to input all symbols and configure the document‚Äôs formatting and structure. What follows is a brisk introduction to LaTeX, that should suffice for the purposes of this book.

The word LaTeX is pronounced like ‚ÄòLAY-tek‚Äô or ‚ÄòLAH-tek‚Äô, with a hard or soft ‚Äòk‚Äô sound‚Äîthe ‚ÄòX‚Äô is meant to resemble the Greek letter chi (œá), so is pronounced by some people as such. It doesn‚Äôt really matter how you say it, but do be warned that if you pronounce it like ‚ÄòLAY-teks‚Äô then people will think you‚Äôre talking about something somewhat different.

### Finding the software

You can use LaTeX by installing it on your computer, or by using a web-based editor on an internet browser. There are advantages and disadvantages to both.

- On a web-based editor, everything is set up and ready to go, but you have less control
</markdown><markdown>
over how everything compiles and you need an internet connection at all times when editing your files.

- On a computer, you have more control over how your files compile, you have access to all the logs and auxiliary files (I won‚Äôt go into this) and you don‚Äôt need an internet connection‚Äîbut it can be harder to use different packages, you‚Äôre at the mercy of your own machine‚Äôs limitations, and it‚Äôs more technically involved.

Installing \(\LaTeX\) on a computer is slightly more complicated. In order to make \(\LaTeX\) documents on your computer, you need both a compiler, for turning the code into a readable document, and an editor, for writing the code and facilitating the compilation process.

There are many online and computer-based options available; you are encouraged to research the options, and I make no recommendations for or against any particular \(\LaTeX\) implementations. As a starting point, though, I present here the \(\LaTeX\) implementations used for writing this book:

- **Computer implementation:**
  - Compiler: TeX Live (https://www.tug.org/texlive/)
  - Editor: TeXstudio (https://www.texstudio.org/)

- **Web-based implementation:** Overleaf (https://www.overleaf.com/).

TeX Live and TeXstudio are both free, open-source and cross-platform. Overleaf is a proprietary service with both free and paid subscriptions.

## Getting started

When you have settled on an online \(\LaTeX\) editor or installed \(\LaTeX\) on your computer, you can start editing. The files containing the \(\LaTeX\) code are plain-text files with the file extension `.tex` and consists of two components: a header and a body. Worrying about the details of what goes in the header and what goes in the body is not recommended if you are new to \(\LaTeX\) so, with this in mind, a template can be downloaded from the book‚Äôs website:

[https://infinitedescent.xyz/latex/](https://infinitedescent.xyz/latex/)

The code is replicated at the end of this section, on page 587.
</markdown><markdown>
# Text mode and math mode

Before we get into the nitty-gritty, I should mention the difference between ‚Äòtext mode‚Äô and ‚Äòmath mode‚Äô.

- **Text mode** is the default mode: the stuff you type will appear as text, and this is the mode you should use when writing anything that isn‚Äôt mathematical notation.

- You should use **math mode** when you‚Äôre typing anything which is mathematical notation, including variables, numbers, fractions, square roots, powers, sums, products, binomial coefficients, and so on.

To enter math mode, enclose whatever mathematical notation you are writing with dollar signs ($). For example, if I type `$E=mc^{2}$` then LaTeX shows \( E = mc^2 \). Sometimes it is convenient to put longer expressions on their own line, in which case you can enclose it between `\[` and `\]`; for example, if I type

\[
a^{2}+b^{2}+c^{2}=ab+bc+ca
\]

then LaTeX displays

\[ 
a^2 + b^2 + c^2 = ab + bc + ca 
\]

on a line all of its own.

If you need to type text inside math mode (enclosed by $ signs), you can do that using `\text{...}`, for example:

**TeX code**

\[
\sum_{i=1}^n i = \frac{n(n+1)}{2} \text{ for all } n \in \mathbb{N}
\]

**Output**

\[
\sum_{i=1}^n i = \frac{n(n+1)}{2} \text{ for all } n \in \mathbb{N}
\]

Note the spaces before and after ‚Äòfor all‚Äô; had I left those out of the code, they would not appear because LaTeX ignores spacing in math mode. You can force a space by putting a backslash before a space, for example `$a\ b$` gives \( ab \) but `$a\ \ b$` gives \( a\ b \).

All mathematical notation should be in math mode, including single variables. Notice the difference between the following two lines:

If a and b are both even then so is a+b.
</markdown><markdown>
If *a* and *b* are both even then so is *a* + *b*.

While the first is written entirely in text mode, the second is written using math mode for the variables and \(+\) sign. Although the differences may not seem big, spread over a whole document it is much clearer when math mode is used (as in the second example). Sometimes ambiguities appear, and in any case \(\LaTeX\) does a much better job at displaying mathematical notation when it is typed in math mode.
</markdown><markdown>
# Table of Mathematical Symbols

The following table is a quick reference for the most commonly-used symbols in this book. A complete index of notation can be found at the end of the book.

| Logic |  |
|-------|--|
| conjunction, disjunction | \(\land, \lor\) | \(\backslash\wedge, \backslash\vee\) |
| negation | \(\neg\) | \(\backslash\neg\) |
| implication, biconditional | \(\Rightarrow, \Leftrightarrow\) | \(\backslash\Rightarrow, \backslash\Leftrightarrow\) |
| exclusive disjunction | \(\oplus\) | \(\backslash\oplus\) |
| true, false (in truth table) | \(\checkmark, \times\) | \(\backslash\checkmark, \backslash\times\) |
| quantifiers (universal, existential) | \(\forall, \exists\) | \(\backslash\forall, \backslash\exists\) |

| Set theory |  |
|------------|--|
| element, subset | \(\in, \subseteq\) | \(\backslash\in, \backslash\subseteq\) |
| not equal, proper subset | \(\ne, \subsetneq\) | \(\backslash\ne, \backslash\subsetneq\) |
| intersection, (indexed) | \(\cap, \bigcap_{i=1}^{n}\) | \(\backslash\cap, \backslash\bigcap_{i=1}^{n}\) |
| union, (indexed) | \(\cup, \bigcup_{i=1}^{n}\) | \(\backslash\cup, \backslash\bigcup_{i=1}^{n}\) |
| relative complement, complement | \(X \setminus Y, X^c\) | \(\backslash\setminus, X^c\) |
| product, (indexed) | \(\times, \prod_{i=1}^{n}\) | \(\backslash\times, \backslash\prod_{i=1}^{n}\) |
| implied lists | \(\{1, \ldots, n\}\) | \(\backslash{1, \ldots, n\backslash}\) |
| indexed sets | \(\{x_i \mid i \in I\}\) | \(\backslash{x_i \mid i \in I\backslash}\) |
| set-builder notation | \(\{x \mid p(x)\}\) | \(\backslash{x \mid p(x)\backslash}\) |
| empty, universal set | \(\emptyset, \mathcal{U}\) | \(\backslash\varnothing, \backslash\mathcal{U}\) |
| number sets | \(\mathbb{N}, \mathbb{Z}, \mathbb{R}\) | \(\backslash\mathbb{N}, \backslash\mathbb{Z}, \backslash\mathbb{R}\), etc. |

| Numbers and combinatorics |  |
|---------------------------|--|
| multiplication | \(m \times n, m \cdot n\) | \(\backslash\times, \backslash\cdot\) |
| fractions, exponents | \(\frac{m}{n}, m^n\) | \(\backslash\frac{m}{n}, m^n\) |
| order relations | \(\le, \ge\) | \(\backslash\le, \backslash\ge\) |
| divisibility, (non-) | \(m \mid n, m \nmid n\) | \(\backslash\mid, \backslash\nmid\) |
| binomial coefficient | \(\binom{n}{k}\) | \(\backslash\binom{n}{k}\) |
| indexed sum, product | \(\sum_{i=1}^{n} a_i, \prod_{i=1}^{n} a_i\) | \(\backslash\sum_{i=1}^{n} a_i, \backslash\prod\) |
| modular arithmetic | \(a \equiv b \pmod{n}\) | \(a \backslash equiv b \backslash pmod{n}\) |

| Functions and relations |  |
|-------------------------|--|
| functions | \(f : X \to Y\) | \(f : X \backslash to Y\) |
| composition | \(g \circ f\) | \(\backslash\circ\) |
| isomorphism | \(\cong\) | \(\backslash\cong\) |
| equivalence relations | \(\sim, \approx\) | \(\backslash\sim, \backslash\approx\) |

| Structured sets |  |
|-----------------|--|
| order relation | \(\preceq, \prec\) | \(\backslash\preceq, \backslash\prec\) |
| group operations | \(\cdot, \star, \circ\) | \(\backslash\cdot, \backslash\star, \backslash\circ\) |
</markdown><markdown>
## Organisation and formatting

When typing up solutions to problem, organisation can be the difference between a masterpiece and an unreadable heap of notation. Here are some tips to help you organise your work:

### Sections and paragraphs

You can split your work up into sections, subsections, subsubsections, and even subsubsubsections. To do this, use `\section{Section title}` or `\section*{Section title}`; the former includes a section number, and the latter omits it. To start a new paragraph, simply make two new lines in the code.

### Bulleted and enumerated lists

Sometimes it is useful to use bullet points or give an enumerated list. For example, in these notes, I separate the base case from the induction step in proofs by induction by using bullet points.

For a bulleted list you can use the `itemize` environment:

```latex
\begin{itemize}
  \item Something here\ldots
  \item You can also make a list inside another list:
    \begin{itemize}
      \item Like this.
      \item Isn‚Äôt it fun?
    \end{itemize}
  \item Well, not that fun.
\end{itemize}
```

**Output**

- Something here‚Ä¶
- You can also make a list inside another list:
  - Like this.
  - Isn‚Äôt it fun?
- Well, not that fun.

For an enumerated list, you can use the `enumerate` environment. You can play around with different methods of enumeration, which you specify in square brackets `[...]`; this book most frequently uses `(i)`, `(a)` and `(1)`:
</markdown><markdown>
## Definitions, results and proofs

If you use the provided templates, you can make definitions, and state and prove results, using the following environments:

```
definition, example, proposition, theorem, lemma, corollary, proof
```

They are given a number, such as Definition 3 or Theorem 2.11, depending on how your document is set up.

Here‚Äôs an example of a theorem appearing in the third section of a document, in which five definitions, results or examples come before it:

### TeX code

```latex
\begin{theorem}
Let $a, b \in \mathbb{Z}$. Then $a^2+b^2 \ge 0$.
\end{theorem}

\begin{proof}
Exercise to the reader.
\end{proof}
```

### Output

**Theorem 3.6.** Let \( a, b \in \mathbb{Z} \). Then \( a^2 + b^2 \ge 0 \).

**Proof.** Exercise to the reader. ‚ñ°

Note that the box (‚ñ°) designating the end of the proof is inserted automatically when you close the `proof` environment.

## Labels

As you change the contents of a document, the numbering of the definitions, examples and results might change. To refer to a specific result, instead of typing the number and having to change it each time the number changes, you can use the `\label` and `\ref` commands.

An example of this in action is as follows:
</markdown><markdown>
## Formatting

### In text mode

To put the icing on the cake, you might want to make some words **bold** or *italicised*. This is simple: for bold text type `\textbf{text here}` and for italic text type `\textit{text here}`. In TeXstudio and Overleaf you can press Ctrl+B and Ctrl+I to avoid having to type all this out. Other useful fonts include monospace (`\texttt{text here}`), sans-serif (`\textsf{text here}`) and underlined (`\underline{text here}`).

### In math mode

There are also various fonts or font styles that you can use inside math mode, including:

- Roman (i.e. not italic): AaBbCc, `\mathrm{AaBbCc}`;
- Bold: **AaBbCc**, `\mathbf{AaBbCc}`;
- Sans-serif: AaBbCc, `\mathsf{AaBbCc}`;
- Blackboard bold: ùî∏ùîπ‚ÑÇùîªùîº, `\mathbb{ABCDE}` ‚Äî only capital letters;
- Fraktur: ùîÑùîûùîÖùîüùîÖùî†, `\mathfrak{AaBbCc}`;
- Calligraphic: ùíú‚Ñ¨ùíûùíü‚Ñ∞, `\mathcal{ABCDE}` ‚Äî only capital letters;

## Tables

Tables can be created using the `tabular` environment. You can specify how columns are aligned and separated as an argument to the command `\begin{tabular}`: write `l` or `c` or `r` to specify that a column should be aligned left, centre or right, respectively. If you want columns to be separated by a single or double line, enter a single or double bar (`|` or `||`), respectively.

---

### TeX code

```latex
\begin{definition}
\label{defDivides}
Say $a$ \textbf{divides} $b$ if there exists $k \in \mathbb{Z}$ such that $ka = b$.
\end{definition}
```

We will use Definition \ref{defDivides} for absolutely nothing.

### Output

**Definition 2.11.** Say *a* divides *b* if there exists *k* ‚àà ‚Ñ§ such that *ka* = *b*.

We will use Definition 2.11 for absolutely nothing.
</markdown><markdown>
Columns are then separated by ampersands (\&) and you can move to a new row by entering a double-backslash (\\). To insert a horizontal line between two rows, simply enter `\hline`.

Here's an example:

**TEX code**

```latex
\begin{tabular}{l c c c}
$\times$ & 1 & 2 & 3 \\ \hline
1 & 1 & 2 & 3 \\
2 & 2 & 4 & 6 \\
3 & 3 & 6 & 9 \\
\end{tabular}
```

**Output**

\[
\begin{array}{|c|c|c|c|}
\hline
\times & 1 & 2 & 3 \\
\hline
1 & 1 & 2 & 3 \\
2 & 2 & 4 & 6 \\
3 & 3 & 6 & 9 \\
\hline
\end{array}
\]

### Aligned equations

Occasionally a proof may require you to demonstrate that two terms are equal by providing a sequence of intermediate equations. This can be done using the `align*` environment, which behaves much like the `tabular` environment.

New lines are introduced by inserting a double-backslash (\\), and alignment points are introduced with an ampersand (&). For example:

**TEX code**

```latex
\begin{align*}
(n+1)! - n! \\
&= (n+1)n! - n! \\
&= n \cdot n! + n! - n! \\
&= n \cdot n!
\end{align*}
```

**Output**

\[
\begin{align*}
(n+1)! - n! &= (n+1)n! - n! \\
&= n \cdot n! + n! - n! \\
&= n \cdot n!
\end{align*}
\]

Note that the `align*` environment automatically enters into math mode, so no dollar signs ($) are needed.

Entering more ampersands will create more columns, whose alignment alternates (right, left, right, left, and so on). For example, to add annotations to each line, you can enter a double ampersand (&&). For example:

**TEX code**

```latex
\begin{align*}
(n+1)! - n! && \text{by recursive def of factorials} \\
&= (n+1)n! - n! && \text{by distributivity} \\
&= n \cdot n! && \text{by cancellation}
\end{align*}
```
</markdown><markdown>
\[
\begin{array}{|c|l|}
\hline
\text{Output} & \\
\hline
(n+1)! - n! = (n+1)n! - n! & \text{by recursive def of factorials} \\
= n \cdot n! + n! - n! & \text{by distributivity} \\
= n \cdot n! & \text{by cancellation} \\
\hline
\end{array}
\]

Note again that, because the \texttt{align*} environment automatically enters math mode, any annotations must be made within the \texttt{\textbackslash text\{\}} command.

## Graphics

Images can then be inserted using the \texttt{\textbackslash includegraphics} command. The format is \texttt{\textbackslash includegraphics[parameters]\{filename\}} where \texttt{parameters} denotes information telling \LaTeX\ how large you want the image to be, and \texttt{filename} is the name of the image file, which includes the path relative to the main \texttt{.tex} file. For example if \texttt{donkey.png} is stored in a directory called \texttt{images}, you would enter \texttt{‚Äòimages/donkey.png‚Äô} instead of \texttt{‚Äòdonkey.png‚Äô}.

The simplest way to control the size of the image is to enter \texttt{[width=k\textbackslash textwidth]}, where \texttt{k} is a scaling factor between 0 and 1.

For example:

\textbf{T\textsc{e}X code}
```latex
\begin{center}
\includegraphics[width=0.3\textwidth]{media/donkey.png}
\end{center}
```
</markdown><markdown>
## More advanced techniques

I should take a moment to emphasise that what really matters is your ability to communicate mathematical arguments clearly and correctly. The \(\LaTeX\) tools discussed so far in this section are more than sufficient for our purposes.

However, if you are interested in pushing your \(\LaTeX\) skills further or there is a feature you‚Äôre unsure about how to implement, then I recommend browsing or searching one of the following websites:

- [http://tex.stackexchange.com](http://tex.stackexchange.com) ‚Äî Q&A website about \(\LaTeX\)

- [https://en.wikibooks.org/wiki/LaTeX](https://en.wikibooks.org/wiki/LaTeX) ‚Äî online \(\LaTeX\) manual
</markdown><markdown>
# Practice page

Try to recreate the following page, remembering to use `\label` and `\ref` to refer to enumerated items (such as ‚ÄòProposition 1.2‚Äô).

---

## Squarefree integers

Carl Friedrich Gauss, Wednesday 14th September 1831

### Introduction

When you‚Äôve written this page, you will be unstoppable, at least as far as typesetting mathematics is concerned. You will need to implement:

- Text mode stuff: sections, paragraphs, text formatting, labels and references, lists;
- Math mode stuff: definitions and results, aligned equations, etc.

So let‚Äôs get on with it!

### 1 Squarefree integers

#### 1.1 Definition and an elementary result

**Definition 1.1.** An integer \( a \) is **squarefree** if it is divisible by no perfect square other than 1. That is, if \( n^2 \) divides \( a \) then \( n^2 = 1 \).

**Proposition 1.2.** A non-zero non-unit \( a \) is squarefree if and only if

\[
a = p_1 \times p_2 \times \cdots \times p_n
\]

for distinct primes \( p_1, p_2, \ldots, p_n \).

*Proof.* We leave the proof as an exercise to the reader. \(\square\)

#### 1.2 Some examples

**Example 1.3.** Some concrete examples include:

(i) 5610 is squarefree by Proposition 1.2, since

\[
5610 = 10 \times 561 = (2 \times 5) \times (11 \times 17)
\]

(ii) 12 is not squarefree since 4 \(|\) 12 and \(4 = 2^2\).
</markdown><markdown>
## Template file

What follows is a template `.tex` file to get you started; it can be downloaded from [https://infinitedescent.xyz/latex/](https://infinitedescent.xyz/latex/).

```latex
\documentclass[11pt]{article}

% Edit the following to change the title, author name and date
\title{A \LaTeX{} document}
\author{Firstnameinita McLastnameerson}
\date{Someday 0th Jantember 3000}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}

% Page setup
\setlength{\parskip}{10pt}
\setlength{\parindent}{0pt}
\geometry{
  paper=letterpaper, % Change to 'a4paper' for A4 size
  marginratio={1:1},
  margin={1.25in}
}

% Theorem environments
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start of document body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Hello world! Did you know that $3^2 + 4^2 = 5^2$?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of document body %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
```
</markdown>The page appears to be blank. There is no text to extract.<markdown>
# Indices
</markdown>It seems the image is blank. Could you please provide a different image or check if the file uploaded correctly?<markdown>
## Index of topics

- absolute convergence, 391
- addition principle, 305
- alphabet, 233
- alternating series test, 389
- alternating sum, 314‚Äì329
- AM‚ÄìGM inequality, 344
- antisymmetric relation, 192
- axiom of choice, 139

- base-b expansion, 6, 283
  - of a real number, 385
- basic element, 487
- Bayes‚Äôs theorem, 447
- Bernoulli distribution, 458
- biconditional, 40
- bijection, 130‚Äì144
- binary expansion, 283
- binomial coefficient, 157, 295
- binomial distribution, 459
- Boolean algebra, 478
- bound variable, 49

- canonical prime factorisation, 260
- Cantor‚Äôs diagonal argument, 229
- Cantor‚ÄìSchr√∂der‚ÄìBernstein theorem, 410
- cardinal number, 406
- cardinal sum, 415
- cardinality, 406
  - of a finite set, 216
- Cauchy sequence, 375
- Cauchy‚ÄìSchwarz inequality, 340
- characteristic function, 124
- closed
  - interval, 89
- codomain, 114
  - of a relation, 186
- comparison test, 388
- complement
  - of event, 438
  - relative, 102
- complete ordered field, 552
- completeness axiom, 552
- component, 335
- conditional probability, 443
- congruence, 197, 264
- congruence class, 201
- conjunction, 31
- constructor, 487
- contradiction, 43
  - (direct) proof by, 44
  - (indirect) proof by, 69
- contraposition
  - proof by, 70
- contrapositive, 70
- convergence
  - absolute, 391
  - of a sequence, 358
- converse, 39
- coprime, 248
- countable additivity, 433
</markdown><markdown>
countable set, 223  
counterexample, 73  
counting, 294‚Äì313  
counting principle  
&nbsp;&nbsp;&nbsp;&nbsp;addition principle, 305  
&nbsp;&nbsp;&nbsp;&nbsp;multiplication principle, 297, 302  

de Morgan‚Äôs laws  
&nbsp;&nbsp;&nbsp;&nbsp;for logical operators, 72  
&nbsp;&nbsp;&nbsp;&nbsp;for quantifiers, 73  
&nbsp;&nbsp;&nbsp;&nbsp;for sets, 103  

decimal expansion, 283  
decreasing sequence, 370  
diagonal subset, 190  
Diophantine equation  
&nbsp;&nbsp;&nbsp;&nbsp;linear, 247, 250  

discrete relation, 189  
discriminant, 18  
disjoint  
&nbsp;&nbsp;&nbsp;&nbsp;pairwise, 202, 304  

disjoint union, 219  
disjunction, 34  
distance, 336  
divergence, 358  
division, 8, 242  
division theorem, 9, 240  
divisor, 8, 242  
domain, 114  
&nbsp;&nbsp;&nbsp;&nbsp;of a relation, 186  

domain of discourse, 49  
dot product, 339  
double counting, 309  

_e_, 398  
element, 86  
&nbsp;&nbsp;&nbsp;&nbsp;basic, 487  

empty function, 121  
empty relation, 189  
empty set, 93, 94  
empty word, 233  
enumeration  
&nbsp;&nbsp;&nbsp;&nbsp;of a countably infinite set, 223  
&nbsp;&nbsp;&nbsp;&nbsp;of a finite set, 213  

equivalence  
&nbsp;&nbsp;&nbsp;&nbsp;logical, 63  

equivalence class, 199  
equivalence relation, 196‚Äì207  
&nbsp;&nbsp;&nbsp;&nbsp;generated by a relation, 208  

Euclidean algorithm, 246  
&nbsp;&nbsp;&nbsp;&nbsp;reverse, 250  

Euler‚Äôs constant, 398  
Euler‚Äôs theorem, 274  
even  
&nbsp;&nbsp;&nbsp;&nbsp;function, 145  
&nbsp;&nbsp;&nbsp;&nbsp;integer, 9  
&nbsp;&nbsp;&nbsp;&nbsp;parity, 319  

event, 433  
&nbsp;&nbsp;&nbsp;&nbsp;that _p(X)_, 452  
&nbsp;&nbsp;&nbsp;&nbsp;that _X = e_, 452  

eventually, 363  
existential quantifier, 56  
expectation, 463  
expected value, 463  
exponential function, 397  
extended real number line, 350  
extensionality, 92, 533  

factor, 8, 242  
factorial, 157, 296  
family of sets, 100  
Fermat‚Äôs little theorem, 272  
field, 547  
finite description, 234  
finite set, 213  
free variable, 49  

function, 114‚Äì129  
&nbsp;&nbsp;&nbsp;&nbsp;bijection, 134  
&nbsp;&nbsp;&nbsp;&nbsp;characteristic, 124  
&nbsp;&nbsp;&nbsp;&nbsp;empty, 121  
&nbsp;&nbsp;&nbsp;&nbsp;exponential, 397  
&nbsp;&nbsp;&nbsp;&nbsp;identity, 121  
&nbsp;&nbsp;&nbsp;&nbsp;injective (one-to-one), 131  
&nbsp;&nbsp;&nbsp;&nbsp;quotient, 205  
&nbsp;&nbsp;&nbsp;&nbsp;surjective, 133  

fundamental theorem of arithmetic, 259  
generated
</markdown><markdown>
equivalence relation, 208  
geometric distribution  
&nbsp;&nbsp;&nbsp;&nbsp;on ‚Ñï, 461  
&nbsp;&nbsp;&nbsp;&nbsp;on ‚Ñï‚Å∫, 462  
geometric series, 380  
GM‚ÄìHM inequality, 348  
graph  
&nbsp;&nbsp;&nbsp;&nbsp;of a function, 118  
&nbsp;&nbsp;&nbsp;&nbsp;of a relation, 187  
greatest common divisor, 243  
greatest element of a poset, 472  

harmonic series, 389  
homogeneous relation, 186  

identity function, 121  
implication, 38  
implied list notation, 87  
inclusion‚Äìexclusion principle, 326  
increasing sequence, 370  
independent  
&nbsp;&nbsp;&nbsp;&nbsp;events, 440  
&nbsp;&nbsp;&nbsp;&nbsp;random variables, 455  
indexed family, 100  
indicator function, 439  
induction  
&nbsp;&nbsp;&nbsp;&nbsp;on ‚Ñï (weak), 159, 529  
&nbsp;&nbsp;&nbsp;&nbsp;on ‚Ñï (strong), 174  
&nbsp;&nbsp;&nbsp;&nbsp;on an inductively defined set, 494  
&nbsp;&nbsp;&nbsp;&nbsp;rule, 484  
&nbsp;&nbsp;&nbsp;&nbsp;strong, 174‚Äì181  
&nbsp;&nbsp;&nbsp;&nbsp;structural, 482‚Äì503  
&nbsp;&nbsp;&nbsp;&nbsp;structural (for quotient-inductive sets), 503  
&nbsp;&nbsp;&nbsp;&nbsp;weak, 159‚Äì173  
inductively defined set, 487  
quotient-inductive, 502  
inequality  
&nbsp;&nbsp;&nbsp;&nbsp;Cauchy‚ÄìSchwarz, 340  
&nbsp;&nbsp;&nbsp;&nbsp;of arithmetic and harmonic means, 344  
&nbsp;&nbsp;&nbsp;&nbsp;of generalised means, 352  
&nbsp;&nbsp;&nbsp;&nbsp;of geometric and harmonic means, 348  
&nbsp;&nbsp;&nbsp;&nbsp;of quadratic and arithmetic means, 349  
&nbsp;&nbsp;&nbsp;&nbsp;triangle, 342  
&nbsp;&nbsp;&nbsp;&nbsp;triangle (one-dimensional), 337  
infimum, 473  
&nbsp;&nbsp;&nbsp;&nbsp;of subset of ‚Ñù, 353  
infinite set, 213  
inhabited set, 93  
injection, 130‚Äì144  
intersection, 101  
&nbsp;&nbsp;&nbsp;&nbsp;indexed, 101  
&nbsp;&nbsp;&nbsp;&nbsp;pairwise, 98  
interval  
&nbsp;&nbsp;&nbsp;&nbsp;closed, 89  
&nbsp;&nbsp;&nbsp;&nbsp;half-open, 89  
&nbsp;&nbsp;&nbsp;&nbsp;open, 89  
inverse  
&nbsp;&nbsp;&nbsp;&nbsp;left inverse, 135  
&nbsp;&nbsp;&nbsp;&nbsp;right inverse, 137  
&nbsp;&nbsp;&nbsp;&nbsp;two-sided, 140  
involution, 317  
involution principle, 322  
irrational number, 13  
irreducible number, 256  

Kleene star, 233  

lattice  
&nbsp;&nbsp;&nbsp;&nbsp;complemented, 478  
&nbsp;&nbsp;&nbsp;&nbsp;distributive, 477  
law of excluded middle, 45  
least common multiple, 254  
least element of a poset, 472  
left inverse, 135  
length  
&nbsp;&nbsp;&nbsp;&nbsp;of a word, 233  
limit  
&nbsp;&nbsp;&nbsp;&nbsp;of a function, 555  
&nbsp;&nbsp;&nbsp;&nbsp;of a sequence, 358  
Lindenbaum‚ÄìTarski algebra, 479  
linear Deiohantine equation, 250  
linear Diophantine equation, 247  
list notation, 87  
logical equivalence, 63  
</markdown><markdown>
logical formula, 51  
maximally negated, 74  

logical operator, 30  
lower bound  
of subset of \(\mathbb{R}\), 353  

magnitude, 336  

mean  
- arithmetic, 344  
- generalised, 351  
- geometric, 344  
- harmonic, 347  
- quadratic, 349  

model  
- probabilistic, 432  

modular arithmetic, 266  
modulo, 197, 264  
modus ponens, 39  
monotone convergence theorem, 371  
monotone sequence, 370  
multiple, 8  
multiplication principle, 297, 302  
multiplicity  
- of a prime, 260  

natural number, 537  
- von Neumann, 537  

natural numbers  
- notion of, 152  

negation, 43  
- maximal, 74  

nonzero nonunit, 243  

number  
- cardinal, 406  
- natural, 537  

number base, 6  
numeral system, 5  
- Hindu‚ÄìArabic, 5  

odd  
- function, 145  
- integer, 9  
- parity, 319  

open  
interval, 89  
subset of \(\mathbb{R}\), 109  

ordered \(n\)-tuple, 105  
ordered pair, 104  
origin, 335  
outcome, 433  

pairwise disjoint, 202, 304  
pairwise independent  
- events, 440  
- random variables, 455  

paradox  
- Russell‚Äôs, 532  

parity, 319  
partial order, 470  
partial sum, 379  
Pascal‚Äôs triangle, 157  
Peano‚Äôs axioms, 152  
permutation, 296  
pigeonhole principle, 307  
polynomial, 16  
poset, 470  
power set, 95  
predicate, 49  
prime, 255  
- canonical prime factorisation, 260  

probability, 433  
- conditional, 443  

probability distribution  
- Bernoulli, 458  

probability distribution, 457  
- binomial, 459  
- geometric (on \(\mathbb{N}\)), 461  
- geometric (on \(\mathbb{N}^+\)), 462  
- uniform, 457  

probability mass function, 453  
probability measure  
- discrete, 433  
- pushforward, 455  

probability space  
- discrete, 433  

product of sets  
- \(n\)-fold, 105  
- pairwise, 104  
</markdown><markdown>
proof, 1  
by cases, 36  
by contradiction (direct), 44  
by contradiction (indirect), 69  
by contraposition, 70  
by counterexample, 73  
by strong induction, 175  
by strong induction with multiple base cases, 177  
by structural induction, 494  
by weak induction, 159  

proposition, 1  
propositional formula, 30  
propositional variable, 30  
pushforward measure, 455  
pushforward probability measure, 455  

QM‚ÄìAM inequality, 349  

quantifier  
- existential, 56  
- unique existential, 58  
- universal, 54  

quantifier alternation, 60  

quotient, 9  
- of a set by an equivalence relation, 199  
- of numbers, 242  
- quotient-inductive set, 502  

quotient function, 205  

random variable, 452  

range  
- of a variable, 49  

rank, 492  

ratio test, 392  

rational number  
- dyadic, 88  

recursion  
- definition by recursion, 154  
- recursion theorem, 153  

reducible number, 256  

reflexive relation, 191  

relation, 186‚Äì195  
- antisymmetric, 192  
- discrete, 189  
- empty, 189  
- equivalence relation, 196‚Äì207  
- left-total, 208  
- partial order, 470  
- reflexive, 191  
- symmetric, 191  
- transitive, 193  

relative complement, 102  

relatively prime, 248  

remainder, 9, 242  

reverse Euclidean algorithm, 250  

right inverse, 137  

root, 17  

root-mean-square, 349  

RSA encryption, 285  

rule, 484  

rule of product, 297, 302  

rule of sum, 305  

Russell‚Äôs paradox, 532  

sample space, 433  

scalar product, 339  

sequence, 356  
- Cauchy, 375  
- constant, 356  
- decreasing, 370  
- increasing, 370  
- monotone, 370  

series, 378  
- geometric, 380  
- harmonic, 389  
- indexed by a countably infinite set, 395  

set, 86‚Äì107  
- empty, 93, 94  
- indexed family of, 100  
- inductively defined, 487  
- inhabited, 93  
- quotient-inductive, 502  
- universal, 86  

set equality, 92  

set-builder notation, 87  

sign, 260  
</markdown><markdown>
size, 216  
squeeze theorem, 368  
strong induction principle, 174  
structural induction  
&nbsp;&nbsp;&nbsp;&nbsp;for quotient-inductive sets, 503  
subformula, 30  
subsequence, 373  
subset, 90  
&nbsp;&nbsp;&nbsp;&nbsp;\(k\)-element subset, 294  
&nbsp;&nbsp;&nbsp;&nbsp;diagonal, 190  
sum  
&nbsp;&nbsp;&nbsp;&nbsp;alternating, 314‚Äì329  
&nbsp;&nbsp;&nbsp;&nbsp;of a series, 379  
&nbsp;&nbsp;&nbsp;&nbsp;of cardinal numbers, 415  
supremum, 473  
&nbsp;&nbsp;&nbsp;&nbsp;of subset of \(\mathbb{R}\), 353  
surjection, 130‚Äì144  
symmetric difference, 109  
symmetric relation, 191  

tautology, 77  
term  
&nbsp;&nbsp;&nbsp;&nbsp;of a sequence, 356  
toggle, 318  
totient, 273  
transitive relation, 193  
triangle inequality, 342  
&nbsp;&nbsp;&nbsp;&nbsp;in one dimension, 337  

trinomial coefficient, 313  
truth table, 66  
truth value, 30  
two-sided inverse, 140  

uniform distribution, 457  
union, 101  
&nbsp;&nbsp;&nbsp;&nbsp;indexed, 101  
&nbsp;&nbsp;&nbsp;&nbsp;pairwise, 99  
unit, 242  
universal quantifier, 54  
universal set, 86  
universe  
&nbsp;&nbsp;&nbsp;&nbsp;Grothendieck, 536  
&nbsp;&nbsp;&nbsp;&nbsp;of discourse, 86  
universe of discourse, 86  
upper bound  
&nbsp;&nbsp;&nbsp;&nbsp;of subset of \(\mathbb{R}\), 353  

value  
&nbsp;&nbsp;&nbsp;&nbsp;of a function, 114  
variable  
&nbsp;&nbsp;&nbsp;&nbsp;bound, 49  
&nbsp;&nbsp;&nbsp;&nbsp;free, 49  
von Neumann natural number, 537  

weak induction principle, 159, 529  
well-ordering principle, 179  
word, 233  
</markdown><markdown>
# Index of vocabulary

- absurd, 523
- arbitrary, 524
- assume, 520, 523
- by, 518
- case, 522
- contradiction, 523
- contrary to, 523
- define, 525
- fix, 524
- it follows that, 518, 528
- given, 524
- goal, 527
- hence, 518, 528
- impossible, 523
- we know that, 518
- let, 524, 526
- nonsense, 523
- to see that, 527
- so, 518
- step, 521
- it suffices to show that, 519
- suppose, 520
- take, 524
- therefore, 528
- we want, 527
- without loss of generality, 526
- wlog, 526
</markdown>The page appears to be blank. There is no text to extract.<markdown>
# Index of notation

\((a, b)\) ‚Äî open interval, 89  
\([a, b]\) ‚Äî closed interval, 89  
\((a, b]\) ‚Äî half-open interval, 89  
\([a, b)\) ‚Äî half-open interval, 89  
\((-\infty, a)\) ‚Äî unbounded interval, 89  
\((a, \infty)\) ‚Äî unbounded interval, 89  
\(\aleph_0\) ‚Äî aleph naught, 407  
\([a]_n\) ‚Äî congruence class, 201  
\(\iff\) ‚Äî biconditional, 40  
\(\text{Card}\) ‚Äî set of cardinal numbers, 406  
\(\times\) ‚Äî cartesian product, 104  
\(X^n\) ‚Äî cartesian product (n-fold), 105  
\(\prod_{k=1}^n\) ‚Äî cartesian product (n-fold), 105  
\(A^c\) ‚Äî complement of event, 438  
\(\setminus\) ‚Äî relative complement, 102  
\(\circ\) ‚Äî composition, 122  
\(\land\) ‚Äî conjunction, 31  
\(\mathfrak{c}\) ‚Äî cardinality of the continuum, 407  
\(\bot\) ‚Äî contradiction, 43  
\((x_n) \to a\) ‚Äî convergence of a sequence, 358  
\(\perp\) ‚Äî coprime, 248  
\(\lor\) ‚Äî disjunction, 34  
\(a \mid b\) ‚Äî division, 242  
\(\Delta X\) ‚Äî diagonal subset, 330  
\(e\) ‚Äî Euler‚Äôs constant, 398  
\(\varepsilon\) ‚Äî epsilon, 358  
\(\preceq, \sqsubseteq\) ‚Äî partial order, 470  
\(\sim, \equiv, \approx\) ‚Äî equivalence relation, 196  

\([a]_\sim\) ‚Äî equivalence class, 199  
\(\sim_R\) ‚Äî equivalence relation generated by \(R\), 208  
\(\mathbb{E}[X]\) ‚Äî expectation, 463  
\(\exists\) ‚Äî ;, 56  
\(\exp\) ‚Äî exponential function, 397  
\(\lfloor \cdot \rfloor\) ‚Äî floor operator, 291  
\(f : X \to Y\) ‚Äî function, 114  
\(f(x)\) ‚Äî value of a function, 114  
\(f[U]\) ‚Äî image, 126  
\(f^{-1}\) ‚Äî inverse function, 142  
\(f^{-1}[V]\) ‚Äî preimage, 128  
\(\gcd\) ‚Äî greatest common divisor, 244  
\(\text{Gr}(f)\) ‚Äî graph of a function, 119  
\(\text{Gr}(R)\) ‚Äî graph of a relation, 187  
\(i_A\) ‚Äî indicator function, 439  
\(\text{id}_X\) ‚Äî identity function, 121  
\(\Rightarrow\) ‚Äî implication, 38  
\(\in\) ‚Äî element, 86  
\(\inf(A)\) ‚Äî infimum, 354  
\(\cap\) ‚Äî intersection, 98  
\(\text{lcm}\) ‚Äî least common multiple, 254  
\(\equiv\) ‚Äî logical equivalence, 63  
\(a \equiv b \mod n\) ‚Äî congruence, 197, 264  
\(\text{mod}\) ‚Äî congruence, 197, 264  
\(\binom{n}{k}\) ‚Äî binomial coefficient, 157, 295  
\(\binom{a, b, c}{n}\) ‚Äî trinomial coefficient, 313  
\(\neg\) ‚Äî negation, 43  
\(n!\) ‚Äî factorial, 157, 296  
</markdown><markdown>
\( n_{\text{vN}} \) ‚Äî von Neumann natural number, 537  
\(\emptyset\) ‚Äî empty set, 94  
\((\Omega, \mathbb{P})\) ‚Äî probability space, 433  
\(\emptyset_{X,Y}\) ‚Äî empty relation, 189  
\(\mathbb{P}\) ‚Äî probability, 433  
\(\mathbb{P}(A \mid B)\) ‚Äî conditional probability, 443  
\(\mathcal{P}(X)\) ‚Äî power set, 95  
\(X/\sim\) ‚Äî quotient, 199  
\(\text{rk}(a)\) ‚Äî rank, 492  
\(\{\ldots\}\) ‚Äî set notation, 87  
\(\Sigma^*\) ‚Äî Kleene star, 233  
\(\subseteq\) ‚Äî subset, 90  
\(\sup(A)\) ‚Äî supremum, 354  
\(\text{Sym}(X)\) ‚Äî set of permutations, 296  

\(\triangle\) ‚Äî symmetric difference, 109  
\(\oplus\) ‚Äî toggle, 318  
\(\varphi(n)\) ‚Äî totient, 273  
\(\mathcal{U}\) ‚Äî universal set, 86  
\(\cup\) ‚Äî union, 99  
\(\bigsqcup\) ‚Äî disjoint union, 219  
\(\binom{X}{k}\) ‚Äî k-element subsets, 294  
\(\vec{x} \cdot \vec{y}\) ‚Äî scalar product, 339  
\(\|\vec{x}\|\) ‚Äî magnitude, 336  
\((x_n)_{n \geq 0}\) ‚Äî sequence, 356  
\(X^-\) ‚Äî set of odd-parity elements, 319  
\(X^+\) ‚Äî set of even-parity elements, 319  
\(\vec{x}\) ‚Äî vector, 335  
\(\{X = e\}\) ‚Äî event that \(X = e\), 452  
\(\mathbb{Z}/n\mathbb{Z}\) ‚Äî set of congruence classes modulo \(n\), 201  
</markdown><markdown>
# Index of LaTeX commands

## Math mode commands

- `\{...\}`, `{...}`, 87
- `\aleph`, ‚Ñµ, 407
- `\approx`, ‚âà, 196
- `\binom`, \(\binom{k}{n}\), 157, 294
- `\bmod`, mod, 197, 264
- `\bot`, ‚ä•, 43, 66, 472
- `\cap`, ‚à©, 98
- `\cdot`, ¬∑, 339
- `\circ`, ‚àò, 122
- `\cong`, ‚âÖ, 196
- `\cup`, ‚à™, 99
- `\dots`, ..., 87
- `\equiv`, ‚â°, 63, 196
- `\forall`, ‚àÄ, 54
- `\ge`, ‚â•, 12
- `\in`, ‚àà, 3, 86
- `\infty`, ‚àû, 89
- `\le`, ‚â§, 12
- `\Leftrightarrow`, ‚áî, 40
- `\Vert...\rVert`, ‚Äñ...‚Äñ, 336
- `\mathbb`, \(\mathbb{A}, \mathbb{B}, \ldots\), 4, 7, 11, 12, 14, 433, 463, 582
- `\mathbf`, \(\mathbf{A}, \mathbf{B}, \ldots\), 582
- `\mathcal`, \(\mathcal{A}, \mathcal{B}, \ldots\), 86, 95, 582
- `\mathfrak`, \(\mathfrak{a}, \mathfrak{b}, \ldots\), 407, 582
- `\mathrel`, R, ..., 186
- `\mathrm`, \(\mathrm{A}, \mathrm{B}, \ldots\), 118, 121, 187, 244, 254, 354, 484, 492, 582
- `\mathsf`, \(\mathsf{A}, \mathsf{B}, \ldots\), 406, 582
- `\mid`, |, 87, 242, 443
- `\neg`, ¬¨, 43
- `\nmid`, ‚à§, 242
</markdown><markdown>
\not, \(\not\equiv, \neq, \ldots\), 86, 197, 264  
\nsubseteq, \(\nsubseteq\), 90  
\oplus, \(\oplus\), 318  
\perp, \(\perp\), 248  
\prec, \(\prec\), 472  
\preceq, \(\preceq\), 470  
\prod, \(\prod_{k=1}^{n}\), 105  
\Rightarrow, \(\Rightarrow\), 38  
\setminus, \(\setminus\), 102  
\sim, \(\sim\), 196, 457  
\sqcup, \(\sqcup\), 219  
\sqsubset, \(\sqsubset\), 472  
\sqsupseteq, \(\sqsupseteq\), 470  
\subseteq, \(\subseteq\), 90  
\subsetneqq, \(\subsetneqq\), 90  
\text, access text mode within math mode, 584  
\times, \(\times\), 66, 104  
\to, \(\to\), 114, 358  
\top, \(T\), 66, 472  
\triangle, \(\triangle\), 109  
\varepsilon, \(\varepsilon\), 358  
\varnothing, \(\varnothing\), 94  
\varphi, \(\varphi\), 273  
\vec, \(\vec{a}, \vec{b}, \ldots\), 335  
\vee, \(\vee\), 34, 475  
\wedge, \(\wedge\), 31, 475  

### Math mode environments

- `align*`, aligned equation, 583

### Text mode commands

- `\includegraphics`, insert image, 584  
- `\label`, label (for use with `\ref`), 581  
- `\ref`, reference (for use with `\label`), 581  
- `\section`, section title with number, 580  
- `\section*`, section title without number, 580  
- `\textbf`, bold, 582  
- `\textit`, italic, 582  
- `\textsf`, sans-serif, 582  
- `\texttt`, monospace, 582  
- `\underline`, underlined, 582  

### Text mode environments

- `corollary`, corollary environment, 581
</markdown><markdown>
**Index of \LaTeX\ commands**

- [definition](#), definition environment, 581
- [enumerate](#), enumerated list, 580
- [example](#), example environment, 581
- [itemize](#), bulleted list, 580
- [lemma](#), lemma environment, 581
- [proof](#), proof environment, 581
- [proposition](#), proposition environment, 581
- [tabular](#), table, 582
- [theorem](#), theorem environment, 581
</markdown>The page appears to be blank. There is no text to extract.<markdown>
# Licence

This book, in both physical and electronic form, is licensed under a Creative Commons Attribution-ShareAlike 4.0 International Public License. The licence is replicated below from the Creative Commons website:

[https://creativecommons.org/licenses/by-sa/4.0/legalcode](https://creativecommons.org/licenses/by-sa/4.0/legalcode)

Please read the content of this licence carefully if you intend to share or adapt the material in this book.

If you have questions or would like to request permissions not granted by this licence, then contact the author (Clive Newstead, clive@infinitedescent.xyz).

## Creative Commons Attribution-ShareAlike 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

### Section 1 ‚Äî Definitions.

a. **Adapted Material** means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under
</markdown><markdown>
the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.

b. **Adapter‚Äôs License** means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.

c. **BY-SA Compatible License** means a license listed at [creativecommons.org/compatiblelicenses](https://creativecommons.org/compatiblelicenses), approved by Creative Commons as essentially the equivalent of this Public License.

d. **Copyright and Similar Rights** means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.

e. **Effective Technological Measures** means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.

f. **Exceptions and Limitations** means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.

g. **License Elements** means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.

h. **Licensed Material** means the artistic or literary work, database, or other material to which the Licensor applied this Public License.

i. **Licensed Rights** means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.

j. **Licensor** means the individual(s) or entity(ies) granting rights under this Public License.

k. **Share** means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.

l. **Sui Generis Database Rights** means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.

m. **You** means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.
</markdown><markdown>
## Section 2 ‚Äì Scope.

a. **License grant.**

1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:
   A. reproduce and Share the Licensed Material, in whole or in part; and
   B. produce, reproduce, and Share Adapted Material.

2. **Exceptions and Limitations.** For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.

3. **Term.** The term of this Public License is specified in Section 6(a).

4. **Media and formats; technical modifications allowed.** The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.

5. **Downstream recipients.**
   A. **Offer from the Licensor ‚Äì Licensed Material.** Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.
   B. **Additional offer from the Licensor ‚Äì Adapted Material.** Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.
   C. **No downstream restrictions.** You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.

6. **No endorsement.** Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).

b. **Other rights.**

1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.

2. Patent and trademark rights are not licensed under this Public License.
</markdown><markdown>
3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.

## Section 3 ‚Äì License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

a. **Attribution.**

1. If You Share the Licensed Material (including in modified form), You must:
   A. retain the following if it is supplied by the Licensor with the Licensed Material: identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated); a copyright notice;
      i. a notice that refers to this Public License;
      ii. a notice that refers to the disclaimer of warranties;
      iii. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;
   B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and
   C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.

2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.

3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.

b. **ShareAlike.**

In addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.

1. The Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.

2. You must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.

3. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.
</markdown><markdown>
## Section 4 ‚Äì Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;

b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and

c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

## Section 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.

a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.

b. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.

c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.
</markdown><markdown>
## Section 6 ‚Äì Term and Termination.

a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.

b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:
   1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or
   2. upon express reinstatement by the Licensor.

   For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.

c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.

d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.

## Section 7 ‚Äì Other Terms and Conditions.

a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.

b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

## Section 8 ‚Äì Interpretation.

a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.

b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.

c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.

d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.
</markdown>