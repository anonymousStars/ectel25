{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: Q: What are the necessary and sufficient conditions on the moduli \\( n_1, n_2, \\ldots, n_r \\) for which the system of congruences \\( x \\equiv a_1 \\mod n_1, x \\equiv a_2 \\mod n_2, \\ldots, x \\equiv a_r \\mod n_r \\) has a solution, and how can we express the solution explicitly? \n",
      "Answers:  A: A solution exists if and only if for every pair of moduli \\( n_i \\) and \\( n_j \\), the greatest common divisor \\( \\gcd(n_i, n_j) \\) divides \\( a_j - a_i \\). An explicit solution for \\( x \\) can be given using the formula: \n",
      "\n",
      "\\[\n",
      "x \\equiv \\sum_{i=1}^{r} a_i \\cdot M_i \\cdot y_i \\mod n\n",
      "\\]\n",
      "\n",
      "where \\( M = n_1 n_2 \\cdots n_r \\), \\( M_i = \\frac{M}{n_i} \\), and \\( y_i \\) is the modular inverse of \\( M_i \\) modulo \\( n_i \\). The modulus \\( n \\) can be chosen as the least common multiple \\( \\text{lcm}(n_1, n_2, \\ldots, n_r) \\).\n",
      "page: 300\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('math_qa_2.json')\n",
    "\n",
    "questions = df['Q'].tolist()\n",
    "answers = df['A'].tolist()\n",
    "pages = df['page'].tolist()\n",
    "\n",
    "print(\"Questions:\", questions[0])\n",
    "print(\"Answers:\", answers[0])\n",
    "print(\"page:\", pages[0])\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1(string1, string2):\n",
    "    set1 = set(string1.split())\n",
    "    set2 = set(string2.split())\n",
    "    tp = len(set1 & set2)\n",
    "    precision = tp / len(set2) if set2 else 0\n",
    "    recall = tp / len(set1) if set1 else 0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "300 0\n",
      "0.3185840707964602\n",
      "16961\n",
      "--------\n",
      "200\n",
      "301 1\n",
      "0.6764705882352942\n",
      "42604\n",
      "--------\n",
      "200\n",
      "302 2\n",
      "0.3733333333333333\n",
      "60528\n",
      "--------\n",
      "200\n",
      "303 3\n",
      "0.4943820224719101\n",
      "149588\n",
      "--------\n",
      "200\n",
      "304 4\n",
      "0.7692307692307692\n",
      "194892\n",
      "--------\n",
      "200\n",
      "305 5\n",
      "0.5277777777777778\n",
      "222026\n",
      "--------\n",
      "200\n",
      "306 6\n",
      "0.8000000000000002\n",
      "261898\n",
      "--------\n",
      "200\n",
      "307 7\n",
      "0.6153846153846154\n",
      "299635\n",
      "--------\n",
      "200\n",
      "308 7\n",
      "0.32989690721649484\n",
      "372167\n",
      "--------\n",
      "200\n",
      "309 7\n",
      "0.36666666666666664\n",
      "452365\n",
      "--------\n",
      "200\n",
      "310 7\n",
      "0.4878048780487805\n",
      "497140\n",
      "--------\n",
      "200\n",
      "311 8\n",
      "0.8148148148148148\n",
      "546914\n",
      "--------\n",
      "200\n",
      "312 8\n",
      "0.25\n",
      "561985\n",
      "--------\n",
      "200\n",
      "313 9\n",
      "0.28571428571428575\n",
      "612619\n",
      "--------\n",
      "200\n",
      "314 10\n",
      "0.4444444444444445\n",
      "624945\n",
      "--------\n",
      "200\n",
      "316 11\n",
      "0.13636363636363638\n",
      "675941\n",
      "--------\n",
      "200\n",
      "317 11\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 181290 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "318 12\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 153196 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "319 13\n",
      "0.46153846153846156\n",
      "715240\n",
      "--------\n",
      "200\n",
      "320 14\n",
      "0.7555555555555555\n",
      "752067\n",
      "--------\n",
      "200\n",
      "321 15\n",
      "0.6285714285714286\n",
      "784681\n",
      "--------\n",
      "200\n",
      "322 16\n",
      "0.4\n",
      "794826\n",
      "--------\n",
      "200\n",
      "323 17\n",
      "0.18181818181818182\n",
      "814190\n",
      "--------\n",
      "200\n",
      "324 17\n",
      "0.44705882352941173\n",
      "851319\n",
      "--------\n",
      "200\n",
      "325 18\n",
      "0.7419354838709677\n",
      "899591\n",
      "--------\n",
      "200\n",
      "326 19\n",
      "0.7083333333333333\n",
      "933672\n",
      "--------\n",
      "200\n",
      "327 20\n",
      "0.547008547008547\n",
      "1029321\n",
      "--------\n",
      "200\n",
      "328 21\n",
      "0.53125\n",
      "1070920\n",
      "--------\n",
      "200\n",
      "329 22\n",
      "0.4778761061946902\n",
      "1145841\n",
      "--------\n",
      "200\n",
      "330 23\n",
      "0.368421052631579\n",
      "1188741\n",
      "--------\n",
      "200\n",
      "331 24\n",
      "0.35616438356164387\n",
      "1245064\n",
      "--------\n",
      "200\n",
      "332 25\n",
      "0.41509433962264153\n",
      "1300459\n",
      "--------\n",
      "200\n",
      "333 26\n",
      "0.6382978723404256\n",
      "1345274\n",
      "--------\n",
      "200\n",
      "334 27\n",
      "0.38532110091743116\n",
      "1379437\n",
      "--------\n",
      "200\n",
      "335 28\n",
      "0.6521739130434783\n",
      "1464426\n",
      "--------\n",
      "200\n",
      "336 29\n",
      "0.5217391304347827\n",
      "1547592\n",
      "--------\n",
      "200\n",
      "337 30\n",
      "0.449438202247191\n",
      "1586419\n",
      "--------\n",
      "200\n",
      "338 31\n",
      "0.5833333333333334\n",
      "1616579\n",
      "--------\n",
      "200\n",
      "339 32\n",
      "0.36\n",
      "1646656\n",
      "--------\n",
      "200\n",
      "340 33\n",
      "0.2929936305732484\n",
      "1704440\n",
      "--------\n",
      "200\n",
      "341 34\n",
      "0.20155038759689922\n",
      "1740858\n",
      "--------\n",
      "200\n",
      "342 35\n",
      "0.44776119402985076\n",
      "1784248\n",
      "--------\n",
      "200\n",
      "343 35\n",
      "0.33999999999999997\n",
      "1823641\n",
      "--------\n",
      "200\n",
      "344 36\n",
      "0.6111111111111113\n",
      "1863331\n",
      "--------\n",
      "200\n",
      "345 37\n",
      "0.6933333333333334\n",
      "1898627\n",
      "--------\n",
      "200\n",
      "346 38\n",
      "0.39999999999999997\n",
      "1933416\n",
      "--------\n",
      "200\n",
      "347 38\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 156823 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "348 38\n",
      "0.9767441860465117\n",
      "1952421\n",
      "--------\n",
      "200\n",
      "349 38\n",
      "0.46835443037974683\n",
      "2016532\n",
      "--------\n",
      "200\n",
      "351 39\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 145247 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "352 40\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 226622 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "353 41\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 130368 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "354 42\n",
      "0.693069306930693\n",
      "2054880\n",
      "--------\n",
      "200\n",
      "355 43\n",
      "0.6885245901639343\n",
      "2101396\n",
      "--------\n",
      "200\n",
      "357 44\n",
      "0.7407407407407408\n",
      "2141754\n",
      "--------\n",
      "200\n",
      "358 45\n",
      "0.6779661016949153\n",
      "2177065\n",
      "--------\n",
      "200\n",
      "359 46\n",
      "0.5531914893617021\n",
      "2206015\n",
      "--------\n",
      "200\n",
      "360 47\n",
      "0.4305555555555555\n",
      "2254547\n",
      "--------\n",
      "200\n",
      "361 48\n",
      "0.43636363636363634\n",
      "2303347\n",
      "--------\n",
      "200\n",
      "362 49\n",
      "0.8717948717948718\n",
      "2348695\n",
      "--------\n",
      "200\n",
      "363 50\n",
      "0.553191489361702\n",
      "2395090\n",
      "--------\n",
      "200\n",
      "364 51\n",
      "0.5263157894736843\n",
      "2421359\n",
      "--------\n",
      "200\n",
      "365 52\n",
      "0.48101265822784806\n",
      "2459253\n",
      "--------\n",
      "200\n",
      "366 53\n",
      "0.7727272727272727\n",
      "2499903\n",
      "--------\n",
      "200\n",
      "367 54\n",
      "0.225\n",
      "2541926\n",
      "--------\n",
      "200\n",
      "368 55\n",
      "0.48484848484848486\n",
      "2579667\n",
      "--------\n",
      "200\n",
      "369 56\n",
      "0.7037037037037038\n",
      "2638068\n",
      "--------\n",
      "200\n",
      "370 56\n",
      "0.2716049382716049\n",
      "2675280\n",
      "--------\n",
      "200\n",
      "371 57\n",
      "0.49438202247191015\n",
      "2713952\n",
      "--------\n",
      "200\n",
      "372 58\n",
      "0.30769230769230765\n",
      "2756942\n",
      "--------\n",
      "200\n",
      "373 59\n",
      "0.5609756097560976\n",
      "2830414\n",
      "--------\n",
      "200\n",
      "374 60\n",
      "0.5098039215686275\n",
      "2865113\n",
      "--------\n",
      "200\n",
      "375 61\n",
      "0.8125\n",
      "2909537\n",
      "--------\n",
      "200\n",
      "376 62\n",
      "0.5542168674698795\n",
      "2969121\n",
      "--------\n",
      "200\n",
      "377 63\n",
      "0.23076923076923075\n",
      "3019482\n",
      "--------\n",
      "200\n",
      "378 64\n",
      "0.30107526881720437\n",
      "3034398\n",
      "--------\n",
      "200\n",
      "379 65\n",
      "0.47727272727272724\n",
      "3073225\n",
      "--------\n",
      "200\n",
      "380 66\n",
      "0.20689655172413793\n",
      "3104360\n",
      "--------\n",
      "200\n",
      "381 67\n",
      "0.5161290322580646\n",
      "3140390\n",
      "--------\n",
      "200\n",
      "382 68\n",
      "0.5806451612903225\n",
      "3183771\n",
      "--------\n",
      "200\n",
      "383 69\n",
      "0.90625\n",
      "3228740\n",
      "--------\n",
      "200\n",
      "384 70\n",
      "0.5531914893617021\n",
      "3285153\n",
      "--------\n",
      "200\n",
      "385 70\n",
      "0.2835820895522388\n",
      "3303917\n",
      "--------\n",
      "200\n",
      "386 71\n",
      "0.5684210526315789\n",
      "3343754\n",
      "--------\n",
      "200\n",
      "387 72\n",
      "0.5217391304347826\n",
      "3366475\n",
      "--------\n",
      "200\n",
      "388 73\n",
      "0.22857142857142856\n",
      "3390071\n",
      "--------\n",
      "200\n",
      "389 74\n",
      "0.4411764705882353\n",
      "3426758\n",
      "--------\n",
      "200\n",
      "390 75\n",
      "0.5625\n",
      "3455137\n",
      "--------\n",
      "200\n",
      "391 76\n",
      "0.6341463414634148\n",
      "3525229\n",
      "--------\n",
      "200\n",
      "392 76\n",
      "0.26666666666666666\n",
      "3541257\n",
      "--------\n",
      "200\n",
      "393 77\n",
      "0.5806451612903225\n",
      "3591241\n",
      "--------\n",
      "200\n",
      "394 78\n",
      "0.5033112582781456\n",
      "3647868\n",
      "--------\n",
      "200\n",
      "395 79\n",
      "0.3333333333333333\n",
      "3700507\n",
      "--------\n",
      "200\n",
      "397 80\n",
      "0.761904761904762\n",
      "3732995\n",
      "--------\n",
      "200\n",
      "398 81\n",
      "0.27692307692307694\n",
      "3764457\n",
      "--------\n",
      "200\n",
      "399 82\n",
      "0.8301886792452831\n",
      "3812161\n",
      "--------\n",
      "200\n",
      "400 83\n",
      "0.6923076923076923\n",
      "3844117\n",
      "--------\n",
      "200\n",
      "401 84\n",
      "0.38636363636363635\n",
      "3962868\n",
      "--------\n",
      "200\n",
      "402 85\n",
      "0.9803921568627451\n",
      "4005199\n",
      "--------\n",
      "200\n",
      "403 86\n",
      "0.2909090909090909\n",
      "4031638\n",
      "--------\n",
      "200\n",
      "404 87\n",
      "0.28205128205128205\n",
      "4063105\n",
      "--------\n",
      "200\n",
      "405 88\n",
      "0.49230769230769234\n",
      "4079537\n",
      "--------\n",
      "200\n",
      "406 89\n",
      "0.3384615384615384\n",
      "4124631\n",
      "--------\n",
      "200\n",
      "407 90\n",
      "0.5428571428571428\n",
      "4165911\n",
      "--------\n",
      "200\n",
      "408 90\n",
      "0.8333333333333334\n",
      "4179884\n",
      "--------\n",
      "200\n",
      "409 91\n",
      "0.6333333333333333\n",
      "4213823\n",
      "--------\n",
      "200\n",
      "410 92\n",
      "0.44\n",
      "4262418\n",
      "--------\n",
      "200\n",
      "411 93\n",
      "0.6981132075471698\n",
      "4276537\n",
      "--------\n",
      "200\n",
      "412 93\n",
      "0.375\n",
      "4315789\n",
      "--------\n",
      "200\n",
      "413 94\n",
      "0.4827586206896552\n",
      "4367596\n",
      "--------\n",
      "200\n",
      "414 95\n",
      "0.523076923076923\n",
      "4416088\n",
      "--------\n",
      "200\n",
      "415 95\n",
      "0.3947368421052631\n",
      "4459140\n",
      "--------\n",
      "200\n",
      "416 96\n",
      "0.3508771929824561\n",
      "4490864\n",
      "--------\n",
      "200\n",
      "417 97\n",
      "0.6461538461538462\n",
      "4522890\n",
      "--------\n",
      "200\n",
      "418 98\n",
      "0.3728813559322034\n",
      "4644337\n",
      "--------\n",
      "200\n",
      "419 99\n",
      "0.3356643356643357\n",
      "4678733\n",
      "--------\n",
      "200\n",
      "420 100\n",
      "0.21686746987951808\n",
      "4718519\n",
      "--------\n",
      "200\n",
      "421 100\n",
      "0.46153846153846156\n",
      "4755681\n",
      "--------\n",
      "200\n",
      "422 100\n",
      "0.2692307692307693\n",
      "4793971\n",
      "--------\n",
      "200\n",
      "423 100\n",
      "0.6578947368421052\n",
      "4841899\n",
      "--------\n",
      "200\n",
      "424 101\n",
      "0.6923076923076923\n",
      "4906525\n",
      "--------\n",
      "200\n",
      "425 102\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 388680 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "426 103\n",
      "0.6129032258064515\n",
      "4988868\n",
      "--------\n",
      "200\n",
      "428 104\n",
      "0.9032258064516129\n",
      "5016526\n",
      "--------\n",
      "200\n",
      "429 104\n",
      "0.32323232323232326\n",
      "5048404\n",
      "--------\n",
      "200\n",
      "431 105\n",
      "0.5135135135135135\n",
      "5115214\n",
      "--------\n",
      "200\n",
      "432 106\n",
      "0.5869565217391305\n",
      "5185518\n",
      "--------\n",
      "200\n",
      "433 106\n",
      "0.05714285714285714\n",
      "5220787\n",
      "--------\n",
      "200\n",
      "434 107\n",
      "0.28571428571428575\n",
      "5246485\n",
      "--------\n",
      "200\n",
      "435 108\n",
      "0.425531914893617\n",
      "5308181\n",
      "--------\n",
      "200\n",
      "436 109\n",
      "0.19999999999999998\n",
      "5330277\n",
      "--------\n",
      "200\n",
      "437 109\n",
      "0.33999999999999997\n",
      "5347135\n",
      "--------\n",
      "200\n",
      "439 110\n",
      "0.3255813953488372\n",
      "5424727\n",
      "--------\n",
      "200\n",
      "440 111\n",
      "0.4912280701754386\n",
      "5486761\n",
      "--------\n",
      "200\n",
      "441 112\n",
      "0.49275362318840576\n",
      "5524951\n",
      "--------\n",
      "200\n",
      "442 113\n",
      "0.558139534883721\n",
      "5534737\n",
      "--------\n",
      "200\n",
      "443 114\n",
      "0.3333333333333333\n",
      "5579107\n",
      "--------\n",
      "200\n",
      "444 115\n",
      "0.4444444444444444\n",
      "5674365\n",
      "--------\n",
      "200\n",
      "445 116\n",
      "0.3384615384615384\n",
      "5688866\n",
      "--------\n",
      "200\n",
      "446 117\n",
      "0.4375\n",
      "5758858\n",
      "--------\n",
      "200\n",
      "447 118\n",
      "0.5142857142857143\n",
      "5862439\n",
      "--------\n",
      "200\n",
      "449 118\n",
      "0.27272727272727276\n",
      "5902031\n",
      "--------\n",
      "200\n",
      "450 119\n",
      "0.52\n",
      "5927071\n",
      "--------\n",
      "200\n",
      "451 120\n",
      "0.4150943396226415\n",
      "5989487\n",
      "--------\n",
      "200\n",
      "452 121\n",
      "0.8\n",
      "6046114\n",
      "--------\n",
      "200\n",
      "453 122\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 140115 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "454 123\n",
      "0.5945945945945946\n",
      "6101487\n",
      "--------\n",
      "200\n",
      "456 124\n",
      "0.75\n",
      "6155112\n",
      "--------\n",
      "200\n",
      "457 125\n",
      "0.7887323943661971\n",
      "6171141\n",
      "--------\n",
      "200\n",
      "458 126\n",
      "0.6206896551724138\n",
      "6225619\n",
      "--------\n",
      "200\n",
      "459 126\n",
      "0.20338983050847456\n",
      "6249843\n",
      "--------\n",
      "200\n",
      "460 127\n",
      "0.2608695652173913\n",
      "6283199\n",
      "--------\n",
      "200\n",
      "461 128\n",
      "0.43636363636363634\n",
      "6321207\n",
      "--------\n",
      "200\n",
      "463 129\n",
      "0.2380952380952381\n",
      "6338864\n",
      "--------\n",
      "200\n",
      "464 129\n",
      "0.38216560509554137\n",
      "6413923\n",
      "--------\n",
      "200\n",
      "465 130\n",
      "0.5999999999999999\n",
      "6449812\n",
      "--------\n",
      "200\n",
      "466 131\n",
      "0.30188679245283023\n",
      "6485000\n",
      "--------\n",
      "200\n",
      "467 132\n",
      "0.391304347826087\n",
      "6513682\n",
      "--------\n",
      "200\n",
      "468 133\n",
      "0.5\n",
      "6544433\n",
      "--------\n",
      "200\n",
      "469 134\n",
      "0.7027027027027027\n",
      "6578417\n",
      "--------\n",
      "200\n",
      "470 134\n",
      "0.5\n",
      "6617195\n",
      "--------\n",
      "200\n",
      "471 134\n",
      "0.4324324324324324\n",
      "6655401\n",
      "--------\n",
      "200\n",
      "472 135\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 185046 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "473 135\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 193779 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "474 136\n",
      "0.3529411764705882\n",
      "6689301\n",
      "--------\n",
      "200\n",
      "475 137\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 192927 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "476 138\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 198974 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "477 139\n",
      "0.5454545454545454\n",
      "6725942\n",
      "--------\n",
      "200\n",
      "478 139\n",
      "0.5\n",
      "6759069\n",
      "--------\n",
      "200\n",
      "479 140\n",
      "0.9142857142857143\n",
      "6808998\n",
      "--------\n",
      "200\n",
      "480 141\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 191665 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "481 142\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 194339 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "482 142\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 193520 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "483 143\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 181069 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "484 144\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 191199 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "485 145\n",
      "0.4329896907216495\n",
      "6835721\n",
      "--------\n",
      "200\n",
      "486 146\n",
      "0.717948717948718\n",
      "6895265\n",
      "--------\n",
      "200\n",
      "487 147\n",
      "0.3561643835616438\n",
      "6983284\n",
      "--------\n",
      "200\n",
      "488 148\n",
      "0.8421052631578948\n",
      "7016474\n",
      "--------\n",
      "200\n",
      "489 149\n",
      "0.47761194029850756\n",
      "7036453\n",
      "--------\n",
      "200\n",
      "490 149\n",
      "0.7727272727272727\n",
      "7078035\n",
      "--------\n",
      "200\n",
      "491 150\n",
      "0.6440677966101694\n",
      "7142874\n",
      "--------\n",
      "200\n",
      "492 150\n",
      "0.4752475247524752\n",
      "7164052\n",
      "--------\n",
      "200\n",
      "493 151\n",
      "0.6428571428571429\n",
      "7197747\n",
      "--------\n",
      "200\n",
      "494 152\n",
      "0.2588235294117647\n",
      "7207957\n",
      "--------\n",
      "200\n",
      "497 153\n",
      "0.37500000000000006\n",
      "7237902\n",
      "--------\n",
      "200\n",
      "498 154\n",
      "0.37037037037037035\n",
      "7295755\n",
      "--------\n",
      "200\n",
      "499 154\n",
      "0.64\n",
      "7311067\n",
      "--------\n",
      "200\n",
      "500 155\n",
      "0.5277777777777777\n",
      "7383054\n",
      "--------\n",
      "200\n",
      "501 156\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 279711 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "502 157\n",
      "0.46341463414634143\n",
      "7414637\n",
      "--------\n",
      "200\n",
      "503 158\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 272164 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "504 159\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 260076 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "--------\n",
      "200\n",
      "505 160\n",
      "0.8292682926829269\n",
      "7442441\n",
      "--------\n",
      "200\n",
      "506 161\n",
      "0.5393258426966292\n",
      "7473317\n",
      "--------\n",
      "200\n",
      "507 162\n",
      "0.5142857142857142\n",
      "7506851\n",
      "--------\n",
      "200\n",
      "508 163\n",
      "0.4761904761904762\n",
      "7529933\n",
      "--------\n",
      "200\n",
      "509 164\n",
      "0.6071428571428572\n",
      "7574234\n",
      "--------\n",
      "200\n",
      "510 164\n",
      "0.6461538461538462\n",
      "7617507\n",
      "--------\n",
      "200\n",
      "511 165\n",
      "0.41739130434782606\n",
      "7657278\n",
      "--------\n",
      "200\n",
      "512 166\n",
      "0.3917525773195877\n",
      "7711063\n",
      "--------\n",
      "200\n",
      "513 167\n",
      "0.34965034965034963\n",
      "7749112\n",
      "--------\n",
      "200\n",
      "514 168\n",
      "0.24\n",
      "7789021\n",
      "--------\n",
      "200\n",
      "515 169\n",
      "0.30434782608695654\n",
      "7826121\n",
      "--------\n",
      "200\n",
      "516 170\n",
      "0.40579710144927533\n",
      "7843755\n",
      "--------\n",
      "200\n",
      "517 171\n",
      "0.46551724137931033\n",
      "7874915\n",
      "--------\n",
      "200\n",
      "518 172\n",
      "0.39999999999999997\n",
      "7984587\n",
      "--------\n",
      "200\n",
      "519 173\n",
      "0.22222222222222224\n",
      "8034056\n",
      "--------\n",
      "200\n",
      "520 173\n",
      "0.4067796610169492\n",
      "8109994\n",
      "--------\n",
      "200\n",
      "521 174\n",
      "0.7272727272727272\n",
      "8137012\n",
      "--------\n",
      "200\n",
      "522 175\n",
      "0.39215686274509803\n",
      "8175679\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "url = \"http://i9ubuntu.eason.best:5020/search\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "Error_L = []\n",
    "F1_L = []\n",
    "Errors = 0\n",
    "page_check = 0\n",
    "tokens = 0\n",
    "for i, q in enumerate(questions):\n",
    "    queryN = q\n",
    "    payload = {\n",
    "    \"query\": queryN\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    print(response.status_code)\n",
    "    pattern = r'\"pages\":\\[(.*?)\\]'\n",
    "    matches = re.findall(pattern, response.text)\n",
    "    contains_page = [match for match in matches if str(pages[i]) in match.split(', ') if match.strip()]\n",
    "    if len(contains_page) != 0:\n",
    "        page_check += 1\n",
    "    print(pages[i], page_check)\n",
    "    try:\n",
    "        client = OpenAI(api_key='OPENAI_API_KEY_PLACEHOLDER')\n",
    "        r = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\", \"content\": \"You are a math expert\",\n",
    "                \"content\": \"user\", \"content\": \"I am giving you a question and a text content. Provide a short answer for the question based on context I gave you. Say nothing else. Context:\" + response.text + \", Question:\" + q\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "        )\n",
    "        res = r.choices[0].message.content\n",
    "        f1 = calculate_f1(answers[i], res)\n",
    "        print(f1)\n",
    "        F1_L.append(f1)\n",
    "        usage = r.usage\n",
    "        tokens += usage.total_tokens\n",
    "        print(tokens)\n",
    "        \n",
    "    except Exception as e:\n",
    "        Errors += 1\n",
    "        Error_L.append(i)\n",
    "        print(e)\n",
    "\n",
    "    print(\"--------\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QAs: 211\n",
      "Percentage of page match: 0.8293838862559242\n",
      "Errors Percentage: 0.0947867298578199\n",
      "Average F1-Score 0.49517604109636026\n",
      "Tokens Completion 8175679\n"
     ]
    }
   ],
   "source": [
    "print(\"Total QAs:\", len(questions))\n",
    "print(\"Percentage of page match:\", page_check / len(questions))\n",
    "print(\"Errors Percentage:\", Errors / len(questions))\n",
    "print(\"Average F1-Score\", sum(F1_L) / len(F1_L))\n",
    "print(\"Tokens Completion\",tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_CG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
